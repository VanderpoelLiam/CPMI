Sender: LSF System <lsfadmin@eu-lo-s4-082>
Subject: Job 217553298: <train_lang_standard_32_5.0000E-04_full> in cluster <euler> Exited

Job <train_lang_standard_32_5.0000E-04_full> was submitted from host <eu-login-05> by user <euler_username> in cluster <euler> at Fri May  6 16:26:26 2022
Job was executed on host(s) <4*eu-lo-s4-082>, in queue <gpu.24h>, as user <euler_username> in cluster <euler> at Fri May  6 16:26:42 2022
</cluster/home/euler_username> was used as the home directory.
</cluster/work/cotterell/liam/master-thesis> was used as the working directory.
Started at Fri May  6 16:26:42 2022
Terminated at Sat May  7 12:26:49 2022
Results reported at Sat May  7 12:26:49 2022

Your job looked like:

------------------------------------------------------------
# LSBATCH: User input
fairseq-train data/xsum-lang-full --save-dir checkpoints/language_model/standard --update-freq 32 --lr 0.0005 --checkpoint-suffix _standard_32_5.0000E-04_full --task language_modeling --arch transformer_lm --share-decoder-input-output-embed --dropout 0.1 --optimizer adam --adam-betas "(0.9, 0.98)" --weight-decay 0.01 --clip-norm 0.0 --lr-scheduler inverse_sqrt --warmup-updates 4000 --warmup-init-lr 1e-07 --tokens-per-sample 512 --sample-break-mode none --max-tokens 2048 --no-epoch-checkpoints --no-last-checkpoints --patience 5
------------------------------------------------------------

TERM_RUNLIMIT: job killed after reaching LSF run time limit.
Exited with exit code 140.

Resource usage summary:

    CPU time :                                   65377.00 sec.
    Max Memory :                                 4056 MB
    Average Memory :                             2809.54 MB
    Total Requested Memory :                     8192.00 MB
    Delta Memory :                               4136.00 MB
    Max Swap :                                   -
    Max Processes :                              4
    Max Threads :                                14
    Run time :                                   72029 sec.
    Turnaround time :                            72023 sec.

The output (if any) follows:

2022-05-06 16:28:39 | INFO | fairseq.tasks.text_to_speech | Please install tensorboardX: pip install tensorboardX
2022-05-06 16:28:52 | INFO | fairseq_cli.train | {'_name': None, 'common': {'_name': None, 'no_progress_bar': False, 'log_interval': 100, 'log_format': None, 'log_file': None, 'tensorboard_logdir': None, 'wandb_project': None, 'azureml_logging': False, 'seed': 1, 'cpu': False, 'tpu': False, 'bf16': False, 'memory_efficient_bf16': False, 'fp16': False, 'memory_efficient_fp16': False, 'fp16_no_flatten_grads': False, 'fp16_init_scale': 128, 'fp16_scale_window': None, 'fp16_scale_tolerance': 0.0, 'on_cpu_convert_precision': False, 'min_loss_scale': 0.0001, 'threshold_loss_scale': None, 'amp': False, 'amp_batch_retries': 2, 'amp_init_scale': 128, 'amp_scale_window': None, 'user_dir': None, 'empty_cache_freq': 0, 'all_gather_list_size': 16384, 'model_parallel_size': 1, 'quantization_config_path': None, 'profile': False, 'reset_logging': False, 'suppress_crashes': False, 'use_plasma_view': False, 'plasma_path': '/tmp/plasma'}, 'common_eval': {'_name': None, 'path': None, 'post_process': None, 'quiet': False, 'model_overrides': '{}', 'results_path': None, 'print_tokens': False}, 'distributed_training': {'_name': None, 'distributed_world_size': 1, 'distributed_num_procs': 1, 'distributed_rank': 0, 'distributed_backend': 'nccl', 'distributed_init_method': None, 'distributed_port': -1, 'device_id': 0, 'distributed_no_spawn': False, 'ddp_backend': 'pytorch_ddp', 'ddp_comm_hook': 'none', 'bucket_cap_mb': 25, 'fix_batches_to_gpus': False, 'find_unused_parameters': False, 'gradient_as_bucket_view': False, 'fast_stat_sync': False, 'heartbeat_timeout': -1, 'broadcast_buffers': False, 'slowmo_momentum': None, 'slowmo_base_algorithm': 'localsgd', 'localsgd_frequency': 3, 'nprocs_per_node': 1, 'pipeline_model_parallel': False, 'pipeline_balance': None, 'pipeline_devices': None, 'pipeline_chunks': 0, 'pipeline_encoder_balance': None, 'pipeline_encoder_devices': None, 'pipeline_decoder_balance': None, 'pipeline_decoder_devices': None, 'pipeline_checkpoint': 'never', 'zero_sharding': 'none', 'fp16': False, 'memory_efficient_fp16': False, 'tpu': False, 'no_reshard_after_forward': False, 'fp32_reduce_scatter': False, 'cpu_offload': False, 'use_sharded_state': False, 'not_fsdp_flatten_parameters': False}, 'dataset': {'_name': None, 'num_workers': 1, 'skip_invalid_size_inputs_valid_test': False, 'max_tokens': 2048, 'batch_size': None, 'required_batch_size_multiple': 8, 'required_seq_len_multiple': 1, 'dataset_impl': None, 'data_buffer_size': 10, 'train_subset': 'train', 'valid_subset': 'valid', 'combine_valid_subsets': None, 'ignore_unused_valid_subsets': False, 'validate_interval': 1, 'validate_interval_updates': 0, 'validate_after_updates': 0, 'fixed_validation_seed': None, 'disable_validation': False, 'max_tokens_valid': 2048, 'batch_size_valid': None, 'max_valid_steps': None, 'curriculum': 0, 'gen_subset': 'test', 'num_shards': 1, 'shard_id': 0, 'grouped_shuffling': False, 'update_epoch_batch_itr': False, 'update_ordered_indices_seed': False}, 'optimization': {'_name': None, 'max_epoch': 0, 'max_update': 0, 'stop_time_hours': 0.0, 'clip_norm': 0.0, 'sentence_avg': False, 'update_freq': [32], 'lr': [0.0005], 'stop_min_lr': -1.0, 'use_bmuf': False, 'skip_remainder_batch': False}, 'checkpoint': {'_name': None, 'save_dir': 'checkpoints/language_model/standard', 'restore_file': 'checkpoint_last.pt', 'continue_once': None, 'finetune_from_model': None, 'reset_dataloader': False, 'reset_lr_scheduler': False, 'reset_meters': False, 'reset_optimizer': False, 'optimizer_overrides': '{}', 'save_interval': 1, 'save_interval_updates': 0, 'keep_interval_updates': -1, 'keep_interval_updates_pattern': -1, 'keep_last_epochs': -1, 'keep_best_checkpoints': -1, 'no_save': False, 'no_epoch_checkpoints': True, 'no_last_checkpoints': True, 'no_save_optimizer_state': False, 'best_checkpoint_metric': 'loss', 'maximize_best_checkpoint_metric': False, 'patience': 5, 'checkpoint_suffix': '_standard_32_5.0000E-04_full', 'checkpoint_shard_count': 1, 'load_checkpoint_on_all_dp_ranks': False, 'write_checkpoints_asynchronously': False, 'model_parallel_size': 1}, 'bmuf': {'_name': None, 'block_lr': 1.0, 'block_momentum': 0.875, 'global_sync_iter': 50, 'warmup_iterations': 500, 'use_nbm': False, 'average_sync': False, 'distributed_world_size': 1}, 'generation': {'_name': None, 'beam': 5, 'nbest': 1, 'max_len_a': 0.0, 'max_len_b': 200, 'min_len': 1, 'match_source_len': False, 'unnormalized': False, 'no_early_stop': False, 'no_beamable_mm': False, 'lenpen': 1.0, 'unkpen': 0.0, 'replace_unk': None, 'sacrebleu': False, 'score_reference': False, 'prefix_size': 0, 'no_repeat_ngram_size': 0, 'sampling': False, 'sampling_topk': -1, 'sampling_topp': -1.0, 'constraints': None, 'temperature': 1.0, 'diverse_beam_groups': -1, 'diverse_beam_strength': 0.5, 'diversity_rate': -1.0, 'print_alignment': None, 'print_step': False, 'lm_path': None, 'ent_threshold': 0.0, 'lm_weight': 0.0, 'iter_decode_eos_penalty': 0.0, 'iter_decode_max_iter': 10, 'iter_decode_force_max_iter': False, 'iter_decode_with_beam': 1, 'iter_decode_with_external_reranker': False, 'retain_iter_history': False, 'retain_dropout': False, 'retain_dropout_modules': None, 'decoding_format': None, 'no_seed_provided': False}, 'eval_lm': {'_name': None, 'output_word_probs': False, 'output_word_stats': False, 'context_window': 0, 'softmax_batch': 9223372036854775807}, 'interactive': {'_name': None, 'buffer_size': 0, 'input': '-'}, 'model': {'_name': 'transformer_lm', 'activation_fn': relu, 'dropout': 0.1, 'attention_dropout': 0.0, 'activation_dropout': 0.0, 'relu_dropout': 0.0, 'decoder_embed_dim': 512, 'decoder_output_dim': 512, 'decoder_input_dim': 512, 'decoder_ffn_embed_dim': 2048, 'decoder_layers': 6, 'decoder_attention_heads': 8, 'decoder_normalize_before': False, 'no_decoder_final_norm': False, 'adaptive_softmax_cutoff': None, 'adaptive_softmax_dropout': 0.0, 'adaptive_softmax_factor': 4.0, 'no_token_positional_embeddings': False, 'share_decoder_input_output_embed': True, 'character_embeddings': False, 'character_filters': '[(1, 64), (2, 128), (3, 192), (4, 256), (5, 256), (6, 256), (7, 256)]', 'character_embedding_dim': 4, 'char_embedder_highway_layers': 2, 'adaptive_input': False, 'adaptive_input_factor': 4.0, 'adaptive_input_cutoff': None, 'tie_adaptive_weights': False, 'tie_adaptive_proj': False, 'decoder_learned_pos': False, 'layernorm_embedding': False, 'no_scale_embedding': False, 'checkpoint_activations': False, 'offload_activations': False, 'decoder_layerdrop': 0.0, 'decoder_layers_to_keep': None, 'quant_noise_pq': 0.0, 'quant_noise_pq_block_size': 8, 'quant_noise_scalar': 0.0, 'min_params_to_wrap': 100000000, 'base_layers': 0, 'base_sublayers': 1, 'base_shuffle': 1, 'scale_fc': False, 'scale_attn': False, 'scale_heads': False, 'scale_resids': False, 'add_bos_token': False, 'tokens_per_sample': 512, 'max_target_positions': None, 'tpu': False}, 'task': {'_name': 'language_modeling', 'data': 'data/xsum-lang-full', 'sample_break_mode': none, 'tokens_per_sample': 512, 'output_dictionary_size': -1, 'self_target': False, 'future_target': False, 'past_target': False, 'add_bos_token': False, 'max_target_positions': None, 'shorten_method': none, 'shorten_data_split_list': '', 'pad_to_fixed_length': False, 'pad_to_fixed_bsz': False, 'seed': 1, 'batch_size': None, 'batch_size_valid': None, 'dataset_impl': None, 'data_buffer_size': 10, 'tpu': False, 'use_plasma_view': False, 'plasma_path': '/tmp/plasma'}, 'criterion': {'_name': 'cross_entropy', 'sentence_avg': False}, 'optimizer': {'_name': 'adam', 'adam_betas': '(0.9, 0.98)', 'adam_eps': 1e-08, 'weight_decay': 0.01, 'use_old_adam': False, 'fp16_adam_stats': False, 'tpu': False, 'lr': [0.0005]}, 'lr_scheduler': {'_name': 'inverse_sqrt', 'warmup_updates': 4000, 'warmup_init_lr': 1e-07, 'lr': [0.0005]}, 'scoring': {'_name': 'bleu', 'pad': 1, 'eos': 2, 'unk': 3}, 'bpe': None, 'tokenizer': None, 'ema': {'_name': None, 'store_ema': False, 'ema_decay': 0.9999, 'ema_start_update': 0, 'ema_seed_model': None, 'ema_update_freq': 1, 'ema_fp32': False}, 'simul_type': None}
2022-05-06 16:28:52 | INFO | fairseq.tasks.language_modeling | dictionary: 49992 types
2022-05-06 16:28:54 | INFO | fairseq_cli.train | TransformerLanguageModel(
  (decoder): TransformerDecoder(
    (dropout_module): FairseqDropout()
    (embed_tokens): Embedding(49992, 512, padding_idx=1)
    (embed_positions): SinusoidalPositionalEmbedding()
    (layers): ModuleList(
      (0): TransformerDecoderLayerBase(
        (dropout_module): FairseqDropout()
        (self_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=512, out_features=512, bias=True)
          (v_proj): Linear(in_features=512, out_features=512, bias=True)
          (q_proj): Linear(in_features=512, out_features=512, bias=True)
          (out_proj): Linear(in_features=512, out_features=512, bias=True)
        )
        (activation_dropout_module): FairseqDropout()
        (self_attn_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
        (fc1): Linear(in_features=512, out_features=2048, bias=True)
        (fc2): Linear(in_features=2048, out_features=512, bias=True)
        (final_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
      )
      (1): TransformerDecoderLayerBase(
        (dropout_module): FairseqDropout()
        (self_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=512, out_features=512, bias=True)
          (v_proj): Linear(in_features=512, out_features=512, bias=True)
          (q_proj): Linear(in_features=512, out_features=512, bias=True)
          (out_proj): Linear(in_features=512, out_features=512, bias=True)
        )
        (activation_dropout_module): FairseqDropout()
        (self_attn_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
        (fc1): Linear(in_features=512, out_features=2048, bias=True)
        (fc2): Linear(in_features=2048, out_features=512, bias=True)
        (final_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
      )
      (2): TransformerDecoderLayerBase(
        (dropout_module): FairseqDropout()
        (self_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=512, out_features=512, bias=True)
          (v_proj): Linear(in_features=512, out_features=512, bias=True)
          (q_proj): Linear(in_features=512, out_features=512, bias=True)
          (out_proj): Linear(in_features=512, out_features=512, bias=True)
        )
        (activation_dropout_module): FairseqDropout()
        (self_attn_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
        (fc1): Linear(in_features=512, out_features=2048, bias=True)
        (fc2): Linear(in_features=2048, out_features=512, bias=True)
        (final_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
      )
      (3): TransformerDecoderLayerBase(
        (dropout_module): FairseqDropout()
        (self_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=512, out_features=512, bias=True)
          (v_proj): Linear(in_features=512, out_features=512, bias=True)
          (q_proj): Linear(in_features=512, out_features=512, bias=True)
          (out_proj): Linear(in_features=512, out_features=512, bias=True)
        )
        (activation_dropout_module): FairseqDropout()
        (self_attn_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
        (fc1): Linear(in_features=512, out_features=2048, bias=True)
        (fc2): Linear(in_features=2048, out_features=512, bias=True)
        (final_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
      )
      (4): TransformerDecoderLayerBase(
        (dropout_module): FairseqDropout()
        (self_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=512, out_features=512, bias=True)
          (v_proj): Linear(in_features=512, out_features=512, bias=True)
          (q_proj): Linear(in_features=512, out_features=512, bias=True)
          (out_proj): Linear(in_features=512, out_features=512, bias=True)
        )
        (activation_dropout_module): FairseqDropout()
        (self_attn_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
        (fc1): Linear(in_features=512, out_features=2048, bias=True)
        (fc2): Linear(in_features=2048, out_features=512, bias=True)
        (final_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
      )
      (5): TransformerDecoderLayerBase(
        (dropout_module): FairseqDropout()
        (self_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=512, out_features=512, bias=True)
          (v_proj): Linear(in_features=512, out_features=512, bias=True)
          (q_proj): Linear(in_features=512, out_features=512, bias=True)
          (out_proj): Linear(in_features=512, out_features=512, bias=True)
        )
        (activation_dropout_module): FairseqDropout()
        (self_attn_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
        (fc1): Linear(in_features=512, out_features=2048, bias=True)
        (fc2): Linear(in_features=2048, out_features=512, bias=True)
        (final_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
      )
    )
    (output_projection): Linear(in_features=512, out_features=49992, bias=False)
  )
)
2022-05-06 16:28:54 | INFO | fairseq_cli.train | task: LanguageModelingTask
2022-05-06 16:28:54 | INFO | fairseq_cli.train | model: TransformerLanguageModel
2022-05-06 16:28:54 | INFO | fairseq_cli.train | criterion: CrossEntropyCriterion
2022-05-06 16:28:54 | INFO | fairseq_cli.train | num. shared model params: 44,510,208 (num. trained: 44,510,208)
2022-05-06 16:28:54 | INFO | fairseq_cli.train | num. expert model params: 0 (num. trained: 0)
2022-05-06 16:28:54 | INFO | fairseq.data.data_utils | loaded 22,664 examples from: data/xsum-lang-full/valid
2022-05-06 16:29:36 | INFO | fairseq.trainer | detected shared parameter: decoder.embed_tokens.weight <- decoder.output_projection.weight
2022-05-06 16:29:36 | INFO | fairseq.utils | ***********************CUDA enviroments for all 1 workers***********************
2022-05-06 16:29:36 | INFO | fairseq.utils | rank   0: capabilities =  7.5  ; total memory = 10.761 GB ; name = NVIDIA GeForce RTX 2080 Ti
2022-05-06 16:29:36 | INFO | fairseq.utils | ***********************CUDA enviroments for all 1 workers***********************
2022-05-06 16:29:36 | INFO | fairseq_cli.train | training on 1 devices (GPUs/TPUs)
2022-05-06 16:29:36 | INFO | fairseq_cli.train | max tokens per device = 2048 and max sentences per device = None
2022-05-06 16:29:36 | INFO | fairseq.trainer | Preparing to load checkpoint checkpoints/language_model/standard/checkpoint_last_standard_32_5.0000E-04_full.pt
2022-05-06 16:29:36 | INFO | fairseq.trainer | No existing checkpoint found checkpoints/language_model/standard/checkpoint_last_standard_32_5.0000E-04_full.pt
2022-05-06 16:29:36 | INFO | fairseq.trainer | loading train data for epoch 1
2022-05-06 16:29:37 | INFO | fairseq.data.data_utils | loaded 408,090 examples from: data/xsum-lang-full/train
2022-05-06 16:29:37 | INFO | fairseq.trainer | NOTE: your device may support faster training with --fp16 or --amp
2022-05-06 16:29:38 | INFO | fairseq.data.iterators | grouped total_num_itrs = 1478
2022-05-06 16:29:38 | INFO | fairseq.trainer | begin training epoch 1
2022-05-06 16:29:38 | INFO | fairseq_cli.train | Start iterating over samples
2022-05-06 16:33:38 | INFO | train_inner | epoch 001:    100 / 1478 loss=14.701, ppl=26633.4, wps=27723.8, ups=0.42, wpb=65531, bsz=128, num_updates=100, lr=1.25975e-05, gnorm=3.206, train_wall=231, gb_free=7.9, wall=242
2022-05-06 16:37:41 | INFO | train_inner | epoch 001:    200 / 1478 loss=12.582, ppl=6130.41, wps=26921, ups=0.41, wpb=65536, bsz=128, num_updates=200, lr=2.5095e-05, gnorm=1.093, train_wall=235, gb_free=7.9, wall=485
2022-05-06 16:43:10 | INFO | train_inner | epoch 001:    300 / 1478 loss=11.141, ppl=2257.49, wps=19922.3, ups=0.3, wpb=65536, bsz=128, num_updates=300, lr=3.75925e-05, gnorm=0.687, train_wall=238, gb_free=7.9, wall=814
2022-05-06 16:48:13 | INFO | train_inner | epoch 001:    400 / 1478 loss=10.192, ppl=1170.05, wps=21672.6, ups=0.33, wpb=65536, bsz=128, num_updates=400, lr=5.009e-05, gnorm=0.503, train_wall=226, gb_free=7.9, wall=1116
2022-05-06 16:52:48 | INFO | train_inner | epoch 001:    500 / 1478 loss=9.71, ppl=837.24, wps=23841.2, ups=0.36, wpb=65536, bsz=128, num_updates=500, lr=6.25875e-05, gnorm=0.497, train_wall=235, gb_free=7.9, wall=1391
2022-05-06 16:57:02 | INFO | train_inner | epoch 001:    600 / 1478 loss=9.352, ppl=653.47, wps=25745.1, ups=0.39, wpb=65536, bsz=128, num_updates=600, lr=7.5085e-05, gnorm=0.568, train_wall=229, gb_free=7.9, wall=1646
2022-05-06 17:01:19 | INFO | train_inner | epoch 001:    700 / 1478 loss=9.048, ppl=529.18, wps=25488, ups=0.39, wpb=65525.8, bsz=128, num_updates=700, lr=8.75825e-05, gnorm=0.617, train_wall=232, gb_free=7.9, wall=1903
2022-05-06 17:06:12 | INFO | train_inner | epoch 001:    800 / 1478 loss=8.8, ppl=445.59, wps=22404.4, ups=0.34, wpb=65536, bsz=128, num_updates=800, lr=0.00010008, gnorm=0.712, train_wall=230, gb_free=7.9, wall=2196
2022-05-06 17:10:49 | INFO | train_inner | epoch 001:    900 / 1478 loss=8.587, ppl=384.54, wps=23648.7, ups=0.36, wpb=65536, bsz=128, num_updates=900, lr=0.000112578, gnorm=0.783, train_wall=230, gb_free=7.9, wall=2473
2022-05-06 17:15:22 | INFO | train_inner | epoch 001:   1000 / 1478 loss=8.393, ppl=336.25, wps=23998.8, ups=0.37, wpb=65536, bsz=128, num_updates=1000, lr=0.000125075, gnorm=0.787, train_wall=231, gb_free=7.9, wall=2746
2022-05-06 17:19:40 | INFO | train_inner | epoch 001:   1100 / 1478 loss=8.213, ppl=296.76, wps=25381.6, ups=0.39, wpb=65536, bsz=128, num_updates=1100, lr=0.000137573, gnorm=0.808, train_wall=232, gb_free=7.9, wall=3004
2022-05-06 17:23:50 | INFO | train_inner | epoch 001:   1200 / 1478 loss=8.059, ppl=266.74, wps=26208.9, ups=0.4, wpb=65536, bsz=128, num_updates=1200, lr=0.00015007, gnorm=0.823, train_wall=234, gb_free=7.9, wall=3254
2022-05-06 17:27:56 | INFO | train_inner | epoch 001:   1300 / 1478 loss=7.918, ppl=241.83, wps=26653.5, ups=0.41, wpb=65536, bsz=128, num_updates=1300, lr=0.000162568, gnorm=0.9, train_wall=232, gb_free=7.9, wall=3500
2022-05-06 17:32:05 | INFO | train_inner | epoch 001:   1400 / 1478 loss=7.784, ppl=220.39, wps=26367.9, ups=0.4, wpb=65536, bsz=128, num_updates=1400, lr=0.000175065, gnorm=0.868, train_wall=236, gb_free=7.9, wall=3748
2022-05-06 17:35:17 | INFO | fairseq_cli.train | begin validation on "valid" subset
/cluster/work/cotterell/liam/fairseq/fairseq/utils.py:374: UserWarning: amp_C fused kernels unavailable, disabling multi_tensor_l2norm; you may get better performance by installing NVIDIA's apex library
  warnings.warn(
2022-05-06 17:36:30 | INFO | valid | epoch 001 | valid on 'valid' subset | loss 7.517 | ppl 183.13 | wps 73833.1 | wpb 2047.4 | bsz 4 | num_updates 1478
2022-05-06 17:36:30 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 1 @ 1478 updates
2022-05-06 17:36:30 | INFO | fairseq.trainer | Saving checkpoint to /cluster/work/cotterell/liam/master-thesis/checkpoints/language_model/standard/checkpoint_best_standard_32_5.0000E-04_full.pt
2022-05-06 17:36:32 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/work/cotterell/liam/master-thesis/checkpoints/language_model/standard/checkpoint_best_standard_32_5.0000E-04_full.pt
2022-05-06 17:36:32 | INFO | fairseq.checkpoint_utils | Saved checkpoint checkpoints/language_model/standard/checkpoint_best_standard_32_5.0000E-04_full.pt (epoch 1 @ 1478 updates, score 7.517) (writing took 2.014473462011665 seconds)
2022-05-06 17:36:32 | INFO | fairseq_cli.train | end of epoch 1 (average epoch stats below)
2022-05-06 17:36:32 | INFO | train | epoch 001 | loss 9.504 | ppl 726.08 | wps 24138.1 | ups 0.37 | wpb 65507.3 | bsz 127.9 | num_updates 1478 | lr 0.000184813 | gnorm 0.918 | train_wall 3435 | gb_free 7.9 | wall 4016
2022-05-06 17:36:32 | INFO | fairseq.data.iterators | grouped total_num_itrs = 1478
2022-05-06 17:36:32 | INFO | fairseq.trainer | begin training epoch 2
2022-05-06 17:36:32 | INFO | fairseq_cli.train | Start iterating over samples
2022-05-06 17:37:29 | INFO | train_inner | epoch 002:     22 / 1478 loss=7.655, ppl=201.53, wps=20073.4, ups=0.31, wpb=65126.4, bsz=127.2, num_updates=1500, lr=0.000187563, gnorm=0.922, train_wall=233, gb_free=7.9, wall=4073
2022-05-06 17:42:03 | INFO | train_inner | epoch 002:    122 / 1478 loss=7.53, ppl=184.77, wps=23922.3, ups=0.37, wpb=65536, bsz=128, num_updates=1600, lr=0.00020006, gnorm=0.865, train_wall=230, gb_free=7.9, wall=4347
2022-05-06 17:46:33 | INFO | train_inner | epoch 002:    222 / 1478 loss=7.429, ppl=172.35, wps=24315.9, ups=0.37, wpb=65536, bsz=128, num_updates=1700, lr=0.000212558, gnorm=0.91, train_wall=230, gb_free=7.9, wall=4616
2022-05-06 17:50:57 | INFO | train_inner | epoch 002:    322 / 1478 loss=7.315, ppl=159.23, wps=24830.8, ups=0.38, wpb=65536, bsz=128, num_updates=1800, lr=0.000225055, gnorm=0.884, train_wall=233, gb_free=7.9, wall=4880
2022-05-06 17:55:07 | INFO | train_inner | epoch 002:    422 / 1478 loss=7.204, ppl=147.4, wps=26137.4, ups=0.4, wpb=65531, bsz=128, num_updates=1900, lr=0.000237553, gnorm=0.867, train_wall=230, gb_free=7.9, wall=5131
2022-05-06 18:00:04 | INFO | train_inner | epoch 002:    522 / 1478 loss=7.119, ppl=138.97, wps=22108.4, ups=0.34, wpb=65536, bsz=128, num_updates=2000, lr=0.00025005, gnorm=0.849, train_wall=229, gb_free=7.9, wall=5427
2022-05-06 18:04:46 | INFO | train_inner | epoch 002:    622 / 1478 loss=7.017, ppl=129.56, wps=23196.2, ups=0.35, wpb=65536, bsz=128, num_updates=2100, lr=0.000262548, gnorm=0.832, train_wall=230, gb_free=7.9, wall=5710
2022-05-06 18:09:14 | INFO | train_inner | epoch 002:    722 / 1478 loss=6.929, ppl=121.86, wps=24459, ups=0.37, wpb=65536, bsz=128, num_updates=2200, lr=0.000275045, gnorm=0.822, train_wall=233, gb_free=7.9, wall=5978
2022-05-06 18:13:35 | INFO | train_inner | epoch 002:    822 / 1478 loss=6.853, ppl=115.57, wps=25149.7, ups=0.38, wpb=65536, bsz=128, num_updates=2300, lr=0.000287543, gnorm=0.804, train_wall=233, gb_free=7.9, wall=6239
2022-05-06 18:18:00 | INFO | train_inner | epoch 002:    922 / 1478 loss=6.761, ppl=108.44, wps=24687.1, ups=0.38, wpb=65536, bsz=128, num_updates=2400, lr=0.00030004, gnorm=0.783, train_wall=230, gb_free=7.9, wall=6504
2022-05-06 18:22:17 | INFO | train_inner | epoch 002:   1022 / 1478 loss=6.694, ppl=103.51, wps=25519, ups=0.39, wpb=65536, bsz=128, num_updates=2500, lr=0.000312538, gnorm=0.787, train_wall=231, gb_free=7.9, wall=6761
2022-05-06 18:26:30 | INFO | train_inner | epoch 002:   1122 / 1478 loss=6.614, ppl=97.97, wps=25961, ups=0.4, wpb=65536, bsz=128, num_updates=2600, lr=0.000325035, gnorm=0.758, train_wall=236, gb_free=7.9, wall=7013
2022-05-06 18:30:41 | INFO | train_inner | epoch 002:   1222 / 1478 loss=6.547, ppl=93.53, wps=26105.9, ups=0.4, wpb=65536, bsz=128, num_updates=2700, lr=0.000337533, gnorm=0.734, train_wall=238, gb_free=7.9, wall=7264
2022-05-06 18:34:56 | INFO | train_inner | epoch 002:   1322 / 1478 loss=6.483, ppl=89.43, wps=25637.6, ups=0.39, wpb=65536, bsz=128, num_updates=2800, lr=0.00035003, gnorm=0.736, train_wall=232, gb_free=7.9, wall=7520
2022-05-06 18:39:57 | INFO | train_inner | epoch 002:   1422 / 1478 loss=6.433, ppl=86.38, wps=21789, ups=0.33, wpb=65525.8, bsz=128, num_updates=2900, lr=0.000362528, gnorm=0.708, train_wall=236, gb_free=7.9, wall=7821
2022-05-06 18:42:34 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-05-06 18:43:52 | INFO | valid | epoch 002 | valid on 'valid' subset | loss 6.252 | ppl 76.2 | wps 69842.9 | wpb 2047.4 | bsz 4 | num_updates 2956 | best_loss 6.252
2022-05-06 18:43:52 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 2 @ 2956 updates
2022-05-06 18:43:52 | INFO | fairseq.trainer | Saving checkpoint to /cluster/work/cotterell/liam/master-thesis/checkpoints/language_model/standard/checkpoint_best_standard_32_5.0000E-04_full.pt
2022-05-06 18:43:54 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/work/cotterell/liam/master-thesis/checkpoints/language_model/standard/checkpoint_best_standard_32_5.0000E-04_full.pt
2022-05-06 18:43:54 | INFO | fairseq.checkpoint_utils | Saved checkpoint checkpoints/language_model/standard/checkpoint_best_standard_32_5.0000E-04_full.pt (epoch 2 @ 2956 updates, score 6.252) (writing took 2.58702825428918 seconds)
2022-05-06 18:43:54 | INFO | fairseq_cli.train | end of epoch 2 (average epoch stats below)
2022-05-06 18:43:54 | INFO | train | epoch 002 | loss 6.914 | ppl 120.55 | wps 23953 | ups 0.37 | wpb 65507.3 | bsz 127.9 | num_updates 2956 | lr 0.000369526 | gnorm 0.807 | train_wall 3428 | gb_free 7.9 | wall 8058
2022-05-06 18:43:55 | INFO | fairseq.data.iterators | grouped total_num_itrs = 1478
2022-05-06 18:43:55 | INFO | fairseq.trainer | begin training epoch 3
2022-05-06 18:43:55 | INFO | fairseq_cli.train | Start iterating over samples
2022-05-06 18:46:03 | INFO | train_inner | epoch 003:     44 / 1478 loss=6.359, ppl=82.06, wps=17805.7, ups=0.27, wpb=65126.4, bsz=127.2, num_updates=3000, lr=0.000375025, gnorm=0.704, train_wall=226, gb_free=7.9, wall=8186
2022-05-06 18:50:35 | INFO | train_inner | epoch 003:    144 / 1478 loss=6.292, ppl=78.36, wps=24048, ups=0.37, wpb=65531, bsz=128, num_updates=3100, lr=0.000387523, gnorm=0.682, train_wall=233, gb_free=7.9, wall=8459
2022-05-06 18:55:41 | INFO | train_inner | epoch 003:    244 / 1478 loss=6.263, ppl=76.78, wps=21445.4, ups=0.33, wpb=65536, bsz=128, num_updates=3200, lr=0.00040002, gnorm=0.679, train_wall=239, gb_free=7.9, wall=8765
2022-05-06 19:00:26 | INFO | train_inner | epoch 003:    344 / 1478 loss=6.207, ppl=73.89, wps=23018, ups=0.35, wpb=65536, bsz=128, num_updates=3300, lr=0.000412518, gnorm=0.667, train_wall=233, gb_free=7.9, wall=9049
2022-05-06 19:04:57 | INFO | train_inner | epoch 003:    444 / 1478 loss=6.168, ppl=71.88, wps=24188.3, ups=0.37, wpb=65536, bsz=128, num_updates=3400, lr=0.000425015, gnorm=0.651, train_wall=235, gb_free=7.9, wall=9320
2022-05-06 19:09:23 | INFO | train_inner | epoch 003:    544 / 1478 loss=6.138, ppl=70.43, wps=24617.1, ups=0.38, wpb=65536, bsz=128, num_updates=3500, lr=0.000437513, gnorm=0.636, train_wall=232, gb_free=7.9, wall=9586
2022-05-06 19:13:48 | INFO | train_inner | epoch 003:    644 / 1478 loss=6.081, ppl=67.72, wps=24686.4, ups=0.38, wpb=65536, bsz=128, num_updates=3600, lr=0.00045001, gnorm=0.637, train_wall=230, gb_free=7.9, wall=9852
2022-05-06 19:18:03 | INFO | train_inner | epoch 003:    744 / 1478 loss=6.046, ppl=66.06, wps=25719.7, ups=0.39, wpb=65536, bsz=128, num_updates=3700, lr=0.000462508, gnorm=0.615, train_wall=238, gb_free=7.9, wall=10107
2022-05-06 19:22:38 | INFO | train_inner | epoch 003:    844 / 1478 loss=6.018, ppl=64.81, wps=23843.3, ups=0.36, wpb=65536, bsz=128, num_updates=3800, lr=0.000475005, gnorm=0.603, train_wall=233, gb_free=7.9, wall=10382
2022-05-06 19:27:26 | INFO | train_inner | epoch 003:    944 / 1478 loss=5.981, ppl=63.17, wps=22766.4, ups=0.35, wpb=65525.8, bsz=128, num_updates=3900, lr=0.000487503, gnorm=0.597, train_wall=230, gb_free=7.9, wall=10669
2022-05-06 19:31:59 | INFO | train_inner | epoch 003:   1044 / 1478 loss=5.958, ppl=62.17, wps=23952.8, ups=0.37, wpb=65536, bsz=128, num_updates=4000, lr=0.0005, gnorm=0.59, train_wall=233, gb_free=7.9, wall=10943
2022-05-06 19:36:30 | INFO | train_inner | epoch 003:   1144 / 1478 loss=5.914, ppl=60.31, wps=24202.7, ups=0.37, wpb=65536, bsz=128, num_updates=4100, lr=0.000493865, gnorm=0.568, train_wall=235, gb_free=7.9, wall=11214
2022-05-06 19:41:00 | INFO | train_inner | epoch 003:   1244 / 1478 loss=5.885, ppl=59.11, wps=24250.1, ups=0.37, wpb=65536, bsz=128, num_updates=4200, lr=0.00048795, gnorm=0.562, train_wall=233, gb_free=7.9, wall=11484
2022-05-06 19:45:49 | INFO | train_inner | epoch 003:   1344 / 1478 loss=5.844, ppl=57.44, wps=22687.4, ups=0.35, wpb=65536, bsz=128, num_updates=4300, lr=0.000482243, gnorm=0.538, train_wall=235, gb_free=7.9, wall=11773
2022-05-06 19:50:42 | INFO | train_inner | epoch 003:   1444 / 1478 loss=5.82, ppl=56.49, wps=22395.4, ups=0.34, wpb=65536, bsz=128, num_updates=4400, lr=0.000476731, gnorm=0.54, train_wall=231, gb_free=7.9, wall=12066
2022-05-06 19:52:24 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-05-06 19:53:39 | INFO | valid | epoch 003 | valid on 'valid' subset | loss 5.674 | ppl 51.06 | wps 71320.9 | wpb 2047.4 | bsz 4 | num_updates 4434 | best_loss 5.674
2022-05-06 19:53:39 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 3 @ 4434 updates
2022-05-06 19:53:39 | INFO | fairseq.trainer | Saving checkpoint to /cluster/work/cotterell/liam/master-thesis/checkpoints/language_model/standard/checkpoint_best_standard_32_5.0000E-04_full.pt
2022-05-06 19:53:42 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/work/cotterell/liam/master-thesis/checkpoints/language_model/standard/checkpoint_best_standard_32_5.0000E-04_full.pt
2022-05-06 19:53:42 | INFO | fairseq.checkpoint_utils | Saved checkpoint checkpoints/language_model/standard/checkpoint_best_standard_32_5.0000E-04_full.pt (epoch 3 @ 4434 updates, score 5.674) (writing took 2.496829941868782 seconds)
2022-05-06 19:53:42 | INFO | fairseq_cli.train | end of epoch 3 (average epoch stats below)
2022-05-06 19:53:42 | INFO | train | epoch 003 | loss 6.047 | ppl 66.11 | wps 23121.1 | ups 0.35 | wpb 65507.3 | bsz 127.9 | num_updates 4434 | lr 0.0004749 | gnorm 0.613 | train_wall 3446 | gb_free 7.9 | wall 12246
2022-05-06 19:53:42 | INFO | fairseq.data.iterators | grouped total_num_itrs = 1478
2022-05-06 19:53:42 | INFO | fairseq.trainer | begin training epoch 4
2022-05-06 19:53:42 | INFO | fairseq_cli.train | Start iterating over samples
2022-05-06 19:56:46 | INFO | train_inner | epoch 004:     66 / 1478 loss=5.753, ppl=53.93, wps=17901.1, ups=0.27, wpb=65126.4, bsz=127.2, num_updates=4500, lr=0.000471405, gnorm=0.527, train_wall=227, gb_free=7.9, wall=12429
2022-05-06 20:01:21 | INFO | train_inner | epoch 004:    166 / 1478 loss=5.702, ppl=52.05, wps=23840, ups=0.36, wpb=65536, bsz=128, num_updates=4600, lr=0.000466252, gnorm=0.516, train_wall=235, gb_free=7.9, wall=12704
2022-05-06 20:06:01 | INFO | train_inner | epoch 004:    266 / 1478 loss=5.694, ppl=51.77, wps=23342.3, ups=0.36, wpb=65536, bsz=128, num_updates=4700, lr=0.000461266, gnorm=0.51, train_wall=228, gb_free=7.9, wall=12985
2022-05-06 20:10:52 | INFO | train_inner | epoch 004:    366 / 1478 loss=5.676, ppl=51.14, wps=22563.7, ups=0.34, wpb=65536, bsz=128, num_updates=4800, lr=0.000456435, gnorm=0.509, train_wall=231, gb_free=7.9, wall=13275
2022-05-06 20:15:12 | INFO | train_inner | epoch 004:    466 / 1478 loss=5.658, ppl=50.48, wps=25142, ups=0.38, wpb=65536, bsz=128, num_updates=4900, lr=0.000451754, gnorm=0.502, train_wall=229, gb_free=7.9, wall=13536
2022-05-06 20:19:27 | INFO | train_inner | epoch 004:    566 / 1478 loss=5.65, ppl=50.22, wps=25761.7, ups=0.39, wpb=65536, bsz=128, num_updates=5000, lr=0.000447214, gnorm=0.499, train_wall=232, gb_free=7.9, wall=13791
2022-05-06 20:24:32 | INFO | train_inner | epoch 004:    666 / 1478 loss=5.629, ppl=49.48, wps=21502.7, ups=0.33, wpb=65536, bsz=128, num_updates=5100, lr=0.000442807, gnorm=0.494, train_wall=231, gb_free=7.9, wall=14095
2022-05-06 20:29:11 | INFO | train_inner | epoch 004:    766 / 1478 loss=5.62, ppl=49.17, wps=23459.9, ups=0.36, wpb=65525.8, bsz=128, num_updates=5200, lr=0.000438529, gnorm=0.484, train_wall=230, gb_free=7.9, wall=14375
2022-05-06 20:33:38 | INFO | train_inner | epoch 004:    866 / 1478 loss=5.584, ppl=47.96, wps=24542, ups=0.37, wpb=65536, bsz=128, num_updates=5300, lr=0.000434372, gnorm=0.495, train_wall=235, gb_free=7.9, wall=14642
2022-05-06 20:38:11 | INFO | train_inner | epoch 004:    966 / 1478 loss=5.574, ppl=47.64, wps=23964.4, ups=0.37, wpb=65536, bsz=128, num_updates=5400, lr=0.000430331, gnorm=0.474, train_wall=230, gb_free=7.9, wall=14915
2022-05-06 20:43:00 | INFO | train_inner | epoch 004:   1066 / 1478 loss=5.556, ppl=47.06, wps=22688.9, ups=0.35, wpb=65531, bsz=128, num_updates=5500, lr=0.000426401, gnorm=0.483, train_wall=232, gb_free=7.9, wall=15204
2022-05-06 20:47:31 | INFO | train_inner | epoch 004:   1166 / 1478 loss=5.545, ppl=46.68, wps=24223.8, ups=0.37, wpb=65536, bsz=128, num_updates=5600, lr=0.000422577, gnorm=0.474, train_wall=230, gb_free=7.9, wall=15475
2022-05-06 20:52:22 | INFO | train_inner | epoch 004:   1266 / 1478 loss=5.534, ppl=46.33, wps=22525.5, ups=0.34, wpb=65536, bsz=128, num_updates=5700, lr=0.000418854, gnorm=0.478, train_wall=232, gb_free=7.9, wall=15765
2022-05-06 20:57:05 | INFO | train_inner | epoch 004:   1366 / 1478 loss=5.525, ppl=46.04, wps=23111.1, ups=0.35, wpb=65536, bsz=128, num_updates=5800, lr=0.000415227, gnorm=0.474, train_wall=230, gb_free=7.9, wall=16049
2022-05-06 21:01:42 | INFO | train_inner | epoch 004:   1466 / 1478 loss=5.495, ppl=45.1, wps=23664, ups=0.36, wpb=65536, bsz=128, num_updates=5900, lr=0.000411693, gnorm=0.462, train_wall=231, gb_free=7.9, wall=16326
2022-05-06 21:02:16 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-05-06 21:03:35 | INFO | valid | epoch 004 | valid on 'valid' subset | loss 5.396 | ppl 42.12 | wps 68572.4 | wpb 2047.4 | bsz 4 | num_updates 5912 | best_loss 5.396
2022-05-06 21:03:35 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 4 @ 5912 updates
2022-05-06 21:03:35 | INFO | fairseq.trainer | Saving checkpoint to /cluster/work/cotterell/liam/master-thesis/checkpoints/language_model/standard/checkpoint_best_standard_32_5.0000E-04_full.pt
2022-05-06 21:03:38 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/work/cotterell/liam/master-thesis/checkpoints/language_model/standard/checkpoint_best_standard_32_5.0000E-04_full.pt
2022-05-06 21:03:38 | INFO | fairseq.checkpoint_utils | Saved checkpoint checkpoints/language_model/standard/checkpoint_best_standard_32_5.0000E-04_full.pt (epoch 4 @ 5912 updates, score 5.396) (writing took 2.703036163933575 seconds)
2022-05-06 21:03:38 | INFO | fairseq_cli.train | end of epoch 4 (average epoch stats below)
2022-05-06 21:03:38 | INFO | train | epoch 004 | loss 5.608 | ppl 48.77 | wps 23072.9 | ups 0.35 | wpb 65507.3 | bsz 127.9 | num_updates 5912 | lr 0.000411275 | gnorm 0.491 | train_wall 3414 | gb_free 7.9 | wall 16442
2022-05-06 21:03:38 | INFO | fairseq.data.iterators | grouped total_num_itrs = 1478
2022-05-06 21:03:38 | INFO | fairseq.trainer | begin training epoch 5
2022-05-06 21:03:38 | INFO | fairseq_cli.train | Start iterating over samples
2022-05-06 21:07:42 | INFO | train_inner | epoch 005:     88 / 1478 loss=5.424, ppl=42.93, wps=18095.2, ups=0.28, wpb=65126.4, bsz=127.2, num_updates=6000, lr=0.000408248, gnorm=0.467, train_wall=227, gb_free=7.9, wall=16686
2022-05-06 21:12:08 | INFO | train_inner | epoch 005:    188 / 1478 loss=5.413, ppl=42.6, wps=24702.6, ups=0.38, wpb=65536, bsz=128, num_updates=6100, lr=0.000404888, gnorm=0.465, train_wall=231, gb_free=7.9, wall=16951
2022-05-06 21:17:26 | INFO | train_inner | epoch 005:    288 / 1478 loss=5.415, ppl=42.66, wps=20599.3, ups=0.31, wpb=65536, bsz=128, num_updates=6200, lr=0.00040161, gnorm=0.468, train_wall=229, gb_free=7.9, wall=17269
2022-05-06 21:22:00 | INFO | train_inner | epoch 005:    388 / 1478 loss=5.4, ppl=42.22, wps=23879, ups=0.36, wpb=65536, bsz=128, num_updates=6300, lr=0.00039841, gnorm=0.47, train_wall=231, gb_free=7.9, wall=17544
2022-05-06 21:26:18 | INFO | train_inner | epoch 005:    488 / 1478 loss=5.414, ppl=42.62, wps=25439.9, ups=0.39, wpb=65536, bsz=128, num_updates=6400, lr=0.000395285, gnorm=0.474, train_wall=230, gb_free=7.9, wall=17801
2022-05-06 21:30:33 | INFO | train_inner | epoch 005:    588 / 1478 loss=5.396, ppl=42.11, wps=25706.6, ups=0.39, wpb=65536, bsz=128, num_updates=6500, lr=0.000392232, gnorm=0.46, train_wall=232, gb_free=7.9, wall=18056
2022-05-06 21:35:32 | INFO | train_inner | epoch 005:    688 / 1478 loss=5.392, ppl=42, wps=21873.6, ups=0.33, wpb=65525.8, bsz=128, num_updates=6600, lr=0.000389249, gnorm=0.463, train_wall=231, gb_free=7.9, wall=18356
2022-05-06 21:40:05 | INFO | train_inner | epoch 005:    788 / 1478 loss=5.368, ppl=41.3, wps=24034.9, ups=0.37, wpb=65536, bsz=128, num_updates=6700, lr=0.000386334, gnorm=0.453, train_wall=230, gb_free=7.9, wall=18629
2022-05-06 21:44:28 | INFO | train_inner | epoch 005:    888 / 1478 loss=5.374, ppl=41.46, wps=24929.6, ups=0.38, wpb=65536, bsz=128, num_updates=6800, lr=0.000383482, gnorm=0.468, train_wall=233, gb_free=7.9, wall=18892
2022-05-06 21:48:41 | INFO | train_inner | epoch 005:    988 / 1478 loss=5.348, ppl=40.73, wps=25839, ups=0.39, wpb=65536, bsz=128, num_updates=6900, lr=0.000380693, gnorm=0.453, train_wall=230, gb_free=7.9, wall=19145
2022-05-06 21:52:59 | INFO | train_inner | epoch 005:   1088 / 1478 loss=5.362, ppl=41.12, wps=25403.5, ups=0.39, wpb=65536, bsz=128, num_updates=7000, lr=0.000377964, gnorm=0.46, train_wall=235, gb_free=7.9, wall=19403
2022-05-06 21:57:05 | INFO | train_inner | epoch 005:   1188 / 1478 loss=5.357, ppl=40.99, wps=26678, ups=0.41, wpb=65536, bsz=128, num_updates=7100, lr=0.000375293, gnorm=0.459, train_wall=232, gb_free=7.9, wall=19649
2022-05-06 22:01:14 | INFO | train_inner | epoch 005:   1288 / 1478 loss=5.331, ppl=40.26, wps=26372.5, ups=0.4, wpb=65531, bsz=128, num_updates=7200, lr=0.000372678, gnorm=0.45, train_wall=234, gb_free=7.9, wall=19897
2022-05-06 22:05:23 | INFO | train_inner | epoch 005:   1388 / 1478 loss=5.33, ppl=40.22, wps=26322.5, ups=0.4, wpb=65536, bsz=128, num_updates=7300, lr=0.000370117, gnorm=0.457, train_wall=231, gb_free=7.9, wall=20146
2022-05-06 22:09:01 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-05-06 22:10:25 | INFO | valid | epoch 005 | valid on 'valid' subset | loss 5.238 | ppl 37.74 | wps 64803.1 | wpb 2047.4 | bsz 4 | num_updates 7390 | best_loss 5.238
2022-05-06 22:10:25 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 5 @ 7390 updates
2022-05-06 22:10:25 | INFO | fairseq.trainer | Saving checkpoint to /cluster/work/cotterell/liam/master-thesis/checkpoints/language_model/standard/checkpoint_best_standard_32_5.0000E-04_full.pt
2022-05-06 22:10:28 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/work/cotterell/liam/master-thesis/checkpoints/language_model/standard/checkpoint_best_standard_32_5.0000E-04_full.pt
2022-05-06 22:10:28 | INFO | fairseq.checkpoint_utils | Saved checkpoint checkpoints/language_model/standard/checkpoint_best_standard_32_5.0000E-04_full.pt (epoch 5 @ 7390 updates, score 5.238) (writing took 2.5649912930093706 seconds)
2022-05-06 22:10:28 | INFO | fairseq_cli.train | end of epoch 5 (average epoch stats below)
2022-05-06 22:10:28 | INFO | train | epoch 005 | loss 5.376 | ppl 41.51 | wps 24148.1 | ups 0.37 | wpb 65507.3 | bsz 127.9 | num_updates 7390 | lr 0.000367856 | gnorm 0.462 | train_wall 3418 | gb_free 7.9 | wall 20451
2022-05-06 22:10:28 | INFO | fairseq.data.iterators | grouped total_num_itrs = 1478
2022-05-06 22:10:28 | INFO | fairseq.trainer | begin training epoch 6
2022-05-06 22:10:28 | INFO | fairseq_cli.train | Start iterating over samples
2022-05-06 22:11:00 | INFO | train_inner | epoch 006:     10 / 1478 loss=5.314, ppl=39.78, wps=19295.2, ups=0.3, wpb=65126.4, bsz=127.2, num_updates=7400, lr=0.000367607, gnorm=0.463, train_wall=231, gb_free=7.9, wall=20484
2022-05-06 22:15:55 | INFO | train_inner | epoch 006:    110 / 1478 loss=5.245, ppl=37.91, wps=22229.9, ups=0.34, wpb=65525.8, bsz=128, num_updates=7500, lr=0.000365148, gnorm=0.46, train_wall=226, gb_free=7.9, wall=20779
2022-05-06 22:20:26 | INFO | train_inner | epoch 006:    210 / 1478 loss=5.248, ppl=38, wps=24165.7, ups=0.37, wpb=65536, bsz=128, num_updates=7600, lr=0.000362738, gnorm=0.455, train_wall=232, gb_free=7.9, wall=21050
2022-05-06 22:25:13 | INFO | train_inner | epoch 006:    310 / 1478 loss=5.243, ppl=37.86, wps=22874, ups=0.35, wpb=65536, bsz=128, num_updates=7700, lr=0.000360375, gnorm=0.454, train_wall=229, gb_free=7.9, wall=21336
2022-05-06 22:29:40 | INFO | train_inner | epoch 006:    410 / 1478 loss=5.248, ppl=38.01, wps=24484.3, ups=0.37, wpb=65536, bsz=128, num_updates=7800, lr=0.000358057, gnorm=0.453, train_wall=229, gb_free=7.9, wall=21604
2022-05-06 22:34:22 | INFO | train_inner | epoch 006:    510 / 1478 loss=5.235, ppl=37.65, wps=23224.8, ups=0.35, wpb=65536, bsz=128, num_updates=7900, lr=0.000355784, gnorm=0.456, train_wall=237, gb_free=7.9, wall=21886
2022-05-06 22:38:36 | INFO | train_inner | epoch 006:    610 / 1478 loss=5.246, ppl=37.96, wps=25887.5, ups=0.4, wpb=65536, bsz=128, num_updates=8000, lr=0.000353553, gnorm=0.453, train_wall=230, gb_free=7.9, wall=22139
2022-05-06 22:43:38 | INFO | train_inner | epoch 006:    710 / 1478 loss=5.245, ppl=37.93, wps=21695.3, ups=0.33, wpb=65536, bsz=128, num_updates=8100, lr=0.000351364, gnorm=0.457, train_wall=236, gb_free=7.9, wall=22441
2022-05-06 22:52:29 | INFO | train_inner | epoch 006:    810 / 1478 loss=5.237, ppl=37.72, wps=12343.2, ups=0.19, wpb=65536, bsz=128, num_updates=8200, lr=0.000349215, gnorm=0.46, train_wall=235, gb_free=7.9, wall=22972
2022-05-06 22:57:38 | INFO | train_inner | epoch 006:    910 / 1478 loss=5.231, ppl=37.55, wps=21168.7, ups=0.32, wpb=65536, bsz=128, num_updates=8300, lr=0.000347105, gnorm=0.45, train_wall=236, gb_free=7.9, wall=23282
2022-05-06 23:09:13 | INFO | train_inner | epoch 006:   1010 / 1478 loss=5.23, ppl=37.54, wps=9435.9, ups=0.14, wpb=65536, bsz=128, num_updates=8400, lr=0.000345033, gnorm=0.452, train_wall=250, gb_free=7.9, wall=23976
2022-05-06 23:17:36 | INFO | train_inner | epoch 006:   1110 / 1478 loss=5.232, ppl=37.59, wps=13017.8, ups=0.2, wpb=65536, bsz=128, num_updates=8500, lr=0.000342997, gnorm=0.453, train_wall=227, gb_free=7.9, wall=24480
2022-05-06 23:22:32 | INFO | train_inner | epoch 006:   1210 / 1478 loss=5.22, ppl=37.28, wps=22147.7, ups=0.34, wpb=65531, bsz=128, num_updates=8600, lr=0.000340997, gnorm=0.451, train_wall=236, gb_free=7.9, wall=24776
2022-05-06 23:26:48 | INFO | train_inner | epoch 006:   1310 / 1478 loss=5.218, ppl=37.22, wps=25595.2, ups=0.39, wpb=65536, bsz=128, num_updates=8700, lr=0.000339032, gnorm=0.457, train_wall=229, gb_free=7.9, wall=25032
2022-05-06 23:31:05 | INFO | train_inner | epoch 006:   1410 / 1478 loss=5.203, ppl=36.84, wps=25476.3, ups=0.39, wpb=65536, bsz=128, num_updates=8800, lr=0.0003371, gnorm=0.453, train_wall=235, gb_free=7.9, wall=25289
2022-05-06 23:34:09 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-05-06 23:35:29 | INFO | valid | epoch 006 | valid on 'valid' subset | loss 5.145 | ppl 35.39 | wps 69000.8 | wpb 2047.4 | bsz 4 | num_updates 8868 | best_loss 5.145
2022-05-06 23:35:29 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 6 @ 8868 updates
2022-05-06 23:35:29 | INFO | fairseq.trainer | Saving checkpoint to /cluster/work/cotterell/liam/master-thesis/checkpoints/language_model/standard/checkpoint_best_standard_32_5.0000E-04_full.pt
2022-05-06 23:35:32 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/work/cotterell/liam/master-thesis/checkpoints/language_model/standard/checkpoint_best_standard_32_5.0000E-04_full.pt
2022-05-06 23:35:32 | INFO | fairseq.checkpoint_utils | Saved checkpoint checkpoints/language_model/standard/checkpoint_best_standard_32_5.0000E-04_full.pt (epoch 6 @ 8868 updates, score 5.145) (writing took 3.4682952109724283 seconds)
2022-05-06 23:35:32 | INFO | fairseq_cli.train | end of epoch 6 (average epoch stats below)
2022-05-06 23:35:32 | INFO | train | epoch 006 | loss 5.234 | ppl 37.63 | wps 18967.3 | ups 0.29 | wpb 65507.3 | bsz 127.9 | num_updates 8868 | lr 0.000335805 | gnorm 0.455 | train_wall 3451 | gb_free 7.9 | wall 25556
2022-05-06 23:35:32 | INFO | fairseq.data.iterators | grouped total_num_itrs = 1478
2022-05-06 23:35:32 | INFO | fairseq.trainer | begin training epoch 7
2022-05-06 23:35:32 | INFO | fairseq_cli.train | Start iterating over samples
2022-05-06 23:37:07 | INFO | train_inner | epoch 007:     32 / 1478 loss=5.185, ppl=36.37, wps=17994.3, ups=0.28, wpb=65126.4, bsz=127.2, num_updates=8900, lr=0.000335201, gnorm=0.461, train_wall=233, gb_free=7.9, wall=25651
2022-05-06 23:41:56 | INFO | train_inner | epoch 007:    132 / 1478 loss=5.126, ppl=34.92, wps=22725.9, ups=0.35, wpb=65536, bsz=128, num_updates=9000, lr=0.000333333, gnorm=0.461, train_wall=228, gb_free=7.9, wall=25939
2022-05-06 23:46:31 | INFO | train_inner | epoch 007:    232 / 1478 loss=5.135, ppl=35.14, wps=23782.7, ups=0.36, wpb=65536, bsz=128, num_updates=9100, lr=0.000331497, gnorm=0.457, train_wall=232, gb_free=7.9, wall=26215
2022-05-06 23:50:59 | INFO | train_inner | epoch 007:    332 / 1478 loss=5.144, ppl=35.37, wps=24493.6, ups=0.37, wpb=65536, bsz=128, num_updates=9200, lr=0.00032969, gnorm=0.464, train_wall=231, gb_free=7.9, wall=26483
2022-05-06 23:55:26 | INFO | train_inner | epoch 007:    432 / 1478 loss=5.151, ppl=35.53, wps=24568.6, ups=0.37, wpb=65536, bsz=128, num_updates=9300, lr=0.000327913, gnorm=0.45, train_wall=228, gb_free=7.9, wall=26749
2022-05-07 00:00:24 | INFO | train_inner | epoch 007:    532 / 1478 loss=5.153, ppl=35.57, wps=21943.9, ups=0.33, wpb=65536, bsz=128, num_updates=9400, lr=0.000326164, gnorm=0.452, train_wall=239, gb_free=7.9, wall=27048
2022-05-07 00:05:01 | INFO | train_inner | epoch 007:    632 / 1478 loss=5.137, ppl=35.18, wps=23670.2, ups=0.36, wpb=65536, bsz=128, num_updates=9500, lr=0.000324443, gnorm=0.455, train_wall=228, gb_free=7.9, wall=27325
2022-05-07 00:09:50 | INFO | train_inner | epoch 007:    732 / 1478 loss=5.142, ppl=35.3, wps=22664.4, ups=0.35, wpb=65536, bsz=128, num_updates=9600, lr=0.000322749, gnorm=0.462, train_wall=230, gb_free=7.9, wall=27614
2022-05-07 00:14:41 | INFO | train_inner | epoch 007:    832 / 1478 loss=5.138, ppl=35.21, wps=22572.8, ups=0.34, wpb=65525.8, bsz=128, num_updates=9700, lr=0.000321081, gnorm=0.454, train_wall=230, gb_free=7.9, wall=27904
2022-05-07 00:19:05 | INFO | train_inner | epoch 007:    932 / 1478 loss=5.136, ppl=35.16, wps=24768.2, ups=0.38, wpb=65531, bsz=128, num_updates=9800, lr=0.000319438, gnorm=0.462, train_wall=233, gb_free=7.9, wall=28169
2022-05-07 00:23:19 | INFO | train_inner | epoch 007:   1032 / 1478 loss=5.132, ppl=35.07, wps=25859.1, ups=0.39, wpb=65536, bsz=128, num_updates=9900, lr=0.000317821, gnorm=0.457, train_wall=232, gb_free=7.9, wall=28422
2022-05-07 00:27:40 | INFO | train_inner | epoch 007:   1132 / 1478 loss=5.134, ppl=35.12, wps=25109.2, ups=0.38, wpb=65536, bsz=128, num_updates=10000, lr=0.000316228, gnorm=0.456, train_wall=237, gb_free=7.9, wall=28683
2022-05-07 00:32:43 | INFO | train_inner | epoch 007:   1232 / 1478 loss=5.125, ppl=34.9, wps=21616.5, ups=0.33, wpb=65536, bsz=128, num_updates=10100, lr=0.000314658, gnorm=0.45, train_wall=230, gb_free=7.9, wall=28986
2022-05-07 00:37:28 | INFO | train_inner | epoch 007:   1332 / 1478 loss=5.138, ppl=35.21, wps=23010.3, ups=0.35, wpb=65536, bsz=128, num_updates=10200, lr=0.000313112, gnorm=0.461, train_wall=231, gb_free=7.9, wall=29271
2022-05-07 00:42:02 | INFO | train_inner | epoch 007:   1432 / 1478 loss=5.114, ppl=34.62, wps=23840.5, ups=0.36, wpb=65536, bsz=128, num_updates=10300, lr=0.000311588, gnorm=0.465, train_wall=233, gb_free=7.9, wall=29546
2022-05-07 00:43:58 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-05-07 00:45:24 | INFO | valid | epoch 007 | valid on 'valid' subset | loss 5.077 | ppl 33.76 | wps 66482 | wpb 2047.4 | bsz 4 | num_updates 10346 | best_loss 5.077
2022-05-07 00:45:24 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 7 @ 10346 updates
2022-05-07 00:45:24 | INFO | fairseq.trainer | Saving checkpoint to /cluster/work/cotterell/liam/master-thesis/checkpoints/language_model/standard/checkpoint_best_standard_32_5.0000E-04_full.pt
2022-05-07 00:45:27 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/work/cotterell/liam/master-thesis/checkpoints/language_model/standard/checkpoint_best_standard_32_5.0000E-04_full.pt
2022-05-07 00:45:27 | INFO | fairseq.checkpoint_utils | Saved checkpoint checkpoints/language_model/standard/checkpoint_best_standard_32_5.0000E-04_full.pt (epoch 7 @ 10346 updates, score 5.077) (writing took 3.554250922985375 seconds)
2022-05-07 00:45:27 | INFO | fairseq_cli.train | end of epoch 7 (average epoch stats below)
2022-05-07 00:45:27 | INFO | train | epoch 007 | loss 5.135 | ppl 35.14 | wps 23078.6 | ups 0.35 | wpb 65507.3 | bsz 127.9 | num_updates 10346 | lr 0.000310895 | gnorm 0.458 | train_wall 3419 | gb_free 7.9 | wall 29751
2022-05-07 00:45:28 | INFO | fairseq.data.iterators | grouped total_num_itrs = 1478
2022-05-07 00:45:28 | INFO | fairseq.trainer | begin training epoch 8
2022-05-07 00:45:28 | INFO | fairseq_cli.train | Start iterating over samples
2022-05-07 00:47:43 | INFO | train_inner | epoch 008:     54 / 1478 loss=5.082, ppl=33.87, wps=19106.1, ups=0.29, wpb=65126.4, bsz=127.2, num_updates=10400, lr=0.000310087, gnorm=0.465, train_wall=228, gb_free=7.9, wall=29887
2022-05-07 00:52:06 | INFO | train_inner | epoch 008:    154 / 1478 loss=5.053, ppl=33.19, wps=24941.7, ups=0.38, wpb=65536, bsz=128, num_updates=10500, lr=0.000308607, gnorm=0.465, train_wall=235, gb_free=7.9, wall=30150
2022-05-07 00:56:48 | INFO | train_inner | epoch 008:    254 / 1478 loss=5.068, ppl=33.55, wps=23261.2, ups=0.35, wpb=65536, bsz=128, num_updates=10600, lr=0.000307148, gnorm=0.471, train_wall=235, gb_free=7.9, wall=30432
2022-05-07 01:01:24 | INFO | train_inner | epoch 008:    354 / 1478 loss=5.061, ppl=33.39, wps=23760.7, ups=0.36, wpb=65536, bsz=128, num_updates=10700, lr=0.000305709, gnorm=0.462, train_wall=230, gb_free=7.9, wall=30707
2022-05-07 01:06:16 | INFO | train_inner | epoch 008:    454 / 1478 loss=5.062, ppl=33.4, wps=22392.9, ups=0.34, wpb=65536, bsz=128, num_updates=10800, lr=0.00030429, gnorm=0.466, train_wall=234, gb_free=7.9, wall=31000
2022-05-07 01:10:35 | INFO | train_inner | epoch 008:    554 / 1478 loss=5.063, ppl=33.43, wps=25336.3, ups=0.39, wpb=65536, bsz=128, num_updates=10900, lr=0.000302891, gnorm=0.468, train_wall=231, gb_free=7.9, wall=31259
2022-05-07 01:14:51 | INFO | train_inner | epoch 008:    654 / 1478 loss=5.072, ppl=33.64, wps=25576.4, ups=0.39, wpb=65536, bsz=128, num_updates=11000, lr=0.000301511, gnorm=0.457, train_wall=234, gb_free=7.9, wall=31515
2022-05-07 01:19:04 | INFO | train_inner | epoch 008:    754 / 1478 loss=5.062, ppl=33.4, wps=25905.7, ups=0.4, wpb=65531, bsz=128, num_updates=11100, lr=0.00030015, gnorm=0.463, train_wall=234, gb_free=7.9, wall=31768
2022-05-07 01:23:11 | INFO | train_inner | epoch 008:    854 / 1478 loss=5.075, ppl=33.72, wps=26578.5, ups=0.41, wpb=65536, bsz=128, num_updates=11200, lr=0.000298807, gnorm=0.465, train_wall=234, gb_free=7.9, wall=32014
2022-05-07 01:27:48 | INFO | train_inner | epoch 008:    954 / 1478 loss=5.066, ppl=33.49, wps=23656, ups=0.36, wpb=65536, bsz=128, num_updates=11300, lr=0.000297482, gnorm=0.468, train_wall=232, gb_free=7.9, wall=32292
2022-05-07 01:32:47 | INFO | train_inner | epoch 008:   1054 / 1478 loss=5.055, ppl=33.24, wps=21909.2, ups=0.33, wpb=65536, bsz=128, num_updates=11400, lr=0.000296174, gnorm=0.462, train_wall=232, gb_free=7.9, wall=32591
2022-05-07 01:37:22 | INFO | train_inner | epoch 008:   1154 / 1478 loss=5.058, ppl=33.31, wps=23813.1, ups=0.36, wpb=65536, bsz=128, num_updates=11500, lr=0.000294884, gnorm=0.464, train_wall=230, gb_free=7.9, wall=32866
2022-05-07 01:42:03 | INFO | train_inner | epoch 008:   1254 / 1478 loss=5.067, ppl=33.51, wps=23368.2, ups=0.36, wpb=65536, bsz=128, num_updates=11600, lr=0.00029361, gnorm=0.465, train_wall=235, gb_free=7.9, wall=33146
2022-05-07 01:46:17 | INFO | train_inner | epoch 008:   1354 / 1478 loss=5.066, ppl=33.49, wps=25726.1, ups=0.39, wpb=65536, bsz=128, num_updates=11700, lr=0.000292353, gnorm=0.468, train_wall=232, gb_free=7.9, wall=33401
2022-05-07 01:50:44 | INFO | train_inner | epoch 008:   1454 / 1478 loss=5.05, ppl=33.14, wps=24539.4, ups=0.37, wpb=65525.8, bsz=128, num_updates=11800, lr=0.000291111, gnorm=0.463, train_wall=239, gb_free=7.9, wall=33668
2022-05-07 01:51:43 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-05-07 01:53:02 | INFO | valid | epoch 008 | valid on 'valid' subset | loss 5.03 | ppl 32.68 | wps 69949.6 | wpb 2047.4 | bsz 4 | num_updates 11824 | best_loss 5.03
2022-05-07 01:53:02 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 8 @ 11824 updates
2022-05-07 01:53:02 | INFO | fairseq.trainer | Saving checkpoint to /cluster/work/cotterell/liam/master-thesis/checkpoints/language_model/standard/checkpoint_best_standard_32_5.0000E-04_full.pt
2022-05-07 01:53:05 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/work/cotterell/liam/master-thesis/checkpoints/language_model/standard/checkpoint_best_standard_32_5.0000E-04_full.pt
2022-05-07 01:53:05 | INFO | fairseq.checkpoint_utils | Saved checkpoint checkpoints/language_model/standard/checkpoint_best_standard_32_5.0000E-04_full.pt (epoch 8 @ 11824 updates, score 5.03) (writing took 3.1000047558918595 seconds)
2022-05-07 01:53:05 | INFO | fairseq_cli.train | end of epoch 8 (average epoch stats below)
2022-05-07 01:53:05 | INFO | train | epoch 008 | loss 5.062 | ppl 33.4 | wps 23862.9 | ups 0.36 | wpb 65507.3 | bsz 127.9 | num_updates 11824 | lr 0.000290816 | gnorm 0.465 | train_wall 3444 | gb_free 7.9 | wall 33808
2022-05-07 01:53:05 | INFO | fairseq.data.iterators | grouped total_num_itrs = 1478
2022-05-07 01:53:05 | INFO | fairseq.trainer | begin training epoch 9
2022-05-07 01:53:05 | INFO | fairseq_cli.train | Start iterating over samples
2022-05-07 01:56:15 | INFO | train_inner | epoch 009:     76 / 1478 loss=4.999, ppl=31.98, wps=19699.6, ups=0.3, wpb=65126.4, bsz=127.2, num_updates=11900, lr=0.000289886, gnorm=0.468, train_wall=230, gb_free=7.9, wall=33999
2022-05-07 02:00:24 | INFO | train_inner | epoch 009:    176 / 1478 loss=4.998, ppl=31.96, wps=26329, ups=0.4, wpb=65531, bsz=128, num_updates=12000, lr=0.000288675, gnorm=0.467, train_wall=233, gb_free=7.9, wall=34248
2022-05-07 02:04:41 | INFO | train_inner | epoch 009:    276 / 1478 loss=5.008, ppl=32.18, wps=25464.4, ups=0.39, wpb=65536, bsz=128, num_updates=12100, lr=0.00028748, gnorm=0.468, train_wall=242, gb_free=7.9, wall=34505
2022-05-07 02:08:52 | INFO | train_inner | epoch 009:    376 / 1478 loss=4.997, ppl=31.93, wps=26131.8, ups=0.4, wpb=65536, bsz=128, num_updates=12200, lr=0.000286299, gnorm=0.468, train_wall=234, gb_free=7.9, wall=34756
2022-05-07 02:13:02 | INFO | train_inner | epoch 009:    476 / 1478 loss=5.001, ppl=32.02, wps=26173.5, ups=0.4, wpb=65536, bsz=128, num_updates=12300, lr=0.000285133, gnorm=0.469, train_wall=237, gb_free=7.9, wall=35006
2022-05-07 02:17:10 | INFO | train_inner | epoch 009:    576 / 1478 loss=5.003, ppl=32.06, wps=26480.4, ups=0.4, wpb=65536, bsz=128, num_updates=12400, lr=0.000283981, gnorm=0.471, train_wall=237, gb_free=7.9, wall=35254
2022-05-07 02:21:24 | INFO | train_inner | epoch 009:    676 / 1478 loss=5.011, ppl=32.25, wps=25802.2, ups=0.39, wpb=65536, bsz=128, num_updates=12500, lr=0.000282843, gnorm=0.466, train_wall=245, gb_free=7.9, wall=35508
2022-05-07 02:25:47 | INFO | train_inner | epoch 009:    776 / 1478 loss=5.003, ppl=32.07, wps=24923.7, ups=0.38, wpb=65536, bsz=128, num_updates=12600, lr=0.000281718, gnorm=0.471, train_wall=237, gb_free=7.9, wall=35771
2022-05-07 02:30:14 | INFO | train_inner | epoch 009:    876 / 1478 loss=5.006, ppl=32.12, wps=24520.7, ups=0.37, wpb=65536, bsz=128, num_updates=12700, lr=0.000280607, gnorm=0.465, train_wall=230, gb_free=7.9, wall=36038
2022-05-07 02:35:52 | INFO | train_inner | epoch 009:    976 / 1478 loss=4.999, ppl=31.97, wps=19415.7, ups=0.3, wpb=65525.8, bsz=128, num_updates=12800, lr=0.000279508, gnorm=0.467, train_wall=230, gb_free=7.9, wall=36375
2022-05-07 02:40:30 | INFO | train_inner | epoch 009:   1076 / 1478 loss=5.018, ppl=32.4, wps=23564.6, ups=0.36, wpb=65536, bsz=128, num_updates=12900, lr=0.000278423, gnorm=0.472, train_wall=228, gb_free=7.9, wall=36653
2022-05-07 02:45:03 | INFO | train_inner | epoch 009:   1176 / 1478 loss=5.018, ppl=32.4, wps=23981.5, ups=0.37, wpb=65536, bsz=128, num_updates=13000, lr=0.00027735, gnorm=0.471, train_wall=235, gb_free=7.9, wall=36927
2022-05-07 02:49:37 | INFO | train_inner | epoch 009:   1276 / 1478 loss=5.005, ppl=32.11, wps=23965.9, ups=0.37, wpb=65536, bsz=128, num_updates=13100, lr=0.000276289, gnorm=0.466, train_wall=231, gb_free=7.9, wall=37200
2022-05-07 02:54:19 | INFO | train_inner | epoch 009:   1376 / 1478 loss=5.004, ppl=32.09, wps=23174.8, ups=0.35, wpb=65536, bsz=128, num_updates=13200, lr=0.000275241, gnorm=0.472, train_wall=237, gb_free=7.9, wall=37483
2022-05-07 02:59:25 | INFO | train_inner | epoch 009:   1476 / 1478 loss=5.007, ppl=32.15, wps=21434.6, ups=0.33, wpb=65536, bsz=128, num_updates=13300, lr=0.000274204, gnorm=0.463, train_wall=229, gb_free=7.9, wall=37789
2022-05-07 02:59:29 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-05-07 03:01:00 | INFO | valid | epoch 009 | valid on 'valid' subset | loss 4.987 | ppl 31.72 | wps 60911.1 | wpb 2047.4 | bsz 4 | num_updates 13302 | best_loss 4.987
2022-05-07 03:01:00 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 9 @ 13302 updates
2022-05-07 03:01:00 | INFO | fairseq.trainer | Saving checkpoint to /cluster/work/cotterell/liam/master-thesis/checkpoints/language_model/standard/checkpoint_best_standard_32_5.0000E-04_full.pt
2022-05-07 03:01:04 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/work/cotterell/liam/master-thesis/checkpoints/language_model/standard/checkpoint_best_standard_32_5.0000E-04_full.pt
2022-05-07 03:01:04 | INFO | fairseq.checkpoint_utils | Saved checkpoint checkpoints/language_model/standard/checkpoint_best_standard_32_5.0000E-04_full.pt (epoch 9 @ 13302 updates, score 4.987) (writing took 3.9021719014272094 seconds)
2022-05-07 03:01:04 | INFO | fairseq_cli.train | end of epoch 9 (average epoch stats below)
2022-05-07 03:01:04 | INFO | train | epoch 009 | loss 5.004 | ppl 32.1 | wps 23735.5 | ups 0.36 | wpb 65507.3 | bsz 127.9 | num_updates 13302 | lr 0.000274184 | gnorm 0.468 | train_wall 3464 | gb_free 7.9 | wall 37888
2022-05-07 03:01:05 | INFO | fairseq.data.iterators | grouped total_num_itrs = 1478
2022-05-07 03:01:05 | INFO | fairseq.trainer | begin training epoch 10
2022-05-07 03:01:05 | INFO | fairseq_cli.train | Start iterating over samples
2022-05-07 03:05:48 | INFO | train_inner | epoch 010:     98 / 1478 loss=4.935, ppl=30.59, wps=17005.6, ups=0.26, wpb=65126.4, bsz=127.2, num_updates=13400, lr=0.000273179, gnorm=0.477, train_wall=228, gb_free=7.9, wall=38172
2022-05-07 03:10:13 | INFO | train_inner | epoch 010:    198 / 1478 loss=4.945, ppl=30.81, wps=24775.7, ups=0.38, wpb=65536, bsz=128, num_updates=13500, lr=0.000272166, gnorm=0.47, train_wall=231, gb_free=7.9, wall=38436
2022-05-07 03:14:37 | INFO | train_inner | epoch 010:    298 / 1478 loss=4.956, ppl=31.04, wps=24813.4, ups=0.38, wpb=65536, bsz=128, num_updates=13600, lr=0.000271163, gnorm=0.47, train_wall=236, gb_free=7.9, wall=38700
2022-05-07 03:18:59 | INFO | train_inner | epoch 010:    398 / 1478 loss=4.952, ppl=30.94, wps=24972.4, ups=0.38, wpb=65536, bsz=128, num_updates=13700, lr=0.000270172, gnorm=0.481, train_wall=232, gb_free=7.9, wall=38963
2022-05-07 03:23:23 | INFO | train_inner | epoch 010:    498 / 1478 loss=4.955, ppl=31.01, wps=24822.6, ups=0.38, wpb=65536, bsz=128, num_updates=13800, lr=0.000269191, gnorm=0.477, train_wall=235, gb_free=7.9, wall=39227
2022-05-07 03:28:34 | INFO | train_inner | epoch 010:    598 / 1478 loss=4.962, ppl=31.18, wps=21091.3, ups=0.32, wpb=65525.8, bsz=128, num_updates=13900, lr=0.000268221, gnorm=0.477, train_wall=227, gb_free=7.9, wall=39537
2022-05-07 03:33:21 | INFO | train_inner | epoch 010:    698 / 1478 loss=4.957, ppl=31.05, wps=22835.5, ups=0.35, wpb=65536, bsz=128, num_updates=14000, lr=0.000267261, gnorm=0.472, train_wall=232, gb_free=7.9, wall=39824
2022-05-07 03:37:51 | INFO | train_inner | epoch 010:    798 / 1478 loss=4.957, ppl=31.05, wps=24266.9, ups=0.37, wpb=65536, bsz=128, num_updates=14100, lr=0.000266312, gnorm=0.483, train_wall=231, gb_free=7.9, wall=40095
2022-05-07 03:42:31 | INFO | train_inner | epoch 010:    898 / 1478 loss=4.961, ppl=31.15, wps=23367.3, ups=0.36, wpb=65536, bsz=128, num_updates=14200, lr=0.000265372, gnorm=0.467, train_wall=233, gb_free=7.9, wall=40375
2022-05-07 03:46:57 | INFO | train_inner | epoch 010:    998 / 1478 loss=4.958, ppl=31.08, wps=24688.3, ups=0.38, wpb=65536, bsz=128, num_updates=14300, lr=0.000264443, gnorm=0.473, train_wall=231, gb_free=7.9, wall=40640
2022-05-07 03:52:10 | INFO | train_inner | epoch 010:   1098 / 1478 loss=4.962, ppl=31.17, wps=20901.5, ups=0.32, wpb=65531, bsz=128, num_updates=14400, lr=0.000263523, gnorm=0.478, train_wall=229, gb_free=7.9, wall=40954
2022-05-07 03:56:55 | INFO | train_inner | epoch 010:   1198 / 1478 loss=4.974, ppl=31.42, wps=23000.6, ups=0.35, wpb=65536, bsz=128, num_updates=14500, lr=0.000262613, gnorm=0.469, train_wall=229, gb_free=7.9, wall=41239
2022-05-07 04:01:25 | INFO | train_inner | epoch 010:   1298 / 1478 loss=4.96, ppl=31.12, wps=24316.3, ups=0.37, wpb=65536, bsz=128, num_updates=14600, lr=0.000261712, gnorm=0.475, train_wall=233, gb_free=7.9, wall=41508
2022-05-07 04:06:30 | INFO | train_inner | epoch 010:   1398 / 1478 loss=4.956, ppl=31.04, wps=21440.2, ups=0.33, wpb=65536, bsz=128, num_updates=14700, lr=0.00026082, gnorm=0.474, train_wall=231, gb_free=7.9, wall=41814
2022-05-07 04:10:09 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-05-07 04:11:27 | INFO | valid | epoch 010 | valid on 'valid' subset | loss 4.955 | ppl 31.02 | wps 69728.1 | wpb 2047.4 | bsz 4 | num_updates 14780 | best_loss 4.955
2022-05-07 04:11:27 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 10 @ 14780 updates
2022-05-07 04:11:27 | INFO | fairseq.trainer | Saving checkpoint to /cluster/work/cotterell/liam/master-thesis/checkpoints/language_model/standard/checkpoint_best_standard_32_5.0000E-04_full.pt
2022-05-07 04:11:30 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/work/cotterell/liam/master-thesis/checkpoints/language_model/standard/checkpoint_best_standard_32_5.0000E-04_full.pt
2022-05-07 04:11:30 | INFO | fairseq.checkpoint_utils | Saved checkpoint checkpoints/language_model/standard/checkpoint_best_standard_32_5.0000E-04_full.pt (epoch 10 @ 14780 updates, score 4.955) (writing took 2.726054365746677 seconds)
2022-05-07 04:11:30 | INFO | fairseq_cli.train | end of epoch 10 (average epoch stats below)
2022-05-07 04:11:30 | INFO | train | epoch 010 | loss 4.958 | ppl 31.08 | wps 22911.1 | ups 0.35 | wpb 65507.3 | bsz 127.9 | num_updates 14780 | lr 0.000260113 | gnorm 0.474 | train_wall 3417 | gb_free 7.9 | wall 42113
2022-05-07 04:11:30 | INFO | fairseq.data.iterators | grouped total_num_itrs = 1478
2022-05-07 04:11:30 | INFO | fairseq.trainer | begin training epoch 11
2022-05-07 04:11:30 | INFO | fairseq_cli.train | Start iterating over samples
2022-05-07 04:12:27 | INFO | train_inner | epoch 011:     20 / 1478 loss=4.967, ppl=31.27, wps=18271.6, ups=0.28, wpb=65126.4, bsz=127.2, num_updates=14800, lr=0.000259938, gnorm=0.479, train_wall=229, gb_free=7.9, wall=42171
2022-05-07 04:17:08 | INFO | train_inner | epoch 011:    120 / 1478 loss=4.894, ppl=29.74, wps=23285.4, ups=0.36, wpb=65531, bsz=128, num_updates=14900, lr=0.000259064, gnorm=0.473, train_wall=228, gb_free=7.9, wall=42452
2022-05-07 04:21:29 | INFO | train_inner | epoch 011:    220 / 1478 loss=4.912, ppl=30.1, wps=25186.7, ups=0.38, wpb=65536, bsz=128, num_updates=15000, lr=0.000258199, gnorm=0.481, train_wall=232, gb_free=7.9, wall=42712
2022-05-07 04:26:03 | INFO | train_inner | epoch 011:    320 / 1478 loss=4.909, ppl=30.04, wps=23895.1, ups=0.36, wpb=65536, bsz=128, num_updates=15100, lr=0.000257343, gnorm=0.485, train_wall=236, gb_free=7.9, wall=42986
2022-05-07 04:30:19 | INFO | train_inner | epoch 011:    420 / 1478 loss=4.911, ppl=30.08, wps=25547.5, ups=0.39, wpb=65536, bsz=128, num_updates=15200, lr=0.000256495, gnorm=0.486, train_wall=232, gb_free=7.9, wall=43243
2022-05-07 04:34:40 | INFO | train_inner | epoch 011:    520 / 1478 loss=4.916, ppl=30.2, wps=25142.7, ups=0.38, wpb=65536, bsz=128, num_updates=15300, lr=0.000255655, gnorm=0.479, train_wall=237, gb_free=7.9, wall=43504
2022-05-07 04:38:48 | INFO | train_inner | epoch 011:    620 / 1478 loss=4.922, ppl=30.32, wps=26406.2, ups=0.4, wpb=65536, bsz=128, num_updates=15400, lr=0.000254824, gnorm=0.481, train_wall=233, gb_free=7.9, wall=43752
2022-05-07 04:44:18 | INFO | train_inner | epoch 011:    720 / 1478 loss=4.924, ppl=30.36, wps=19864.3, ups=0.3, wpb=65536, bsz=128, num_updates=15500, lr=0.000254, gnorm=0.479, train_wall=237, gb_free=7.9, wall=44082
2022-05-07 04:49:01 | INFO | train_inner | epoch 011:    820 / 1478 loss=4.922, ppl=30.32, wps=23173.9, ups=0.35, wpb=65525.8, bsz=128, num_updates=15600, lr=0.000253185, gnorm=0.477, train_wall=229, gb_free=7.9, wall=44365
2022-05-07 04:54:13 | INFO | train_inner | epoch 011:    920 / 1478 loss=4.929, ppl=30.47, wps=20965.4, ups=0.32, wpb=65536, bsz=128, num_updates=15700, lr=0.000252377, gnorm=0.484, train_wall=232, gb_free=7.9, wall=44677
2022-05-07 04:59:16 | INFO | train_inner | epoch 011:   1020 / 1478 loss=4.924, ppl=30.36, wps=21681.4, ups=0.33, wpb=65536, bsz=128, num_updates=15800, lr=0.000251577, gnorm=0.48, train_wall=230, gb_free=7.9, wall=44979
2022-05-07 05:04:39 | INFO | train_inner | epoch 011:   1120 / 1478 loss=4.924, ppl=30.37, wps=20247.6, ups=0.31, wpb=65536, bsz=128, num_updates=15900, lr=0.000250785, gnorm=0.479, train_wall=235, gb_free=7.9, wall=45303
2022-05-07 05:09:21 | INFO | train_inner | epoch 011:   1220 / 1478 loss=4.922, ppl=30.32, wps=23283.8, ups=0.36, wpb=65536, bsz=128, num_updates=16000, lr=0.00025, gnorm=0.474, train_wall=230, gb_free=7.9, wall=45585
2022-05-07 05:14:01 | INFO | train_inner | epoch 011:   1320 / 1478 loss=4.93, ppl=30.48, wps=23382.4, ups=0.36, wpb=65536, bsz=128, num_updates=16100, lr=0.000249222, gnorm=0.487, train_wall=234, gb_free=7.9, wall=45865
2022-05-07 05:18:29 | INFO | train_inner | epoch 011:   1420 / 1478 loss=4.931, ppl=30.51, wps=24472.9, ups=0.37, wpb=65536, bsz=128, num_updates=16200, lr=0.000248452, gnorm=0.483, train_wall=229, gb_free=7.9, wall=46133
2022-05-07 05:21:20 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-05-07 05:22:59 | INFO | valid | epoch 011 | valid on 'valid' subset | loss 4.931 | ppl 30.5 | wps 56294.1 | wpb 2047.4 | bsz 4 | num_updates 16258 | best_loss 4.931
2022-05-07 05:22:59 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 11 @ 16258 updates
2022-05-07 05:22:59 | INFO | fairseq.trainer | Saving checkpoint to /cluster/work/cotterell/liam/master-thesis/checkpoints/language_model/standard/checkpoint_best_standard_32_5.0000E-04_full.pt
2022-05-07 05:23:02 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/work/cotterell/liam/master-thesis/checkpoints/language_model/standard/checkpoint_best_standard_32_5.0000E-04_full.pt
2022-05-07 05:23:03 | INFO | fairseq.checkpoint_utils | Saved checkpoint checkpoints/language_model/standard/checkpoint_best_standard_32_5.0000E-04_full.pt (epoch 11 @ 16258 updates, score 4.931) (writing took 3.481465939898044 seconds)
2022-05-07 05:23:03 | INFO | fairseq_cli.train | end of epoch 11 (average epoch stats below)
2022-05-07 05:23:03 | INFO | train | epoch 011 | loss 4.919 | ppl 30.26 | wps 22553.8 | ups 0.34 | wpb 65507.3 | bsz 127.9 | num_updates 16258 | lr 0.000248008 | gnorm 0.481 | train_wall 3432 | gb_free 7.9 | wall 46406
2022-05-07 05:23:03 | INFO | fairseq.data.iterators | grouped total_num_itrs = 1478
2022-05-07 05:23:03 | INFO | fairseq.trainer | begin training epoch 12
2022-05-07 05:23:03 | INFO | fairseq_cli.train | Start iterating over samples
2022-05-07 05:25:11 | INFO | train_inner | epoch 012:     42 / 1478 loss=4.9, ppl=29.86, wps=16204.7, ups=0.25, wpb=65126.4, bsz=127.2, num_updates=16300, lr=0.000247689, gnorm=0.485, train_wall=228, gb_free=7.9, wall=46535
2022-05-07 05:30:00 | INFO | train_inner | epoch 012:    142 / 1478 loss=4.876, ppl=29.36, wps=22631.8, ups=0.35, wpb=65536, bsz=128, num_updates=16400, lr=0.000246932, gnorm=0.484, train_wall=228, gb_free=7.9, wall=46824
2022-05-07 05:35:14 | INFO | train_inner | epoch 012:    242 / 1478 loss=4.868, ppl=29.19, wps=20869.7, ups=0.32, wpb=65536, bsz=128, num_updates=16500, lr=0.000246183, gnorm=0.489, train_wall=242, gb_free=7.9, wall=47138
2022-05-07 05:39:55 | INFO | train_inner | epoch 012:    342 / 1478 loss=4.863, ppl=29.1, wps=23369.5, ups=0.36, wpb=65536, bsz=128, num_updates=16600, lr=0.00024544, gnorm=0.486, train_wall=228, gb_free=7.9, wall=47419
2022-05-07 05:44:48 | INFO | train_inner | epoch 012:    442 / 1478 loss=4.872, ppl=29.28, wps=22395.6, ups=0.34, wpb=65536, bsz=128, num_updates=16700, lr=0.000244704, gnorm=0.477, train_wall=230, gb_free=7.9, wall=47711
2022-05-07 05:49:25 | INFO | train_inner | epoch 012:    542 / 1478 loss=4.889, ppl=29.64, wps=23630.9, ups=0.36, wpb=65536, bsz=128, num_updates=16800, lr=0.000243975, gnorm=0.487, train_wall=230, gb_free=7.9, wall=47989
2022-05-07 05:54:52 | INFO | train_inner | epoch 012:    642 / 1478 loss=4.887, ppl=29.6, wps=20059.7, ups=0.31, wpb=65536, bsz=128, num_updates=16900, lr=0.000243252, gnorm=0.485, train_wall=234, gb_free=7.9, wall=48315
2022-05-07 05:59:24 | INFO | train_inner | epoch 012:    742 / 1478 loss=4.894, ppl=29.73, wps=24031.8, ups=0.37, wpb=65536, bsz=128, num_updates=17000, lr=0.000242536, gnorm=0.479, train_wall=229, gb_free=7.9, wall=48588
2022-05-07 06:04:14 | INFO | train_inner | epoch 012:    842 / 1478 loss=4.899, ppl=29.84, wps=22643.8, ups=0.35, wpb=65536, bsz=128, num_updates=17100, lr=0.000241825, gnorm=0.484, train_wall=239, gb_free=7.9, wall=48877
2022-05-07 06:09:20 | INFO | train_inner | epoch 012:    942 / 1478 loss=4.875, ppl=29.34, wps=21392.4, ups=0.33, wpb=65536, bsz=128, num_updates=17200, lr=0.000241121, gnorm=0.488, train_wall=227, gb_free=7.9, wall=49184
2022-05-07 06:14:12 | INFO | train_inner | epoch 012:   1042 / 1478 loss=4.894, ppl=29.73, wps=22459, ups=0.34, wpb=65536, bsz=128, num_updates=17300, lr=0.000240424, gnorm=0.483, train_wall=238, gb_free=7.9, wall=49476
2022-05-07 06:18:42 | INFO | train_inner | epoch 012:   1142 / 1478 loss=4.903, ppl=29.91, wps=24252.6, ups=0.37, wpb=65536, bsz=128, num_updates=17400, lr=0.000239732, gnorm=0.49, train_wall=228, gb_free=7.9, wall=49746
2022-05-07 06:23:09 | INFO | train_inner | epoch 012:   1242 / 1478 loss=4.89, ppl=29.66, wps=24525.4, ups=0.37, wpb=65536, bsz=128, num_updates=17500, lr=0.000239046, gnorm=0.487, train_wall=229, gb_free=7.9, wall=50013
2022-05-07 06:27:35 | INFO | train_inner | epoch 012:   1342 / 1478 loss=4.902, ppl=29.89, wps=24669.8, ups=0.38, wpb=65531, bsz=128, num_updates=17600, lr=0.000238366, gnorm=0.483, train_wall=241, gb_free=7.9, wall=50279
2022-05-07 06:31:44 | INFO | train_inner | epoch 012:   1442 / 1478 loss=4.897, ppl=29.8, wps=26267, ups=0.4, wpb=65525.8, bsz=128, num_updates=17700, lr=0.000237691, gnorm=0.481, train_wall=234, gb_free=7.9, wall=50528
2022-05-07 06:33:13 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-05-07 06:34:40 | INFO | valid | epoch 012 | valid on 'valid' subset | loss 4.908 | ppl 30.01 | wps 63399 | wpb 2047.4 | bsz 4 | num_updates 17736 | best_loss 4.908
2022-05-07 06:34:40 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 12 @ 17736 updates
2022-05-07 06:34:40 | INFO | fairseq.trainer | Saving checkpoint to /cluster/work/cotterell/liam/master-thesis/checkpoints/language_model/standard/checkpoint_best_standard_32_5.0000E-04_full.pt
2022-05-07 06:34:44 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/work/cotterell/liam/master-thesis/checkpoints/language_model/standard/checkpoint_best_standard_32_5.0000E-04_full.pt
2022-05-07 06:34:44 | INFO | fairseq.checkpoint_utils | Saved checkpoint checkpoints/language_model/standard/checkpoint_best_standard_32_5.0000E-04_full.pt (epoch 12 @ 17736 updates, score 4.908) (writing took 3.982753269840032 seconds)
2022-05-07 06:34:44 | INFO | fairseq_cli.train | end of epoch 12 (average epoch stats below)
2022-05-07 06:34:44 | INFO | train | epoch 012 | loss 4.886 | ppl 29.58 | wps 22510 | ups 0.34 | wpb 65507.3 | bsz 127.9 | num_updates 17736 | lr 0.00023745 | gnorm 0.485 | train_wall 3436 | gb_free 7.9 | wall 50707
2022-05-07 06:34:46 | INFO | fairseq.data.iterators | grouped total_num_itrs = 1478
2022-05-07 06:34:46 | INFO | fairseq.trainer | begin training epoch 13
2022-05-07 06:34:46 | INFO | fairseq_cli.train | Start iterating over samples
2022-05-07 06:37:22 | INFO | train_inner | epoch 013:     64 / 1478 loss=4.861, ppl=29.06, wps=19295.8, ups=0.3, wpb=65126.4, bsz=127.2, num_updates=17800, lr=0.000237023, gnorm=0.488, train_wall=232, gb_free=7.9, wall=50866
2022-05-07 06:41:46 | INFO | train_inner | epoch 013:    164 / 1478 loss=4.836, ppl=28.55, wps=24856.8, ups=0.38, wpb=65536, bsz=128, num_updates=17900, lr=0.00023636, gnorm=0.489, train_wall=234, gb_free=7.9, wall=51129
2022-05-07 06:46:39 | INFO | train_inner | epoch 013:    264 / 1478 loss=4.839, ppl=28.62, wps=22357.4, ups=0.34, wpb=65536, bsz=128, num_updates=18000, lr=0.000235702, gnorm=0.494, train_wall=237, gb_free=7.9, wall=51422
2022-05-07 06:51:24 | INFO | train_inner | epoch 013:    364 / 1478 loss=4.842, ppl=28.69, wps=22967.1, ups=0.35, wpb=65536, bsz=128, num_updates=18100, lr=0.00023505, gnorm=0.49, train_wall=227, gb_free=7.9, wall=51708
2022-05-07 06:56:10 | INFO | train_inner | epoch 013:    464 / 1478 loss=4.856, ppl=28.97, wps=22922.8, ups=0.35, wpb=65536, bsz=128, num_updates=18200, lr=0.000234404, gnorm=0.486, train_wall=240, gb_free=7.9, wall=51994
2022-05-07 07:00:39 | INFO | train_inner | epoch 013:    564 / 1478 loss=4.857, ppl=28.99, wps=24362.9, ups=0.37, wpb=65525.8, bsz=128, num_updates=18300, lr=0.000233762, gnorm=0.489, train_wall=228, gb_free=7.9, wall=52263
2022-05-07 07:05:00 | INFO | train_inner | epoch 013:    664 / 1478 loss=4.864, ppl=29.12, wps=25099.7, ups=0.38, wpb=65531, bsz=128, num_updates=18400, lr=0.000233126, gnorm=0.491, train_wall=233, gb_free=7.9, wall=52524
2022-05-07 07:09:14 | INFO | train_inner | epoch 013:    764 / 1478 loss=4.863, ppl=29.09, wps=25850.9, ups=0.39, wpb=65536, bsz=128, num_updates=18500, lr=0.000232495, gnorm=0.491, train_wall=230, gb_free=7.9, wall=52777
2022-05-07 07:13:21 | INFO | train_inner | epoch 013:    864 / 1478 loss=4.87, ppl=29.25, wps=26534.6, ups=0.4, wpb=65536, bsz=128, num_updates=18600, lr=0.000231869, gnorm=0.494, train_wall=234, gb_free=7.9, wall=53024
2022-05-07 07:17:43 | INFO | train_inner | epoch 013:    964 / 1478 loss=4.863, ppl=29.1, wps=24988.5, ups=0.38, wpb=65536, bsz=128, num_updates=18700, lr=0.000231249, gnorm=0.495, train_wall=235, gb_free=7.9, wall=53286
2022-05-07 07:22:13 | INFO | train_inner | epoch 013:   1064 / 1478 loss=4.866, ppl=29.15, wps=24227.5, ups=0.37, wpb=65536, bsz=128, num_updates=18800, lr=0.000230633, gnorm=0.488, train_wall=235, gb_free=7.9, wall=53557
2022-05-07 07:26:25 | INFO | train_inner | epoch 013:   1164 / 1478 loss=4.877, ppl=29.38, wps=26040.8, ups=0.4, wpb=65536, bsz=128, num_updates=18900, lr=0.000230022, gnorm=0.491, train_wall=235, gb_free=7.9, wall=53809
2022-05-07 07:30:38 | INFO | train_inner | epoch 013:   1264 / 1478 loss=4.866, ppl=29.16, wps=25900.3, ups=0.4, wpb=65536, bsz=128, num_updates=19000, lr=0.000229416, gnorm=0.49, train_wall=239, gb_free=7.9, wall=54062
2022-05-07 07:34:45 | INFO | train_inner | epoch 013:   1364 / 1478 loss=4.867, ppl=29.18, wps=26531.3, ups=0.4, wpb=65536, bsz=128, num_updates=19100, lr=0.000228814, gnorm=0.488, train_wall=234, gb_free=7.9, wall=54309
2022-05-07 07:39:04 | INFO | train_inner | epoch 013:   1464 / 1478 loss=4.863, ppl=29.1, wps=25327.2, ups=0.39, wpb=65536, bsz=128, num_updates=19200, lr=0.000228218, gnorm=0.49, train_wall=236, gb_free=7.9, wall=54567
2022-05-07 07:39:40 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-05-07 07:41:04 | INFO | valid | epoch 013 | valid on 'valid' subset | loss 4.892 | ppl 29.69 | wps 66930.6 | wpb 2047.4 | bsz 4 | num_updates 19214 | best_loss 4.892
2022-05-07 07:41:04 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 13 @ 19214 updates
2022-05-07 07:41:04 | INFO | fairseq.trainer | Saving checkpoint to /cluster/work/cotterell/liam/master-thesis/checkpoints/language_model/standard/checkpoint_best_standard_32_5.0000E-04_full.pt
2022-05-07 07:41:06 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/work/cotterell/liam/master-thesis/checkpoints/language_model/standard/checkpoint_best_standard_32_5.0000E-04_full.pt
2022-05-07 07:41:06 | INFO | fairseq.checkpoint_utils | Saved checkpoint checkpoints/language_model/standard/checkpoint_best_standard_32_5.0000E-04_full.pt (epoch 13 @ 19214 updates, score 4.892) (writing took 2.2681567282415926 seconds)
2022-05-07 07:41:06 | INFO | fairseq_cli.train | end of epoch 13 (average epoch stats below)
2022-05-07 07:41:06 | INFO | train | epoch 013 | loss 4.858 | ppl 29 | wps 24311.6 | ups 0.37 | wpb 65507.3 | bsz 127.9 | num_updates 19214 | lr 0.000228135 | gnorm 0.491 | train_wall 3457 | gb_free 7.9 | wall 54690
2022-05-07 07:41:06 | INFO | fairseq.data.iterators | grouped total_num_itrs = 1478
2022-05-07 07:41:06 | INFO | fairseq.trainer | begin training epoch 14
2022-05-07 07:41:06 | INFO | fairseq_cli.train | Start iterating over samples
2022-05-07 07:45:34 | INFO | train_inner | epoch 014:     86 / 1478 loss=4.824, ppl=28.33, wps=16675.6, ups=0.26, wpb=65126.4, bsz=127.2, num_updates=19300, lr=0.000227626, gnorm=0.494, train_wall=231, gb_free=7.9, wall=54958
2022-05-07 07:50:16 | INFO | train_inner | epoch 014:    186 / 1478 loss=4.809, ppl=28.04, wps=23291.8, ups=0.36, wpb=65536, bsz=128, num_updates=19400, lr=0.000227038, gnorm=0.492, train_wall=228, gb_free=7.9, wall=55239
2022-05-07 07:54:48 | INFO | train_inner | epoch 014:    286 / 1478 loss=4.819, ppl=28.23, wps=24101.9, ups=0.37, wpb=65536, bsz=128, num_updates=19500, lr=0.000226455, gnorm=0.491, train_wall=233, gb_free=7.9, wall=55511
2022-05-07 07:59:27 | INFO | train_inner | epoch 014:    386 / 1478 loss=4.834, ppl=28.53, wps=23447.6, ups=0.36, wpb=65536, bsz=128, num_updates=19600, lr=0.000225877, gnorm=0.496, train_wall=231, gb_free=7.9, wall=55791
2022-05-07 08:04:16 | INFO | train_inner | epoch 014:    486 / 1478 loss=4.835, ppl=28.53, wps=22684.8, ups=0.35, wpb=65536, bsz=128, num_updates=19700, lr=0.000225303, gnorm=0.492, train_wall=234, gb_free=7.9, wall=56080
2022-05-07 08:09:03 | INFO | train_inner | epoch 014:    586 / 1478 loss=4.837, ppl=28.58, wps=22853, ups=0.35, wpb=65536, bsz=128, num_updates=19800, lr=0.000224733, gnorm=0.49, train_wall=229, gb_free=7.9, wall=56366
2022-05-07 08:13:51 | INFO | train_inner | epoch 014:    686 / 1478 loss=4.833, ppl=28.51, wps=22734.3, ups=0.35, wpb=65536, bsz=128, num_updates=19900, lr=0.000224168, gnorm=0.49, train_wall=235, gb_free=7.9, wall=56655
2022-05-07 08:18:39 | INFO | train_inner | epoch 014:    786 / 1478 loss=4.842, ppl=28.69, wps=22798.4, ups=0.35, wpb=65536, bsz=128, num_updates=20000, lr=0.000223607, gnorm=0.492, train_wall=229, gb_free=7.9, wall=56942
2022-05-07 08:23:03 | INFO | train_inner | epoch 014:    886 / 1478 loss=4.841, ppl=28.67, wps=24818.5, ups=0.38, wpb=65525.8, bsz=128, num_updates=20100, lr=0.00022305, gnorm=0.504, train_wall=235, gb_free=7.9, wall=57206
2022-05-07 08:27:21 | INFO | train_inner | epoch 014:    986 / 1478 loss=4.831, ppl=28.47, wps=25402.6, ups=0.39, wpb=65536, bsz=128, num_updates=20200, lr=0.000222497, gnorm=0.493, train_wall=233, gb_free=7.9, wall=57464
2022-05-07 08:31:36 | INFO | train_inner | epoch 014:   1086 / 1478 loss=4.838, ppl=28.6, wps=25668.9, ups=0.39, wpb=65536, bsz=128, num_updates=20300, lr=0.000221948, gnorm=0.493, train_wall=236, gb_free=7.9, wall=57720
2022-05-07 08:36:09 | INFO | train_inner | epoch 014:   1186 / 1478 loss=4.84, ppl=28.64, wps=24022.1, ups=0.37, wpb=65536, bsz=128, num_updates=20400, lr=0.000221404, gnorm=0.485, train_wall=242, gb_free=7.9, wall=57992
2022-05-07 08:40:38 | INFO | train_inner | epoch 014:   1286 / 1478 loss=4.838, ppl=28.6, wps=24313.5, ups=0.37, wpb=65536, bsz=128, num_updates=20500, lr=0.000220863, gnorm=0.496, train_wall=230, gb_free=7.9, wall=58262
2022-05-07 08:45:38 | INFO | train_inner | epoch 014:   1386 / 1478 loss=4.837, ppl=28.58, wps=21854.4, ups=0.33, wpb=65531, bsz=128, num_updates=20600, lr=0.000220326, gnorm=0.492, train_wall=236, gb_free=7.9, wall=58562
2022-05-07 08:49:45 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-05-07 08:51:07 | INFO | valid | epoch 014 | valid on 'valid' subset | loss 4.874 | ppl 29.33 | wps 67308 | wpb 2047.4 | bsz 4 | num_updates 20692 | best_loss 4.874
2022-05-07 08:51:07 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 14 @ 20692 updates
2022-05-07 08:51:07 | INFO | fairseq.trainer | Saving checkpoint to /cluster/work/cotterell/liam/master-thesis/checkpoints/language_model/standard/checkpoint_best_standard_32_5.0000E-04_full.pt
2022-05-07 08:51:10 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/work/cotterell/liam/master-thesis/checkpoints/language_model/standard/checkpoint_best_standard_32_5.0000E-04_full.pt
2022-05-07 08:51:10 | INFO | fairseq.checkpoint_utils | Saved checkpoint checkpoints/language_model/standard/checkpoint_best_standard_32_5.0000E-04_full.pt (epoch 14 @ 20692 updates, score 4.874) (writing took 2.864653184078634 seconds)
2022-05-07 08:51:10 | INFO | fairseq_cli.train | end of epoch 14 (average epoch stats below)
2022-05-07 08:51:10 | INFO | train | epoch 014 | loss 4.834 | ppl 28.51 | wps 23031.5 | ups 0.35 | wpb 65507.3 | bsz 127.9 | num_updates 20692 | lr 0.000219836 | gnorm 0.493 | train_wall 3438 | gb_free 7.9 | wall 58894
2022-05-07 08:51:10 | INFO | fairseq.data.iterators | grouped total_num_itrs = 1478
2022-05-07 08:51:10 | INFO | fairseq.trainer | begin training epoch 15
2022-05-07 08:51:10 | INFO | fairseq_cli.train | Start iterating over samples
2022-05-07 08:51:31 | INFO | train_inner | epoch 015:      8 / 1478 loss=4.842, ppl=28.69, wps=18432.8, ups=0.28, wpb=65126.4, bsz=127.2, num_updates=20700, lr=0.000219793, gnorm=0.492, train_wall=226, gb_free=7.9, wall=58915
2022-05-07 08:56:07 | INFO | train_inner | epoch 015:    108 / 1478 loss=4.775, ppl=27.37, wps=23818.9, ups=0.36, wpb=65536, bsz=128, num_updates=20800, lr=0.000219265, gnorm=0.495, train_wall=230, gb_free=7.9, wall=59190
2022-05-07 09:00:32 | INFO | train_inner | epoch 015:    208 / 1478 loss=4.786, ppl=27.59, wps=24642.8, ups=0.38, wpb=65525.8, bsz=128, num_updates=20900, lr=0.000218739, gnorm=0.496, train_wall=228, gb_free=7.9, wall=59456
2022-05-07 09:05:09 | INFO | train_inner | epoch 015:    308 / 1478 loss=4.786, ppl=27.58, wps=23664.2, ups=0.36, wpb=65536, bsz=128, num_updates=21000, lr=0.000218218, gnorm=0.497, train_wall=236, gb_free=7.9, wall=59733
2022-05-07 09:09:44 | INFO | train_inner | epoch 015:    408 / 1478 loss=4.795, ppl=27.76, wps=23865, ups=0.36, wpb=65536, bsz=128, num_updates=21100, lr=0.0002177, gnorm=0.496, train_wall=230, gb_free=7.9, wall=60008
2022-05-07 09:14:10 | INFO | train_inner | epoch 015:    508 / 1478 loss=4.807, ppl=27.99, wps=24623.3, ups=0.38, wpb=65536, bsz=128, num_updates=21200, lr=0.000217186, gnorm=0.498, train_wall=236, gb_free=7.9, wall=60274
2022-05-07 09:18:25 | INFO | train_inner | epoch 015:    608 / 1478 loss=4.812, ppl=28.08, wps=25678.3, ups=0.39, wpb=65536, bsz=128, num_updates=21300, lr=0.000216676, gnorm=0.495, train_wall=233, gb_free=7.9, wall=60529
2022-05-07 09:22:35 | INFO | train_inner | epoch 015:    708 / 1478 loss=4.823, ppl=28.3, wps=26257.9, ups=0.4, wpb=65531, bsz=128, num_updates=21400, lr=0.000216169, gnorm=0.499, train_wall=234, gb_free=7.9, wall=60779
2022-05-07 09:27:02 | INFO | train_inner | epoch 015:    808 / 1478 loss=4.81, ppl=28.05, wps=24586.1, ups=0.38, wpb=65536, bsz=128, num_updates=21500, lr=0.000215666, gnorm=0.494, train_wall=232, gb_free=7.9, wall=61045
2022-05-07 09:32:05 | INFO | train_inner | epoch 015:    908 / 1478 loss=4.817, ppl=28.19, wps=21629, ups=0.33, wpb=65536, bsz=128, num_updates=21600, lr=0.000215166, gnorm=0.495, train_wall=233, gb_free=7.9, wall=61348
2022-05-07 09:36:41 | INFO | train_inner | epoch 015:   1008 / 1478 loss=4.822, ppl=28.28, wps=23711.5, ups=0.36, wpb=65536, bsz=128, num_updates=21700, lr=0.000214669, gnorm=0.504, train_wall=229, gb_free=7.9, wall=61625
2022-05-07 09:41:40 | INFO | train_inner | epoch 015:   1108 / 1478 loss=4.83, ppl=28.44, wps=21884.4, ups=0.33, wpb=65536, bsz=128, num_updates=21800, lr=0.000214176, gnorm=0.5, train_wall=236, gb_free=7.9, wall=61924
2022-05-07 09:46:04 | INFO | train_inner | epoch 015:   1208 / 1478 loss=4.827, ppl=28.39, wps=24883.7, ups=0.38, wpb=65536, bsz=128, num_updates=21900, lr=0.000213687, gnorm=0.501, train_wall=231, gb_free=7.9, wall=62187
2022-05-07 09:50:21 | INFO | train_inner | epoch 015:   1308 / 1478 loss=4.822, ppl=28.28, wps=25467.3, ups=0.39, wpb=65536, bsz=128, num_updates=22000, lr=0.000213201, gnorm=0.49, train_wall=233, gb_free=7.9, wall=62445
2022-05-07 09:54:41 | INFO | train_inner | epoch 015:   1408 / 1478 loss=4.836, ppl=28.57, wps=25236.2, ups=0.39, wpb=65536, bsz=128, num_updates=22100, lr=0.000212718, gnorm=0.498, train_wall=235, gb_free=7.9, wall=62704
2022-05-07 09:58:07 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-05-07 09:59:27 | INFO | valid | epoch 015 | valid on 'valid' subset | loss 4.863 | ppl 29.1 | wps 68173.5 | wpb 2047.4 | bsz 4 | num_updates 22170 | best_loss 4.863
2022-05-07 09:59:27 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 15 @ 22170 updates
2022-05-07 09:59:27 | INFO | fairseq.trainer | Saving checkpoint to /cluster/work/cotterell/liam/master-thesis/checkpoints/language_model/standard/checkpoint_best_standard_32_5.0000E-04_full.pt
2022-05-07 09:59:30 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/work/cotterell/liam/master-thesis/checkpoints/language_model/standard/checkpoint_best_standard_32_5.0000E-04_full.pt
2022-05-07 09:59:30 | INFO | fairseq.checkpoint_utils | Saved checkpoint checkpoints/language_model/standard/checkpoint_best_standard_32_5.0000E-04_full.pt (epoch 15 @ 22170 updates, score 4.863) (writing took 2.905640622600913 seconds)
2022-05-07 09:59:30 | INFO | fairseq_cli.train | end of epoch 15 (average epoch stats below)
2022-05-07 09:59:30 | INFO | train | epoch 015 | loss 4.812 | ppl 28.08 | wps 23613.5 | ups 0.36 | wpb 65507.3 | bsz 127.9 | num_updates 22170 | lr 0.000212382 | gnorm 0.497 | train_wall 3445 | gb_free 7.9 | wall 62994
2022-05-07 09:59:30 | INFO | fairseq.data.iterators | grouped total_num_itrs = 1478
2022-05-07 09:59:30 | INFO | fairseq.trainer | begin training epoch 16
2022-05-07 09:59:30 | INFO | fairseq_cli.train | Start iterating over samples
2022-05-07 10:00:43 | INFO | train_inner | epoch 016:     30 / 1478 loss=4.817, ppl=28.18, wps=17960.8, ups=0.28, wpb=65126.4, bsz=127.2, num_updates=22200, lr=0.000212238, gnorm=0.505, train_wall=239, gb_free=7.9, wall=63067
2022-05-07 10:04:49 | INFO | train_inner | epoch 016:    130 / 1478 loss=4.757, ppl=27.04, wps=26697.2, ups=0.41, wpb=65536, bsz=128, num_updates=22300, lr=0.000211762, gnorm=0.502, train_wall=234, gb_free=7.9, wall=63313
2022-05-07 10:08:57 | INFO | train_inner | epoch 016:    230 / 1478 loss=4.778, ppl=27.44, wps=26464.2, ups=0.4, wpb=65531, bsz=128, num_updates=22400, lr=0.000211289, gnorm=0.508, train_wall=236, gb_free=7.9, wall=63560
2022-05-07 10:13:24 | INFO | train_inner | epoch 016:    330 / 1478 loss=4.777, ppl=27.41, wps=24462, ups=0.37, wpb=65536, bsz=128, num_updates=22500, lr=0.000210819, gnorm=0.505, train_wall=236, gb_free=7.9, wall=63828
2022-05-07 10:17:31 | INFO | train_inner | epoch 016:    430 / 1478 loss=4.795, ppl=27.76, wps=26573, ups=0.41, wpb=65536, bsz=128, num_updates=22600, lr=0.000210352, gnorm=0.496, train_wall=235, gb_free=7.9, wall=64075
2022-05-07 10:21:43 | INFO | train_inner | epoch 016:    530 / 1478 loss=4.791, ppl=27.68, wps=26044.2, ups=0.4, wpb=65536, bsz=128, num_updates=22700, lr=0.000209888, gnorm=0.501, train_wall=240, gb_free=7.9, wall=64326
2022-05-07 10:25:49 | INFO | train_inner | epoch 016:    630 / 1478 loss=4.792, ppl=27.7, wps=26562.4, ups=0.41, wpb=65536, bsz=128, num_updates=22800, lr=0.000209427, gnorm=0.503, train_wall=238, gb_free=7.9, wall=64573
2022-05-07 10:30:15 | INFO | train_inner | epoch 016:    730 / 1478 loss=4.798, ppl=27.81, wps=24696, ups=0.38, wpb=65525.8, bsz=128, num_updates=22900, lr=0.000208969, gnorm=0.504, train_wall=243, gb_free=7.9, wall=64838
2022-05-07 10:34:18 | INFO | train_inner | epoch 016:    830 / 1478 loss=4.791, ppl=27.68, wps=26894.5, ups=0.41, wpb=65536, bsz=128, num_updates=23000, lr=0.000208514, gnorm=0.5, train_wall=233, gb_free=7.9, wall=65082
2022-05-07 10:38:27 | INFO | train_inner | epoch 016:    930 / 1478 loss=4.786, ppl=27.59, wps=26346.1, ups=0.4, wpb=65536, bsz=128, num_updates=23100, lr=0.000208063, gnorm=0.504, train_wall=240, gb_free=7.9, wall=65331
2022-05-07 10:42:37 | INFO | train_inner | epoch 016:   1030 / 1478 loss=4.804, ppl=27.93, wps=26208, ups=0.4, wpb=65536, bsz=128, num_updates=23200, lr=0.000207614, gnorm=0.503, train_wall=240, gb_free=7.9, wall=65581
2022-05-07 10:46:41 | INFO | train_inner | epoch 016:   1130 / 1478 loss=4.812, ppl=28.09, wps=26927, ups=0.41, wpb=65536, bsz=128, num_updates=23300, lr=0.000207168, gnorm=0.5, train_wall=235, gb_free=7.9, wall=65824
2022-05-07 10:51:34 | INFO | train_inner | epoch 016:   1230 / 1478 loss=4.805, ppl=27.95, wps=22363.2, ups=0.34, wpb=65536, bsz=128, num_updates=23400, lr=0.000206725, gnorm=0.502, train_wall=239, gb_free=7.9, wall=66117
2022-05-07 10:56:49 | INFO | train_inner | epoch 016:   1330 / 1478 loss=4.81, ppl=28.05, wps=20776.9, ups=0.32, wpb=65536, bsz=128, num_updates=23500, lr=0.000206284, gnorm=0.505, train_wall=227, gb_free=7.9, wall=66433
2022-05-07 11:01:37 | INFO | train_inner | epoch 016:   1430 / 1478 loss=4.798, ppl=27.82, wps=22798.1, ups=0.35, wpb=65536, bsz=128, num_updates=23600, lr=0.000205847, gnorm=0.497, train_wall=232, gb_free=7.9, wall=66720
2022-05-07 11:03:43 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-05-07 11:04:59 | INFO | valid | epoch 016 | valid on 'valid' subset | loss 4.849 | ppl 28.82 | wps 71540.6 | wpb 2047.4 | bsz 4 | num_updates 23648 | best_loss 4.849
2022-05-07 11:04:59 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 16 @ 23648 updates
2022-05-07 11:04:59 | INFO | fairseq.trainer | Saving checkpoint to /cluster/work/cotterell/liam/master-thesis/checkpoints/language_model/standard/checkpoint_best_standard_32_5.0000E-04_full.pt
2022-05-07 11:05:02 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/work/cotterell/liam/master-thesis/checkpoints/language_model/standard/checkpoint_best_standard_32_5.0000E-04_full.pt
2022-05-07 11:05:02 | INFO | fairseq.checkpoint_utils | Saved checkpoint checkpoints/language_model/standard/checkpoint_best_standard_32_5.0000E-04_full.pt (epoch 16 @ 23648 updates, score 4.849) (writing took 3.178193928208202 seconds)
2022-05-07 11:05:02 | INFO | fairseq_cli.train | end of epoch 16 (average epoch stats below)
2022-05-07 11:05:02 | INFO | train | epoch 016 | loss 4.792 | ppl 27.71 | wps 24624.4 | ups 0.38 | wpb 65507.3 | bsz 127.9 | num_updates 23648 | lr 0.000205638 | gnorm 0.503 | train_wall 3486 | gb_free 7.9 | wall 66926
2022-05-07 11:05:02 | INFO | fairseq.data.iterators | grouped total_num_itrs = 1478
2022-05-07 11:05:02 | INFO | fairseq.trainer | begin training epoch 17
2022-05-07 11:05:02 | INFO | fairseq_cli.train | Start iterating over samples
2022-05-07 11:07:23 | INFO | train_inner | epoch 017:     52 / 1478 loss=4.773, ppl=27.35, wps=18778.5, ups=0.29, wpb=65126.4, bsz=127.2, num_updates=23700, lr=0.000205412, gnorm=0.507, train_wall=229, gb_free=7.9, wall=67067
2022-05-07 11:11:40 | INFO | train_inner | epoch 017:    152 / 1478 loss=4.753, ppl=26.97, wps=25497.6, ups=0.39, wpb=65536, bsz=128, num_updates=23800, lr=0.00020498, gnorm=0.503, train_wall=234, gb_free=7.9, wall=67324
2022-05-07 11:16:43 | INFO | train_inner | epoch 017:    252 / 1478 loss=4.752, ppl=26.95, wps=21649.3, ups=0.33, wpb=65536, bsz=128, num_updates=23900, lr=0.000204551, gnorm=0.509, train_wall=236, gb_free=7.9, wall=67627
2022-05-07 11:21:27 | INFO | train_inner | epoch 017:    352 / 1478 loss=4.755, ppl=27.01, wps=23094.6, ups=0.35, wpb=65536, bsz=128, num_updates=24000, lr=0.000204124, gnorm=0.506, train_wall=230, gb_free=7.9, wall=67911
2022-05-07 11:26:00 | INFO | train_inner | epoch 017:    452 / 1478 loss=4.769, ppl=27.26, wps=24045.6, ups=0.37, wpb=65536, bsz=128, num_updates=24100, lr=0.0002037, gnorm=0.507, train_wall=231, gb_free=7.9, wall=68183
2022-05-07 11:30:24 | INFO | train_inner | epoch 017:    552 / 1478 loss=4.764, ppl=27.16, wps=24793.8, ups=0.38, wpb=65536, bsz=128, num_updates=24200, lr=0.000203279, gnorm=0.506, train_wall=229, gb_free=7.9, wall=68448
2022-05-07 11:35:09 | INFO | train_inner | epoch 017:    652 / 1478 loss=4.764, ppl=27.18, wps=23001.7, ups=0.35, wpb=65536, bsz=128, num_updates=24300, lr=0.00020286, gnorm=0.502, train_wall=236, gb_free=7.9, wall=68732
2022-05-07 11:39:24 | INFO | train_inner | epoch 017:    752 / 1478 loss=4.782, ppl=27.51, wps=25686.3, ups=0.39, wpb=65536, bsz=128, num_updates=24400, lr=0.000202444, gnorm=0.508, train_wall=233, gb_free=7.9, wall=68988
2022-05-07 11:43:46 | INFO | train_inner | epoch 017:    852 / 1478 loss=4.774, ppl=27.36, wps=24998.2, ups=0.38, wpb=65536, bsz=128, num_updates=24500, lr=0.000202031, gnorm=0.504, train_wall=242, gb_free=7.9, wall=69250
2022-05-07 11:48:00 | INFO | train_inner | epoch 017:    952 / 1478 loss=4.785, ppl=27.58, wps=25819.9, ups=0.39, wpb=65536, bsz=128, num_updates=24600, lr=0.000201619, gnorm=0.508, train_wall=235, gb_free=7.9, wall=69504
2022-05-07 11:52:20 | INFO | train_inner | epoch 017:   1052 / 1478 loss=4.784, ppl=27.55, wps=25221.7, ups=0.38, wpb=65536, bsz=128, num_updates=24700, lr=0.000201211, gnorm=0.504, train_wall=233, gb_free=7.9, wall=69763
2022-05-07 11:56:33 | INFO | train_inner | epoch 017:   1152 / 1478 loss=4.792, ppl=27.71, wps=25846.5, ups=0.39, wpb=65525.8, bsz=128, num_updates=24800, lr=0.000200805, gnorm=0.506, train_wall=240, gb_free=7.9, wall=70017
2022-05-07 12:00:39 | INFO | train_inner | epoch 017:   1252 / 1478 loss=4.79, ppl=27.66, wps=26685.1, ups=0.41, wpb=65536, bsz=128, num_updates=24900, lr=0.000200401, gnorm=0.51, train_wall=236, gb_free=7.9, wall=70263
2022-05-07 12:04:51 | INFO | train_inner | epoch 017:   1352 / 1478 loss=4.795, ppl=27.75, wps=25984.9, ups=0.4, wpb=65536, bsz=128, num_updates=25000, lr=0.0002, gnorm=0.502, train_wall=241, gb_free=7.9, wall=70515
2022-05-07 12:10:22 | INFO | train_inner | epoch 017:   1452 / 1478 loss=4.791, ppl=27.67, wps=19777.6, ups=0.3, wpb=65531, bsz=128, num_updates=25100, lr=0.000199601, gnorm=0.504, train_wall=229, gb_free=7.9, wall=70846
2022-05-07 12:11:39 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-05-07 12:12:58 | INFO | valid | epoch 017 | valid on 'valid' subset | loss 4.841 | ppl 28.65 | wps 68643.3 | wpb 2047.4 | bsz 4 | num_updates 25126 | best_loss 4.841
2022-05-07 12:12:58 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 17 @ 25126 updates
2022-05-07 12:12:58 | INFO | fairseq.trainer | Saving checkpoint to /cluster/work/cotterell/liam/master-thesis/checkpoints/language_model/standard/checkpoint_best_standard_32_5.0000E-04_full.pt
2022-05-07 12:13:01 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/work/cotterell/liam/master-thesis/checkpoints/language_model/standard/checkpoint_best_standard_32_5.0000E-04_full.pt
2022-05-07 12:13:01 | INFO | fairseq.checkpoint_utils | Saved checkpoint checkpoints/language_model/standard/checkpoint_best_standard_32_5.0000E-04_full.pt (epoch 17 @ 25126 updates, score 4.841) (writing took 2.969059166032821 seconds)
2022-05-07 12:13:01 | INFO | fairseq_cli.train | end of epoch 17 (average epoch stats below)
2022-05-07 12:13:01 | INFO | train | epoch 017 | loss 4.775 | ppl 27.37 | wps 23738 | ups 0.36 | wpb 65507.3 | bsz 127.9 | num_updates 25126 | lr 0.000199498 | gnorm 0.506 | train_wall 3463 | gb_free 7.9 | wall 71004
2022-05-07 12:13:01 | INFO | fairseq.data.iterators | grouped total_num_itrs = 1478
2022-05-07 12:13:01 | INFO | fairseq.trainer | begin training epoch 18
2022-05-07 12:13:01 | INFO | fairseq_cli.train | Start iterating over samples
2022-05-07 12:16:30 | INFO | train_inner | epoch 018:     74 / 1478 loss=4.741, ppl=26.74, wps=17719.7, ups=0.27, wpb=65126.4, bsz=127.2, num_updates=25200, lr=0.000199205, gnorm=0.51, train_wall=228, gb_free=7.9, wall=71214
2022-05-07 12:21:02 | INFO | train_inner | epoch 018:    174 / 1478 loss=4.726, ppl=26.46, wps=24126, ups=0.37, wpb=65536, bsz=128, num_updates=25300, lr=0.000198811, gnorm=0.509, train_wall=232, gb_free=7.9, wall=71485
2022-05-07 12:26:27 | INFO | train_inner | epoch 018:    274 / 1478 loss=4.746, ppl=26.83, wps=20132.9, ups=0.31, wpb=65536, bsz=128, num_updates=25400, lr=0.000198419, gnorm=0.513, train_wall=238, gb_free=7.9, wall=71811
User defined signal 2
Sender: LSF System <lsfadmin@eu-g2-04>
Subject: Job 218695090: <train_lang_standard_32_5.0000E-04_full> in cluster <euler> Exited

Job <train_lang_standard_32_5.0000E-04_full> was submitted from host <eu-login-07> by user <euler_username> in cluster <euler> at Mon May 16 17:44:38 2022
Job was executed on host(s) <4*eu-g2-04>, in queue <gpu.24h>, as user <euler_username> in cluster <euler> at Mon May 16 17:45:13 2022
</cluster/home/euler_username> was used as the home directory.
</cluster/work/cotterell/liam/master-thesis> was used as the working directory.
Started at Mon May 16 17:45:13 2022
Terminated at Tue May 17 13:45:24 2022
Results reported at Tue May 17 13:45:24 2022

Your job looked like:

------------------------------------------------------------
# LSBATCH: User input
fairseq-train data/xsum-lang-full --save-dir checkpoints/language_model/standard --update-freq 32 --lr 0.0005 --checkpoint-suffix _standard_32_5.0000E-04_full --restore-file checkpoints/language_model/standard/checkpoint_best.pt --task language_modeling --arch transformer_lm --share-decoder-input-output-embed --dropout 0.1 --optimizer adam --adam-betas "(0.9, 0.98)" --weight-decay 0.01 --clip-norm 0.0 --lr-scheduler inverse_sqrt --warmup-updates 4000 --warmup-init-lr 1e-07 --tokens-per-sample 512 --sample-break-mode none --max-tokens 2048 --no-epoch-checkpoints --no-last-checkpoints --patience 5
------------------------------------------------------------

TERM_RUNLIMIT: job killed after reaching LSF run time limit.
Exited with exit code 140.

Resource usage summary:

    CPU time :                                   73798.00 sec.
    Max Memory :                                 4689 MB
    Average Memory :                             2846.70 MB
    Total Requested Memory :                     8192.00 MB
    Delta Memory :                               3503.00 MB
    Max Swap :                                   -
    Max Processes :                              4
    Max Threads :                                14
    Run time :                                   72010 sec.
    Turnaround time :                            72046 sec.

The output (if any) follows:

2022-05-16 17:46:45 | INFO | fairseq.tasks.text_to_speech | Please install tensorboardX: pip install tensorboardX
2022-05-16 17:46:51 | INFO | fairseq_cli.train | {'_name': None, 'common': {'_name': None, 'no_progress_bar': False, 'log_interval': 100, 'log_format': None, 'log_file': None, 'tensorboard_logdir': None, 'wandb_project': None, 'azureml_logging': False, 'seed': 1, 'cpu': False, 'tpu': False, 'bf16': False, 'memory_efficient_bf16': False, 'fp16': False, 'memory_efficient_fp16': False, 'fp16_no_flatten_grads': False, 'fp16_init_scale': 128, 'fp16_scale_window': None, 'fp16_scale_tolerance': 0.0, 'on_cpu_convert_precision': False, 'min_loss_scale': 0.0001, 'threshold_loss_scale': None, 'amp': False, 'amp_batch_retries': 2, 'amp_init_scale': 128, 'amp_scale_window': None, 'user_dir': None, 'empty_cache_freq': 0, 'all_gather_list_size': 16384, 'model_parallel_size': 1, 'quantization_config_path': None, 'profile': False, 'reset_logging': False, 'suppress_crashes': False, 'use_plasma_view': False, 'plasma_path': '/tmp/plasma'}, 'common_eval': {'_name': None, 'path': None, 'post_process': None, 'quiet': False, 'model_overrides': '{}', 'results_path': None, 'print_tokens': False}, 'distributed_training': {'_name': None, 'distributed_world_size': 1, 'distributed_num_procs': 1, 'distributed_rank': 0, 'distributed_backend': 'nccl', 'distributed_init_method': None, 'distributed_port': -1, 'device_id': 0, 'distributed_no_spawn': False, 'ddp_backend': 'pytorch_ddp', 'ddp_comm_hook': 'none', 'bucket_cap_mb': 25, 'fix_batches_to_gpus': False, 'find_unused_parameters': False, 'gradient_as_bucket_view': False, 'fast_stat_sync': False, 'heartbeat_timeout': -1, 'broadcast_buffers': False, 'slowmo_momentum': None, 'slowmo_base_algorithm': 'localsgd', 'localsgd_frequency': 3, 'nprocs_per_node': 1, 'pipeline_model_parallel': False, 'pipeline_balance': None, 'pipeline_devices': None, 'pipeline_chunks': 0, 'pipeline_encoder_balance': None, 'pipeline_encoder_devices': None, 'pipeline_decoder_balance': None, 'pipeline_decoder_devices': None, 'pipeline_checkpoint': 'never', 'zero_sharding': 'none', 'fp16': False, 'memory_efficient_fp16': False, 'tpu': False, 'no_reshard_after_forward': False, 'fp32_reduce_scatter': False, 'cpu_offload': False, 'use_sharded_state': False, 'not_fsdp_flatten_parameters': False}, 'dataset': {'_name': None, 'num_workers': 1, 'skip_invalid_size_inputs_valid_test': False, 'max_tokens': 2048, 'batch_size': None, 'required_batch_size_multiple': 8, 'required_seq_len_multiple': 1, 'dataset_impl': None, 'data_buffer_size': 10, 'train_subset': 'train', 'valid_subset': 'valid', 'combine_valid_subsets': None, 'ignore_unused_valid_subsets': False, 'validate_interval': 1, 'validate_interval_updates': 0, 'validate_after_updates': 0, 'fixed_validation_seed': None, 'disable_validation': False, 'max_tokens_valid': 2048, 'batch_size_valid': None, 'max_valid_steps': None, 'curriculum': 0, 'gen_subset': 'test', 'num_shards': 1, 'shard_id': 0, 'grouped_shuffling': False, 'update_epoch_batch_itr': False, 'update_ordered_indices_seed': False}, 'optimization': {'_name': None, 'max_epoch': 0, 'max_update': 0, 'stop_time_hours': 0.0, 'clip_norm': 0.0, 'sentence_avg': False, 'update_freq': [32], 'lr': [0.0005], 'stop_min_lr': -1.0, 'use_bmuf': False, 'skip_remainder_batch': False}, 'checkpoint': {'_name': None, 'save_dir': 'checkpoints/language_model/standard', 'restore_file': 'checkpoints/language_model/standard/checkpoint_best.pt', 'continue_once': None, 'finetune_from_model': None, 'reset_dataloader': False, 'reset_lr_scheduler': False, 'reset_meters': False, 'reset_optimizer': False, 'optimizer_overrides': '{}', 'save_interval': 1, 'save_interval_updates': 0, 'keep_interval_updates': -1, 'keep_interval_updates_pattern': -1, 'keep_last_epochs': -1, 'keep_best_checkpoints': -1, 'no_save': False, 'no_epoch_checkpoints': True, 'no_last_checkpoints': True, 'no_save_optimizer_state': False, 'best_checkpoint_metric': 'loss', 'maximize_best_checkpoint_metric': False, 'patience': 5, 'checkpoint_suffix': '_standard_32_5.0000E-04_full', 'checkpoint_shard_count': 1, 'load_checkpoint_on_all_dp_ranks': False, 'write_checkpoints_asynchronously': False, 'model_parallel_size': 1}, 'bmuf': {'_name': None, 'block_lr': 1.0, 'block_momentum': 0.875, 'global_sync_iter': 50, 'warmup_iterations': 500, 'use_nbm': False, 'average_sync': False, 'distributed_world_size': 1}, 'generation': {'_name': None, 'beam': 5, 'nbest': 1, 'max_len_a': 0.0, 'max_len_b': 200, 'min_len': 1, 'match_source_len': False, 'unnormalized': False, 'no_early_stop': False, 'no_beamable_mm': False, 'lenpen': 1.0, 'unkpen': 0.0, 'replace_unk': None, 'sacrebleu': False, 'score_reference': False, 'prefix_size': 0, 'no_repeat_ngram_size': 0, 'sampling': False, 'sampling_topk': -1, 'sampling_topp': -1.0, 'constraints': None, 'temperature': 1.0, 'diverse_beam_groups': -1, 'diverse_beam_strength': 0.5, 'diversity_rate': -1.0, 'print_alignment': None, 'print_step': False, 'lm_path': None, 'ent_threshold': 0.0, 'lm_weight': 0.0, 'iter_decode_eos_penalty': 0.0, 'iter_decode_max_iter': 10, 'iter_decode_force_max_iter': False, 'iter_decode_with_beam': 1, 'iter_decode_with_external_reranker': False, 'retain_iter_history': False, 'retain_dropout': False, 'retain_dropout_modules': None, 'decoding_format': None, 'no_seed_provided': False}, 'eval_lm': {'_name': None, 'output_word_probs': False, 'output_word_stats': False, 'context_window': 0, 'softmax_batch': 9223372036854775807}, 'interactive': {'_name': None, 'buffer_size': 0, 'input': '-'}, 'model': {'_name': 'transformer_lm', 'activation_fn': relu, 'dropout': 0.1, 'attention_dropout': 0.0, 'activation_dropout': 0.0, 'relu_dropout': 0.0, 'decoder_embed_dim': 512, 'decoder_output_dim': 512, 'decoder_input_dim': 512, 'decoder_ffn_embed_dim': 2048, 'decoder_layers': 6, 'decoder_attention_heads': 8, 'decoder_normalize_before': False, 'no_decoder_final_norm': False, 'adaptive_softmax_cutoff': None, 'adaptive_softmax_dropout': 0.0, 'adaptive_softmax_factor': 4.0, 'no_token_positional_embeddings': False, 'share_decoder_input_output_embed': True, 'character_embeddings': False, 'character_filters': '[(1, 64), (2, 128), (3, 192), (4, 256), (5, 256), (6, 256), (7, 256)]', 'character_embedding_dim': 4, 'char_embedder_highway_layers': 2, 'adaptive_input': False, 'adaptive_input_factor': 4.0, 'adaptive_input_cutoff': None, 'tie_adaptive_weights': False, 'tie_adaptive_proj': False, 'decoder_learned_pos': False, 'layernorm_embedding': False, 'no_scale_embedding': False, 'checkpoint_activations': False, 'offload_activations': False, 'decoder_layerdrop': 0.0, 'decoder_layers_to_keep': None, 'quant_noise_pq': 0.0, 'quant_noise_pq_block_size': 8, 'quant_noise_scalar': 0.0, 'min_params_to_wrap': 100000000, 'base_layers': 0, 'base_sublayers': 1, 'base_shuffle': 1, 'scale_fc': False, 'scale_attn': False, 'scale_heads': False, 'scale_resids': False, 'add_bos_token': False, 'tokens_per_sample': 512, 'max_target_positions': None, 'tpu': False}, 'task': {'_name': 'language_modeling', 'data': 'data/xsum-lang-full', 'sample_break_mode': none, 'tokens_per_sample': 512, 'output_dictionary_size': -1, 'self_target': False, 'future_target': False, 'past_target': False, 'add_bos_token': False, 'max_target_positions': None, 'shorten_method': none, 'shorten_data_split_list': '', 'pad_to_fixed_length': False, 'pad_to_fixed_bsz': False, 'seed': 1, 'batch_size': None, 'batch_size_valid': None, 'dataset_impl': None, 'data_buffer_size': 10, 'tpu': False, 'use_plasma_view': False, 'plasma_path': '/tmp/plasma'}, 'criterion': {'_name': 'cross_entropy', 'sentence_avg': False}, 'optimizer': {'_name': 'adam', 'adam_betas': '(0.9, 0.98)', 'adam_eps': 1e-08, 'weight_decay': 0.01, 'use_old_adam': False, 'fp16_adam_stats': False, 'tpu': False, 'lr': [0.0005]}, 'lr_scheduler': {'_name': 'inverse_sqrt', 'warmup_updates': 4000, 'warmup_init_lr': 1e-07, 'lr': [0.0005]}, 'scoring': {'_name': 'bleu', 'pad': 1, 'eos': 2, 'unk': 3}, 'bpe': None, 'tokenizer': None, 'ema': {'_name': None, 'store_ema': False, 'ema_decay': 0.9999, 'ema_start_update': 0, 'ema_seed_model': None, 'ema_update_freq': 1, 'ema_fp32': False}, 'simul_type': None}
2022-05-16 17:46:51 | INFO | fairseq.tasks.language_modeling | dictionary: 49992 types
2022-05-16 17:46:54 | INFO | fairseq_cli.train | TransformerLanguageModel(
  (decoder): TransformerDecoder(
    (dropout_module): FairseqDropout()
    (embed_tokens): Embedding(49992, 512, padding_idx=1)
    (embed_positions): SinusoidalPositionalEmbedding()
    (layers): ModuleList(
      (0): TransformerDecoderLayerBase(
        (dropout_module): FairseqDropout()
        (self_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=512, out_features=512, bias=True)
          (v_proj): Linear(in_features=512, out_features=512, bias=True)
          (q_proj): Linear(in_features=512, out_features=512, bias=True)
          (out_proj): Linear(in_features=512, out_features=512, bias=True)
        )
        (activation_dropout_module): FairseqDropout()
        (self_attn_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
        (fc1): Linear(in_features=512, out_features=2048, bias=True)
        (fc2): Linear(in_features=2048, out_features=512, bias=True)
        (final_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
      )
      (1): TransformerDecoderLayerBase(
        (dropout_module): FairseqDropout()
        (self_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=512, out_features=512, bias=True)
          (v_proj): Linear(in_features=512, out_features=512, bias=True)
          (q_proj): Linear(in_features=512, out_features=512, bias=True)
          (out_proj): Linear(in_features=512, out_features=512, bias=True)
        )
        (activation_dropout_module): FairseqDropout()
        (self_attn_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
        (fc1): Linear(in_features=512, out_features=2048, bias=True)
        (fc2): Linear(in_features=2048, out_features=512, bias=True)
        (final_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
      )
      (2): TransformerDecoderLayerBase(
        (dropout_module): FairseqDropout()
        (self_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=512, out_features=512, bias=True)
          (v_proj): Linear(in_features=512, out_features=512, bias=True)
          (q_proj): Linear(in_features=512, out_features=512, bias=True)
          (out_proj): Linear(in_features=512, out_features=512, bias=True)
        )
        (activation_dropout_module): FairseqDropout()
        (self_attn_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
        (fc1): Linear(in_features=512, out_features=2048, bias=True)
        (fc2): Linear(in_features=2048, out_features=512, bias=True)
        (final_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
      )
      (3): TransformerDecoderLayerBase(
        (dropout_module): FairseqDropout()
        (self_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=512, out_features=512, bias=True)
          (v_proj): Linear(in_features=512, out_features=512, bias=True)
          (q_proj): Linear(in_features=512, out_features=512, bias=True)
          (out_proj): Linear(in_features=512, out_features=512, bias=True)
        )
        (activation_dropout_module): FairseqDropout()
        (self_attn_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
        (fc1): Linear(in_features=512, out_features=2048, bias=True)
        (fc2): Linear(in_features=2048, out_features=512, bias=True)
        (final_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
      )
      (4): TransformerDecoderLayerBase(
        (dropout_module): FairseqDropout()
        (self_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=512, out_features=512, bias=True)
          (v_proj): Linear(in_features=512, out_features=512, bias=True)
          (q_proj): Linear(in_features=512, out_features=512, bias=True)
          (out_proj): Linear(in_features=512, out_features=512, bias=True)
        )
        (activation_dropout_module): FairseqDropout()
        (self_attn_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
        (fc1): Linear(in_features=512, out_features=2048, bias=True)
        (fc2): Linear(in_features=2048, out_features=512, bias=True)
        (final_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
      )
      (5): TransformerDecoderLayerBase(
        (dropout_module): FairseqDropout()
        (self_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=512, out_features=512, bias=True)
          (v_proj): Linear(in_features=512, out_features=512, bias=True)
          (q_proj): Linear(in_features=512, out_features=512, bias=True)
          (out_proj): Linear(in_features=512, out_features=512, bias=True)
        )
        (activation_dropout_module): FairseqDropout()
        (self_attn_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
        (fc1): Linear(in_features=512, out_features=2048, bias=True)
        (fc2): Linear(in_features=2048, out_features=512, bias=True)
        (final_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
      )
    )
    (output_projection): Linear(in_features=512, out_features=49992, bias=False)
  )
)
2022-05-16 17:46:54 | INFO | fairseq_cli.train | task: LanguageModelingTask
2022-05-16 17:46:54 | INFO | fairseq_cli.train | model: TransformerLanguageModel
2022-05-16 17:46:54 | INFO | fairseq_cli.train | criterion: CrossEntropyCriterion
2022-05-16 17:46:54 | INFO | fairseq_cli.train | num. shared model params: 44,510,208 (num. trained: 44,510,208)
2022-05-16 17:46:54 | INFO | fairseq_cli.train | num. expert model params: 0 (num. trained: 0)
2022-05-16 17:46:55 | INFO | fairseq.data.data_utils | loaded 22,664 examples from: data/xsum-lang-full/valid
2022-05-16 17:47:36 | INFO | fairseq.trainer | detected shared parameter: decoder.embed_tokens.weight <- decoder.output_projection.weight
2022-05-16 17:47:36 | INFO | fairseq.utils | ***********************CUDA enviroments for all 1 workers***********************
2022-05-16 17:47:36 | INFO | fairseq.utils | rank   0: capabilities =  7.5  ; total memory = 10.761 GB ; name = NVIDIA GeForce RTX 2080 Ti              
2022-05-16 17:47:36 | INFO | fairseq.utils | ***********************CUDA enviroments for all 1 workers***********************
2022-05-16 17:47:36 | INFO | fairseq_cli.train | training on 1 devices (GPUs/TPUs)
2022-05-16 17:47:36 | INFO | fairseq_cli.train | max tokens per device = 2048 and max sentences per device = None
2022-05-16 17:47:36 | INFO | fairseq.trainer | Preparing to load checkpoint checkpoints/language_model/standard/checkpoint_best_standard_32_5.0000E-04_full.pt
2022-05-16 17:47:39 | INFO | fairseq.trainer | NOTE: your device may support faster training with --fp16 or --amp
2022-05-16 17:47:39 | INFO | fairseq.trainer | Loaded checkpoint checkpoints/language_model/standard/checkpoint_best_standard_32_5.0000E-04_full.pt (epoch 35 @ 50252 updates)
2022-05-16 17:47:39 | INFO | fairseq.trainer | loading train data for epoch 35
2022-05-16 17:47:40 | INFO | fairseq.data.data_utils | loaded 408,090 examples from: data/xsum-lang-full/train
2022-05-16 17:47:40 | INFO | fairseq.data.iterators | grouped total_num_itrs = 1478
2022-05-16 17:47:40 | INFO | fairseq.trainer | begin training epoch 35
2022-05-16 17:47:40 | INFO | fairseq_cli.train | Start iterating over samples
2022-05-16 17:49:38 | INFO | train_inner | epoch 035:     48 / 1478 loss=4.582, ppl=23.96, wps=27522.2, ups=0.42, wpb=65536, bsz=128, num_updates=50300, lr=0.000140999, gnorm=0.548, train_wall=111, gb_free=7.9, wall=122
2022-05-16 17:53:37 | INFO | train_inner | epoch 035:    148 / 1478 loss=4.586, ppl=24.01, wps=27427.6, ups=0.42, wpb=65536, bsz=128, num_updates=50400, lr=0.000140859, gnorm=0.548, train_wall=227, gb_free=7.9, wall=361
2022-05-16 17:57:48 | INFO | train_inner | epoch 035:    248 / 1478 loss=4.586, ppl=24.01, wps=26084.3, ups=0.4, wpb=65531, bsz=128, num_updates=50500, lr=0.00014072, gnorm=0.551, train_wall=227, gb_free=7.9, wall=612
2022-05-16 18:02:34 | INFO | train_inner | epoch 035:    348 / 1478 loss=4.603, ppl=24.3, wps=22954.3, ups=0.35, wpb=65536, bsz=128, num_updates=50600, lr=0.00014058, gnorm=0.552, train_wall=226, gb_free=7.9, wall=898
2022-05-16 18:07:07 | INFO | train_inner | epoch 035:    448 / 1478 loss=4.6, ppl=24.25, wps=23973.2, ups=0.37, wpb=65536, bsz=128, num_updates=50700, lr=0.000140442, gnorm=0.548, train_wall=227, gb_free=7.9, wall=1171
2022-05-16 18:11:22 | INFO | train_inner | epoch 035:    548 / 1478 loss=4.595, ppl=24.17, wps=25700.8, ups=0.39, wpb=65536, bsz=128, num_updates=50800, lr=0.000140303, gnorm=0.547, train_wall=227, gb_free=7.9, wall=1426
2022-05-16 18:15:30 | INFO | train_inner | epoch 035:    648 / 1478 loss=4.604, ppl=24.32, wps=26446.8, ups=0.4, wpb=65536, bsz=128, num_updates=50900, lr=0.000140165, gnorm=0.548, train_wall=227, gb_free=7.9, wall=1674
2022-05-16 18:19:37 | INFO | train_inner | epoch 035:    748 / 1478 loss=4.616, ppl=24.52, wps=26523.5, ups=0.4, wpb=65525.8, bsz=128, num_updates=51000, lr=0.000140028, gnorm=0.547, train_wall=231, gb_free=7.9, wall=1921
2022-05-16 18:23:37 | INFO | train_inner | epoch 035:    848 / 1478 loss=4.61, ppl=24.42, wps=27275.1, ups=0.42, wpb=65536, bsz=128, num_updates=51100, lr=0.000139891, gnorm=0.548, train_wall=227, gb_free=7.9, wall=2161
2022-05-16 18:27:39 | INFO | train_inner | epoch 035:    948 / 1478 loss=4.622, ppl=24.62, wps=27114.5, ups=0.41, wpb=65536, bsz=128, num_updates=51200, lr=0.000139754, gnorm=0.551, train_wall=228, gb_free=7.9, wall=2403
2022-05-16 18:31:43 | INFO | train_inner | epoch 035:   1048 / 1478 loss=4.616, ppl=24.52, wps=26828.1, ups=0.41, wpb=65536, bsz=128, num_updates=51300, lr=0.000139618, gnorm=0.55, train_wall=231, gb_free=7.9, wall=2647
2022-05-16 18:35:45 | INFO | train_inner | epoch 035:   1148 / 1478 loss=4.625, ppl=24.67, wps=27175.7, ups=0.41, wpb=65536, bsz=128, num_updates=51400, lr=0.000139482, gnorm=0.553, train_wall=227, gb_free=7.9, wall=2888
2022-05-16 18:39:49 | INFO | train_inner | epoch 035:   1248 / 1478 loss=4.625, ppl=24.68, wps=26861.6, ups=0.41, wpb=65536, bsz=128, num_updates=51500, lr=0.000139347, gnorm=0.544, train_wall=231, gb_free=7.9, wall=3132
2022-05-16 18:43:47 | INFO | train_inner | epoch 035:   1348 / 1478 loss=4.641, ppl=24.95, wps=27423.1, ups=0.42, wpb=65536, bsz=128, num_updates=51600, lr=0.000139212, gnorm=0.548, train_wall=227, gb_free=7.9, wall=3371
2022-05-16 18:47:47 | INFO | train_inner | epoch 035:   1448 / 1478 loss=4.625, ppl=24.67, wps=27310.5, ups=0.42, wpb=65536, bsz=128, num_updates=51700, lr=0.000139077, gnorm=0.548, train_wall=227, gb_free=7.9, wall=3611
2022-05-16 18:48:59 | INFO | fairseq_cli.train | begin validation on "valid" subset
/cluster/work/cotterell/liam/fairseq/fairseq/utils.py:374: UserWarning: amp_C fused kernels unavailable, disabling multi_tensor_l2norm; you may get better performance by installing NVIDIA's apex library
  warnings.warn(
2022-05-16 18:50:14 | INFO | valid | epoch 035 | valid on 'valid' subset | loss 4.751 | ppl 26.93 | wps 73029.4 | wpb 2047.4 | bsz 4 | num_updates 51730 | best_loss 4.751
2022-05-16 18:50:14 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 35 @ 51730 updates
2022-05-16 18:50:14 | INFO | fairseq.trainer | Saving checkpoint to /cluster/work/cotterell/liam/master-thesis/checkpoints/language_model/standard/checkpoint_best_standard_32_5.0000E-04_full.pt
2022-05-16 18:50:15 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/work/cotterell/liam/master-thesis/checkpoints/language_model/standard/checkpoint_best_standard_32_5.0000E-04_full.pt
2022-05-16 18:50:15 | INFO | fairseq.checkpoint_utils | Saved checkpoint checkpoints/language_model/standard/checkpoint_best_standard_32_5.0000E-04_full.pt (epoch 35 @ 51730 updates, score 4.751) (writing took 1.9335208591073751 seconds)
2022-05-16 18:50:15 | INFO | fairseq_cli.train | end of epoch 35 (average epoch stats below)
2022-05-16 18:50:15 | INFO | train | epoch 035 | loss 4.61 | ppl 24.42 | wps 25806.6 | ups 0.39 | wpb 65507.3 | bsz 127.9 | num_updates 51730 | lr 0.000139036 | gnorm 0.549 | train_wall 3366 | gb_free 7.9 | wall 3759
2022-05-16 18:50:16 | INFO | fairseq.data.iterators | grouped total_num_itrs = 1478
2022-05-16 18:50:16 | INFO | fairseq.trainer | begin training epoch 36
2022-05-16 18:50:16 | INFO | fairseq_cli.train | Start iterating over samples
2022-05-16 18:53:38 | INFO | train_inner | epoch 036:     70 / 1478 loss=4.582, ppl=23.96, wps=18584.7, ups=0.29, wpb=65116.2, bsz=127.2, num_updates=51800, lr=0.000138943, gnorm=0.551, train_wall=226, gb_free=7.9, wall=3962
2022-05-16 18:58:04 | INFO | train_inner | epoch 036:    170 / 1478 loss=4.588, ppl=24.05, wps=24602.1, ups=0.38, wpb=65536, bsz=128, num_updates=51900, lr=0.000138809, gnorm=0.551, train_wall=226, gb_free=7.9, wall=4228
2022-05-16 19:02:23 | INFO | train_inner | epoch 036:    270 / 1478 loss=4.591, ppl=24.09, wps=25310.7, ups=0.39, wpb=65536, bsz=128, num_updates=52000, lr=0.000138675, gnorm=0.551, train_wall=230, gb_free=7.9, wall=4487
2022-05-16 19:06:36 | INFO | train_inner | epoch 036:    370 / 1478 loss=4.585, ppl=24, wps=25924.2, ups=0.4, wpb=65536, bsz=128, num_updates=52100, lr=0.000138542, gnorm=0.551, train_wall=229, gb_free=7.9, wall=4740
2022-05-16 19:10:42 | INFO | train_inner | epoch 036:    470 / 1478 loss=4.601, ppl=24.27, wps=26615.1, ups=0.41, wpb=65531, bsz=128, num_updates=52200, lr=0.000138409, gnorm=0.547, train_wall=228, gb_free=7.9, wall=4986
2022-05-16 19:14:47 | INFO | train_inner | epoch 036:    570 / 1478 loss=4.603, ppl=24.31, wps=26813.4, ups=0.41, wpb=65536, bsz=128, num_updates=52300, lr=0.000138277, gnorm=0.552, train_wall=229, gb_free=7.9, wall=5230
2022-05-16 19:18:51 | INFO | train_inner | epoch 036:    670 / 1478 loss=4.601, ppl=24.27, wps=26769.1, ups=0.41, wpb=65536, bsz=128, num_updates=52400, lr=0.000138145, gnorm=0.548, train_wall=230, gb_free=7.9, wall=5475
2022-05-16 19:22:54 | INFO | train_inner | epoch 036:    770 / 1478 loss=4.616, ppl=24.52, wps=26986.1, ups=0.41, wpb=65536, bsz=128, num_updates=52500, lr=0.000138013, gnorm=0.545, train_wall=228, gb_free=7.9, wall=5718
2022-05-16 19:26:59 | INFO | train_inner | epoch 036:    870 / 1478 loss=4.611, ppl=24.44, wps=26788.4, ups=0.41, wpb=65536, bsz=128, num_updates=52600, lr=0.000137882, gnorm=0.545, train_wall=229, gb_free=7.9, wall=5963
2022-05-16 19:30:59 | INFO | train_inner | epoch 036:    970 / 1478 loss=4.604, ppl=24.32, wps=27249.6, ups=0.42, wpb=65536, bsz=128, num_updates=52700, lr=0.000137751, gnorm=0.558, train_wall=227, gb_free=7.9, wall=6203
2022-05-16 19:35:00 | INFO | train_inner | epoch 036:   1070 / 1478 loss=4.606, ppl=24.35, wps=27277.3, ups=0.42, wpb=65536, bsz=128, num_updates=52800, lr=0.00013762, gnorm=0.549, train_wall=228, gb_free=7.9, wall=6444
2022-05-16 19:39:01 | INFO | train_inner | epoch 036:   1170 / 1478 loss=4.622, ppl=24.63, wps=27121.8, ups=0.41, wpb=65536, bsz=128, num_updates=52900, lr=0.00013749, gnorm=0.552, train_wall=229, gb_free=7.9, wall=6685
2022-05-16 19:43:01 | INFO | train_inner | epoch 036:   1270 / 1478 loss=4.619, ppl=24.57, wps=27335.6, ups=0.42, wpb=65536, bsz=128, num_updates=53000, lr=0.000137361, gnorm=0.545, train_wall=227, gb_free=7.9, wall=6925
2022-05-16 19:47:16 | INFO | train_inner | epoch 036:   1370 / 1478 loss=4.628, ppl=24.72, wps=25705.8, ups=0.39, wpb=65536, bsz=128, num_updates=53100, lr=0.000137231, gnorm=0.549, train_wall=230, gb_free=7.9, wall=7180
2022-05-16 19:51:57 | INFO | train_inner | epoch 036:   1470 / 1478 loss=4.624, ppl=24.65, wps=23344.2, ups=0.36, wpb=65536, bsz=128, num_updates=53200, lr=0.000137102, gnorm=0.551, train_wall=225, gb_free=7.9, wall=7461
2022-05-16 19:52:16 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-05-16 19:53:27 | INFO | valid | epoch 036 | valid on 'valid' subset | loss 4.748 | ppl 26.86 | wps 75797.2 | wpb 2047.4 | bsz 4 | num_updates 53208 | best_loss 4.748
2022-05-16 19:53:27 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 36 @ 53208 updates
2022-05-16 19:53:27 | INFO | fairseq.trainer | Saving checkpoint to /cluster/work/cotterell/liam/master-thesis/checkpoints/language_model/standard/checkpoint_best_standard_32_5.0000E-04_full.pt
2022-05-16 19:53:29 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/work/cotterell/liam/master-thesis/checkpoints/language_model/standard/checkpoint_best_standard_32_5.0000E-04_full.pt
2022-05-16 19:53:29 | INFO | fairseq.checkpoint_utils | Saved checkpoint checkpoints/language_model/standard/checkpoint_best_standard_32_5.0000E-04_full.pt (epoch 36 @ 53208 updates, score 4.748) (writing took 1.6736087650060654 seconds)
2022-05-16 19:53:29 | INFO | fairseq_cli.train | end of epoch 36 (average epoch stats below)
2022-05-16 19:53:29 | INFO | train | epoch 036 | loss 4.605 | ppl 24.34 | wps 25524.6 | ups 0.39 | wpb 65507.3 | bsz 127.9 | num_updates 53208 | lr 0.000137092 | gnorm 0.55 | train_wall 3369 | gb_free 7.9 | wall 7553
2022-05-16 19:53:29 | INFO | fairseq.data.iterators | grouped total_num_itrs = 1478
2022-05-16 19:53:29 | INFO | fairseq.trainer | begin training epoch 37
2022-05-16 19:53:29 | INFO | fairseq_cli.train | Start iterating over samples
2022-05-16 19:57:33 | INFO | train_inner | epoch 037:     92 / 1478 loss=4.579, ppl=23.91, wps=19349.3, ups=0.3, wpb=65126.4, bsz=127.2, num_updates=53300, lr=0.000136973, gnorm=0.552, train_wall=226, gb_free=7.9, wall=7797
2022-05-16 20:01:45 | INFO | train_inner | epoch 037:    192 / 1478 loss=4.575, ppl=23.84, wps=26051.3, ups=0.4, wpb=65536, bsz=128, num_updates=53400, lr=0.000136845, gnorm=0.552, train_wall=226, gb_free=7.9, wall=8049
2022-05-16 20:05:56 | INFO | train_inner | epoch 037:    292 / 1478 loss=4.584, ppl=23.99, wps=26074, ups=0.4, wpb=65536, bsz=128, num_updates=53500, lr=0.000136717, gnorm=0.557, train_wall=230, gb_free=7.9, wall=8300
2022-05-16 20:10:00 | INFO | train_inner | epoch 037:    392 / 1478 loss=4.58, ppl=23.92, wps=26832, ups=0.41, wpb=65531, bsz=128, num_updates=53600, lr=0.00013659, gnorm=0.554, train_wall=229, gb_free=7.9, wall=8544
2022-05-16 20:14:03 | INFO | train_inner | epoch 037:    492 / 1478 loss=4.588, ppl=24.05, wps=27022.4, ups=0.41, wpb=65536, bsz=128, num_updates=53700, lr=0.000136462, gnorm=0.552, train_wall=227, gb_free=7.9, wall=8787
2022-05-16 20:18:05 | INFO | train_inner | epoch 037:    592 / 1478 loss=4.594, ppl=24.15, wps=27104.6, ups=0.41, wpb=65536, bsz=128, num_updates=53800, lr=0.000136335, gnorm=0.545, train_wall=227, gb_free=7.9, wall=9029
2022-05-16 20:22:08 | INFO | train_inner | epoch 037:    692 / 1478 loss=4.606, ppl=24.36, wps=26941.4, ups=0.41, wpb=65536, bsz=128, num_updates=53900, lr=0.000136209, gnorm=0.552, train_wall=227, gb_free=7.9, wall=9272
2022-05-16 20:26:12 | INFO | train_inner | epoch 037:    792 / 1478 loss=4.601, ppl=24.27, wps=26816, ups=0.41, wpb=65536, bsz=128, num_updates=54000, lr=0.000136083, gnorm=0.552, train_wall=228, gb_free=7.9, wall=9516
2022-05-16 20:30:14 | INFO | train_inner | epoch 037:    892 / 1478 loss=4.602, ppl=24.29, wps=27160.7, ups=0.41, wpb=65525.8, bsz=128, num_updates=54100, lr=0.000135957, gnorm=0.548, train_wall=227, gb_free=7.9, wall=9758
2022-05-16 20:34:14 | INFO | train_inner | epoch 037:    992 / 1478 loss=4.61, ppl=24.42, wps=27238.1, ups=0.42, wpb=65536, bsz=128, num_updates=54200, lr=0.000135831, gnorm=0.558, train_wall=227, gb_free=7.9, wall=9998
2022-05-16 20:38:20 | INFO | train_inner | epoch 037:   1092 / 1478 loss=4.616, ppl=24.52, wps=26699.4, ups=0.41, wpb=65536, bsz=128, num_updates=54300, lr=0.000135706, gnorm=0.552, train_wall=232, gb_free=7.9, wall=10244
2022-05-16 20:42:19 | INFO | train_inner | epoch 037:   1192 / 1478 loss=4.622, ppl=24.62, wps=27392.3, ups=0.42, wpb=65536, bsz=128, num_updates=54400, lr=0.000135582, gnorm=0.551, train_wall=227, gb_free=7.9, wall=10483
2022-05-16 20:46:23 | INFO | train_inner | epoch 037:   1292 / 1478 loss=4.621, ppl=24.61, wps=26835.5, ups=0.41, wpb=65536, bsz=128, num_updates=54500, lr=0.000135457, gnorm=0.55, train_wall=228, gb_free=7.9, wall=10727
2022-05-16 20:50:23 | INFO | train_inner | epoch 037:   1392 / 1478 loss=4.611, ppl=24.43, wps=27301.8, ups=0.42, wpb=65536, bsz=128, num_updates=54600, lr=0.000135333, gnorm=0.552, train_wall=228, gb_free=7.9, wall=10967
2022-05-16 20:53:48 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-05-16 20:54:58 | INFO | valid | epoch 037 | valid on 'valid' subset | loss 4.748 | ppl 26.87 | wps 76057.3 | wpb 2047.4 | bsz 4 | num_updates 54686 | best_loss 4.748
2022-05-16 20:54:58 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 37 @ 54686 updates
2022-05-16 20:54:58 | INFO | fairseq.trainer | Saving checkpoint to /cluster/work/cotterell/liam/master-thesis/checkpoints/language_model/standard/checkpoint_best_standard_32_5.0000E-04_full.pt
2022-05-16 20:55:00 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/work/cotterell/liam/master-thesis/checkpoints/language_model/standard/checkpoint_best_standard_32_5.0000E-04_full.pt
2022-05-16 20:55:00 | INFO | fairseq.checkpoint_utils | Saved checkpoint checkpoints/language_model/standard/checkpoint_best_standard_32_5.0000E-04_full.pt (epoch 37 @ 54686 updates, score 4.748) (writing took 1.7515699323266745 seconds)
2022-05-16 20:55:00 | INFO | fairseq_cli.train | end of epoch 37 (average epoch stats below)
2022-05-16 20:55:00 | INFO | train | epoch 037 | loss 4.6 | ppl 24.26 | wps 26228.1 | ups 0.4 | wpb 65507.3 | bsz 127.9 | num_updates 54686 | lr 0.000135227 | gnorm 0.552 | train_wall 3365 | gb_free 7.9 | wall 11244
2022-05-16 20:55:00 | INFO | fairseq.data.iterators | grouped total_num_itrs = 1478
2022-05-16 20:55:00 | INFO | fairseq.trainer | begin training epoch 38
2022-05-16 20:55:00 | INFO | fairseq_cli.train | Start iterating over samples
2022-05-16 20:55:34 | INFO | train_inner | epoch 038:     14 / 1478 loss=4.615, ppl=24.5, wps=20963.3, ups=0.32, wpb=65126.4, bsz=127.2, num_updates=54700, lr=0.000135209, gnorm=0.556, train_wall=225, gb_free=7.9, wall=11278
2022-05-16 20:59:34 | INFO | train_inner | epoch 038:    114 / 1478 loss=4.563, ppl=23.64, wps=27283.4, ups=0.42, wpb=65536, bsz=128, num_updates=54800, lr=0.000135086, gnorm=0.547, train_wall=228, gb_free=7.9, wall=11518
2022-05-16 21:03:36 | INFO | train_inner | epoch 038:    214 / 1478 loss=4.579, ppl=23.89, wps=27142.4, ups=0.41, wpb=65531, bsz=128, num_updates=54900, lr=0.000134963, gnorm=0.554, train_wall=227, gb_free=7.9, wall=11759
2022-05-16 21:08:25 | INFO | train_inner | epoch 038:    314 / 1478 loss=4.578, ppl=23.88, wps=22641.1, ups=0.35, wpb=65536, bsz=128, num_updates=55000, lr=0.00013484, gnorm=0.55, train_wall=231, gb_free=7.9, wall=12049
2022-05-16 21:12:46 | INFO | train_inner | epoch 038:    414 / 1478 loss=4.589, ppl=24.06, wps=25070, ups=0.38, wpb=65536, bsz=128, num_updates=55100, lr=0.000134718, gnorm=0.556, train_wall=226, gb_free=7.9, wall=12310
2022-05-16 21:16:57 | INFO | train_inner | epoch 038:    514 / 1478 loss=4.587, ppl=24.04, wps=26114.2, ups=0.4, wpb=65536, bsz=128, num_updates=55200, lr=0.000134595, gnorm=0.551, train_wall=227, gb_free=7.9, wall=12561
2022-05-16 21:21:05 | INFO | train_inner | epoch 038:    614 / 1478 loss=4.594, ppl=24.15, wps=26444.9, ups=0.4, wpb=65536, bsz=128, num_updates=55300, lr=0.000134474, gnorm=0.554, train_wall=227, gb_free=7.9, wall=12809
2022-05-16 21:25:08 | INFO | train_inner | epoch 038:    714 / 1478 loss=4.591, ppl=24.1, wps=26955.2, ups=0.41, wpb=65536, bsz=128, num_updates=55400, lr=0.000134352, gnorm=0.551, train_wall=227, gb_free=7.9, wall=13052
2022-05-16 21:29:14 | INFO | train_inner | epoch 038:    814 / 1478 loss=4.594, ppl=24.16, wps=26641.6, ups=0.41, wpb=65525.8, bsz=128, num_updates=55500, lr=0.000134231, gnorm=0.547, train_wall=231, gb_free=7.9, wall=13298
2022-05-16 21:33:15 | INFO | train_inner | epoch 038:    914 / 1478 loss=4.599, ppl=24.24, wps=27275.6, ups=0.42, wpb=65536, bsz=128, num_updates=55600, lr=0.00013411, gnorm=0.55, train_wall=227, gb_free=7.9, wall=13538
2022-05-16 21:37:17 | INFO | train_inner | epoch 038:   1014 / 1478 loss=4.613, ppl=24.46, wps=27047.1, ups=0.41, wpb=65536, bsz=128, num_updates=55700, lr=0.00013399, gnorm=0.55, train_wall=227, gb_free=7.9, wall=13781
2022-05-16 21:41:19 | INFO | train_inner | epoch 038:   1114 / 1478 loss=4.612, ppl=24.46, wps=27020.9, ups=0.41, wpb=65536, bsz=128, num_updates=55800, lr=0.00013387, gnorm=0.553, train_wall=229, gb_free=7.9, wall=14023
2022-05-16 21:45:27 | INFO | train_inner | epoch 038:   1214 / 1478 loss=4.604, ppl=24.32, wps=26425.5, ups=0.4, wpb=65536, bsz=128, num_updates=55900, lr=0.00013375, gnorm=0.548, train_wall=233, gb_free=7.9, wall=14271
2022-05-16 21:49:28 | INFO | train_inner | epoch 038:   1314 / 1478 loss=4.61, ppl=24.41, wps=27187.1, ups=0.41, wpb=65536, bsz=128, num_updates=56000, lr=0.000133631, gnorm=0.562, train_wall=228, gb_free=7.9, wall=14512
2022-05-16 21:53:28 | INFO | train_inner | epoch 038:   1414 / 1478 loss=4.623, ppl=24.64, wps=27406.7, ups=0.42, wpb=65536, bsz=128, num_updates=56100, lr=0.000133511, gnorm=0.554, train_wall=227, gb_free=7.9, wall=14751
2022-05-16 21:56:04 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-05-16 21:57:16 | INFO | valid | epoch 038 | valid on 'valid' subset | loss 4.744 | ppl 26.79 | wps 75094.1 | wpb 2047.4 | bsz 4 | num_updates 56164 | best_loss 4.744
2022-05-16 21:57:16 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 38 @ 56164 updates
2022-05-16 21:57:16 | INFO | fairseq.trainer | Saving checkpoint to /cluster/work/cotterell/liam/master-thesis/checkpoints/language_model/standard/checkpoint_best_standard_32_5.0000E-04_full.pt
2022-05-16 21:57:18 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/work/cotterell/liam/master-thesis/checkpoints/language_model/standard/checkpoint_best_standard_32_5.0000E-04_full.pt
2022-05-16 21:57:18 | INFO | fairseq.checkpoint_utils | Saved checkpoint checkpoints/language_model/standard/checkpoint_best_standard_32_5.0000E-04_full.pt (epoch 38 @ 56164 updates, score 4.744) (writing took 2.081825285218656 seconds)
2022-05-16 21:57:18 | INFO | fairseq_cli.train | end of epoch 38 (average epoch stats below)
2022-05-16 21:57:18 | INFO | train | epoch 038 | loss 4.596 | ppl 24.18 | wps 25904.1 | ups 0.4 | wpb 65507.3 | bsz 127.9 | num_updates 56164 | lr 0.000133435 | gnorm 0.552 | train_wall 3372 | gb_free 7.9 | wall 14982
2022-05-16 21:57:18 | INFO | fairseq.data.iterators | grouped total_num_itrs = 1478
2022-05-16 21:57:18 | INFO | fairseq.trainer | begin training epoch 39
2022-05-16 21:57:18 | INFO | fairseq_cli.train | Start iterating over samples
2022-05-16 21:58:46 | INFO | train_inner | epoch 039:     36 / 1478 loss=4.58, ppl=23.92, wps=20463.1, ups=0.31, wpb=65126.4, bsz=127.2, num_updates=56200, lr=0.000133393, gnorm=0.557, train_wall=231, gb_free=7.9, wall=15070
2022-05-16 22:02:47 | INFO | train_inner | epoch 039:    136 / 1478 loss=4.561, ppl=23.6, wps=27156.5, ups=0.41, wpb=65536, bsz=128, num_updates=56300, lr=0.000133274, gnorm=0.551, train_wall=227, gb_free=7.9, wall=15311
2022-05-16 22:06:58 | INFO | train_inner | epoch 039:    236 / 1478 loss=4.57, ppl=23.75, wps=26107.1, ups=0.4, wpb=65536, bsz=128, num_updates=56400, lr=0.000133156, gnorm=0.555, train_wall=227, gb_free=7.9, wall=15562
2022-05-16 22:11:50 | INFO | train_inner | epoch 039:    336 / 1478 loss=4.581, ppl=23.94, wps=22451.8, ups=0.34, wpb=65536, bsz=128, num_updates=56500, lr=0.000133038, gnorm=0.555, train_wall=232, gb_free=7.9, wall=15854
2022-05-16 22:16:12 | INFO | train_inner | epoch 039:    436 / 1478 loss=4.575, ppl=23.84, wps=24998.8, ups=0.38, wpb=65536, bsz=128, num_updates=56600, lr=0.00013292, gnorm=0.555, train_wall=226, gb_free=7.9, wall=16116
2022-05-16 22:20:28 | INFO | train_inner | epoch 039:    536 / 1478 loss=4.593, ppl=24.13, wps=25676.8, ups=0.39, wpb=65536, bsz=128, num_updates=56700, lr=0.000132803, gnorm=0.554, train_wall=228, gb_free=7.9, wall=16371
2022-05-16 22:25:01 | INFO | train_inner | epoch 039:    636 / 1478 loss=4.585, ppl=24, wps=23927.9, ups=0.37, wpb=65536, bsz=128, num_updates=56800, lr=0.000132686, gnorm=0.551, train_wall=226, gb_free=7.9, wall=16645
2022-05-16 22:29:34 | INFO | train_inner | epoch 039:    736 / 1478 loss=4.602, ppl=24.29, wps=24010.7, ups=0.37, wpb=65536, bsz=128, num_updates=56900, lr=0.00013257, gnorm=0.553, train_wall=230, gb_free=7.9, wall=16918
2022-05-16 22:33:50 | INFO | train_inner | epoch 039:    836 / 1478 loss=4.59, ppl=24.08, wps=25592.7, ups=0.39, wpb=65536, bsz=128, num_updates=57000, lr=0.000132453, gnorm=0.555, train_wall=227, gb_free=7.9, wall=17174
2022-05-16 22:37:58 | INFO | train_inner | epoch 039:    936 / 1478 loss=4.596, ppl=24.18, wps=26492.8, ups=0.4, wpb=65536, bsz=128, num_updates=57100, lr=0.000132337, gnorm=0.555, train_wall=226, gb_free=7.9, wall=17422
2022-05-16 22:42:03 | INFO | train_inner | epoch 039:   1036 / 1478 loss=4.603, ppl=24.31, wps=26725.1, ups=0.41, wpb=65525.8, bsz=128, num_updates=57200, lr=0.000132221, gnorm=0.554, train_wall=227, gb_free=7.9, wall=17667
2022-05-16 22:46:08 | INFO | train_inner | epoch 039:   1136 / 1478 loss=4.609, ppl=24.41, wps=26800.9, ups=0.41, wpb=65536, bsz=128, num_updates=57300, lr=0.000132106, gnorm=0.553, train_wall=228, gb_free=7.9, wall=17911
2022-05-16 22:50:16 | INFO | train_inner | epoch 039:   1236 / 1478 loss=4.609, ppl=24.4, wps=26360, ups=0.4, wpb=65531, bsz=128, num_updates=57400, lr=0.000131991, gnorm=0.553, train_wall=232, gb_free=7.9, wall=18160
2022-05-16 22:54:16 | INFO | train_inner | epoch 039:   1336 / 1478 loss=4.608, ppl=24.38, wps=27320, ups=0.42, wpb=65536, bsz=128, num_updates=57500, lr=0.000131876, gnorm=0.561, train_wall=227, gb_free=7.9, wall=18400
2022-05-16 22:58:16 | INFO | train_inner | epoch 039:   1436 / 1478 loss=4.605, ppl=24.34, wps=27332.3, ups=0.42, wpb=65536, bsz=128, num_updates=57600, lr=0.000131762, gnorm=0.561, train_wall=227, gb_free=7.9, wall=18640
2022-05-16 22:59:55 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-05-16 23:01:08 | INFO | valid | epoch 039 | valid on 'valid' subset | loss 4.739 | ppl 26.71 | wps 74151 | wpb 2047.4 | bsz 4 | num_updates 57642 | best_loss 4.739
2022-05-16 23:01:08 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 39 @ 57642 updates
2022-05-16 23:01:08 | INFO | fairseq.trainer | Saving checkpoint to /cluster/work/cotterell/liam/master-thesis/checkpoints/language_model/standard/checkpoint_best_standard_32_5.0000E-04_full.pt
2022-05-16 23:01:09 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/work/cotterell/liam/master-thesis/checkpoints/language_model/standard/checkpoint_best_standard_32_5.0000E-04_full.pt
2022-05-16 23:01:09 | INFO | fairseq.checkpoint_utils | Saved checkpoint checkpoints/language_model/standard/checkpoint_best_standard_32_5.0000E-04_full.pt (epoch 39 @ 57642 updates, score 4.739) (writing took 1.8924523680470884 seconds)
2022-05-16 23:01:09 | INFO | fairseq_cli.train | end of epoch 39 (average epoch stats below)
2022-05-16 23:01:09 | INFO | train | epoch 039 | loss 4.591 | ppl 24.1 | wps 25267.8 | ups 0.39 | wpb 65507.3 | bsz 127.9 | num_updates 57642 | lr 0.000131714 | gnorm 0.555 | train_wall 3368 | gb_free 7.9 | wall 18813
2022-05-16 23:01:10 | INFO | fairseq.data.iterators | grouped total_num_itrs = 1478
2022-05-16 23:01:10 | INFO | fairseq.trainer | begin training epoch 40
2022-05-16 23:01:10 | INFO | fairseq_cli.train | Start iterating over samples
2022-05-16 23:03:29 | INFO | train_inner | epoch 040:     58 / 1478 loss=4.579, ppl=23.9, wps=20819.6, ups=0.32, wpb=65126.4, bsz=127.2, num_updates=57700, lr=0.000131647, gnorm=0.557, train_wall=226, gb_free=7.9, wall=18952
2022-05-16 23:07:28 | INFO | train_inner | epoch 040:    158 / 1478 loss=4.566, ppl=23.68, wps=27378.1, ups=0.42, wpb=65536, bsz=128, num_updates=57800, lr=0.000131533, gnorm=0.557, train_wall=227, gb_free=7.9, wall=19192
2022-05-16 23:11:56 | INFO | train_inner | epoch 040:    258 / 1478 loss=4.558, ppl=23.55, wps=24474.4, ups=0.37, wpb=65525.8, bsz=128, num_updates=57900, lr=0.00013142, gnorm=0.557, train_wall=232, gb_free=7.9, wall=19460
2022-05-16 23:16:29 | INFO | train_inner | epoch 040:    358 / 1478 loss=4.578, ppl=23.88, wps=23978, ups=0.37, wpb=65536, bsz=128, num_updates=58000, lr=0.000131306, gnorm=0.552, train_wall=225, gb_free=7.9, wall=19733
2022-05-16 23:20:48 | INFO | train_inner | epoch 040:    458 / 1478 loss=4.58, ppl=23.92, wps=25324.5, ups=0.39, wpb=65536, bsz=128, num_updates=58100, lr=0.000131193, gnorm=0.556, train_wall=228, gb_free=7.9, wall=19992
2022-05-16 23:25:05 | INFO | train_inner | epoch 040:    558 / 1478 loss=4.572, ppl=23.78, wps=25456.3, ups=0.39, wpb=65536, bsz=128, num_updates=58200, lr=0.000131081, gnorm=0.554, train_wall=230, gb_free=7.9, wall=20249
2022-05-16 23:29:14 | INFO | train_inner | epoch 040:    658 / 1478 loss=4.581, ppl=23.93, wps=26349.7, ups=0.4, wpb=65536, bsz=128, num_updates=58300, lr=0.000130968, gnorm=0.552, train_wall=228, gb_free=7.9, wall=20498
2022-05-16 23:33:25 | INFO | train_inner | epoch 040:    758 / 1478 loss=4.591, ppl=24.1, wps=26095.7, ups=0.4, wpb=65536, bsz=128, num_updates=58400, lr=0.000130856, gnorm=0.555, train_wall=234, gb_free=7.9, wall=20749
2022-05-16 23:37:27 | INFO | train_inner | epoch 040:    858 / 1478 loss=4.592, ppl=24.12, wps=27135.5, ups=0.41, wpb=65536, bsz=128, num_updates=58500, lr=0.000130744, gnorm=0.557, train_wall=227, gb_free=7.9, wall=20990
2022-05-16 23:41:30 | INFO | train_inner | epoch 040:    958 / 1478 loss=4.591, ppl=24.1, wps=26914.5, ups=0.41, wpb=65531, bsz=128, num_updates=58600, lr=0.000130632, gnorm=0.554, train_wall=227, gb_free=7.9, wall=21234
2022-05-16 23:45:37 | INFO | train_inner | epoch 040:   1058 / 1478 loss=4.605, ppl=24.34, wps=26528.9, ups=0.4, wpb=65536, bsz=128, num_updates=58700, lr=0.000130521, gnorm=0.556, train_wall=227, gb_free=7.9, wall=21481
2022-05-16 23:49:48 | INFO | train_inner | epoch 040:   1158 / 1478 loss=4.606, ppl=24.35, wps=26081.1, ups=0.4, wpb=65536, bsz=128, num_updates=58800, lr=0.00013041, gnorm=0.561, train_wall=232, gb_free=7.9, wall=21732
2022-05-16 23:53:52 | INFO | train_inner | epoch 040:   1258 / 1478 loss=4.601, ppl=24.27, wps=26918.2, ups=0.41, wpb=65536, bsz=128, num_updates=58900, lr=0.000130299, gnorm=0.556, train_wall=227, gb_free=7.9, wall=21976
2022-05-16 23:57:53 | INFO | train_inner | epoch 040:   1358 / 1478 loss=4.597, ppl=24.2, wps=27153.5, ups=0.41, wpb=65536, bsz=128, num_updates=59000, lr=0.000130189, gnorm=0.556, train_wall=227, gb_free=7.9, wall=22217
2022-05-17 00:01:57 | INFO | train_inner | epoch 040:   1458 / 1478 loss=4.612, ppl=24.45, wps=26874.6, ups=0.41, wpb=65536, bsz=128, num_updates=59100, lr=0.000130079, gnorm=0.56, train_wall=230, gb_free=7.9, wall=22461
2022-05-17 00:02:44 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-05-17 00:03:57 | INFO | valid | epoch 040 | valid on 'valid' subset | loss 4.74 | ppl 26.73 | wps 74031.8 | wpb 2047.4 | bsz 4 | num_updates 59120 | best_loss 4.739
2022-05-17 00:03:57 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 40 @ 59120 updates
2022-05-17 00:03:57 | INFO | fairseq_cli.train | end of epoch 40 (average epoch stats below)
2022-05-17 00:03:57 | INFO | train | epoch 040 | loss 4.587 | ppl 24.03 | wps 25700.5 | ups 0.39 | wpb 65507.3 | bsz 127.9 | num_updates 59120 | lr 0.000130057 | gnorm 0.556 | train_wall 3378 | gb_free 7.9 | wall 22581
2022-05-17 00:03:57 | INFO | fairseq.data.iterators | grouped total_num_itrs = 1478
2022-05-17 00:03:57 | INFO | fairseq.trainer | begin training epoch 41
2022-05-17 00:03:57 | INFO | fairseq_cli.train | Start iterating over samples
2022-05-17 00:07:10 | INFO | train_inner | epoch 041:     80 / 1478 loss=4.563, ppl=23.64, wps=20842.4, ups=0.32, wpb=65126.4, bsz=127.2, num_updates=59200, lr=0.000129969, gnorm=0.563, train_wall=226, gb_free=7.9, wall=22773
2022-05-17 00:11:20 | INFO | train_inner | epoch 041:    180 / 1478 loss=4.561, ppl=23.6, wps=26186.4, ups=0.4, wpb=65536, bsz=128, num_updates=59300, lr=0.000129859, gnorm=0.56, train_wall=233, gb_free=7.9, wall=23024
2022-05-17 00:15:25 | INFO | train_inner | epoch 041:    280 / 1478 loss=4.564, ppl=23.65, wps=26693.9, ups=0.41, wpb=65536, bsz=128, num_updates=59400, lr=0.00012975, gnorm=0.556, train_wall=228, gb_free=7.9, wall=23269
2022-05-17 00:19:30 | INFO | train_inner | epoch 041:    380 / 1478 loss=4.573, ppl=23.8, wps=26763.2, ups=0.41, wpb=65531, bsz=128, num_updates=59500, lr=0.000129641, gnorm=0.56, train_wall=230, gb_free=7.9, wall=23514
2022-05-17 00:23:32 | INFO | train_inner | epoch 041:    480 / 1478 loss=4.571, ppl=23.77, wps=27127.5, ups=0.41, wpb=65536, bsz=128, num_updates=59600, lr=0.000129532, gnorm=0.558, train_wall=227, gb_free=7.9, wall=23756
2022-05-17 00:27:35 | INFO | train_inner | epoch 041:    580 / 1478 loss=4.577, ppl=23.87, wps=26910.4, ups=0.41, wpb=65536, bsz=128, num_updates=59700, lr=0.000129423, gnorm=0.556, train_wall=228, gb_free=7.9, wall=23999
2022-05-17 00:31:40 | INFO | train_inner | epoch 041:    680 / 1478 loss=4.579, ppl=23.9, wps=26741.3, ups=0.41, wpb=65536, bsz=128, num_updates=59800, lr=0.000129315, gnorm=0.558, train_wall=231, gb_free=7.9, wall=24244
2022-05-17 00:35:43 | INFO | train_inner | epoch 041:    780 / 1478 loss=4.586, ppl=24.02, wps=27028, ups=0.41, wpb=65525.8, bsz=128, num_updates=59900, lr=0.000129207, gnorm=0.553, train_wall=228, gb_free=7.9, wall=24487
2022-05-17 00:39:54 | INFO | train_inner | epoch 041:    880 / 1478 loss=4.588, ppl=24.06, wps=26117.1, ups=0.4, wpb=65536, bsz=128, num_updates=60000, lr=0.000129099, gnorm=0.55, train_wall=237, gb_free=7.9, wall=24738
2022-05-17 00:43:56 | INFO | train_inner | epoch 041:    980 / 1478 loss=4.591, ppl=24.1, wps=27038.7, ups=0.41, wpb=65536, bsz=128, num_updates=60100, lr=0.000128992, gnorm=0.562, train_wall=227, gb_free=7.9, wall=24980
2022-05-17 00:47:58 | INFO | train_inner | epoch 041:   1080 / 1478 loss=4.595, ppl=24.16, wps=27121.4, ups=0.41, wpb=65536, bsz=128, num_updates=60200, lr=0.000128885, gnorm=0.568, train_wall=228, gb_free=7.9, wall=25222
2022-05-17 00:52:07 | INFO | train_inner | epoch 041:   1180 / 1478 loss=4.593, ppl=24.14, wps=26309.2, ups=0.4, wpb=65536, bsz=128, num_updates=60300, lr=0.000128778, gnorm=0.555, train_wall=236, gb_free=7.9, wall=25471
2022-05-17 00:56:08 | INFO | train_inner | epoch 041:   1280 / 1478 loss=4.594, ppl=24.15, wps=27175.7, ups=0.41, wpb=65536, bsz=128, num_updates=60400, lr=0.000128671, gnorm=0.554, train_wall=228, gb_free=7.9, wall=25712
2022-05-17 01:00:13 | INFO | train_inner | epoch 041:   1380 / 1478 loss=4.609, ppl=24.4, wps=26768, ups=0.41, wpb=65536, bsz=128, num_updates=60500, lr=0.000128565, gnorm=0.556, train_wall=228, gb_free=7.9, wall=25957
2022-05-17 01:04:39 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-05-17 01:05:52 | INFO | valid | epoch 041 | valid on 'valid' subset | loss 4.735 | ppl 26.63 | wps 74066.5 | wpb 2047.4 | bsz 4 | num_updates 60598 | best_loss 4.735
2022-05-17 01:05:52 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 41 @ 60598 updates
2022-05-17 01:05:52 | INFO | fairseq.trainer | Saving checkpoint to /cluster/work/cotterell/liam/master-thesis/checkpoints/language_model/standard/checkpoint_best_standard_32_5.0000E-04_full.pt
2022-05-17 01:05:55 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/work/cotterell/liam/master-thesis/checkpoints/language_model/standard/checkpoint_best_standard_32_5.0000E-04_full.pt
2022-05-17 01:05:55 | INFO | fairseq.checkpoint_utils | Saved checkpoint checkpoints/language_model/standard/checkpoint_best_standard_32_5.0000E-04_full.pt (epoch 41 @ 60598 updates, score 4.735) (writing took 2.2138409418985248 seconds)
2022-05-17 01:05:55 | INFO | fairseq_cli.train | end of epoch 41 (average epoch stats below)
2022-05-17 01:05:55 | INFO | train | epoch 041 | loss 4.583 | ppl 23.96 | wps 26042 | ups 0.4 | wpb 65507.3 | bsz 127.9 | num_updates 60598 | lr 0.000128461 | gnorm 0.557 | train_wall 3395 | gb_free 7.9 | wall 26298
2022-05-17 01:05:55 | INFO | fairseq.data.iterators | grouped total_num_itrs = 1478
2022-05-17 01:05:55 | INFO | fairseq.trainer | begin training epoch 42
2022-05-17 01:05:55 | INFO | fairseq_cli.train | Start iterating over samples
2022-05-17 01:06:01 | INFO | train_inner | epoch 042:      2 / 1478 loss=4.597, ppl=24.19, wps=18690.4, ups=0.29, wpb=65126.4, bsz=127.2, num_updates=60600, lr=0.000128459, gnorm=0.555, train_wall=230, gb_free=7.9, wall=26305
2022-05-17 01:10:37 | INFO | train_inner | epoch 042:    102 / 1478 loss=4.545, ppl=23.35, wps=23746.5, ups=0.36, wpb=65536, bsz=128, num_updates=60700, lr=0.000128353, gnorm=0.562, train_wall=226, gb_free=7.9, wall=26581
2022-05-17 01:15:00 | INFO | train_inner | epoch 042:    202 / 1478 loss=4.561, ppl=23.61, wps=24966.5, ups=0.38, wpb=65536, bsz=128, num_updates=60800, lr=0.000128247, gnorm=0.565, train_wall=226, gb_free=7.9, wall=26844
2022-05-17 01:19:56 | INFO | train_inner | epoch 042:    302 / 1478 loss=4.558, ppl=23.56, wps=22112.3, ups=0.34, wpb=65536, bsz=128, num_updates=60900, lr=0.000128142, gnorm=0.563, train_wall=233, gb_free=7.9, wall=27140
2022-05-17 01:24:21 | INFO | train_inner | epoch 042:    402 / 1478 loss=4.574, ppl=23.82, wps=24759.1, ups=0.38, wpb=65536, bsz=128, num_updates=61000, lr=0.000128037, gnorm=0.555, train_wall=226, gb_free=7.9, wall=27405
2022-05-17 01:28:36 | INFO | train_inner | epoch 042:    502 / 1478 loss=4.568, ppl=23.71, wps=25663.5, ups=0.39, wpb=65531, bsz=128, num_updates=61100, lr=0.000127932, gnorm=0.56, train_wall=227, gb_free=7.9, wall=27660
2022-05-17 01:32:46 | INFO | train_inner | epoch 042:    602 / 1478 loss=4.572, ppl=23.78, wps=26200.1, ups=0.4, wpb=65536, bsz=128, num_updates=61200, lr=0.000127827, gnorm=0.558, train_wall=227, gb_free=7.9, wall=27910
2022-05-17 01:37:01 | INFO | train_inner | epoch 042:    702 / 1478 loss=4.579, ppl=23.9, wps=25684.7, ups=0.39, wpb=65536, bsz=128, num_updates=61300, lr=0.000127723, gnorm=0.559, train_wall=233, gb_free=7.9, wall=28165
2022-05-17 01:41:09 | INFO | train_inner | epoch 042:    802 / 1478 loss=4.569, ppl=23.74, wps=26530.3, ups=0.4, wpb=65536, bsz=128, num_updates=61400, lr=0.000127619, gnorm=0.554, train_wall=231, gb_free=7.9, wall=28412
2022-05-17 01:45:43 | INFO | train_inner | epoch 042:    902 / 1478 loss=4.584, ppl=23.98, wps=23903.1, ups=0.36, wpb=65536, bsz=128, num_updates=61500, lr=0.000127515, gnorm=0.552, train_wall=228, gb_free=7.9, wall=28687
2022-05-17 01:50:23 | INFO | train_inner | epoch 042:   1002 / 1478 loss=4.601, ppl=24.26, wps=23349.7, ups=0.36, wpb=65536, bsz=128, num_updates=61600, lr=0.000127412, gnorm=0.558, train_wall=230, gb_free=7.9, wall=28967
2022-05-17 01:54:46 | INFO | train_inner | epoch 042:   1102 / 1478 loss=4.595, ppl=24.16, wps=24953.5, ups=0.38, wpb=65536, bsz=128, num_updates=61700, lr=0.000127309, gnorm=0.563, train_wall=228, gb_free=7.9, wall=29230
2022-05-17 01:58:59 | INFO | train_inner | epoch 042:   1202 / 1478 loss=4.585, ppl=24, wps=25920.9, ups=0.4, wpb=65536, bsz=128, num_updates=61800, lr=0.000127205, gnorm=0.557, train_wall=228, gb_free=7.9, wall=29483
2022-05-17 02:03:09 | INFO | train_inner | epoch 042:   1302 / 1478 loss=4.586, ppl=24.03, wps=26241.9, ups=0.4, wpb=65525.8, bsz=128, num_updates=61900, lr=0.000127103, gnorm=0.562, train_wall=230, gb_free=7.9, wall=29732
2022-05-17 02:07:15 | INFO | train_inner | epoch 042:   1402 / 1478 loss=4.606, ppl=24.35, wps=26616, ups=0.41, wpb=65536, bsz=128, num_updates=62000, lr=0.000127, gnorm=0.558, train_wall=228, gb_free=7.9, wall=29979
2022-05-17 02:10:22 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-05-17 02:11:36 | INFO | valid | epoch 042 | valid on 'valid' subset | loss 4.735 | ppl 26.63 | wps 73437.8 | wpb 2047.4 | bsz 4 | num_updates 62076 | best_loss 4.735
2022-05-17 02:11:36 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 42 @ 62076 updates
2022-05-17 02:11:36 | INFO | fairseq.trainer | Saving checkpoint to /cluster/work/cotterell/liam/master-thesis/checkpoints/language_model/standard/checkpoint_best_standard_32_5.0000E-04_full.pt
2022-05-17 02:11:38 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/work/cotterell/liam/master-thesis/checkpoints/language_model/standard/checkpoint_best_standard_32_5.0000E-04_full.pt
2022-05-17 02:11:38 | INFO | fairseq.checkpoint_utils | Saved checkpoint checkpoints/language_model/standard/checkpoint_best_standard_32_5.0000E-04_full.pt (epoch 42 @ 62076 updates, score 4.735) (writing took 2.1681552631780505 seconds)
2022-05-17 02:11:38 | INFO | fairseq_cli.train | end of epoch 42 (average epoch stats below)
2022-05-17 02:11:38 | INFO | train | epoch 042 | loss 4.579 | ppl 23.9 | wps 24551.8 | ups 0.37 | wpb 65507.3 | bsz 127.9 | num_updates 62076 | lr 0.000126922 | gnorm 0.56 | train_wall 3380 | gb_free 7.9 | wall 30242
2022-05-17 02:11:38 | INFO | fairseq.data.iterators | grouped total_num_itrs = 1478
2022-05-17 02:11:38 | INFO | fairseq.trainer | begin training epoch 43
2022-05-17 02:11:38 | INFO | fairseq_cli.train | Start iterating over samples
2022-05-17 02:12:40 | INFO | train_inner | epoch 043:     24 / 1478 loss=4.589, ppl=24.07, wps=20025.2, ups=0.31, wpb=65126.4, bsz=127.2, num_updates=62100, lr=0.000126898, gnorm=0.563, train_wall=230, gb_free=7.9, wall=30304
2022-05-17 02:16:42 | INFO | train_inner | epoch 043:    124 / 1478 loss=4.545, ppl=23.34, wps=27045.4, ups=0.41, wpb=65531, bsz=128, num_updates=62200, lr=0.000126796, gnorm=0.557, train_wall=227, gb_free=7.9, wall=30546
2022-05-17 02:20:50 | INFO | train_inner | epoch 043:    224 / 1478 loss=4.563, ppl=23.64, wps=26447.3, ups=0.4, wpb=65536, bsz=128, num_updates=62300, lr=0.000126694, gnorm=0.565, train_wall=228, gb_free=7.9, wall=30794
2022-05-17 02:25:42 | INFO | train_inner | epoch 043:    324 / 1478 loss=4.555, ppl=23.5, wps=22472.9, ups=0.34, wpb=65536, bsz=128, num_updates=62400, lr=0.000126592, gnorm=0.566, train_wall=231, gb_free=7.9, wall=31086
2022-05-17 02:30:10 | INFO | train_inner | epoch 043:    424 / 1478 loss=4.56, ppl=23.59, wps=24439.5, ups=0.37, wpb=65536, bsz=128, num_updates=62500, lr=0.000126491, gnorm=0.562, train_wall=229, gb_free=7.9, wall=31354
2022-05-17 02:34:29 | INFO | train_inner | epoch 043:    524 / 1478 loss=4.56, ppl=23.59, wps=25325.2, ups=0.39, wpb=65536, bsz=128, num_updates=62600, lr=0.00012639, gnorm=0.561, train_wall=229, gb_free=7.9, wall=31612
2022-05-17 02:38:38 | INFO | train_inner | epoch 043:    624 / 1478 loss=4.572, ppl=23.79, wps=26265, ups=0.4, wpb=65536, bsz=128, num_updates=62700, lr=0.000126289, gnorm=0.56, train_wall=227, gb_free=7.9, wall=31862
2022-05-17 02:42:49 | INFO | train_inner | epoch 043:    724 / 1478 loss=4.563, ppl=23.64, wps=26129, ups=0.4, wpb=65536, bsz=128, num_updates=62800, lr=0.000126189, gnorm=0.559, train_wall=232, gb_free=7.9, wall=32113
2022-05-17 02:46:54 | INFO | train_inner | epoch 043:    824 / 1478 loss=4.582, ppl=23.95, wps=26745.4, ups=0.41, wpb=65536, bsz=128, num_updates=62900, lr=0.000126088, gnorm=0.56, train_wall=227, gb_free=7.9, wall=32358
2022-05-17 02:50:55 | INFO | train_inner | epoch 043:    924 / 1478 loss=4.575, ppl=23.83, wps=27141.2, ups=0.41, wpb=65536, bsz=128, num_updates=63000, lr=0.000125988, gnorm=0.554, train_wall=228, gb_free=7.9, wall=32599
2022-05-17 02:54:58 | INFO | train_inner | epoch 043:   1024 / 1478 loss=4.577, ppl=23.87, wps=27023.3, ups=0.41, wpb=65525.8, bsz=128, num_updates=63100, lr=0.000125888, gnorm=0.559, train_wall=229, gb_free=7.9, wall=32842
2022-05-17 02:58:58 | INFO | train_inner | epoch 043:   1124 / 1478 loss=4.594, ppl=24.16, wps=27253.1, ups=0.42, wpb=65536, bsz=128, num_updates=63200, lr=0.000125789, gnorm=0.569, train_wall=228, gb_free=7.9, wall=33082
2022-05-17 03:03:08 | INFO | train_inner | epoch 043:   1224 / 1478 loss=4.595, ppl=24.16, wps=26256.8, ups=0.4, wpb=65536, bsz=128, num_updates=63300, lr=0.000125689, gnorm=0.559, train_wall=232, gb_free=7.9, wall=33332
2022-05-17 03:07:10 | INFO | train_inner | epoch 043:   1324 / 1478 loss=4.604, ppl=24.32, wps=27030.6, ups=0.41, wpb=65536, bsz=128, num_updates=63400, lr=0.00012559, gnorm=0.561, train_wall=228, gb_free=7.9, wall=33574
2022-05-17 03:11:16 | INFO | train_inner | epoch 043:   1424 / 1478 loss=4.601, ppl=24.27, wps=26692.9, ups=0.41, wpb=65536, bsz=128, num_updates=63500, lr=0.000125491, gnorm=0.565, train_wall=231, gb_free=7.9, wall=33820
2022-05-17 03:13:25 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-05-17 03:14:39 | INFO | valid | epoch 043 | valid on 'valid' subset | loss 4.734 | ppl 26.61 | wps 73983.2 | wpb 2047.4 | bsz 4 | num_updates 63554 | best_loss 4.734
2022-05-17 03:14:39 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 43 @ 63554 updates
2022-05-17 03:14:39 | INFO | fairseq.trainer | Saving checkpoint to /cluster/work/cotterell/liam/master-thesis/checkpoints/language_model/standard/checkpoint_best_standard_32_5.0000E-04_full.pt
2022-05-17 03:14:41 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/work/cotterell/liam/master-thesis/checkpoints/language_model/standard/checkpoint_best_standard_32_5.0000E-04_full.pt
2022-05-17 03:14:41 | INFO | fairseq.checkpoint_utils | Saved checkpoint checkpoints/language_model/standard/checkpoint_best_standard_32_5.0000E-04_full.pt (epoch 43 @ 63554 updates, score 4.734) (writing took 1.980485511943698 seconds)
2022-05-17 03:14:41 | INFO | fairseq_cli.train | end of epoch 43 (average epoch stats below)
2022-05-17 03:14:41 | INFO | train | epoch 043 | loss 4.575 | ppl 23.83 | wps 25595.6 | ups 0.39 | wpb 65507.3 | bsz 127.9 | num_updates 63554 | lr 0.000125438 | gnorm 0.561 | train_wall 3383 | gb_free 7.9 | wall 34025
2022-05-17 03:14:41 | INFO | fairseq.data.iterators | grouped total_num_itrs = 1478
2022-05-17 03:14:41 | INFO | fairseq.trainer | begin training epoch 44
2022-05-17 03:14:41 | INFO | fairseq_cli.train | Start iterating over samples
2022-05-17 03:16:32 | INFO | train_inner | epoch 044:     46 / 1478 loss=4.562, ppl=23.63, wps=20618.9, ups=0.32, wpb=65126.4, bsz=127.2, num_updates=63600, lr=0.000125392, gnorm=0.56, train_wall=226, gb_free=7.9, wall=34136
2022-05-17 03:20:38 | INFO | train_inner | epoch 044:    146 / 1478 loss=4.538, ppl=23.23, wps=26648.4, ups=0.41, wpb=65536, bsz=128, num_updates=63700, lr=0.000125294, gnorm=0.562, train_wall=230, gb_free=7.9, wall=34382
2022-05-17 03:25:00 | INFO | train_inner | epoch 044:    246 / 1478 loss=4.558, ppl=23.56, wps=25029.1, ups=0.38, wpb=65536, bsz=128, num_updates=63800, lr=0.000125196, gnorm=0.56, train_wall=228, gb_free=7.9, wall=34643
2022-05-17 03:29:39 | INFO | train_inner | epoch 044:    346 / 1478 loss=4.555, ppl=23.5, wps=23485.5, ups=0.36, wpb=65531, bsz=128, num_updates=63900, lr=0.000125098, gnorm=0.562, train_wall=226, gb_free=7.9, wall=34923
2022-05-17 03:34:06 | INFO | train_inner | epoch 044:    446 / 1478 loss=4.553, ppl=23.48, wps=24492.1, ups=0.37, wpb=65536, bsz=128, num_updates=64000, lr=0.000125, gnorm=0.561, train_wall=229, gb_free=7.9, wall=35190
2022-05-17 03:38:33 | INFO | train_inner | epoch 044:    546 / 1478 loss=4.568, ppl=23.71, wps=24568.3, ups=0.37, wpb=65536, bsz=128, num_updates=64100, lr=0.000124902, gnorm=0.559, train_wall=232, gb_free=7.9, wall=35457
2022-05-17 03:42:45 | INFO | train_inner | epoch 044:    646 / 1478 loss=4.569, ppl=23.74, wps=26050.6, ups=0.4, wpb=65525.8, bsz=128, num_updates=64200, lr=0.000124805, gnorm=0.561, train_wall=229, gb_free=7.9, wall=35708
2022-05-17 03:47:27 | INFO | train_inner | epoch 044:    746 / 1478 loss=4.573, ppl=23.81, wps=23234.7, ups=0.35, wpb=65536, bsz=128, num_updates=64300, lr=0.000124708, gnorm=0.567, train_wall=231, gb_free=7.9, wall=35990
2022-05-17 03:52:09 | INFO | train_inner | epoch 044:    846 / 1478 loss=4.58, ppl=23.92, wps=23234.8, ups=0.35, wpb=65536, bsz=128, num_updates=64400, lr=0.000124611, gnorm=0.56, train_wall=230, gb_free=7.9, wall=36273
2022-05-17 03:56:39 | INFO | train_inner | epoch 044:    946 / 1478 loss=4.578, ppl=23.89, wps=24283.2, ups=0.37, wpb=65536, bsz=128, num_updates=64500, lr=0.000124515, gnorm=0.563, train_wall=232, gb_free=7.9, wall=36542
2022-05-17 04:00:54 | INFO | train_inner | epoch 044:   1046 / 1478 loss=4.579, ppl=23.91, wps=25689.3, ups=0.39, wpb=65536, bsz=128, num_updates=64600, lr=0.000124418, gnorm=0.561, train_wall=230, gb_free=7.9, wall=36797
2022-05-17 04:05:03 | INFO | train_inner | epoch 044:   1146 / 1478 loss=4.578, ppl=23.88, wps=26325.4, ups=0.4, wpb=65536, bsz=128, num_updates=64700, lr=0.000124322, gnorm=0.557, train_wall=228, gb_free=7.9, wall=37046
2022-05-17 04:09:10 | INFO | train_inner | epoch 044:   1246 / 1478 loss=4.583, ppl=23.96, wps=26496.1, ups=0.4, wpb=65536, bsz=128, num_updates=64800, lr=0.000124226, gnorm=0.564, train_wall=231, gb_free=7.9, wall=37294
2022-05-17 04:13:19 | INFO | train_inner | epoch 044:   1346 / 1478 loss=4.59, ppl=24.09, wps=26284, ups=0.4, wpb=65536, bsz=128, num_updates=64900, lr=0.00012413, gnorm=0.564, train_wall=231, gb_free=7.9, wall=37543
2022-05-17 04:17:27 | INFO | train_inner | epoch 044:   1446 / 1478 loss=4.602, ppl=24.28, wps=26421.2, ups=0.4, wpb=65536, bsz=128, num_updates=65000, lr=0.000124035, gnorm=0.562, train_wall=233, gb_free=7.9, wall=37791
2022-05-17 04:18:43 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-05-17 04:19:57 | INFO | valid | epoch 044 | valid on 'valid' subset | loss 4.732 | ppl 26.57 | wps 73565.1 | wpb 2047.4 | bsz 4 | num_updates 65032 | best_loss 4.732
2022-05-17 04:19:57 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 44 @ 65032 updates
2022-05-17 04:19:57 | INFO | fairseq.trainer | Saving checkpoint to /cluster/work/cotterell/liam/master-thesis/checkpoints/language_model/standard/checkpoint_best_standard_32_5.0000E-04_full.pt
2022-05-17 04:19:59 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/work/cotterell/liam/master-thesis/checkpoints/language_model/standard/checkpoint_best_standard_32_5.0000E-04_full.pt
2022-05-17 04:19:59 | INFO | fairseq.checkpoint_utils | Saved checkpoint checkpoints/language_model/standard/checkpoint_best_standard_32_5.0000E-04_full.pt (epoch 44 @ 65032 updates, score 4.732) (writing took 1.9576605479232967 seconds)
2022-05-17 04:19:59 | INFO | fairseq_cli.train | end of epoch 44 (average epoch stats below)
2022-05-17 04:19:59 | INFO | train | epoch 044 | loss 4.571 | ppl 23.77 | wps 24712.6 | ups 0.38 | wpb 65507.3 | bsz 127.9 | num_updates 65032 | lr 0.000124004 | gnorm 0.562 | train_wall 3397 | gb_free 7.9 | wall 37942
2022-05-17 04:19:59 | INFO | fairseq.data.iterators | grouped total_num_itrs = 1478
2022-05-17 04:19:59 | INFO | fairseq.trainer | begin training epoch 45
2022-05-17 04:19:59 | INFO | fairseq_cli.train | Start iterating over samples
2022-05-17 04:22:43 | INFO | train_inner | epoch 045:     68 / 1478 loss=4.55, ppl=23.42, wps=20655.6, ups=0.32, wpb=65126.4, bsz=127.2, num_updates=65100, lr=0.000123939, gnorm=0.564, train_wall=226, gb_free=7.9, wall=38106
2022-05-17 04:26:43 | INFO | train_inner | epoch 045:    168 / 1478 loss=4.537, ppl=23.21, wps=27252.1, ups=0.42, wpb=65536, bsz=128, num_updates=65200, lr=0.000123844, gnorm=0.563, train_wall=228, gb_free=7.9, wall=38347
2022-05-17 04:30:48 | INFO | train_inner | epoch 045:    268 / 1478 loss=4.544, ppl=23.33, wps=26799.6, ups=0.41, wpb=65536, bsz=128, num_updates=65300, lr=0.000123749, gnorm=0.565, train_wall=230, gb_free=7.9, wall=38591
2022-05-17 04:34:52 | INFO | train_inner | epoch 045:    368 / 1478 loss=4.559, ppl=23.57, wps=26800.8, ups=0.41, wpb=65531, bsz=128, num_updates=65400, lr=0.000123655, gnorm=0.563, train_wall=229, gb_free=7.9, wall=38836
2022-05-17 04:38:55 | INFO | train_inner | epoch 045:    468 / 1478 loss=4.567, ppl=23.7, wps=27020.1, ups=0.41, wpb=65536, bsz=128, num_updates=65500, lr=0.00012356, gnorm=0.563, train_wall=228, gb_free=7.9, wall=39079
2022-05-17 04:43:00 | INFO | train_inner | epoch 045:    568 / 1478 loss=4.55, ppl=23.43, wps=26738.1, ups=0.41, wpb=65536, bsz=128, num_updates=65600, lr=0.000123466, gnorm=0.561, train_wall=230, gb_free=7.9, wall=39324
2022-05-17 04:47:01 | INFO | train_inner | epoch 045:    668 / 1478 loss=4.58, ppl=23.92, wps=27137.1, ups=0.41, wpb=65536, bsz=128, num_updates=65700, lr=0.000123372, gnorm=0.561, train_wall=228, gb_free=7.9, wall=39565
2022-05-17 04:51:12 | INFO | train_inner | epoch 045:    768 / 1478 loss=4.571, ppl=23.76, wps=26099, ups=0.4, wpb=65536, bsz=128, num_updates=65800, lr=0.000123278, gnorm=0.562, train_wall=235, gb_free=7.9, wall=39816
2022-05-17 04:55:14 | INFO | train_inner | epoch 045:    868 / 1478 loss=4.581, ppl=23.93, wps=27159.6, ups=0.41, wpb=65525.8, bsz=128, num_updates=65900, lr=0.000123185, gnorm=0.56, train_wall=227, gb_free=7.9, wall=40058
2022-05-17 04:59:14 | INFO | train_inner | epoch 045:    968 / 1478 loss=4.577, ppl=23.87, wps=27298.9, ups=0.42, wpb=65536, bsz=128, num_updates=66000, lr=0.000123091, gnorm=0.562, train_wall=227, gb_free=7.9, wall=40298
2022-05-17 05:03:21 | INFO | train_inner | epoch 045:   1068 / 1478 loss=4.573, ppl=23.8, wps=26450.9, ups=0.4, wpb=65536, bsz=128, num_updates=66100, lr=0.000122998, gnorm=0.566, train_wall=234, gb_free=7.9, wall=40545
2022-05-17 05:08:03 | INFO | train_inner | epoch 045:   1168 / 1478 loss=4.582, ppl=23.95, wps=23314, ups=0.36, wpb=65536, bsz=128, num_updates=66200, lr=0.000122905, gnorm=0.563, train_wall=226, gb_free=7.9, wall=40826
2022-05-17 05:12:37 | INFO | train_inner | epoch 045:   1268 / 1478 loss=4.581, ppl=23.93, wps=23911.4, ups=0.36, wpb=65536, bsz=128, num_updates=66300, lr=0.000122813, gnorm=0.569, train_wall=229, gb_free=7.9, wall=41101
2022-05-17 05:16:55 | INFO | train_inner | epoch 045:   1368 / 1478 loss=4.58, ppl=23.91, wps=25369.2, ups=0.39, wpb=65536, bsz=128, num_updates=66400, lr=0.00012272, gnorm=0.566, train_wall=227, gb_free=7.9, wall=41359
2022-05-17 05:21:11 | INFO | train_inner | epoch 045:   1468 / 1478 loss=4.587, ppl=24.03, wps=25584.5, ups=0.39, wpb=65536, bsz=128, num_updates=66500, lr=0.000122628, gnorm=0.57, train_wall=233, gb_free=7.9, wall=41615
2022-05-17 05:21:38 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-05-17 05:22:51 | INFO | valid | epoch 045 | valid on 'valid' subset | loss 4.729 | ppl 26.53 | wps 73155.8 | wpb 2047.4 | bsz 4 | num_updates 66510 | best_loss 4.729
2022-05-17 05:22:51 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 45 @ 66510 updates
2022-05-17 05:22:51 | INFO | fairseq.trainer | Saving checkpoint to /cluster/work/cotterell/liam/master-thesis/checkpoints/language_model/standard/checkpoint_best_standard_32_5.0000E-04_full.pt
2022-05-17 05:22:53 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/work/cotterell/liam/master-thesis/checkpoints/language_model/standard/checkpoint_best_standard_32_5.0000E-04_full.pt
2022-05-17 05:22:53 | INFO | fairseq.checkpoint_utils | Saved checkpoint checkpoints/language_model/standard/checkpoint_best_standard_32_5.0000E-04_full.pt (epoch 45 @ 66510 updates, score 4.729) (writing took 2.011658715084195 seconds)
2022-05-17 05:22:53 | INFO | fairseq_cli.train | end of epoch 45 (average epoch stats below)
2022-05-17 05:22:53 | INFO | train | epoch 045 | loss 4.568 | ppl 23.71 | wps 25648.1 | ups 0.39 | wpb 65507.3 | bsz 127.9 | num_updates 66510 | lr 0.000122619 | gnorm 0.564 | train_wall 3389 | gb_free 7.9 | wall 41717
2022-05-17 05:22:54 | INFO | fairseq.data.iterators | grouped total_num_itrs = 1478
2022-05-17 05:22:54 | INFO | fairseq.trainer | begin training epoch 46
2022-05-17 05:22:54 | INFO | fairseq_cli.train | Start iterating over samples
2022-05-17 05:26:33 | INFO | train_inner | epoch 046:     90 / 1478 loss=4.53, ppl=23.11, wps=20247.1, ups=0.31, wpb=65126.4, bsz=127.2, num_updates=66600, lr=0.000122536, gnorm=0.563, train_wall=229, gb_free=7.9, wall=41937
2022-05-17 05:30:40 | INFO | train_inner | epoch 046:    190 / 1478 loss=4.541, ppl=23.28, wps=26535.1, ups=0.4, wpb=65536, bsz=128, num_updates=66700, lr=0.000122444, gnorm=0.56, train_wall=227, gb_free=7.9, wall=42184
2022-05-17 05:34:48 | INFO | train_inner | epoch 046:    290 / 1478 loss=4.55, ppl=23.43, wps=26404.7, ups=0.4, wpb=65536, bsz=128, num_updates=66800, lr=0.000122352, gnorm=0.569, train_wall=232, gb_free=7.9, wall=42432
2022-05-17 05:38:49 | INFO | train_inner | epoch 046:    390 / 1478 loss=4.56, ppl=23.58, wps=27144.5, ups=0.41, wpb=65536, bsz=128, num_updates=66900, lr=0.000122261, gnorm=0.563, train_wall=227, gb_free=7.9, wall=42673
2022-05-17 05:42:54 | INFO | train_inner | epoch 046:    490 / 1478 loss=4.55, ppl=23.43, wps=26747.3, ups=0.41, wpb=65536, bsz=128, num_updates=67000, lr=0.000122169, gnorm=0.558, train_wall=231, gb_free=7.9, wall=42918
2022-05-17 05:47:00 | INFO | train_inner | epoch 046:    590 / 1478 loss=4.549, ppl=23.41, wps=26637.6, ups=0.41, wpb=65536, bsz=128, num_updates=67100, lr=0.000122078, gnorm=0.569, train_wall=228, gb_free=7.9, wall=43164
2022-05-17 05:51:02 | INFO | train_inner | epoch 046:    690 / 1478 loss=4.56, ppl=23.59, wps=27096.3, ups=0.41, wpb=65536, bsz=128, num_updates=67200, lr=0.000121988, gnorm=0.559, train_wall=228, gb_free=7.9, wall=43406
2022-05-17 05:55:06 | INFO | train_inner | epoch 046:    790 / 1478 loss=4.577, ppl=23.88, wps=26866.9, ups=0.41, wpb=65531, bsz=128, num_updates=67300, lr=0.000121897, gnorm=0.564, train_wall=230, gb_free=7.9, wall=43650
2022-05-17 05:59:06 | INFO | train_inner | epoch 046:    890 / 1478 loss=4.567, ppl=23.7, wps=27286.8, ups=0.42, wpb=65536, bsz=128, num_updates=67400, lr=0.000121806, gnorm=0.565, train_wall=227, gb_free=7.9, wall=43890
2022-05-17 06:03:11 | INFO | train_inner | epoch 046:    990 / 1478 loss=4.584, ppl=23.99, wps=26816.2, ups=0.41, wpb=65536, bsz=128, num_updates=67500, lr=0.000121716, gnorm=0.564, train_wall=227, gb_free=7.9, wall=44135
2022-05-17 06:07:18 | INFO | train_inner | epoch 046:   1090 / 1478 loss=4.56, ppl=23.58, wps=26566.4, ups=0.41, wpb=65536, bsz=128, num_updates=67600, lr=0.000121626, gnorm=0.559, train_wall=233, gb_free=7.9, wall=44381
2022-05-17 06:11:17 | INFO | train_inner | epoch 046:   1190 / 1478 loss=4.58, ppl=23.91, wps=27315.6, ups=0.42, wpb=65536, bsz=128, num_updates=67700, lr=0.000121536, gnorm=0.561, train_wall=227, gb_free=7.9, wall=44621
2022-05-17 06:15:22 | INFO | train_inner | epoch 046:   1290 / 1478 loss=4.582, ppl=23.95, wps=26840, ups=0.41, wpb=65536, bsz=128, num_updates=67800, lr=0.000121447, gnorm=0.564, train_wall=231, gb_free=7.9, wall=44865
2022-05-17 06:19:22 | INFO | train_inner | epoch 046:   1390 / 1478 loss=4.584, ppl=23.98, wps=27316.3, ups=0.42, wpb=65536, bsz=128, num_updates=67900, lr=0.000121357, gnorm=0.564, train_wall=227, gb_free=7.9, wall=45105
2022-05-17 06:23:13 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-05-17 06:24:26 | INFO | valid | epoch 046 | valid on 'valid' subset | loss 4.727 | ppl 26.49 | wps 73942.6 | wpb 2047.4 | bsz 4 | num_updates 67988 | best_loss 4.727
2022-05-17 06:24:26 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 46 @ 67988 updates
2022-05-17 06:24:26 | INFO | fairseq.trainer | Saving checkpoint to /cluster/work/cotterell/liam/master-thesis/checkpoints/language_model/standard/checkpoint_best_standard_32_5.0000E-04_full.pt
2022-05-17 06:24:29 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/work/cotterell/liam/master-thesis/checkpoints/language_model/standard/checkpoint_best_standard_32_5.0000E-04_full.pt
2022-05-17 06:24:29 | INFO | fairseq.checkpoint_utils | Saved checkpoint checkpoints/language_model/standard/checkpoint_best_standard_32_5.0000E-04_full.pt (epoch 46 @ 67988 updates, score 4.727) (writing took 2.07519961707294 seconds)
2022-05-17 06:24:29 | INFO | fairseq_cli.train | end of epoch 46 (average epoch stats below)
2022-05-17 06:24:29 | INFO | train | epoch 046 | loss 4.564 | ppl 23.65 | wps 26202.4 | ups 0.4 | wpb 65507.3 | bsz 127.9 | num_updates 67988 | lr 0.000121279 | gnorm 0.563 | train_wall 3384 | gb_free 7.9 | wall 45412
2022-05-17 06:24:29 | INFO | fairseq.data.iterators | grouped total_num_itrs = 1478
2022-05-17 06:24:29 | INFO | fairseq.trainer | begin training epoch 47
2022-05-17 06:24:29 | INFO | fairseq_cli.train | Start iterating over samples
2022-05-17 06:25:08 | INFO | train_inner | epoch 047:     12 / 1478 loss=4.584, ppl=23.99, wps=18812.5, ups=0.29, wpb=65116.2, bsz=127.2, num_updates=68000, lr=0.000121268, gnorm=0.569, train_wall=231, gb_free=7.9, wall=45452
2022-05-17 06:29:45 | INFO | train_inner | epoch 047:    112 / 1478 loss=4.536, ppl=23.2, wps=23609.9, ups=0.36, wpb=65536, bsz=128, num_updates=68100, lr=0.000121179, gnorm=0.564, train_wall=225, gb_free=7.9, wall=45729
2022-05-17 06:34:15 | INFO | train_inner | epoch 047:    212 / 1478 loss=4.556, ppl=23.53, wps=24297.7, ups=0.37, wpb=65525.8, bsz=128, num_updates=68200, lr=0.00012109, gnorm=0.566, train_wall=226, gb_free=7.9, wall=45999
2022-05-17 06:39:07 | INFO | train_inner | epoch 047:    312 / 1478 loss=4.548, ppl=23.4, wps=22464.5, ups=0.34, wpb=65536, bsz=128, num_updates=68300, lr=0.000121001, gnorm=0.568, train_wall=228, gb_free=7.9, wall=46291
2022-05-17 06:43:38 | INFO | train_inner | epoch 047:    412 / 1478 loss=4.545, ppl=23.35, wps=24161.3, ups=0.37, wpb=65536, bsz=128, num_updates=68400, lr=0.000120913, gnorm=0.566, train_wall=226, gb_free=7.9, wall=46562
2022-05-17 06:48:03 | INFO | train_inner | epoch 047:    512 / 1478 loss=4.543, ppl=23.31, wps=24734, ups=0.38, wpb=65536, bsz=128, num_updates=68500, lr=0.000120824, gnorm=0.563, train_wall=233, gb_free=7.9, wall=46827
2022-05-17 06:52:12 | INFO | train_inner | epoch 047:    612 / 1478 loss=4.563, ppl=23.64, wps=26340.1, ups=0.4, wpb=65536, bsz=128, num_updates=68600, lr=0.000120736, gnorm=0.569, train_wall=229, gb_free=7.9, wall=47076
2022-05-17 06:56:16 | INFO | train_inner | epoch 047:    712 / 1478 loss=4.557, ppl=23.54, wps=26849.2, ups=0.41, wpb=65536, bsz=128, num_updates=68700, lr=0.000120648, gnorm=0.564, train_wall=228, gb_free=7.9, wall=47320
2022-05-17 07:00:17 | INFO | train_inner | epoch 047:    812 / 1478 loss=4.56, ppl=23.59, wps=27177, ups=0.41, wpb=65536, bsz=128, num_updates=68800, lr=0.000120561, gnorm=0.564, train_wall=227, gb_free=7.9, wall=47561
2022-05-17 07:04:25 | INFO | train_inner | epoch 047:    912 / 1478 loss=4.563, ppl=23.63, wps=26392.8, ups=0.4, wpb=65536, bsz=128, num_updates=68900, lr=0.000120473, gnorm=0.567, train_wall=234, gb_free=7.9, wall=47809
2022-05-17 07:08:26 | INFO | train_inner | epoch 047:   1012 / 1478 loss=4.559, ppl=23.56, wps=27226, ups=0.42, wpb=65536, bsz=128, num_updates=69000, lr=0.000120386, gnorm=0.559, train_wall=227, gb_free=7.9, wall=48050
2022-05-17 07:12:26 | INFO | train_inner | epoch 047:   1112 / 1478 loss=4.58, ppl=23.92, wps=27280.5, ups=0.42, wpb=65536, bsz=128, num_updates=69100, lr=0.000120299, gnorm=0.566, train_wall=227, gb_free=7.9, wall=48290
2022-05-17 07:17:01 | INFO | train_inner | epoch 047:   1212 / 1478 loss=4.583, ppl=23.97, wps=23812, ups=0.36, wpb=65536, bsz=128, num_updates=69200, lr=0.000120212, gnorm=0.568, train_wall=231, gb_free=7.9, wall=48565
2022-05-17 07:21:36 | INFO | train_inner | epoch 047:   1312 / 1478 loss=4.572, ppl=23.79, wps=23831, ups=0.36, wpb=65531, bsz=128, num_updates=69300, lr=0.000120125, gnorm=0.568, train_wall=226, gb_free=7.9, wall=48840
2022-05-17 07:25:58 | INFO | train_inner | epoch 047:   1412 / 1478 loss=4.573, ppl=23.79, wps=25069, ups=0.38, wpb=65536, bsz=128, num_updates=69400, lr=0.000120038, gnorm=0.567, train_wall=228, gb_free=7.9, wall=49102
2022-05-17 07:28:42 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-05-17 07:29:54 | INFO | valid | epoch 047 | valid on 'valid' subset | loss 4.728 | ppl 26.49 | wps 74562.2 | wpb 2047.4 | bsz 4 | num_updates 69466 | best_loss 4.727
2022-05-17 07:29:54 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 47 @ 69466 updates
2022-05-17 07:29:54 | INFO | fairseq_cli.train | end of epoch 47 (average epoch stats below)
2022-05-17 07:29:54 | INFO | train | epoch 047 | loss 4.561 | ppl 23.6 | wps 24666.2 | ups 0.38 | wpb 65507.3 | bsz 127.9 | num_updates 69466 | lr 0.000119981 | gnorm 0.566 | train_wall 3372 | gb_free 7.9 | wall 49338
2022-05-17 07:29:54 | INFO | fairseq.data.iterators | grouped total_num_itrs = 1478
2022-05-17 07:29:54 | INFO | fairseq.trainer | begin training epoch 48
2022-05-17 07:29:54 | INFO | fairseq_cli.train | Start iterating over samples
2022-05-17 07:31:25 | INFO | train_inner | epoch 048:     34 / 1478 loss=4.56, ppl=23.59, wps=19914.3, ups=0.31, wpb=65126.4, bsz=127.2, num_updates=69500, lr=0.000119952, gnorm=0.566, train_wall=230, gb_free=7.9, wall=49429
2022-05-17 07:35:30 | INFO | train_inner | epoch 048:    134 / 1478 loss=4.533, ppl=23.15, wps=26763.3, ups=0.41, wpb=65536, bsz=128, num_updates=69600, lr=0.000119866, gnorm=0.567, train_wall=227, gb_free=7.9, wall=49674
2022-05-17 07:39:48 | INFO | train_inner | epoch 048:    234 / 1478 loss=4.548, ppl=23.39, wps=25376.3, ups=0.39, wpb=65531, bsz=128, num_updates=69700, lr=0.00011978, gnorm=0.565, train_wall=228, gb_free=7.9, wall=49932
2022-05-17 07:44:32 | INFO | train_inner | epoch 048:    334 / 1478 loss=4.536, ppl=23.19, wps=23045, ups=0.35, wpb=65536, bsz=128, num_updates=69800, lr=0.000119694, gnorm=0.564, train_wall=229, gb_free=7.9, wall=50216
2022-05-17 07:48:55 | INFO | train_inner | epoch 048:    434 / 1478 loss=4.545, ppl=23.34, wps=24924.1, ups=0.38, wpb=65536, bsz=128, num_updates=69900, lr=0.000119608, gnorm=0.565, train_wall=226, gb_free=7.9, wall=50479
2022-05-17 07:53:37 | INFO | train_inner | epoch 048:    534 / 1478 loss=4.553, ppl=23.47, wps=23255.7, ups=0.35, wpb=65536, bsz=128, num_updates=70000, lr=0.000119523, gnorm=0.568, train_wall=230, gb_free=7.9, wall=50761
2022-05-17 07:58:10 | INFO | train_inner | epoch 048:    634 / 1478 loss=4.554, ppl=23.5, wps=24032.9, ups=0.37, wpb=65536, bsz=128, num_updates=70100, lr=0.000119438, gnorm=0.569, train_wall=226, gb_free=7.9, wall=51034
2022-05-17 08:02:29 | INFO | train_inner | epoch 048:    734 / 1478 loss=4.561, ppl=23.61, wps=25327.1, ups=0.39, wpb=65536, bsz=128, num_updates=70200, lr=0.000119352, gnorm=0.564, train_wall=228, gb_free=7.9, wall=51292
2022-05-17 08:06:45 | INFO | train_inner | epoch 048:    834 / 1478 loss=4.556, ppl=23.53, wps=25582.4, ups=0.39, wpb=65536, bsz=128, num_updates=70300, lr=0.000119268, gnorm=0.562, train_wall=234, gb_free=7.9, wall=51549
2022-05-17 08:10:51 | INFO | train_inner | epoch 048:    934 / 1478 loss=4.567, ppl=23.7, wps=26597.8, ups=0.41, wpb=65536, bsz=128, num_updates=70400, lr=0.000119183, gnorm=0.569, train_wall=228, gb_free=7.9, wall=51795
2022-05-17 08:15:02 | INFO | train_inner | epoch 048:   1034 / 1478 loss=4.572, ppl=23.78, wps=26150.6, ups=0.4, wpb=65536, bsz=128, num_updates=70500, lr=0.000119098, gnorm=0.568, train_wall=235, gb_free=7.9, wall=52046
2022-05-17 08:19:07 | INFO | train_inner | epoch 048:   1134 / 1478 loss=4.562, ppl=23.62, wps=26740.3, ups=0.41, wpb=65536, bsz=128, num_updates=70600, lr=0.000119014, gnorm=0.565, train_wall=227, gb_free=7.9, wall=52291
2022-05-17 08:23:08 | INFO | train_inner | epoch 048:   1234 / 1478 loss=4.57, ppl=23.75, wps=27129.5, ups=0.41, wpb=65536, bsz=128, num_updates=70700, lr=0.00011893, gnorm=0.569, train_wall=228, gb_free=7.9, wall=52532
2022-05-17 08:27:18 | INFO | train_inner | epoch 048:   1334 / 1478 loss=4.58, ppl=23.91, wps=26272.2, ups=0.4, wpb=65525.8, bsz=128, num_updates=70800, lr=0.000118846, gnorm=0.568, train_wall=230, gb_free=7.9, wall=52782
2022-05-17 08:31:22 | INFO | train_inner | epoch 048:   1434 / 1478 loss=4.584, ppl=23.99, wps=26838.9, ups=0.41, wpb=65536, bsz=128, num_updates=70900, lr=0.000118762, gnorm=0.564, train_wall=227, gb_free=7.9, wall=53026
2022-05-17 08:33:07 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-05-17 08:34:21 | INFO | valid | epoch 048 | valid on 'valid' subset | loss 4.728 | ppl 26.51 | wps 73915.2 | wpb 2047.4 | bsz 4 | num_updates 70944 | best_loss 4.727
2022-05-17 08:34:21 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 48 @ 70944 updates
2022-05-17 08:34:21 | INFO | fairseq_cli.train | end of epoch 48 (average epoch stats below)
2022-05-17 08:34:21 | INFO | train | epoch 048 | loss 4.558 | ppl 23.55 | wps 25038.7 | ups 0.38 | wpb 65507.3 | bsz 127.9 | num_updates 70944 | lr 0.000118725 | gnorm 0.567 | train_wall 3385 | gb_free 7.9 | wall 53204
2022-05-17 08:34:21 | INFO | fairseq.data.iterators | grouped total_num_itrs = 1478
2022-05-17 08:34:21 | INFO | fairseq.trainer | begin training epoch 49
2022-05-17 08:34:21 | INFO | fairseq_cli.train | Start iterating over samples
2022-05-17 08:36:36 | INFO | train_inner | epoch 049:     56 / 1478 loss=4.543, ppl=23.31, wps=20725, ups=0.32, wpb=65126.4, bsz=127.2, num_updates=71000, lr=0.000118678, gnorm=0.572, train_wall=226, gb_free=7.9, wall=53340
2022-05-17 08:40:37 | INFO | train_inner | epoch 049:    156 / 1478 loss=4.536, ppl=23.19, wps=27266.8, ups=0.42, wpb=65536, bsz=128, num_updates=71100, lr=0.000118595, gnorm=0.571, train_wall=227, gb_free=7.9, wall=53580
2022-05-17 08:45:05 | INFO | train_inner | epoch 049:    256 / 1478 loss=4.535, ppl=23.18, wps=24443.4, ups=0.37, wpb=65536, bsz=128, num_updates=71200, lr=0.000118511, gnorm=0.564, train_wall=230, gb_free=7.9, wall=53849
2022-05-17 08:49:41 | INFO | train_inner | epoch 049:    356 / 1478 loss=4.541, ppl=23.28, wps=23701.5, ups=0.36, wpb=65536, bsz=128, num_updates=71300, lr=0.000118428, gnorm=0.571, train_wall=226, gb_free=7.9, wall=54125
2022-05-17 08:54:03 | INFO | train_inner | epoch 049:    456 / 1478 loss=4.545, ppl=23.34, wps=25009.9, ups=0.38, wpb=65536, bsz=128, num_updates=71400, lr=0.000118345, gnorm=0.572, train_wall=229, gb_free=7.9, wall=54387
2022-05-17 08:58:15 | INFO | train_inner | epoch 049:    556 / 1478 loss=4.546, ppl=23.36, wps=26038.2, ups=0.4, wpb=65536, bsz=128, num_updates=71500, lr=0.000118262, gnorm=0.569, train_wall=227, gb_free=7.9, wall=54639
2022-05-17 09:02:40 | INFO | train_inner | epoch 049:    656 / 1478 loss=4.548, ppl=23.39, wps=24749.7, ups=0.38, wpb=65536, bsz=128, num_updates=71600, lr=0.00011818, gnorm=0.568, train_wall=227, gb_free=7.9, wall=54904
2022-05-17 09:07:31 | INFO | train_inner | epoch 049:    756 / 1478 loss=4.546, ppl=23.35, wps=22525, ups=0.34, wpb=65536, bsz=128, num_updates=71700, lr=0.000118097, gnorm=0.567, train_wall=227, gb_free=7.9, wall=55194
2022-05-17 09:11:59 | INFO | train_inner | epoch 049:    856 / 1478 loss=4.56, ppl=23.58, wps=24406.3, ups=0.37, wpb=65536, bsz=128, num_updates=71800, lr=0.000118015, gnorm=0.567, train_wall=226, gb_free=7.9, wall=55463
2022-05-17 09:16:17 | INFO | train_inner | epoch 049:    956 / 1478 loss=4.567, ppl=23.71, wps=25413, ups=0.39, wpb=65536, bsz=128, num_updates=71900, lr=0.000117933, gnorm=0.577, train_wall=228, gb_free=7.9, wall=55721
2022-05-17 09:20:25 | INFO | train_inner | epoch 049:   1056 / 1478 loss=4.565, ppl=23.68, wps=26478.1, ups=0.4, wpb=65525.8, bsz=128, num_updates=72000, lr=0.000117851, gnorm=0.57, train_wall=227, gb_free=7.9, wall=55968
2022-05-17 09:24:27 | INFO | train_inner | epoch 049:   1156 / 1478 loss=4.573, ppl=23.81, wps=27005.6, ups=0.41, wpb=65531, bsz=128, num_updates=72100, lr=0.000117769, gnorm=0.566, train_wall=227, gb_free=7.9, wall=56211
2022-05-17 09:28:28 | INFO | train_inner | epoch 049:   1256 / 1478 loss=4.567, ppl=23.7, wps=27207.3, ups=0.42, wpb=65536, bsz=128, num_updates=72200, lr=0.000117688, gnorm=0.562, train_wall=227, gb_free=7.9, wall=56452
2022-05-17 09:32:29 | INFO | train_inner | epoch 049:   1356 / 1478 loss=4.575, ppl=23.84, wps=27250.6, ups=0.42, wpb=65536, bsz=128, num_updates=72300, lr=0.000117606, gnorm=0.565, train_wall=227, gb_free=7.9, wall=56692
2022-05-17 09:36:38 | INFO | train_inner | epoch 049:   1456 / 1478 loss=4.568, ppl=23.72, wps=26229.8, ups=0.4, wpb=65536, bsz=128, num_updates=72400, lr=0.000117525, gnorm=0.562, train_wall=234, gb_free=7.9, wall=56942
2022-05-17 09:37:30 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-05-17 09:38:43 | INFO | valid | epoch 049 | valid on 'valid' subset | loss 4.723 | ppl 26.41 | wps 73563.4 | wpb 2047.4 | bsz 4 | num_updates 72422 | best_loss 4.723
2022-05-17 09:38:43 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 49 @ 72422 updates
2022-05-17 09:38:43 | INFO | fairseq.trainer | Saving checkpoint to /cluster/work/cotterell/liam/master-thesis/checkpoints/language_model/standard/checkpoint_best_standard_32_5.0000E-04_full.pt
2022-05-17 09:38:45 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/work/cotterell/liam/master-thesis/checkpoints/language_model/standard/checkpoint_best_standard_32_5.0000E-04_full.pt
2022-05-17 09:38:45 | INFO | fairseq.checkpoint_utils | Saved checkpoint checkpoints/language_model/standard/checkpoint_best_standard_32_5.0000E-04_full.pt (epoch 49 @ 72422 updates, score 4.723) (writing took 1.9695897148922086 seconds)
2022-05-17 09:38:45 | INFO | fairseq_cli.train | end of epoch 49 (average epoch stats below)
2022-05-17 09:38:45 | INFO | train | epoch 049 | loss 4.554 | ppl 23.49 | wps 25054.2 | ups 0.38 | wpb 65507.3 | bsz 127.9 | num_updates 72422 | lr 0.000117507 | gnorm 0.568 | train_wall 3365 | gb_free 7.9 | wall 57069
2022-05-17 09:38:45 | INFO | fairseq.data.iterators | grouped total_num_itrs = 1478
2022-05-17 09:38:45 | INFO | fairseq.trainer | begin training epoch 50
2022-05-17 09:38:45 | INFO | fairseq_cli.train | Start iterating over samples
2022-05-17 09:41:52 | INFO | train_inner | epoch 050:     78 / 1478 loss=4.528, ppl=23.07, wps=20740.1, ups=0.32, wpb=65126.4, bsz=127.2, num_updates=72500, lr=0.000117444, gnorm=0.584, train_wall=226, gb_free=7.9, wall=57256
2022-05-17 09:45:53 | INFO | train_inner | epoch 050:    178 / 1478 loss=4.526, ppl=23.04, wps=27273.7, ups=0.42, wpb=65536, bsz=128, num_updates=72600, lr=0.000117363, gnorm=0.572, train_wall=227, gb_free=7.9, wall=57497
2022-05-17 09:50:26 | INFO | train_inner | epoch 050:    278 / 1478 loss=4.531, ppl=23.12, wps=23957.2, ups=0.37, wpb=65536, bsz=128, num_updates=72700, lr=0.000117282, gnorm=0.569, train_wall=227, gb_free=7.9, wall=57770
2022-05-17 09:55:00 | INFO | train_inner | epoch 050:    378 / 1478 loss=4.543, ppl=23.3, wps=23918.5, ups=0.36, wpb=65536, bsz=128, num_updates=72800, lr=0.000117202, gnorm=0.567, train_wall=228, gb_free=7.9, wall=58044
2022-05-17 09:59:22 | INFO | train_inner | epoch 050:    478 / 1478 loss=4.552, ppl=23.46, wps=25051.1, ups=0.38, wpb=65536, bsz=128, num_updates=72900, lr=0.000117121, gnorm=0.577, train_wall=230, gb_free=7.9, wall=58306
2022-05-17 10:03:33 | INFO | train_inner | epoch 050:    578 / 1478 loss=4.546, ppl=23.37, wps=26057.9, ups=0.4, wpb=65536, bsz=128, num_updates=73000, lr=0.000117041, gnorm=0.574, train_wall=228, gb_free=7.9, wall=58557
2022-05-17 10:07:42 | INFO | train_inner | epoch 050:    678 / 1478 loss=4.548, ppl=23.39, wps=26325.5, ups=0.4, wpb=65536, bsz=128, num_updates=73100, lr=0.000116961, gnorm=0.57, train_wall=228, gb_free=7.9, wall=58806
2022-05-17 10:11:53 | INFO | train_inner | epoch 050:    778 / 1478 loss=4.551, ppl=23.45, wps=26128, ups=0.4, wpb=65536, bsz=128, num_updates=73200, lr=0.000116881, gnorm=0.573, train_wall=230, gb_free=7.9, wall=59057
2022-05-17 10:15:57 | INFO | train_inner | epoch 050:    878 / 1478 loss=4.555, ppl=23.51, wps=26920.5, ups=0.41, wpb=65536, bsz=128, num_updates=73300, lr=0.000116801, gnorm=0.571, train_wall=229, gb_free=7.9, wall=59300
2022-05-17 10:20:03 | INFO | train_inner | epoch 050:    978 / 1478 loss=4.544, ppl=23.33, wps=26601.7, ups=0.41, wpb=65536, bsz=128, num_updates=73400, lr=0.000116722, gnorm=0.578, train_wall=232, gb_free=7.9, wall=59547
2022-05-17 10:24:05 | INFO | train_inner | epoch 050:   1078 / 1478 loss=4.568, ppl=23.72, wps=27063.7, ups=0.41, wpb=65531, bsz=128, num_updates=73500, lr=0.000116642, gnorm=0.572, train_wall=229, gb_free=7.9, wall=59789
2022-05-17 10:28:07 | INFO | train_inner | epoch 050:   1178 / 1478 loss=4.563, ppl=23.64, wps=27095.6, ups=0.41, wpb=65536, bsz=128, num_updates=73600, lr=0.000116563, gnorm=0.573, train_wall=227, gb_free=7.9, wall=60031
2022-05-17 10:32:08 | INFO | train_inner | epoch 050:   1278 / 1478 loss=4.559, ppl=23.57, wps=27240.9, ups=0.42, wpb=65536, bsz=128, num_updates=73700, lr=0.000116484, gnorm=0.57, train_wall=227, gb_free=7.9, wall=60271
2022-05-17 10:36:12 | INFO | train_inner | epoch 050:   1378 / 1478 loss=4.583, ppl=23.97, wps=26762.3, ups=0.41, wpb=65536, bsz=128, num_updates=73800, lr=0.000116405, gnorm=0.568, train_wall=232, gb_free=7.9, wall=60516
2022-05-17 10:40:13 | INFO | train_inner | epoch 050:   1478 / 1478 loss=4.578, ppl=23.88, wps=27105.3, ups=0.42, wpb=65116.2, bsz=127.2, num_updates=73900, lr=0.000116326, gnorm=0.571, train_wall=226, gb_free=7.9, wall=60757
2022-05-17 10:40:13 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-05-17 10:41:25 | INFO | valid | epoch 050 | valid on 'valid' subset | loss 4.721 | ppl 26.38 | wps 74577.2 | wpb 2047.4 | bsz 4 | num_updates 73900 | best_loss 4.721
2022-05-17 10:41:25 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 50 @ 73900 updates
2022-05-17 10:41:25 | INFO | fairseq.trainer | Saving checkpoint to /cluster/work/cotterell/liam/master-thesis/checkpoints/language_model/standard/checkpoint_best_standard_32_5.0000E-04_full.pt
2022-05-17 10:41:27 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/work/cotterell/liam/master-thesis/checkpoints/language_model/standard/checkpoint_best_standard_32_5.0000E-04_full.pt
2022-05-17 10:41:27 | INFO | fairseq.checkpoint_utils | Saved checkpoint checkpoints/language_model/standard/checkpoint_best_standard_32_5.0000E-04_full.pt (epoch 50 @ 73900 updates, score 4.721) (writing took 2.039627131074667 seconds)
2022-05-17 10:41:27 | INFO | fairseq_cli.train | end of epoch 50 (average epoch stats below)
2022-05-17 10:41:27 | INFO | train | epoch 050 | loss 4.551 | ppl 23.45 | wps 25734.7 | ups 0.39 | wpb 65507.3 | bsz 127.9 | num_updates 73900 | lr 0.000116326 | gnorm 0.573 | train_wall 3377 | gb_free 7.9 | wall 60831
2022-05-17 10:41:27 | INFO | fairseq.data.iterators | grouped total_num_itrs = 1478
2022-05-17 10:41:27 | INFO | fairseq.trainer | begin training epoch 51
2022-05-17 10:41:27 | INFO | fairseq_cli.train | Start iterating over samples
2022-05-17 10:46:15 | INFO | train_inner | epoch 051:    100 / 1478 loss=4.519, ppl=22.92, wps=18091.3, ups=0.28, wpb=65536, bsz=128, num_updates=74000, lr=0.000116248, gnorm=0.574, train_wall=228, gb_free=7.9, wall=61119
2022-05-17 10:50:41 | INFO | train_inner | epoch 051:    200 / 1478 loss=4.525, ppl=23.02, wps=24640.2, ups=0.38, wpb=65536, bsz=128, num_updates=74100, lr=0.000116169, gnorm=0.57, train_wall=227, gb_free=7.9, wall=61385
2022-05-17 10:54:56 | INFO | train_inner | epoch 051:    300 / 1478 loss=4.521, ppl=22.96, wps=25733.6, ups=0.39, wpb=65536, bsz=128, num_updates=74200, lr=0.000116091, gnorm=0.571, train_wall=228, gb_free=7.9, wall=61639
2022-05-17 10:59:02 | INFO | train_inner | epoch 051:    400 / 1478 loss=4.534, ppl=23.17, wps=26618.9, ups=0.41, wpb=65536, bsz=128, num_updates=74300, lr=0.000116013, gnorm=0.57, train_wall=227, gb_free=7.9, wall=61886
2022-05-17 11:03:07 | INFO | train_inner | epoch 051:    500 / 1478 loss=4.537, ppl=23.22, wps=26769.7, ups=0.41, wpb=65536, bsz=128, num_updates=74400, lr=0.000115935, gnorm=0.573, train_wall=227, gb_free=7.9, wall=62130
2022-05-17 11:07:22 | INFO | train_inner | epoch 051:    600 / 1478 loss=4.547, ppl=23.38, wps=25692.1, ups=0.39, wpb=65536, bsz=128, num_updates=74500, lr=0.000115857, gnorm=0.572, train_wall=234, gb_free=7.9, wall=62386
2022-05-17 11:11:34 | INFO | train_inner | epoch 051:    700 / 1478 loss=4.548, ppl=23.39, wps=25963.2, ups=0.4, wpb=65531, bsz=128, num_updates=74600, lr=0.000115779, gnorm=0.571, train_wall=226, gb_free=7.9, wall=62638
2022-05-17 11:15:42 | INFO | train_inner | epoch 051:    800 / 1478 loss=4.562, ppl=23.61, wps=26421.6, ups=0.4, wpb=65536, bsz=128, num_updates=74700, lr=0.000115702, gnorm=0.576, train_wall=228, gb_free=7.9, wall=62886
2022-05-17 11:19:51 | INFO | train_inner | epoch 051:    900 / 1478 loss=4.554, ppl=23.49, wps=26308.7, ups=0.4, wpb=65536, bsz=128, num_updates=74800, lr=0.000115624, gnorm=0.572, train_wall=230, gb_free=7.9, wall=63135
2022-05-17 11:23:54 | INFO | train_inner | epoch 051:   1000 / 1478 loss=4.558, ppl=23.56, wps=26991.4, ups=0.41, wpb=65536, bsz=128, num_updates=74900, lr=0.000115547, gnorm=0.576, train_wall=228, gb_free=7.9, wall=63378
2022-05-17 11:27:57 | INFO | train_inner | epoch 051:   1100 / 1478 loss=4.571, ppl=23.76, wps=27000.4, ups=0.41, wpb=65536, bsz=128, num_updates=75000, lr=0.00011547, gnorm=0.572, train_wall=229, gb_free=7.9, wall=63621
2022-05-17 11:31:59 | INFO | train_inner | epoch 051:   1200 / 1478 loss=4.547, ppl=23.37, wps=27037.7, ups=0.41, wpb=65536, bsz=128, num_updates=75100, lr=0.000115393, gnorm=0.566, train_wall=229, gb_free=7.9, wall=63863
2022-05-17 11:36:01 | INFO | train_inner | epoch 051:   1300 / 1478 loss=4.567, ppl=23.7, wps=27082.4, ups=0.41, wpb=65536, bsz=128, num_updates=75200, lr=0.000115316, gnorm=0.575, train_wall=229, gb_free=7.9, wall=64105
2022-05-17 11:40:09 | INFO | train_inner | epoch 051:   1400 / 1478 loss=4.566, ppl=23.69, wps=26459.7, ups=0.4, wpb=65536, bsz=128, num_updates=75300, lr=0.00011524, gnorm=0.574, train_wall=231, gb_free=7.9, wall=64353
2022-05-17 11:43:21 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-05-17 11:44:34 | INFO | valid | epoch 051 | valid on 'valid' subset | loss 4.721 | ppl 26.38 | wps 74621.2 | wpb 2047.4 | bsz 4 | num_updates 75378 | best_loss 4.721
2022-05-17 11:44:34 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 51 @ 75378 updates
2022-05-17 11:44:34 | INFO | fairseq.trainer | Saving checkpoint to /cluster/work/cotterell/liam/master-thesis/checkpoints/language_model/standard/checkpoint_best_standard_32_5.0000E-04_full.pt
2022-05-17 11:44:36 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/work/cotterell/liam/master-thesis/checkpoints/language_model/standard/checkpoint_best_standard_32_5.0000E-04_full.pt
2022-05-17 11:44:36 | INFO | fairseq.checkpoint_utils | Saved checkpoint checkpoints/language_model/standard/checkpoint_best_standard_32_5.0000E-04_full.pt (epoch 51 @ 75378 updates, score 4.721) (writing took 1.9650609781965613 seconds)
2022-05-17 11:44:36 | INFO | fairseq_cli.train | end of epoch 51 (average epoch stats below)
2022-05-17 11:44:36 | INFO | train | epoch 051 | loss 4.548 | ppl 23.4 | wps 25556.9 | ups 0.39 | wpb 65507.3 | bsz 127.9 | num_updates 75378 | lr 0.00011518 | gnorm 0.573 | train_wall 3374 | gb_free 7.9 | wall 64619
2022-05-17 11:44:36 | INFO | fairseq.data.iterators | grouped total_num_itrs = 1478
2022-05-17 11:44:36 | INFO | fairseq.trainer | begin training epoch 52
2022-05-17 11:44:36 | INFO | fairseq_cli.train | Start iterating over samples
2022-05-17 11:45:30 | INFO | train_inner | epoch 052:     22 / 1478 loss=4.563, ppl=23.63, wps=20263.3, ups=0.31, wpb=65116.2, bsz=127.2, num_updates=75400, lr=0.000115163, gnorm=0.575, train_wall=225, gb_free=7.9, wall=64674
2022-05-17 11:49:34 | INFO | train_inner | epoch 052:    122 / 1478 loss=4.514, ppl=22.85, wps=26874.8, ups=0.41, wpb=65536, bsz=128, num_updates=75500, lr=0.000115087, gnorm=0.573, train_wall=227, gb_free=7.9, wall=64918
2022-05-17 11:53:36 | INFO | train_inner | epoch 052:    222 / 1478 loss=4.525, ppl=23.02, wps=27101.3, ups=0.41, wpb=65536, bsz=128, num_updates=75600, lr=0.000115011, gnorm=0.574, train_wall=227, gb_free=7.9, wall=65160
2022-05-17 11:57:47 | INFO | train_inner | epoch 052:    322 / 1478 loss=4.527, ppl=23.05, wps=26104.9, ups=0.4, wpb=65536, bsz=128, num_updates=75700, lr=0.000114935, gnorm=0.573, train_wall=232, gb_free=7.9, wall=65411
2022-05-17 12:01:48 | INFO | train_inner | epoch 052:    422 / 1478 loss=4.537, ppl=23.22, wps=27229.6, ups=0.42, wpb=65536, bsz=128, num_updates=75800, lr=0.000114859, gnorm=0.572, train_wall=227, gb_free=7.9, wall=65651
2022-05-17 12:05:49 | INFO | train_inner | epoch 052:    522 / 1478 loss=4.546, ppl=23.36, wps=27120.2, ups=0.41, wpb=65536, bsz=128, num_updates=75900, lr=0.000114783, gnorm=0.573, train_wall=228, gb_free=7.9, wall=65893
2022-05-17 12:09:50 | INFO | train_inner | epoch 052:    622 / 1478 loss=4.551, ppl=23.44, wps=27240.5, ups=0.42, wpb=65536, bsz=128, num_updates=76000, lr=0.000114708, gnorm=0.571, train_wall=227, gb_free=7.9, wall=66134
2022-05-17 12:13:51 | INFO | train_inner | epoch 052:    722 / 1478 loss=4.55, ppl=23.42, wps=27148.4, ups=0.41, wpb=65536, bsz=128, num_updates=76100, lr=0.000114632, gnorm=0.571, train_wall=227, gb_free=7.9, wall=66375
2022-05-17 12:17:56 | INFO | train_inner | epoch 052:    822 / 1478 loss=4.541, ppl=23.27, wps=26788.9, ups=0.41, wpb=65531, bsz=128, num_updates=76200, lr=0.000114557, gnorm=0.566, train_wall=231, gb_free=7.9, wall=66620
2022-05-17 12:21:56 | INFO | train_inner | epoch 052:    922 / 1478 loss=4.549, ppl=23.41, wps=27339.7, ups=0.42, wpb=65536, bsz=128, num_updates=76300, lr=0.000114482, gnorm=0.568, train_wall=227, gb_free=7.9, wall=66859
2022-05-17 12:25:57 | INFO | train_inner | epoch 052:   1022 / 1478 loss=4.567, ppl=23.7, wps=27139.4, ups=0.41, wpb=65536, bsz=128, num_updates=76400, lr=0.000114407, gnorm=0.575, train_wall=227, gb_free=7.9, wall=67101
2022-05-17 12:30:02 | INFO | train_inner | epoch 052:   1122 / 1478 loss=4.564, ppl=23.66, wps=26699.6, ups=0.41, wpb=65536, bsz=128, num_updates=76500, lr=0.000114332, gnorm=0.575, train_wall=227, gb_free=7.9, wall=67346
2022-05-17 12:34:08 | INFO | train_inner | epoch 052:   1222 / 1478 loss=4.555, ppl=23.51, wps=26677.3, ups=0.41, wpb=65536, bsz=128, num_updates=76600, lr=0.000114258, gnorm=0.575, train_wall=227, gb_free=7.9, wall=67592
2022-05-17 12:38:15 | INFO | train_inner | epoch 052:   1322 / 1478 loss=4.562, ppl=23.62, wps=26523.1, ups=0.4, wpb=65525.8, bsz=128, num_updates=76700, lr=0.000114183, gnorm=0.574, train_wall=231, gb_free=7.9, wall=67839
2022-05-17 12:42:16 | INFO | train_inner | epoch 052:   1422 / 1478 loss=4.554, ppl=23.5, wps=27192, ups=0.41, wpb=65536, bsz=128, num_updates=76800, lr=0.000114109, gnorm=0.57, train_wall=227, gb_free=7.9, wall=68080
2022-05-17 12:44:29 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-05-17 12:45:44 | INFO | valid | epoch 052 | valid on 'valid' subset | loss 4.722 | ppl 26.39 | wps 72537.1 | wpb 2047.4 | bsz 4 | num_updates 76856 | best_loss 4.721
2022-05-17 12:45:44 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 52 @ 76856 updates
2022-05-17 12:45:44 | INFO | fairseq_cli.train | end of epoch 52 (average epoch stats below)
2022-05-17 12:45:44 | INFO | train | epoch 052 | loss 4.545 | ppl 23.35 | wps 26394.6 | ups 0.4 | wpb 65507.3 | bsz 127.9 | num_updates 76856 | lr 0.000114067 | gnorm 0.572 | train_wall 3367 | gb_free 7.9 | wall 68288
2022-05-17 12:45:44 | INFO | fairseq.data.iterators | grouped total_num_itrs = 1478
2022-05-17 12:45:44 | INFO | fairseq.trainer | begin training epoch 53
2022-05-17 12:45:44 | INFO | fairseq_cli.train | Start iterating over samples
2022-05-17 12:47:33 | INFO | train_inner | epoch 053:     44 / 1478 loss=4.528, ppl=23.07, wps=20585, ups=0.32, wpb=65126.4, bsz=127.2, num_updates=76900, lr=0.000114035, gnorm=0.579, train_wall=226, gb_free=7.9, wall=68396
2022-05-17 12:51:33 | INFO | train_inner | epoch 053:    144 / 1478 loss=4.511, ppl=22.8, wps=27223.9, ups=0.42, wpb=65536, bsz=128, num_updates=77000, lr=0.000113961, gnorm=0.573, train_wall=227, gb_free=7.9, wall=68637
2022-05-17 12:55:37 | INFO | train_inner | epoch 053:    244 / 1478 loss=4.52, ppl=22.94, wps=26855.7, ups=0.41, wpb=65536, bsz=128, num_updates=77100, lr=0.000113887, gnorm=0.572, train_wall=230, gb_free=7.9, wall=68881
2022-05-17 12:59:41 | INFO | train_inner | epoch 053:    344 / 1478 loss=4.539, ppl=23.25, wps=26891.7, ups=0.41, wpb=65536, bsz=128, num_updates=77200, lr=0.000113813, gnorm=0.579, train_wall=230, gb_free=7.9, wall=69125
2022-05-17 13:03:41 | INFO | train_inner | epoch 053:    444 / 1478 loss=4.526, ppl=23.04, wps=27309.5, ups=0.42, wpb=65525.8, bsz=128, num_updates=77300, lr=0.000113739, gnorm=0.57, train_wall=227, gb_free=7.9, wall=69365
2022-05-17 13:07:49 | INFO | train_inner | epoch 053:    544 / 1478 loss=4.542, ppl=23.3, wps=26393, ups=0.4, wpb=65536, bsz=128, num_updates=77400, lr=0.000113666, gnorm=0.58, train_wall=233, gb_free=7.9, wall=69613
2022-05-17 13:11:50 | INFO | train_inner | epoch 053:    644 / 1478 loss=4.544, ppl=23.33, wps=27180.1, ups=0.41, wpb=65536, bsz=128, num_updates=77500, lr=0.000113592, gnorm=0.571, train_wall=227, gb_free=7.9, wall=69854
2022-05-17 13:15:55 | INFO | train_inner | epoch 053:    744 / 1478 loss=4.54, ppl=23.26, wps=26825.7, ups=0.41, wpb=65536, bsz=128, num_updates=77600, lr=0.000113519, gnorm=0.571, train_wall=230, gb_free=7.9, wall=70099
2022-05-17 13:19:56 | INFO | train_inner | epoch 053:    844 / 1478 loss=4.551, ppl=23.44, wps=27136.8, ups=0.41, wpb=65536, bsz=128, num_updates=77700, lr=0.000113446, gnorm=0.57, train_wall=228, gb_free=7.9, wall=70340
2022-05-17 13:23:58 | INFO | train_inner | epoch 053:    944 / 1478 loss=4.562, ppl=23.63, wps=27093.5, ups=0.41, wpb=65536, bsz=128, num_updates=77800, lr=0.000113373, gnorm=0.579, train_wall=227, gb_free=7.9, wall=70582
2022-05-17 13:28:06 | INFO | train_inner | epoch 053:   1044 / 1478 loss=4.551, ppl=23.44, wps=26384.5, ups=0.4, wpb=65531, bsz=128, num_updates=77900, lr=0.0001133, gnorm=0.571, train_wall=234, gb_free=7.9, wall=70830
2022-05-17 13:32:06 | INFO | train_inner | epoch 053:   1144 / 1478 loss=4.549, ppl=23.41, wps=27351.9, ups=0.42, wpb=65536, bsz=128, num_updates=78000, lr=0.000113228, gnorm=0.573, train_wall=227, gb_free=7.9, wall=71070
2022-05-17 13:36:06 | INFO | train_inner | epoch 053:   1244 / 1478 loss=4.556, ppl=23.53, wps=27279.8, ups=0.42, wpb=65536, bsz=128, num_updates=78100, lr=0.000113155, gnorm=0.573, train_wall=227, gb_free=7.9, wall=71310
2022-05-17 13:40:21 | INFO | train_inner | epoch 053:   1344 / 1478 loss=4.558, ppl=23.55, wps=25766.1, ups=0.39, wpb=65536, bsz=128, num_updates=78200, lr=0.000113083, gnorm=0.573, train_wall=229, gb_free=7.9, wall=71564
2022-05-17 13:45:03 | INFO | train_inner | epoch 053:   1444 / 1478 loss=4.558, ppl=23.56, wps=23203.5, ups=0.35, wpb=65536, bsz=128, num_updates=78300, lr=0.000113011, gnorm=0.571, train_wall=226, gb_free=7.9, wall=71847
User defined signal 2
