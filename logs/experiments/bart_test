Sender: LSF System <lsfadmin@eu-g2-18>
Subject: Job 221679536: <bart_no_500_0.0000E+00_0.0000E+00> in cluster <euler> Done

Job <bart_no_500_0.0000E+00_0.0000E+00> was submitted from host <eu-login-06> by user <euler_username> in cluster <euler> at Wed Jun 15 14:15:33 2022
Job was executed on host(s) <4*eu-g2-18>, in queue <gpu.4h>, as user <euler_username> in cluster <euler> at Wed Jun 15 14:15:54 2022
</cluster/home/euler_username> was used as the home directory.
</cluster/work/cotterell/liam/master-thesis> was used as the working directory.
Started at Wed Jun 15 14:15:54 2022
Terminated at Wed Jun 15 15:06:28 2022
Results reported at Wed Jun 15 15:06:28 2022

Your job looked like:

------------------------------------------------------------
# LSBATCH: User input
src/generate.sh bart xsum-summarizer-no-500-bart 0.0 0.0
------------------------------------------------------------

Successfully completed.

Resource usage summary:

    CPU time :                                   2785.46 sec.
    Max Memory :                                 5992 MB
    Average Memory :                             3806.68 MB
    Total Requested Memory :                     8192.00 MB
    Delta Memory :                               2200.00 MB
    Max Swap :                                   -
    Max Processes :                              5
    Max Threads :                                14
    Run time :                                   3030 sec.
    Turnaround time :                            3055 sec.

The output (if any) follows:

2022-06-15 14:17:44 | INFO | fairseq.tasks.text_to_speech | Please install tensorboardX: pip install tensorboardX
2022-06-15 14:17:52 | INFO | fairseq_cli.generate | {'_name': None, 'common': {'_name': None, 'no_progress_bar': True, 'log_interval': 100, 'log_format': 'none', 'log_file': None, 'tensorboard_logdir': None, 'wandb_project': None, 'azureml_logging': False, 'seed': 1, 'cpu': False, 'tpu': False, 'bf16': False, 'memory_efficient_bf16': False, 'fp16': False, 'memory_efficient_fp16': False, 'fp16_no_flatten_grads': False, 'fp16_init_scale': 128, 'fp16_scale_window': None, 'fp16_scale_tolerance': 0.0, 'on_cpu_convert_precision': False, 'min_loss_scale': 0.0001, 'threshold_loss_scale': None, 'amp': False, 'amp_batch_retries': 2, 'amp_init_scale': 128, 'amp_scale_window': None, 'user_dir': None, 'empty_cache_freq': 0, 'all_gather_list_size': 16384, 'model_parallel_size': 1, 'quantization_config_path': None, 'profile': False, 'reset_logging': False, 'suppress_crashes': False, 'use_plasma_view': False, 'plasma_path': '/tmp/plasma'}, 'common_eval': {'_name': None, 'path': 'checkpoints/summarization_model/bart/checkpoint_best.pt', 'post_process': None, 'quiet': False, 'model_overrides': '{}', 'results_path': None, 'print_tokens': False}, 'distributed_training': {'_name': None, 'distributed_world_size': 1, 'distributed_num_procs': 1, 'distributed_rank': 0, 'distributed_backend': 'nccl', 'distributed_init_method': None, 'distributed_port': -1, 'device_id': 0, 'distributed_no_spawn': False, 'ddp_backend': 'pytorch_ddp', 'ddp_comm_hook': 'none', 'bucket_cap_mb': 25, 'fix_batches_to_gpus': False, 'find_unused_parameters': False, 'gradient_as_bucket_view': False, 'fast_stat_sync': False, 'heartbeat_timeout': -1, 'broadcast_buffers': False, 'slowmo_momentum': None, 'slowmo_base_algorithm': 'localsgd', 'localsgd_frequency': 3, 'nprocs_per_node': 1, 'pipeline_model_parallel': False, 'pipeline_balance': None, 'pipeline_devices': None, 'pipeline_chunks': 0, 'pipeline_encoder_balance': None, 'pipeline_encoder_devices': None, 'pipeline_decoder_balance': None, 'pipeline_decoder_devices': None, 'pipeline_checkpoint': 'never', 'zero_sharding': 'none', 'fp16': False, 'memory_efficient_fp16': False, 'tpu': False, 'no_reshard_after_forward': False, 'fp32_reduce_scatter': False, 'cpu_offload': False, 'use_sharded_state': False, 'not_fsdp_flatten_parameters': False}, 'dataset': {'_name': None, 'num_workers': 1, 'skip_invalid_size_inputs_valid_test': False, 'max_tokens': None, 'batch_size': 4, 'required_batch_size_multiple': 8, 'required_seq_len_multiple': 1, 'dataset_impl': None, 'data_buffer_size': 10, 'train_subset': 'train', 'valid_subset': 'valid', 'combine_valid_subsets': None, 'ignore_unused_valid_subsets': False, 'validate_interval': 1, 'validate_interval_updates': 0, 'validate_after_updates': 0, 'fixed_validation_seed': None, 'disable_validation': False, 'max_tokens_valid': None, 'batch_size_valid': 4, 'max_valid_steps': None, 'curriculum': 0, 'gen_subset': 'test', 'num_shards': 1, 'shard_id': 0, 'grouped_shuffling': False, 'update_epoch_batch_itr': False, 'update_ordered_indices_seed': False}, 'optimization': {'_name': None, 'max_epoch': 0, 'max_update': 0, 'stop_time_hours': 0.0, 'clip_norm': 0.0, 'sentence_avg': False, 'update_freq': [1], 'lr': [0.25], 'stop_min_lr': -1.0, 'use_bmuf': False, 'skip_remainder_batch': False}, 'checkpoint': {'_name': None, 'save_dir': 'checkpoints', 'restore_file': 'checkpoint_last.pt', 'continue_once': None, 'finetune_from_model': None, 'reset_dataloader': False, 'reset_lr_scheduler': False, 'reset_meters': False, 'reset_optimizer': False, 'optimizer_overrides': '{}', 'save_interval': 1, 'save_interval_updates': 0, 'keep_interval_updates': -1, 'keep_interval_updates_pattern': -1, 'keep_last_epochs': -1, 'keep_best_checkpoints': -1, 'no_save': False, 'no_epoch_checkpoints': False, 'no_last_checkpoints': False, 'no_save_optimizer_state': False, 'best_checkpoint_metric': 'loss', 'maximize_best_checkpoint_metric': False, 'patience': -1, 'checkpoint_suffix': '', 'checkpoint_shard_count': 1, 'load_checkpoint_on_all_dp_ranks': False, 'write_checkpoints_asynchronously': False, 'model_parallel_size': 1}, 'bmuf': {'_name': None, 'block_lr': 1.0, 'block_momentum': 0.875, 'global_sync_iter': 50, 'warmup_iterations': 500, 'use_nbm': False, 'average_sync': False, 'distributed_world_size': 1}, 'generation': {'_name': None, 'beam': 5, 'nbest': 1, 'max_len_a': 0.0, 'max_len_b': 200, 'min_len': 1, 'match_source_len': False, 'unnormalized': False, 'no_early_stop': False, 'no_beamable_mm': False, 'lenpen': 1.0, 'unkpen': 0.0, 'replace_unk': None, 'sacrebleu': False, 'score_reference': False, 'prefix_size': 0, 'no_repeat_ngram_size': 0, 'sampling': False, 'sampling_topk': -1, 'sampling_topp': -1.0, 'constraints': None, 'temperature': 1.0, 'diverse_beam_groups': -1, 'diverse_beam_strength': 0.5, 'diversity_rate': -1.0, 'print_alignment': None, 'print_step': False, 'lm_path': 'checkpoints/language_model/bart/checkpoint_best.pt', 'ent_threshold': 0.0, 'lm_weight': -0.0, 'iter_decode_eos_penalty': 0.0, 'iter_decode_max_iter': 10, 'iter_decode_force_max_iter': False, 'iter_decode_with_beam': 1, 'iter_decode_with_external_reranker': False, 'retain_iter_history': False, 'retain_dropout': False, 'retain_dropout_modules': None, 'decoding_format': None, 'no_seed_provided': False}, 'eval_lm': {'_name': None, 'output_word_probs': False, 'output_word_stats': False, 'context_window': 0, 'softmax_batch': 9223372036854775807}, 'interactive': {'_name': None, 'buffer_size': 0, 'input': '-'}, 'model': {'_name': 'wav2vec2', 'extractor_mode': 'default', 'encoder_layers': 12, 'encoder_embed_dim': 768, 'encoder_ffn_embed_dim': 3072, 'encoder_attention_heads': 12, 'activation_fn': 'gelu', 'layer_type': 'transformer', 'dropout': 0.1, 'attention_dropout': 0.1, 'activation_dropout': 0.0, 'encoder_layerdrop': 0.0, 'dropout_input': 0.0, 'dropout_features': 0.0, 'final_dim': 0, 'layer_norm_first': False, 'conv_feature_layers': '[(512, 10, 5)] + [(512, 3, 2)] * 4 + [(512,2,2)] + [(512,2,2)]', 'conv_bias': False, 'logit_temp': 0.1, 'quantize_targets': False, 'quantize_input': False, 'same_quantizer': False, 'target_glu': False, 'feature_grad_mult': 1.0, 'quantizer_depth': 1, 'quantizer_factor': 3, 'latent_vars': 320, 'latent_groups': 2, 'latent_dim': 0, 'mask_length': 10, 'mask_prob': 0.65, 'mask_selection': 'static', 'mask_other': 0.0, 'no_mask_overlap': False, 'mask_min_space': 1, 'require_same_masks': True, 'mask_dropout': 0.0, 'mask_channel_length': 10, 'mask_channel_prob': 0.0, 'mask_channel_before': False, 'mask_channel_selection': 'static', 'mask_channel_other': 0.0, 'no_mask_channel_overlap': False, 'mask_channel_min_space': 1, 'num_negatives': 100, 'negatives_from_everywhere': False, 'cross_sample_negatives': 0, 'codebook_negatives': 0, 'conv_pos': 128, 'conv_pos_groups': 16, 'pos_conv_depth': 1, 'latent_temp': [2.0, 0.5, 0.999995], 'max_positions': 100000, 'checkpoint_activations': False, 'required_seq_len_multiple': 1, 'crop_seq_to_multiple': 1, 'depthwise_conv_kernel_size': 31, 'attn_type': '', 'pos_enc_type': 'abs', 'fp16': False}, 'task': {'_name': 'translation', 'data': 'data/xsum-summarizer-no-500-bart', 'source_lang': None, 'target_lang': None, 'load_alignments': False, 'left_pad_source': True, 'left_pad_target': False, 'max_source_positions': 1024, 'max_target_positions': 1024, 'upsample_primary': -1, 'truncate_source': True, 'num_batch_buckets': 0, 'train_subset': 'train', 'dataset_impl': None, 'required_seq_len_multiple': 1, 'eval_bleu': False, 'eval_bleu_args': '{}', 'eval_bleu_detok': 'space', 'eval_bleu_detok_args': '{}', 'eval_tokenized_bleu': False, 'eval_bleu_remove_bpe': None, 'eval_bleu_print_samples': False}, 'criterion': {'_name': 'cross_entropy', 'sentence_avg': True}, 'optimizer': None, 'lr_scheduler': {'_name': 'fixed', 'force_anneal': None, 'lr_shrink': 0.1, 'warmup_updates': 0, 'lr': [0.25]}, 'scoring': {'_name': 'bleu', 'pad': 1, 'eos': 2, 'unk': 3}, 'bpe': None, 'tokenizer': None, 'ema': {'_name': None, 'store_ema': False, 'ema_decay': 0.9999, 'ema_start_update': 0, 'ema_seed_model': None, 'ema_update_freq': 1, 'ema_fp32': False}, 'simul_type': None}
2022-06-15 14:17:53 | INFO | fairseq.tasks.translation | [source] dictionary: 50264 types
2022-06-15 14:17:53 | INFO | fairseq.tasks.translation | [target] dictionary: 50264 types
2022-06-15 14:17:53 | INFO | fairseq_cli.generate | loading model(s) from checkpoints/summarization_model/bart/checkpoint_best.pt
2022-06-15 14:18:07 | INFO | fairseq.data.data_utils | loaded 10,834 examples from: data/xsum-summarizer-no-500-bart/test.source-target.source
2022-06-15 14:18:08 | INFO | fairseq.data.data_utils | loaded 10,834 examples from: data/xsum-summarizer-no-500-bart/test.source-target.target
2022-06-15 14:18:08 | INFO | fairseq.tasks.translation | data/xsum-summarizer-no-500-bart test source-target 10834 examples
2022-06-15 14:18:10 | INFO | fairseq.tasks.language_modeling | dictionary: 50264 types
S-8715	36705 11 37886 290 3296 423 587 5989 17547 319 511 1919 2056 5504 13
T-8715	14731 6075 1008 11 262 8502 26777 508 2630 262 15694 12 14463 4776 329 41184 11 468 3724 287 257 3442 6614 7014 9722 8454 13
H-8715	-1.1740493774414062	51 7657 423 587 3432 284 3517 14015 12 34050 16002 1717 1375 263 272 11 508 468 3724 379 262 2479 286 6073 13
P-8715	-2.7989 -17.5116 -9.2762 -11.6856 -14.6495 -15.0404 -11.7402 -12.2657 -17.0733 -14.8511 -16.4289 -12.2906 -21.2077 -18.1812 -18.0618 -14.0007 -12.7983 -12.1410 -13.8663 -14.4114 -13.8411 -14.8047 -12.5178 -19.3766 -13.4936 -1.5458
RANK-8715	36572 21315 1614 11510 5637 3529 1522 86 418 10899 33423 940 25440 18436 1034 20 12797 1475 12100 3558 8928 9995 3558 5894 13 7
P_SM-8715	-2.7989 -17.5116 -9.2762 -11.6856 -14.6495 -15.0404 -11.7402 -12.2657 -17.0733 -14.8511 -16.4289 -12.2906 -21.2077 -18.1812 -18.0618 -14.0007 -12.7983 -12.1410 -13.8663 -14.4114 -13.8411 -14.8047 -12.5178 -19.3766 -13.4936 -1.5458
P_LM-8715	-6.3211 -14.8363 -13.8759 -9.0467 -11.7141 -14.8972 -12.1280 -11.0576 -18.9233 -18.2548 -11.7356 -7.7829 -22.0220 -22.7374 -16.3728 -14.9403 -10.4397 -14.1518 -17.6800 -14.2900 -20.1773 -18.5293 -16.8929 -19.5102 -15.7593 -15.3978
ENT_LM-8715	6.9740 2.4536 0.5935 2.0064 0.9206 3.6734 5.7376 5.5816 0.0319 0.0151 5.9030 1.3898 0.0264 0.0030 2.1641 1.0399 3.1936 2.4793 1.5557 0.5353 0.0459 0.0148 3.9931 0.8112 2.6301 4.9368
ENT_SM-8715	5.7424 1.9350 2.4740 2.0839 2.3919 6.7754 4.5803 6.4702 1.1872 1.0139 6.6071 0.9713 0.2041 0.1786 2.8048 1.7191 1.8412 1.0676 2.7347 1.7217 0.9245 1.5054 4.8123 2.4494 1.3695 6.8192
S-6584	32 6356 286 5205 422 1973 262 5510 15549 428 1285 25
T-6584	29398 12537 286 8916 11 3486 11 10193 290 8428
H-6584	-0.29274946451187134	29398 12537 286 8916 11 10193 11 7136 5382 290 8428 13
P-6584	-0.1534 -9.2769 -14.9729 -12.9156 -16.9883 -15.4986 -17.5144 -10.8338 -15.9936 -19.3686 -18.2111 -12.8036 -2.7820
RANK-6584	25647 6650 1328 955 978 13366 212 17984 16134 978 9619 9 5
P_SM-6584	-0.1534 -9.2769 -14.9729 -12.9156 -16.9883 -15.4986 -17.5144 -10.8338 -15.9936 -19.3686 -18.2111 -12.8036 -2.7820
P_LM-6584	-14.6911 -18.5256 -16.0312 -10.2617 -19.4381 -6.6950 -16.6170 -15.7705 -10.4283 -20.2182 -6.6108 -15.7445 -15.7038
ENT_LM-6584	2.4502 0.0454 2.4349 1.2307 1.9132 1.0895 3.2013 0.2542 0.9375 0.1398 2.2711 4.3759 4.7074
ENT_SM-6584	0.8519 3.6447 2.3301 2.4191 1.9384 2.5957 3.2671 4.2536 3.3378 0.7658 1.4175 1.0282 6.9279
S-3961	4342 318 607 1204 287 5986 25
T-3961	34 5049 2619 11 3025 3451 355 257 14015 290 8204 263 599 3577 517 621 2026 812 11 468 3724 379 262 2479 286 7724 13
H-3961	-1.310870885848999	464 40289 286 14457 468 3414 326 673 318 284 1577 4082 284 607 717 1200 11 257 2576 3706 12568 20970 13
P-3961	-3.1896 -11.1715 -13.7745 -15.7059 -11.1498 -13.4587 -14.8821 -15.3484 -13.0422 -12.7762 -12.1813 -14.2598 -15.3596 -14.7055 -15.5690 -15.3794 -14.2559 -14.9207 -11.6348 -9.1784 -13.3247 -15.1256 -13.1967 -0.4565
RANK-3961	5321 30678 11800 29036 11253 1646 53 1018 8297 147 1988 14737 278 2762 18378 23899 42 201 15704 7556 9993 49032 12 338
P_SM-3961	-3.1896 -11.1715 -13.7745 -15.7059 -11.1498 -13.4587 -14.8821 -15.3484 -13.0422 -12.7762 -12.1813 -14.2598 -15.3596 -14.7055 -15.5690 -15.3794 -14.2559 -14.9207 -11.6348 -9.1784 -13.3247 -15.1256 -13.1967 -0.4565
P_LM-3961	-20.6668 -7.4779 -23.1742 -12.8661 -12.4694 -13.0109 -10.2783 -13.5464 -11.5456 -12.3865 -17.3212 -15.9128 -15.2656 -13.9632 -11.4729 -10.9062 -12.7888 -13.8429 -9.2872 -10.6312 -15.1675 -12.5506 -15.6498 -15.1333
ENT_LM-3961	7.9681 0.9034 0.5633 3.6675 4.4172 2.1011 2.6302 1.2026 3.2955 4.3008 0.5627 1.2440 2.6122 2.9042 1.3124 2.5879 4.7659 5.2187 3.5228 5.7756 1.1916 1.4284 1.0052 4.9709
ENT_SM-3961	6.0757 1.3207 2.2119 4.0861 4.9430 2.8900 2.1962 2.5172 3.7864 5.1143 0.8523 2.0247 2.4632 2.3556 2.3926 2.5815 4.7429 1.2369 2.1859 6.5802 1.1458 2.3875 1.2779 3.2188
S-2820	5167 284 1061
T-2820	26886 15183 10972 6989 262 717 7433 286 465 4708 3451 355 19632 8909 8742 1839 319 2173 284 21509 262 370 4339 17972 6551 3670 287 10123 9621 13
H-2820	-1.3981016874313354	32 804 379 617 286 262 1994 3923 625 262 1613 1987 2250 13
P-2820	-5.1850 -14.3284 -13.2601 -11.4895 -12.8855 -12.0045 -11.9004 -8.9427 -12.7448 -12.7438 -13.8819 -12.9252 -18.9597 -12.3520 -0.4444
RANK-2820	43297 3497 95 2060 379 300 24345 9094 2405 769 53 21137 4221 37 49653
P_SM-2820	-5.1850 -14.3284 -13.2601 -11.4895 -12.8855 -12.0045 -11.9004 -8.9427 -12.7448 -12.7438 -13.8819 -12.9252 -18.9597 -12.3520 -0.4444
P_LM-2820	-12.3548 -18.2333 -11.3911 -11.7227 -13.0612 -11.5190 -7.9442 -11.0811 -11.4326 -12.7361 -11.0276 -14.2624 -18.6976 -15.6864 -15.2774
ENT_LM-2820	7.4565 0.8097 3.6074 1.5433 1.3799 6.5125 5.7320 3.6354 1.7637 2.1580 2.9105 1.4490 3.0451 4.0648 4.8044
ENT_SM-2820	7.3775 2.3488 3.9860 1.8698 3.3065 5.1588 2.9525 4.2651 1.3298 2.1221 2.7924 0.9675 3.6380 1.2852 3.0396
S-10252	1065 2805 1584 4586 6153 379 1511 25 1065 16987 7823 11045 2688 338 1964 5464 3362 2409 2528 1773 3136 13
T-10252	13798 1586 2706 287 262 2688 12946 910 484 389 1719 284 10960 14297 3259 422 10522 780 286 257 18772 286 8776 12037 13
H-10252	-1.457751750946045	464 3482 318 900 284 3015 329 257 649 6994 5342 11 20251 1737 11 287 1737 338 2276 3071 13
P-10252	-2.1774 -14.8442 -12.7789 -13.9313 -13.4475 -13.9945 -17.6748 -13.5231 -13.0778 -11.5730 -17.0447 -15.5949 -12.6398 -17.5925 -11.7511 -14.6243 -16.3850 -19.8852 -11.0225 -16.5395 -13.0984 -0.2681
RANK-10252	2523 285 2812 817 193 41466 2655 938 3795 34363 15351 54 3255 1568 512 396 3986 3097 35281 33497 13 282
P_SM-10252	-2.1774 -14.8442 -12.7789 -13.9313 -13.4475 -13.9945 -17.6748 -13.5231 -13.0778 -11.5730 -17.0447 -15.5949 -12.6398 -17.5925 -11.7511 -14.6243 -16.3850 -19.8852 -11.0225 -16.5395 -13.0984 -0.2681
P_LM-10252	-20.6668 -12.5762 -13.0050 -13.5179 -12.1372 -12.8869 -12.8258 -13.7973 -9.8216 -12.9281 -15.3606 -10.9997 -15.9655 -11.8761 -10.1956 -14.8957 -8.4796 -14.9329 -14.1089 -6.5383 -13.9734 -14.9881
ENT_LM-10252	7.9681 3.7462 4.9923 0.5540 4.8324 2.0605 3.4244 4.3473 4.6028 0.0508 3.2162 4.3070 0.0096 2.4385 3.5228 3.1999 1.3925 4.5200 0.0047 3.0179 3.8475 4.8794
ENT_SM-10252	5.7807 3.9131 5.2005 1.9646 4.9270 2.8817 2.9152 2.5117 3.6298 1.5700 4.1453 2.6887 0.7445 1.4378 4.0930 3.1226 3.1596 1.5166 0.8566 2.8803 1.3659 2.2096
S-4812	19 3945 2177 4586 6153 379 8870 25 2624 16987 7623 10099 25 48664 1872 6997 7745 1031 37040 7745 78
T-4812	37482 286 28589 272 9214 1666 389 4137 503 286 511 5682 416 511 287 12 29317 1123 614 13
H-4812	-1.0660890340805054	57 27175 338 1992 5199 25841 11231 468 3414 339 481 2239 866 379 262 886 286 262 614 13
P-4812	-2.1744 -13.8496 -17.3087 -14.4725 -10.3397 -7.3578 -20.6984 -12.7104 -13.0596 -15.2192 -15.2318 -11.9708 -13.8173 -11.8505 -14.6478 -15.9108 -13.8948 -12.6376 -17.1911 -13.0230 -1.9344
RANK-4812	22187 3270 7519 10872 34737 23660 2791 1661 9895 237 905 43228 619 292 1728 1568 645 187 1438 6 261
P_SM-4812	-2.1744 -13.8496 -17.3087 -14.4725 -10.3397 -7.3578 -20.6984 -12.7104 -13.0596 -15.2192 -15.2318 -11.9708 -13.8173 -11.8505 -14.6478 -15.9108 -13.8948 -12.6376 -17.1911 -13.0230 -1.9344
P_LM-4812	-8.8963 -12.7225 -14.4069 -10.0466 -21.7848 -12.4375 -17.6740 -13.8639 -11.0398 -14.0771 -15.6834 -13.2388 -14.1548 -11.9762 -14.8466 -17.9558 -15.8720 -13.3584 -17.0598 -17.6890 -14.7490
ENT_LM-4812	6.4732 4.0005 6.3293 4.2071 0.0037 0.0130 3.7289 4.2501 2.4997 1.0279 3.7686 0.1092 2.8737 0.7240 0.3386 0.0179 2.2881 1.7071 1.3252 0.4051 5.0451
ENT_SM-4812	5.2434 3.6977 5.1842 0.6499 0.0968 2.9111 4.1459 5.5687 3.5887 2.2796 3.2538 1.0519 3.8096 1.7809 1.7661 1.2848 3.1856 3.7025 2.8907 1.2328 7.0401
S-10069	37316 26244 4479 26244 4041 34761 37213 25911 32646 46000 6663 17277 8829 642 2107 290 642 2107 5701 3131
T-10069	32 45602 286 477 262 3452 7823 5243 2912 3166 2691 13
H-10069	-1.2045484781265259	33833 8829 642 2107 5701 3131 6774 345 2107 14604 286 790 983 287 262 2177 26244 4041 2159 5454 13
P-10069	-1.8019 -13.7021 -17.6302 -12.1019 -12.0181 -11.8948 -14.8644 -11.7413 -11.1497 -12.3521 -17.0932 -16.3549 -13.8692 -15.2896 -14.8999 -12.9755 -11.2481 -9.6620 -21.4981 -17.4901 -13.8710 -0.4319
RANK-10069	6895 7506 439 47458 10142 34938 46285 2276 4721 33070 111 18821 3253 295 200 2617 40691 22443 34277 23845 9 49653
P_SM-10069	-1.8019 -13.7021 -17.6302 -12.1019 -12.0181 -11.8948 -14.8644 -11.7413 -11.1497 -12.3521 -17.0932 -16.3549 -13.8692 -15.2896 -14.8999 -12.9755 -11.2481 -9.6620 -21.4981 -17.4901 -13.8710 -0.4319
P_LM-10069	-10.8662 -8.4596 -21.3106 -8.3746 -11.8282 -8.9939 -9.8017 -13.6591 -4.9760 -12.9610 -13.8570 -11.5541 -11.7430 -13.7781 -12.0267 -13.3269 -14.8054 -6.6206 -14.3663 -17.3128 -14.3436 -15.3265
ENT_LM-10069	1.4227 3.2125 0.1045 3.1474 0.3926 4.3255 3.6723 2.5335 3.4933 1.0890 3.2115 4.3316 3.4188 1.9208 3.8748 3.8244 1.4652 0.5127 0.0651 2.5712 3.9151 4.9657
ENT_SM-10069	5.2137 1.7193 1.2664 4.4600 2.6450 5.5848 2.0077 1.8669 1.6737 2.4182 2.2977 3.7045 5.8558 2.9393 3.1933 4.5363 4.3229 3.9830 0.2841 2.6989 1.2044 3.0963
S-9108	32 13027 25872 373 7424 287 4505 11 3354 286 2520 3687 7229 290 262 8211 13
T-9108	8061 1973 16256 290 262 8211 423 13923 257 2472 6591 25872 11 351 617 3354 286 16256 287 2472 11854 329 510 284 1115 2431 13
H-9108	-1.2714908123016357	32 2472 6591 25872 1718 1295 319 2310 2932 1584 11 351 262 8824 6427 1022 262 3825 290 262 3668 13
P-9108	-2.0842 -6.9541 -11.7350 -10.5017 -12.8666 -15.3743 -13.4720 -15.4089 -14.9570 -17.6907 -12.9879 -15.9155 -14.2077 -17.6643 -8.4952 -12.2357 -12.9180 -15.2741 -14.6450 -14.1244 -15.5077 -12.7648 -0.6502
RANK-9108	10170 42021 28989 36090 30995 45529 380 16834 14738 14723 7098 15074 10785 36346 35039 231 6951 16844 18169 21642 39425 1438 331
P_SM-9108	-2.0842 -6.9541 -11.7350 -10.5017 -12.8666 -15.3743 -13.4720 -15.4089 -14.9570 -17.6907 -12.9879 -15.9155 -14.2077 -17.6643 -8.4952 -12.2357 -12.9180 -15.2741 -14.6450 -14.1244 -15.5077 -12.7648 -0.6502
P_LM-9108	-12.3548 -9.9806 -8.9864 -8.2623 -18.2549 -10.3704 -12.3166 -17.1552 -13.0073 -12.7590 -12.8098 -13.1700 -12.9407 -13.4642 -11.6859 -10.4162 -12.5759 -9.3019 -11.8920 -13.0770 -13.7764 -13.8219 -15.7531
ENT_LM-9108	7.4565 0.2837 4.1895 3.9630 0.8140 3.0110 3.5330 2.4736 3.5709 2.2049 4.2718 4.1230 5.6565 5.5724 4.3862 4.1237 4.6155 0.4112 1.0650 2.9951 3.1430 4.4677 4.8799
ENT_SM-9108	3.9625 4.3171 1.8738 3.9889 0.5345 3.0294 4.7365 2.9494 4.0682 3.7014 4.2546 4.6193 4.7788 3.3189 3.6183 3.2436 3.0446 1.2314 2.1423 1.8170 2.6397 1.4032 4.2020
S-3508	17 3426 1584 4586 6153 379 1248 25 2624 16987 775 804 736 379 262 2534 12 1941 3896 286 262 8381 290 27687 3554 13
T-3508	38 4131 544 338 35473 3972 9986 1326 71 11 508 1752 531 339 561 3896 262 1499 329 366 505 2997 812 1600 468 2626 262 4787 3071 284 3119 7842 378 1215 1689 2409 808 13
H-3508	-0.8264526724815369	464 33330 544 338 1992 35473 3972 9986 1326 71 468 3414 339 481 2239 866 379 262 886 286 262 614 13
P-3508	-2.6653 -15.2236 -13.4220 -15.1729 -14.9033 -10.1616 -11.9238 -14.1506 -13.6699 -16.7017 -13.1456 -12.4694 -13.9126 -15.8417 -11.5985 -14.5794 -11.0145 -14.1120 -16.3512 -13.3387 -12.3753 -17.3329 -13.1772 -0.6671
RANK-3508	38625 34147 5117 6376 9807 49473 43896 17408 120 5613 279 20698 1150 586 45839 546 514 1787 45835 11 596 625 6 5
P_SM-3508	-2.6653 -15.2236 -13.4220 -15.1729 -14.9033 -10.1616 -11.9238 -14.1506 -13.6699 -16.7017 -13.1456 -12.4694 -13.9126 -15.8417 -11.5985 -14.5794 -11.0145 -14.1120 -16.3512 -13.3387 -12.3753 -17.3329 -13.1772 -0.6671
P_LM-3508	-20.6668 -16.9530 -11.6337 -15.3910 -10.2147 -25.9766 -19.5659 -20.2505 -16.8926 -17.2682 -12.0111 -11.7915 -12.8579 -16.3627 -14.4849 -13.3316 -12.0564 -14.5433 -18.5860 -15.2372 -13.2722 -17.8622 -15.3442 -15.4988
ENT_LM-3508	7.9681 0.8074 3.0983 5.1719 2.1195 0.0012 0.0175 0.0115 0.0040 2.8624 4.7804 2.5369 1.1738 3.1932 0.0492 2.9398 1.0180 0.3885 0.0158 2.1450 2.2273 1.7574 1.3628 4.7393
ENT_SM-3508	5.1734 1.5192 3.5722 3.2209 1.3281 0.2183 3.3355 0.3256 3.1993 3.9596 4.0823 2.6166 2.0165 1.5859 0.8930 3.6192 1.5063 1.2186 1.5241 3.1258 2.4800 2.6122 1.3916 4.2879
S-6605	3633 663 6224 468 1716 257 2723 286 10386 319 1111 5389 286 262 11102 11 617 389 635 7954 379 6370 284 4117 340 13
T-6605	11522 50014 3466 532 9763 510 416 16352 1644 532 389 41928 3354 286 262 23088 1413 287 48025 11 1900 355 262 26411 11 543 468 1716 1363 284 4138 286 661 12111 284 3151 5491 13
H-6605	-1.376808524131775	464 11102 35863 318 530 286 262 995 338 14069 22642 290 468 587 3170 739 262 3594 11102 13
