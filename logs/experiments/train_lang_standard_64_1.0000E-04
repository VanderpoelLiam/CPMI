Sender: LSF System <lsfadmin@eu-g3-045>
Subject: Job 218287450: <train_lang_standard_64_1.0000E-04> in cluster <euler> Done

Job <train_lang_standard_64_1.0000E-04> was submitted from host <eu-login-18> by user <euler_username> in cluster <euler> at Thu May 12 23:34:15 2022
Job was executed on host(s) <4*eu-g3-045>, in queue <gpu.24h>, as user <euler_username> in cluster <euler> at Thu May 12 23:35:05 2022
</cluster/home/euler_username> was used as the home directory.
</cluster/work/cotterell/liam/master-thesis> was used as the working directory.
Started at Thu May 12 23:35:05 2022
Terminated at Fri May 13 08:41:24 2022
Results reported at Fri May 13 08:41:24 2022

Your job looked like:

------------------------------------------------------------
# LSBATCH: User input
fairseq-train data/xsum-lang --save-dir checkpoints/language_model/standard --update-freq 64 --lr 0.0001 --checkpoint-suffix _standard_64_1.0000E-04 --task language_modeling --arch transformer_lm --share-decoder-input-output-embed --dropout 0.1 --optimizer adam --adam-betas "(0.9, 0.98)" --weight-decay 0.01 --clip-norm 0.0 --lr-scheduler inverse_sqrt --warmup-updates 4000 --warmup-init-lr 1e-07 --tokens-per-sample 512 --sample-break-mode none --max-tokens 2048 --no-epoch-checkpoints --no-last-checkpoints --patience 5
------------------------------------------------------------

Successfully completed.

Resource usage summary:

    CPU time :                                   32426.29 sec.
    Max Memory :                                 5067 MB
    Average Memory :                             3037.04 MB
    Total Requested Memory :                     8192.00 MB
    Delta Memory :                               3125.00 MB
    Max Swap :                                   -
    Max Processes :                              4
    Max Threads :                                14
    Run time :                                   32778 sec.
    Turnaround time :                            32829 sec.

The output (if any) follows:

2022-05-12 23:45:59 | INFO | fairseq.tasks.text_to_speech | Please install tensorboardX: pip install tensorboardX
2022-05-12 23:46:40 | INFO | fairseq_cli.train | {'_name': None, 'common': {'_name': None, 'no_progress_bar': False, 'log_interval': 100, 'log_format': None, 'log_file': None, 'tensorboard_logdir': None, 'wandb_project': None, 'azureml_logging': False, 'seed': 1, 'cpu': False, 'tpu': False, 'bf16': False, 'memory_efficient_bf16': False, 'fp16': False, 'memory_efficient_fp16': False, 'fp16_no_flatten_grads': False, 'fp16_init_scale': 128, 'fp16_scale_window': None, 'fp16_scale_tolerance': 0.0, 'on_cpu_convert_precision': False, 'min_loss_scale': 0.0001, 'threshold_loss_scale': None, 'amp': False, 'amp_batch_retries': 2, 'amp_init_scale': 128, 'amp_scale_window': None, 'user_dir': None, 'empty_cache_freq': 0, 'all_gather_list_size': 16384, 'model_parallel_size': 1, 'quantization_config_path': None, 'profile': False, 'reset_logging': False, 'suppress_crashes': False, 'use_plasma_view': False, 'plasma_path': '/tmp/plasma'}, 'common_eval': {'_name': None, 'path': None, 'post_process': None, 'quiet': False, 'model_overrides': '{}', 'results_path': None, 'print_tokens': False}, 'distributed_training': {'_name': None, 'distributed_world_size': 1, 'distributed_num_procs': 1, 'distributed_rank': 0, 'distributed_backend': 'nccl', 'distributed_init_method': None, 'distributed_port': -1, 'device_id': 0, 'distributed_no_spawn': False, 'ddp_backend': 'pytorch_ddp', 'ddp_comm_hook': 'none', 'bucket_cap_mb': 25, 'fix_batches_to_gpus': False, 'find_unused_parameters': False, 'gradient_as_bucket_view': False, 'fast_stat_sync': False, 'heartbeat_timeout': -1, 'broadcast_buffers': False, 'slowmo_momentum': None, 'slowmo_base_algorithm': 'localsgd', 'localsgd_frequency': 3, 'nprocs_per_node': 1, 'pipeline_model_parallel': False, 'pipeline_balance': None, 'pipeline_devices': None, 'pipeline_chunks': 0, 'pipeline_encoder_balance': None, 'pipeline_encoder_devices': None, 'pipeline_decoder_balance': None, 'pipeline_decoder_devices': None, 'pipeline_checkpoint': 'never', 'zero_sharding': 'none', 'fp16': False, 'memory_efficient_fp16': False, 'tpu': False, 'no_reshard_after_forward': False, 'fp32_reduce_scatter': False, 'cpu_offload': False, 'use_sharded_state': False, 'not_fsdp_flatten_parameters': False}, 'dataset': {'_name': None, 'num_workers': 1, 'skip_invalid_size_inputs_valid_test': False, 'max_tokens': 2048, 'batch_size': None, 'required_batch_size_multiple': 8, 'required_seq_len_multiple': 1, 'dataset_impl': None, 'data_buffer_size': 10, 'train_subset': 'train', 'valid_subset': 'valid', 'combine_valid_subsets': None, 'ignore_unused_valid_subsets': False, 'validate_interval': 1, 'validate_interval_updates': 0, 'validate_after_updates': 0, 'fixed_validation_seed': None, 'disable_validation': False, 'max_tokens_valid': 2048, 'batch_size_valid': None, 'max_valid_steps': None, 'curriculum': 0, 'gen_subset': 'test', 'num_shards': 1, 'shard_id': 0, 'grouped_shuffling': False, 'update_epoch_batch_itr': False, 'update_ordered_indices_seed': False}, 'optimization': {'_name': None, 'max_epoch': 0, 'max_update': 0, 'stop_time_hours': 0.0, 'clip_norm': 0.0, 'sentence_avg': False, 'update_freq': [64], 'lr': [0.0001], 'stop_min_lr': -1.0, 'use_bmuf': False, 'skip_remainder_batch': False}, 'checkpoint': {'_name': None, 'save_dir': 'checkpoints/language_model/standard', 'restore_file': 'checkpoint_last.pt', 'continue_once': None, 'finetune_from_model': None, 'reset_dataloader': False, 'reset_lr_scheduler': False, 'reset_meters': False, 'reset_optimizer': False, 'optimizer_overrides': '{}', 'save_interval': 1, 'save_interval_updates': 0, 'keep_interval_updates': -1, 'keep_interval_updates_pattern': -1, 'keep_last_epochs': -1, 'keep_best_checkpoints': -1, 'no_save': False, 'no_epoch_checkpoints': True, 'no_last_checkpoints': True, 'no_save_optimizer_state': False, 'best_checkpoint_metric': 'loss', 'maximize_best_checkpoint_metric': False, 'patience': 5, 'checkpoint_suffix': '_standard_64_1.0000E-04', 'checkpoint_shard_count': 1, 'load_checkpoint_on_all_dp_ranks': False, 'write_checkpoints_asynchronously': False, 'model_parallel_size': 1}, 'bmuf': {'_name': None, 'block_lr': 1.0, 'block_momentum': 0.875, 'global_sync_iter': 50, 'warmup_iterations': 500, 'use_nbm': False, 'average_sync': False, 'distributed_world_size': 1}, 'generation': {'_name': None, 'beam': 5, 'nbest': 1, 'max_len_a': 0.0, 'max_len_b': 200, 'min_len': 1, 'match_source_len': False, 'unnormalized': False, 'no_early_stop': False, 'no_beamable_mm': False, 'lenpen': 1.0, 'unkpen': 0.0, 'replace_unk': None, 'sacrebleu': False, 'score_reference': False, 'prefix_size': 0, 'no_repeat_ngram_size': 0, 'sampling': False, 'sampling_topk': -1, 'sampling_topp': -1.0, 'constraints': None, 'temperature': 1.0, 'diverse_beam_groups': -1, 'diverse_beam_strength': 0.5, 'diversity_rate': -1.0, 'print_alignment': None, 'print_step': False, 'lm_path': None, 'ent_threshold': 0.0, 'lm_weight': 0.0, 'iter_decode_eos_penalty': 0.0, 'iter_decode_max_iter': 10, 'iter_decode_force_max_iter': False, 'iter_decode_with_beam': 1, 'iter_decode_with_external_reranker': False, 'retain_iter_history': False, 'retain_dropout': False, 'retain_dropout_modules': None, 'decoding_format': None, 'no_seed_provided': False}, 'eval_lm': {'_name': None, 'output_word_probs': False, 'output_word_stats': False, 'context_window': 0, 'softmax_batch': 9223372036854775807}, 'interactive': {'_name': None, 'buffer_size': 0, 'input': '-'}, 'model': {'_name': 'transformer_lm', 'activation_fn': relu, 'dropout': 0.1, 'attention_dropout': 0.0, 'activation_dropout': 0.0, 'relu_dropout': 0.0, 'decoder_embed_dim': 512, 'decoder_output_dim': 512, 'decoder_input_dim': 512, 'decoder_ffn_embed_dim': 2048, 'decoder_layers': 6, 'decoder_attention_heads': 8, 'decoder_normalize_before': False, 'no_decoder_final_norm': False, 'adaptive_softmax_cutoff': None, 'adaptive_softmax_dropout': 0.0, 'adaptive_softmax_factor': 4.0, 'no_token_positional_embeddings': False, 'share_decoder_input_output_embed': True, 'character_embeddings': False, 'character_filters': '[(1, 64), (2, 128), (3, 192), (4, 256), (5, 256), (6, 256), (7, 256)]', 'character_embedding_dim': 4, 'char_embedder_highway_layers': 2, 'adaptive_input': False, 'adaptive_input_factor': 4.0, 'adaptive_input_cutoff': None, 'tie_adaptive_weights': False, 'tie_adaptive_proj': False, 'decoder_learned_pos': False, 'layernorm_embedding': False, 'no_scale_embedding': False, 'checkpoint_activations': False, 'offload_activations': False, 'decoder_layerdrop': 0.0, 'decoder_layers_to_keep': None, 'quant_noise_pq': 0.0, 'quant_noise_pq_block_size': 8, 'quant_noise_scalar': 0.0, 'min_params_to_wrap': 100000000, 'base_layers': 0, 'base_sublayers': 1, 'base_shuffle': 1, 'scale_fc': False, 'scale_attn': False, 'scale_heads': False, 'scale_resids': False, 'add_bos_token': False, 'tokens_per_sample': 512, 'max_target_positions': None, 'tpu': False}, 'task': {'_name': 'language_modeling', 'data': 'data/xsum-lang', 'sample_break_mode': none, 'tokens_per_sample': 512, 'output_dictionary_size': -1, 'self_target': False, 'future_target': False, 'past_target': False, 'add_bos_token': False, 'max_target_positions': None, 'shorten_method': none, 'shorten_data_split_list': '', 'pad_to_fixed_length': False, 'pad_to_fixed_bsz': False, 'seed': 1, 'batch_size': None, 'batch_size_valid': None, 'dataset_impl': None, 'data_buffer_size': 10, 'tpu': False, 'use_plasma_view': False, 'plasma_path': '/tmp/plasma'}, 'criterion': {'_name': 'cross_entropy', 'sentence_avg': False}, 'optimizer': {'_name': 'adam', 'adam_betas': '(0.9, 0.98)', 'adam_eps': 1e-08, 'weight_decay': 0.01, 'use_old_adam': False, 'fp16_adam_stats': False, 'tpu': False, 'lr': [0.0001]}, 'lr_scheduler': {'_name': 'inverse_sqrt', 'warmup_updates': 4000, 'warmup_init_lr': 1e-07, 'lr': [0.0001]}, 'scoring': {'_name': 'bleu', 'pad': 1, 'eos': 2, 'unk': 3}, 'bpe': None, 'tokenizer': None, 'ema': {'_name': None, 'store_ema': False, 'ema_decay': 0.9999, 'ema_start_update': 0, 'ema_seed_model': None, 'ema_update_freq': 1, 'ema_fp32': False}, 'simul_type': None}
2022-05-12 23:46:41 | INFO | fairseq.tasks.language_modeling | dictionary: 49992 types
2022-05-12 23:47:08 | INFO | fairseq_cli.train | TransformerLanguageModel(
  (decoder): TransformerDecoder(
    (dropout_module): FairseqDropout()
    (embed_tokens): Embedding(49992, 512, padding_idx=1)
    (embed_positions): SinusoidalPositionalEmbedding()
    (layers): ModuleList(
      (0): TransformerDecoderLayerBase(
        (dropout_module): FairseqDropout()
        (self_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=512, out_features=512, bias=True)
          (v_proj): Linear(in_features=512, out_features=512, bias=True)
          (q_proj): Linear(in_features=512, out_features=512, bias=True)
          (out_proj): Linear(in_features=512, out_features=512, bias=True)
        )
        (activation_dropout_module): FairseqDropout()
        (self_attn_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
        (fc1): Linear(in_features=512, out_features=2048, bias=True)
        (fc2): Linear(in_features=2048, out_features=512, bias=True)
        (final_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
      )
      (1): TransformerDecoderLayerBase(
        (dropout_module): FairseqDropout()
        (self_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=512, out_features=512, bias=True)
          (v_proj): Linear(in_features=512, out_features=512, bias=True)
          (q_proj): Linear(in_features=512, out_features=512, bias=True)
          (out_proj): Linear(in_features=512, out_features=512, bias=True)
        )
        (activation_dropout_module): FairseqDropout()
        (self_attn_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
        (fc1): Linear(in_features=512, out_features=2048, bias=True)
        (fc2): Linear(in_features=2048, out_features=512, bias=True)
        (final_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
      )
      (2): TransformerDecoderLayerBase(
        (dropout_module): FairseqDropout()
        (self_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=512, out_features=512, bias=True)
          (v_proj): Linear(in_features=512, out_features=512, bias=True)
          (q_proj): Linear(in_features=512, out_features=512, bias=True)
          (out_proj): Linear(in_features=512, out_features=512, bias=True)
        )
        (activation_dropout_module): FairseqDropout()
        (self_attn_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
        (fc1): Linear(in_features=512, out_features=2048, bias=True)
        (fc2): Linear(in_features=2048, out_features=512, bias=True)
        (final_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
      )
      (3): TransformerDecoderLayerBase(
        (dropout_module): FairseqDropout()
        (self_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=512, out_features=512, bias=True)
          (v_proj): Linear(in_features=512, out_features=512, bias=True)
          (q_proj): Linear(in_features=512, out_features=512, bias=True)
          (out_proj): Linear(in_features=512, out_features=512, bias=True)
        )
        (activation_dropout_module): FairseqDropout()
        (self_attn_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
        (fc1): Linear(in_features=512, out_features=2048, bias=True)
        (fc2): Linear(in_features=2048, out_features=512, bias=True)
        (final_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
      )
      (4): TransformerDecoderLayerBase(
        (dropout_module): FairseqDropout()
        (self_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=512, out_features=512, bias=True)
          (v_proj): Linear(in_features=512, out_features=512, bias=True)
          (q_proj): Linear(in_features=512, out_features=512, bias=True)
          (out_proj): Linear(in_features=512, out_features=512, bias=True)
        )
        (activation_dropout_module): FairseqDropout()
        (self_attn_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
        (fc1): Linear(in_features=512, out_features=2048, bias=True)
        (fc2): Linear(in_features=2048, out_features=512, bias=True)
        (final_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
      )
      (5): TransformerDecoderLayerBase(
        (dropout_module): FairseqDropout()
        (self_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=512, out_features=512, bias=True)
          (v_proj): Linear(in_features=512, out_features=512, bias=True)
          (q_proj): Linear(in_features=512, out_features=512, bias=True)
          (out_proj): Linear(in_features=512, out_features=512, bias=True)
        )
        (activation_dropout_module): FairseqDropout()
        (self_attn_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
        (fc1): Linear(in_features=512, out_features=2048, bias=True)
        (fc2): Linear(in_features=2048, out_features=512, bias=True)
        (final_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
      )
    )
    (output_projection): Linear(in_features=512, out_features=49992, bias=False)
  )
)
2022-05-12 23:47:08 | INFO | fairseq_cli.train | task: LanguageModelingTask
2022-05-12 23:47:08 | INFO | fairseq_cli.train | model: TransformerLanguageModel
2022-05-12 23:47:08 | INFO | fairseq_cli.train | criterion: CrossEntropyCriterion
2022-05-12 23:47:08 | INFO | fairseq_cli.train | num. shared model params: 44,510,208 (num. trained: 44,510,208)
2022-05-12 23:47:08 | INFO | fairseq_cli.train | num. expert model params: 0 (num. trained: 0)
2022-05-12 23:47:08 | INFO | fairseq.data.data_utils | loaded 11,332 examples from: data/xsum-lang/valid
2022-05-12 23:48:10 | INFO | fairseq.trainer | detected shared parameter: decoder.embed_tokens.weight <- decoder.output_projection.weight
2022-05-12 23:48:10 | INFO | fairseq.utils | ***********************CUDA enviroments for all 1 workers***********************
2022-05-12 23:48:10 | INFO | fairseq.utils | rank   0: capabilities =  7.5  ; total memory = 10.761 GB ; name = NVIDIA GeForce RTX 2080 Ti              
2022-05-12 23:48:10 | INFO | fairseq.utils | ***********************CUDA enviroments for all 1 workers***********************
2022-05-12 23:48:10 | INFO | fairseq_cli.train | training on 1 devices (GPUs/TPUs)
2022-05-12 23:48:10 | INFO | fairseq_cli.train | max tokens per device = 2048 and max sentences per device = None
2022-05-12 23:48:10 | INFO | fairseq.trainer | Preparing to load checkpoint checkpoints/language_model/standard/checkpoint_last_standard_64_1.0000E-04.pt
2022-05-12 23:48:10 | INFO | fairseq.trainer | No existing checkpoint found checkpoints/language_model/standard/checkpoint_last_standard_64_1.0000E-04.pt
2022-05-12 23:48:10 | INFO | fairseq.trainer | loading train data for epoch 1
2022-05-12 23:48:14 | INFO | fairseq.data.data_utils | loaded 204,045 examples from: data/xsum-lang/train
2022-05-12 23:48:22 | INFO | fairseq.trainer | NOTE: your device may support faster training with --fp16 or --amp
2022-05-12 23:48:22 | INFO | fairseq.data.iterators | grouped total_num_itrs = 39
2022-05-12 23:48:22 | INFO | fairseq.trainer | begin training epoch 1
2022-05-12 23:48:22 | INFO | fairseq_cli.train | Start iterating over samples
2022-05-12 23:52:16 | INFO | fairseq_cli.train | begin validation on "valid" subset
/cluster/work/cotterell/liam/fairseq/fairseq/utils.py:374: UserWarning: amp_C fused kernels unavailable, disabling multi_tensor_l2norm; you may get better performance by installing NVIDIA's apex library
  warnings.warn(
2022-05-12 23:52:20 | INFO | valid | epoch 001 | valid on 'valid' subset | loss 15.928 | ppl 62329.4 | wps 76790.8 | wpb 2041.1 | bsz 4 | num_updates 39
2022-05-12 23:52:20 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 1 @ 39 updates
2022-05-12 23:52:20 | INFO | fairseq.trainer | Saving checkpoint to /cluster/work/cotterell/liam/master-thesis/checkpoints/language_model/standard/checkpoint_best_standard_64_1.0000E-04.pt
2022-05-12 23:52:21 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/work/cotterell/liam/master-thesis/checkpoints/language_model/standard/checkpoint_best_standard_64_1.0000E-04.pt
2022-05-12 23:52:21 | INFO | fairseq.checkpoint_utils | Saved checkpoint checkpoints/language_model/standard/checkpoint_best_standard_64_1.0000E-04.pt (epoch 1 @ 39 updates, score 15.928) (writing took 1.1773686329834163 seconds)
2022-05-12 23:52:21 | INFO | fairseq_cli.train | end of epoch 1 (average epoch stats below)
2022-05-12 23:52:21 | INFO | train | epoch 001 | loss 16.156 | ppl 73018.3 | wps 26741.3 | ups 0.2 | wpb 130930 | bsz 255.7 | num_updates 39 | lr 1.07402e-06 | gnorm 5.517 | train_wall 220 | gb_free 7.9 | wall 251
2022-05-12 23:52:21 | INFO | fairseq.data.iterators | grouped total_num_itrs = 39
2022-05-12 23:52:21 | INFO | fairseq.trainer | begin training epoch 2
2022-05-12 23:52:21 | INFO | fairseq_cli.train | Start iterating over samples
2022-05-12 23:55:28 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-05-12 23:55:31 | INFO | valid | epoch 002 | valid on 'valid' subset | loss 15.02 | ppl 33221.7 | wps 76678.1 | wpb 2041.1 | bsz 4 | num_updates 78 | best_loss 15.02
2022-05-12 23:55:31 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 2 @ 78 updates
2022-05-12 23:55:31 | INFO | fairseq.trainer | Saving checkpoint to /cluster/work/cotterell/liam/master-thesis/checkpoints/language_model/standard/checkpoint_best_standard_64_1.0000E-04.pt
2022-05-12 23:55:33 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/work/cotterell/liam/master-thesis/checkpoints/language_model/standard/checkpoint_best_standard_64_1.0000E-04.pt
2022-05-12 23:55:33 | INFO | fairseq.checkpoint_utils | Saved checkpoint checkpoints/language_model/standard/checkpoint_best_standard_64_1.0000E-04.pt (epoch 2 @ 78 updates, score 15.02) (writing took 1.0909852553158998 seconds)
2022-05-12 23:55:33 | INFO | fairseq_cli.train | end of epoch 2 (average epoch stats below)
2022-05-12 23:55:33 | INFO | train | epoch 002 | loss 15.511 | ppl 46683.2 | wps 26714.2 | ups 0.2 | wpb 130930 | bsz 255.7 | num_updates 78 | lr 2.04805e-06 | gnorm 4.815 | train_wall 177 | gb_free 7.9 | wall 442
2022-05-12 23:55:33 | INFO | fairseq.data.iterators | grouped total_num_itrs = 39
2022-05-12 23:55:33 | INFO | fairseq.trainer | begin training epoch 3
2022-05-12 23:55:33 | INFO | fairseq_cli.train | Start iterating over samples
2022-05-12 23:57:18 | INFO | train_inner | epoch 003:     22 / 39 loss=15.605, ppl=49850.6, wps=26858.2, ups=0.21, wpb=130951, bsz=255.8, num_updates=100, lr=2.5975e-06, gnorm=4.855, train_wall=498, gb_free=7.9, wall=548
2022-05-13 00:00:30 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-05-13 00:00:35 | INFO | valid | epoch 003 | valid on 'valid' subset | loss 14.129 | ppl 17916.3 | wps 77481.4 | wpb 2041.1 | bsz 4 | num_updates 117 | best_loss 14.129
2022-05-13 00:00:35 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 3 @ 117 updates
2022-05-13 00:00:35 | INFO | fairseq.trainer | Saving checkpoint to /cluster/work/cotterell/liam/master-thesis/checkpoints/language_model/standard/checkpoint_best_standard_64_1.0000E-04.pt
2022-05-13 00:00:36 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/work/cotterell/liam/master-thesis/checkpoints/language_model/standard/checkpoint_best_standard_64_1.0000E-04.pt
2022-05-13 00:00:36 | INFO | fairseq.checkpoint_utils | Saved checkpoint checkpoints/language_model/standard/checkpoint_best_standard_64_1.0000E-04.pt (epoch 3 @ 117 updates, score 14.129) (writing took 1.1054589501582086 seconds)
2022-05-13 00:00:36 | INFO | fairseq_cli.train | end of epoch 3 (average epoch stats below)
2022-05-13 00:00:36 | INFO | train | epoch 003 | loss 14.611 | ppl 25022.6 | wps 16830.1 | ups 0.13 | wpb 130930 | bsz 255.7 | num_updates 117 | lr 3.02207e-06 | gnorm 3.439 | train_wall 270 | gb_free 7.9 | wall 746
2022-05-13 00:00:36 | INFO | fairseq.data.iterators | grouped total_num_itrs = 39
2022-05-13 00:00:36 | INFO | fairseq.trainer | begin training epoch 4
2022-05-13 00:00:36 | INFO | fairseq_cli.train | Start iterating over samples
2022-05-13 00:03:52 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-05-13 00:03:56 | INFO | valid | epoch 004 | valid on 'valid' subset | loss 13.526 | ppl 11795.2 | wps 76951.9 | wpb 2041.1 | bsz 4 | num_updates 156 | best_loss 13.526
2022-05-13 00:03:56 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 4 @ 156 updates
2022-05-13 00:03:56 | INFO | fairseq.trainer | Saving checkpoint to /cluster/work/cotterell/liam/master-thesis/checkpoints/language_model/standard/checkpoint_best_standard_64_1.0000E-04.pt
2022-05-13 00:03:58 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/work/cotterell/liam/master-thesis/checkpoints/language_model/standard/checkpoint_best_standard_64_1.0000E-04.pt
2022-05-13 00:03:58 | INFO | fairseq.checkpoint_utils | Saved checkpoint checkpoints/language_model/standard/checkpoint_best_standard_64_1.0000E-04.pt (epoch 4 @ 156 updates, score 13.526) (writing took 1.2202776838093996 seconds)
2022-05-13 00:03:58 | INFO | fairseq_cli.train | end of epoch 4 (average epoch stats below)
2022-05-13 00:03:58 | INFO | train | epoch 004 | loss 13.884 | ppl 15119.5 | wps 25319.2 | ups 0.19 | wpb 130930 | bsz 255.7 | num_updates 156 | lr 3.9961e-06 | gnorm 2.26 | train_wall 183 | gb_free 7.9 | wall 947
2022-05-13 00:03:58 | INFO | fairseq.data.iterators | grouped total_num_itrs = 39
2022-05-13 00:03:58 | INFO | fairseq.trainer | begin training epoch 5
2022-05-13 00:03:58 | INFO | fairseq_cli.train | Start iterating over samples
2022-05-13 00:07:05 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-05-13 00:07:09 | INFO | valid | epoch 005 | valid on 'valid' subset | loss 13.123 | ppl 8923.48 | wps 76964.3 | wpb 2041.1 | bsz 4 | num_updates 195 | best_loss 13.123
2022-05-13 00:07:09 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 5 @ 195 updates
2022-05-13 00:07:09 | INFO | fairseq.trainer | Saving checkpoint to /cluster/work/cotterell/liam/master-thesis/checkpoints/language_model/standard/checkpoint_best_standard_64_1.0000E-04.pt
2022-05-13 00:07:10 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/work/cotterell/liam/master-thesis/checkpoints/language_model/standard/checkpoint_best_standard_64_1.0000E-04.pt
2022-05-13 00:07:10 | INFO | fairseq.checkpoint_utils | Saved checkpoint checkpoints/language_model/standard/checkpoint_best_standard_64_1.0000E-04.pt (epoch 5 @ 195 updates, score 13.123) (writing took 1.2409580829553306 seconds)
2022-05-13 00:07:10 | INFO | fairseq_cli.train | end of epoch 5 (average epoch stats below)
2022-05-13 00:07:10 | INFO | train | epoch 005 | loss 13.398 | ppl 10795.8 | wps 26567.5 | ups 0.2 | wpb 130930 | bsz 255.7 | num_updates 195 | lr 4.97013e-06 | gnorm 1.596 | train_wall 178 | gb_free 7.9 | wall 1140
2022-05-13 00:07:10 | INFO | fairseq.data.iterators | grouped total_num_itrs = 39
2022-05-13 00:07:10 | INFO | fairseq.trainer | begin training epoch 6
2022-05-13 00:07:10 | INFO | fairseq_cli.train | Start iterating over samples
2022-05-13 00:07:34 | INFO | train_inner | epoch 006:      5 / 39 loss=13.742, ppl=13702.8, wps=21258.7, ups=0.16, wpb=130911, bsz=255.7, num_updates=200, lr=5.095e-06, gnorm=2.089, train_wall=553, gb_free=7.9, wall=1164
2022-05-13 00:10:19 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-05-13 00:10:23 | INFO | valid | epoch 006 | valid on 'valid' subset | loss 12.816 | ppl 7209.49 | wps 77225.5 | wpb 2041.1 | bsz 4 | num_updates 234 | best_loss 12.816
2022-05-13 00:10:23 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 6 @ 234 updates
2022-05-13 00:10:23 | INFO | fairseq.trainer | Saving checkpoint to /cluster/work/cotterell/liam/master-thesis/checkpoints/language_model/standard/checkpoint_best_standard_64_1.0000E-04.pt
2022-05-13 00:10:24 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/work/cotterell/liam/master-thesis/checkpoints/language_model/standard/checkpoint_best_standard_64_1.0000E-04.pt
2022-05-13 00:10:24 | INFO | fairseq.checkpoint_utils | Saved checkpoint checkpoints/language_model/standard/checkpoint_best_standard_64_1.0000E-04.pt (epoch 6 @ 234 updates, score 12.816) (writing took 1.2370779998600483 seconds)
2022-05-13 00:10:24 | INFO | fairseq_cli.train | end of epoch 6 (average epoch stats below)
2022-05-13 00:10:24 | INFO | train | epoch 006 | loss 13.057 | ppl 8521.53 | wps 26245.4 | ups 0.2 | wpb 130930 | bsz 255.7 | num_updates 234 | lr 5.94415e-06 | gnorm 1.314 | train_wall 179 | gb_free 7.9 | wall 1334
2022-05-13 00:10:24 | INFO | fairseq.data.iterators | grouped total_num_itrs = 39
2022-05-13 00:10:24 | INFO | fairseq.trainer | begin training epoch 7
2022-05-13 00:10:24 | INFO | fairseq_cli.train | Start iterating over samples
2022-05-13 00:13:32 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-05-13 00:13:36 | INFO | valid | epoch 007 | valid on 'valid' subset | loss 12.502 | ppl 5799.25 | wps 76698.1 | wpb 2041.1 | bsz 4 | num_updates 273 | best_loss 12.502
2022-05-13 00:13:36 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 7 @ 273 updates
2022-05-13 00:13:36 | INFO | fairseq.trainer | Saving checkpoint to /cluster/work/cotterell/liam/master-thesis/checkpoints/language_model/standard/checkpoint_best_standard_64_1.0000E-04.pt
2022-05-13 00:13:37 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/work/cotterell/liam/master-thesis/checkpoints/language_model/standard/checkpoint_best_standard_64_1.0000E-04.pt
2022-05-13 00:13:37 | INFO | fairseq.checkpoint_utils | Saved checkpoint checkpoints/language_model/standard/checkpoint_best_standard_64_1.0000E-04.pt (epoch 7 @ 273 updates, score 12.502) (writing took 1.1012650127522647 seconds)
2022-05-13 00:13:37 | INFO | fairseq_cli.train | end of epoch 7 (average epoch stats below)
2022-05-13 00:13:37 | INFO | train | epoch 007 | loss 12.754 | ppl 6907.31 | wps 26514.7 | ups 0.2 | wpb 130930 | bsz 255.7 | num_updates 273 | lr 6.91818e-06 | gnorm 1.205 | train_wall 178 | gb_free 7.9 | wall 1527
2022-05-13 00:13:37 | INFO | fairseq.data.iterators | grouped total_num_itrs = 39
2022-05-13 00:13:37 | INFO | fairseq.trainer | begin training epoch 8
2022-05-13 00:13:37 | INFO | fairseq_cli.train | Start iterating over samples
2022-05-13 00:15:46 | INFO | train_inner | epoch 008:     27 / 39 loss=12.776, ppl=7015.52, wps=26584.4, ups=0.2, wpb=130955, bsz=255.8, num_updates=300, lr=7.5925e-06, gnorm=1.21, train_wall=457, gb_free=7.9, wall=1656
2022-05-13 00:16:44 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-05-13 00:16:48 | INFO | valid | epoch 008 | valid on 'valid' subset | loss 12.206 | ppl 4723.19 | wps 77079.5 | wpb 2041.1 | bsz 4 | num_updates 312 | best_loss 12.206
2022-05-13 00:16:48 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 8 @ 312 updates
2022-05-13 00:16:48 | INFO | fairseq.trainer | Saving checkpoint to /cluster/work/cotterell/liam/master-thesis/checkpoints/language_model/standard/checkpoint_best_standard_64_1.0000E-04.pt
2022-05-13 00:16:49 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/work/cotterell/liam/master-thesis/checkpoints/language_model/standard/checkpoint_best_standard_64_1.0000E-04.pt
2022-05-13 00:16:49 | INFO | fairseq.checkpoint_utils | Saved checkpoint checkpoints/language_model/standard/checkpoint_best_standard_64_1.0000E-04.pt (epoch 8 @ 312 updates, score 12.206) (writing took 1.0348883750848472 seconds)
2022-05-13 00:16:49 | INFO | fairseq_cli.train | end of epoch 8 (average epoch stats below)
2022-05-13 00:16:49 | INFO | train | epoch 008 | loss 12.434 | ppl 5531.81 | wps 26570.7 | ups 0.2 | wpb 130930 | bsz 255.7 | num_updates 312 | lr 7.8922e-06 | gnorm 1.085 | train_wall 178 | gb_free 7.9 | wall 1719
2022-05-13 00:16:49 | INFO | fairseq.data.iterators | grouped total_num_itrs = 39
2022-05-13 00:16:49 | INFO | fairseq.trainer | begin training epoch 9
2022-05-13 00:16:49 | INFO | fairseq_cli.train | Start iterating over samples
2022-05-13 00:19:55 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-05-13 00:19:59 | INFO | valid | epoch 009 | valid on 'valid' subset | loss 11.909 | ppl 3846.08 | wps 76930.5 | wpb 2041.1 | bsz 4 | num_updates 351 | best_loss 11.909
2022-05-13 00:19:59 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 9 @ 351 updates
2022-05-13 00:19:59 | INFO | fairseq.trainer | Saving checkpoint to /cluster/work/cotterell/liam/master-thesis/checkpoints/language_model/standard/checkpoint_best_standard_64_1.0000E-04.pt
2022-05-13 00:20:00 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/work/cotterell/liam/master-thesis/checkpoints/language_model/standard/checkpoint_best_standard_64_1.0000E-04.pt
2022-05-13 00:20:00 | INFO | fairseq.checkpoint_utils | Saved checkpoint checkpoints/language_model/standard/checkpoint_best_standard_64_1.0000E-04.pt (epoch 9 @ 351 updates, score 11.909) (writing took 1.6260917410254478 seconds)
2022-05-13 00:20:00 | INFO | fairseq_cli.train | end of epoch 9 (average epoch stats below)
2022-05-13 00:20:00 | INFO | train | epoch 009 | loss 12.134 | ppl 4495.59 | wps 26689.6 | ups 0.2 | wpb 130930 | bsz 255.7 | num_updates 351 | lr 8.86623e-06 | gnorm 1.007 | train_wall 177 | gb_free 7.9 | wall 1910
2022-05-13 00:20:00 | INFO | fairseq.data.iterators | grouped total_num_itrs = 39
2022-05-13 00:20:00 | INFO | fairseq.trainer | begin training epoch 10
2022-05-13 00:20:00 | INFO | fairseq_cli.train | Start iterating over samples
2022-05-13 00:23:30 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-05-13 00:23:35 | INFO | valid | epoch 010 | valid on 'valid' subset | loss 11.588 | ppl 3078.85 | wps 76934.2 | wpb 2041.1 | bsz 4 | num_updates 390 | best_loss 11.588
2022-05-13 00:23:35 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 10 @ 390 updates
2022-05-13 00:23:35 | INFO | fairseq.trainer | Saving checkpoint to /cluster/work/cotterell/liam/master-thesis/checkpoints/language_model/standard/checkpoint_best_standard_64_1.0000E-04.pt
2022-05-13 00:23:36 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/work/cotterell/liam/master-thesis/checkpoints/language_model/standard/checkpoint_best_standard_64_1.0000E-04.pt
2022-05-13 00:23:36 | INFO | fairseq.checkpoint_utils | Saved checkpoint checkpoints/language_model/standard/checkpoint_best_standard_64_1.0000E-04.pt (epoch 10 @ 390 updates, score 11.588) (writing took 1.2130771232768893 seconds)
2022-05-13 00:23:36 | INFO | fairseq_cli.train | end of epoch 10 (average epoch stats below)
2022-05-13 00:23:36 | INFO | train | epoch 010 | loss 11.818 | ppl 3610.89 | wps 23711.5 | ups 0.18 | wpb 130930 | bsz 255.7 | num_updates 390 | lr 9.84025e-06 | gnorm 0.934 | train_wall 190 | gb_free 7.9 | wall 2126
2022-05-13 00:23:36 | INFO | fairseq.data.iterators | grouped total_num_itrs = 39
2022-05-13 00:23:36 | INFO | fairseq.trainer | begin training epoch 11
2022-05-13 00:23:36 | INFO | fairseq_cli.train | Start iterating over samples
2022-05-13 00:24:34 | INFO | train_inner | epoch 011:     10 / 39 loss=11.982, ppl=4044.93, wps=24806.9, ups=0.19, wpb=130916, bsz=255.7, num_updates=400, lr=1.009e-05, gnorm=0.971, train_wall=478, gb_free=7.9, wall=2184
2022-05-13 00:27:02 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-05-13 00:27:06 | INFO | valid | epoch 011 | valid on 'valid' subset | loss 11.265 | ppl 2460.73 | wps 77125 | wpb 2041.1 | bsz 4 | num_updates 429 | best_loss 11.265
2022-05-13 00:27:06 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 11 @ 429 updates
2022-05-13 00:27:06 | INFO | fairseq.trainer | Saving checkpoint to /cluster/work/cotterell/liam/master-thesis/checkpoints/language_model/standard/checkpoint_best_standard_64_1.0000E-04.pt
2022-05-13 00:27:08 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/work/cotterell/liam/master-thesis/checkpoints/language_model/standard/checkpoint_best_standard_64_1.0000E-04.pt
2022-05-13 00:27:08 | INFO | fairseq.checkpoint_utils | Saved checkpoint checkpoints/language_model/standard/checkpoint_best_standard_64_1.0000E-04.pt (epoch 11 @ 429 updates, score 11.265) (writing took 1.2571869026869535 seconds)
2022-05-13 00:27:08 | INFO | fairseq_cli.train | end of epoch 11 (average epoch stats below)
2022-05-13 00:27:08 | INFO | train | epoch 011 | loss 11.488 | ppl 2871.93 | wps 24098.2 | ups 0.18 | wpb 130930 | bsz 255.7 | num_updates 429 | lr 1.08143e-05 | gnorm 0.846 | train_wall 193 | gb_free 7.9 | wall 2338
2022-05-13 00:27:08 | INFO | fairseq.data.iterators | grouped total_num_itrs = 39
2022-05-13 00:27:08 | INFO | fairseq.trainer | begin training epoch 12
2022-05-13 00:27:08 | INFO | fairseq_cli.train | Start iterating over samples
2022-05-13 00:30:25 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-05-13 00:30:29 | INFO | valid | epoch 012 | valid on 'valid' subset | loss 10.956 | ppl 1985.83 | wps 76708.4 | wpb 2041.1 | bsz 4 | num_updates 468 | best_loss 10.956
2022-05-13 00:30:29 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 12 @ 468 updates
2022-05-13 00:30:29 | INFO | fairseq.trainer | Saving checkpoint to /cluster/work/cotterell/liam/master-thesis/checkpoints/language_model/standard/checkpoint_best_standard_64_1.0000E-04.pt
2022-05-13 00:30:30 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/work/cotterell/liam/master-thesis/checkpoints/language_model/standard/checkpoint_best_standard_64_1.0000E-04.pt
2022-05-13 00:30:30 | INFO | fairseq.checkpoint_utils | Saved checkpoint checkpoints/language_model/standard/checkpoint_best_standard_64_1.0000E-04.pt (epoch 12 @ 468 updates, score 10.956) (writing took 1.1139054200612009 seconds)
2022-05-13 00:30:30 | INFO | fairseq_cli.train | end of epoch 12 (average epoch stats below)
2022-05-13 00:30:30 | INFO | train | epoch 012 | loss 11.168 | ppl 2301.67 | wps 25190.1 | ups 0.19 | wpb 130930 | bsz 255.7 | num_updates 468 | lr 1.17883e-05 | gnorm 0.757 | train_wall 184 | gb_free 7.9 | wall 2540
2022-05-13 00:30:30 | INFO | fairseq.data.iterators | grouped total_num_itrs = 39
2022-05-13 00:30:30 | INFO | fairseq.trainer | begin training epoch 13
2022-05-13 00:30:30 | INFO | fairseq_cli.train | Start iterating over samples
2022-05-13 00:33:06 | INFO | train_inner | epoch 013:     32 / 39 loss=11.161, ppl=2289.24, wps=25600.8, ups=0.2, wpb=130946, bsz=255.8, num_updates=500, lr=1.25875e-05, gnorm=0.754, train_wall=467, gb_free=7.9, wall=2695
2022-05-13 00:33:39 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-05-13 00:33:43 | INFO | valid | epoch 013 | valid on 'valid' subset | loss 10.666 | ppl 1624.57 | wps 76761.7 | wpb 2041.1 | bsz 4 | num_updates 507 | best_loss 10.666
2022-05-13 00:33:43 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 13 @ 507 updates
2022-05-13 00:33:43 | INFO | fairseq.trainer | Saving checkpoint to /cluster/work/cotterell/liam/master-thesis/checkpoints/language_model/standard/checkpoint_best_standard_64_1.0000E-04.pt
2022-05-13 00:33:44 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/work/cotterell/liam/master-thesis/checkpoints/language_model/standard/checkpoint_best_standard_64_1.0000E-04.pt
2022-05-13 00:33:44 | INFO | fairseq.checkpoint_utils | Saved checkpoint checkpoints/language_model/standard/checkpoint_best_standard_64_1.0000E-04.pt (epoch 13 @ 507 updates, score 10.666) (writing took 1.0699218329973519 seconds)
2022-05-13 00:33:44 | INFO | fairseq_cli.train | end of epoch 13 (average epoch stats below)
2022-05-13 00:33:44 | INFO | train | epoch 013 | loss 10.867 | ppl 1868.24 | wps 26403.6 | ups 0.2 | wpb 130930 | bsz 255.7 | num_updates 507 | lr 1.27623e-05 | gnorm 0.672 | train_wall 177 | gb_free 7.9 | wall 2734
2022-05-13 00:33:44 | INFO | fairseq.data.iterators | grouped total_num_itrs = 39
2022-05-13 00:33:44 | INFO | fairseq.trainer | begin training epoch 14
2022-05-13 00:33:44 | INFO | fairseq_cli.train | Start iterating over samples
2022-05-13 00:36:50 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-05-13 00:36:54 | INFO | valid | epoch 014 | valid on 'valid' subset | loss 10.403 | ppl 1354.14 | wps 76848.2 | wpb 2041.1 | bsz 4 | num_updates 546 | best_loss 10.403
2022-05-13 00:36:54 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 14 @ 546 updates
2022-05-13 00:36:54 | INFO | fairseq.trainer | Saving checkpoint to /cluster/work/cotterell/liam/master-thesis/checkpoints/language_model/standard/checkpoint_best_standard_64_1.0000E-04.pt
2022-05-13 00:36:55 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/work/cotterell/liam/master-thesis/checkpoints/language_model/standard/checkpoint_best_standard_64_1.0000E-04.pt
2022-05-13 00:36:55 | INFO | fairseq.checkpoint_utils | Saved checkpoint checkpoints/language_model/standard/checkpoint_best_standard_64_1.0000E-04.pt (epoch 14 @ 546 updates, score 10.403) (writing took 1.1274244179949164 seconds)
2022-05-13 00:36:55 | INFO | fairseq_cli.train | end of epoch 14 (average epoch stats below)
2022-05-13 00:36:55 | INFO | train | epoch 014 | loss 10.592 | ppl 1543.64 | wps 26660.4 | ups 0.2 | wpb 130930 | bsz 255.7 | num_updates 546 | lr 1.37364e-05 | gnorm 0.59 | train_wall 178 | gb_free 7.9 | wall 2925
2022-05-13 00:36:55 | INFO | fairseq.data.iterators | grouped total_num_itrs = 39
2022-05-13 00:36:55 | INFO | fairseq.trainer | begin training epoch 15
2022-05-13 00:36:55 | INFO | fairseq_cli.train | Start iterating over samples
2022-05-13 00:40:03 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-05-13 00:40:07 | INFO | valid | epoch 015 | valid on 'valid' subset | loss 10.167 | ppl 1149.61 | wps 76816.7 | wpb 2041.1 | bsz 4 | num_updates 585 | best_loss 10.167
2022-05-13 00:40:07 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 15 @ 585 updates
2022-05-13 00:40:07 | INFO | fairseq.trainer | Saving checkpoint to /cluster/work/cotterell/liam/master-thesis/checkpoints/language_model/standard/checkpoint_best_standard_64_1.0000E-04.pt
2022-05-13 00:40:08 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/work/cotterell/liam/master-thesis/checkpoints/language_model/standard/checkpoint_best_standard_64_1.0000E-04.pt
2022-05-13 00:40:08 | INFO | fairseq.checkpoint_utils | Saved checkpoint checkpoints/language_model/standard/checkpoint_best_standard_64_1.0000E-04.pt (epoch 15 @ 585 updates, score 10.167) (writing took 1.1915116370655596 seconds)
2022-05-13 00:40:08 | INFO | fairseq_cli.train | end of epoch 15 (average epoch stats below)
2022-05-13 00:40:08 | INFO | train | epoch 015 | loss 10.344 | ppl 1299.75 | wps 26472.6 | ups 0.2 | wpb 130930 | bsz 255.7 | num_updates 585 | lr 1.47104e-05 | gnorm 0.529 | train_wall 178 | gb_free 7.9 | wall 3118
2022-05-13 00:40:08 | INFO | fairseq.data.iterators | grouped total_num_itrs = 39
2022-05-13 00:40:08 | INFO | fairseq.trainer | begin training epoch 16
2022-05-13 00:40:08 | INFO | fairseq_cli.train | Start iterating over samples
2022-05-13 00:41:22 | INFO | train_inner | epoch 016:     15 / 39 loss=10.445, ppl=1394.14, wps=26401.7, ups=0.2, wpb=130916, bsz=255.7, num_updates=600, lr=1.5085e-05, gnorm=0.556, train_wall=456, gb_free=7.9, wall=3191
2022-05-13 00:43:29 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-05-13 00:43:34 | INFO | valid | epoch 016 | valid on 'valid' subset | loss 9.955 | ppl 992.67 | wps 76932 | wpb 2041.1 | bsz 4 | num_updates 624 | best_loss 9.955
2022-05-13 00:43:34 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 16 @ 624 updates
2022-05-13 00:43:34 | INFO | fairseq.trainer | Saving checkpoint to /cluster/work/cotterell/liam/master-thesis/checkpoints/language_model/standard/checkpoint_best_standard_64_1.0000E-04.pt
2022-05-13 00:43:35 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/work/cotterell/liam/master-thesis/checkpoints/language_model/standard/checkpoint_best_standard_64_1.0000E-04.pt
2022-05-13 00:43:35 | INFO | fairseq.checkpoint_utils | Saved checkpoint checkpoints/language_model/standard/checkpoint_best_standard_64_1.0000E-04.pt (epoch 16 @ 624 updates, score 9.955) (writing took 1.2655902998521924 seconds)
2022-05-13 00:43:35 | INFO | fairseq_cli.train | end of epoch 16 (average epoch stats below)
2022-05-13 00:43:35 | INFO | train | epoch 016 | loss 10.123 | ppl 1114.96 | wps 24713 | ups 0.19 | wpb 130930 | bsz 255.7 | num_updates 624 | lr 1.56844e-05 | gnorm 0.484 | train_wall 188 | gb_free 7.9 | wall 3325
2022-05-13 00:43:35 | INFO | fairseq.data.iterators | grouped total_num_itrs = 39
2022-05-13 00:43:35 | INFO | fairseq.trainer | begin training epoch 17
2022-05-13 00:43:35 | INFO | fairseq_cli.train | Start iterating over samples
2022-05-13 00:46:48 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-05-13 00:46:52 | INFO | valid | epoch 017 | valid on 'valid' subset | loss 9.773 | ppl 875.21 | wps 76911.5 | wpb 2041.1 | bsz 4 | num_updates 663 | best_loss 9.773
2022-05-13 00:46:52 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 17 @ 663 updates
2022-05-13 00:46:52 | INFO | fairseq.trainer | Saving checkpoint to /cluster/work/cotterell/liam/master-thesis/checkpoints/language_model/standard/checkpoint_best_standard_64_1.0000E-04.pt
2022-05-13 00:46:53 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/work/cotterell/liam/master-thesis/checkpoints/language_model/standard/checkpoint_best_standard_64_1.0000E-04.pt
2022-05-13 00:46:53 | INFO | fairseq.checkpoint_utils | Saved checkpoint checkpoints/language_model/standard/checkpoint_best_standard_64_1.0000E-04.pt (epoch 17 @ 663 updates, score 9.773) (writing took 1.1191537259146571 seconds)
2022-05-13 00:46:53 | INFO | fairseq_cli.train | end of epoch 17 (average epoch stats below)
2022-05-13 00:46:53 | INFO | train | epoch 017 | loss 9.929 | ppl 974.55 | wps 25789.8 | ups 0.2 | wpb 130930 | bsz 255.7 | num_updates 663 | lr 1.66584e-05 | gnorm 0.438 | train_wall 180 | gb_free 7.9 | wall 3523
2022-05-13 00:46:53 | INFO | fairseq.data.iterators | grouped total_num_itrs = 39
2022-05-13 00:46:53 | INFO | fairseq.trainer | begin training epoch 18
2022-05-13 00:46:53 | INFO | fairseq_cli.train | Start iterating over samples
2022-05-13 00:49:51 | INFO | train_inner | epoch 018:     37 / 39 loss=9.905, ppl=958.46, wps=25728.2, ups=0.2, wpb=130951, bsz=255.8, num_updates=700, lr=1.75825e-05, gnorm=0.426, train_wall=469, gb_free=7.9, wall=3700
2022-05-13 00:50:01 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-05-13 00:50:05 | INFO | valid | epoch 018 | valid on 'valid' subset | loss 9.616 | ppl 784.96 | wps 76771.7 | wpb 2041.1 | bsz 4 | num_updates 702 | best_loss 9.616
2022-05-13 00:50:05 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 18 @ 702 updates
2022-05-13 00:50:05 | INFO | fairseq.trainer | Saving checkpoint to /cluster/work/cotterell/liam/master-thesis/checkpoints/language_model/standard/checkpoint_best_standard_64_1.0000E-04.pt
2022-05-13 00:50:06 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/work/cotterell/liam/master-thesis/checkpoints/language_model/standard/checkpoint_best_standard_64_1.0000E-04.pt
2022-05-13 00:50:06 | INFO | fairseq.checkpoint_utils | Saved checkpoint checkpoints/language_model/standard/checkpoint_best_standard_64_1.0000E-04.pt (epoch 18 @ 702 updates, score 9.616) (writing took 1.1523902816697955 seconds)
2022-05-13 00:50:06 | INFO | fairseq_cli.train | end of epoch 18 (average epoch stats below)
2022-05-13 00:50:06 | INFO | train | epoch 018 | loss 9.759 | ppl 866.66 | wps 26457.9 | ups 0.2 | wpb 130930 | bsz 255.7 | num_updates 702 | lr 1.76325e-05 | gnorm 0.38 | train_wall 178 | gb_free 7.9 | wall 3716
2022-05-13 00:50:06 | INFO | fairseq.data.iterators | grouped total_num_itrs = 39
2022-05-13 00:50:06 | INFO | fairseq.trainer | begin training epoch 19
2022-05-13 00:50:06 | INFO | fairseq_cli.train | Start iterating over samples
2022-05-13 00:53:25 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-05-13 00:53:29 | INFO | valid | epoch 019 | valid on 'valid' subset | loss 9.477 | ppl 712.49 | wps 77069 | wpb 2041.1 | bsz 4 | num_updates 741 | best_loss 9.477
2022-05-13 00:53:29 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 19 @ 741 updates
2022-05-13 00:53:29 | INFO | fairseq.trainer | Saving checkpoint to /cluster/work/cotterell/liam/master-thesis/checkpoints/language_model/standard/checkpoint_best_standard_64_1.0000E-04.pt
2022-05-13 00:53:30 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/work/cotterell/liam/master-thesis/checkpoints/language_model/standard/checkpoint_best_standard_64_1.0000E-04.pt
2022-05-13 00:53:30 | INFO | fairseq.checkpoint_utils | Saved checkpoint checkpoints/language_model/standard/checkpoint_best_standard_64_1.0000E-04.pt (epoch 19 @ 741 updates, score 9.477) (writing took 1.3056437796913087 seconds)
2022-05-13 00:53:30 | INFO | fairseq_cli.train | end of epoch 19 (average epoch stats below)
2022-05-13 00:53:30 | INFO | train | epoch 019 | loss 9.611 | ppl 782.24 | wps 25000 | ups 0.19 | wpb 130930 | bsz 255.7 | num_updates 741 | lr 1.86065e-05 | gnorm 0.385 | train_wall 186 | gb_free 7.9 | wall 3920
2022-05-13 00:53:30 | INFO | fairseq.data.iterators | grouped total_num_itrs = 39
2022-05-13 00:53:30 | INFO | fairseq.trainer | begin training epoch 20
2022-05-13 00:53:30 | INFO | fairseq_cli.train | Start iterating over samples
2022-05-13 00:56:36 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-05-13 00:56:40 | INFO | valid | epoch 020 | valid on 'valid' subset | loss 9.354 | ppl 654.45 | wps 76745.8 | wpb 2041.1 | bsz 4 | num_updates 780 | best_loss 9.354
2022-05-13 00:56:40 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 20 @ 780 updates
2022-05-13 00:56:40 | INFO | fairseq.trainer | Saving checkpoint to /cluster/work/cotterell/liam/master-thesis/checkpoints/language_model/standard/checkpoint_best_standard_64_1.0000E-04.pt
2022-05-13 00:56:41 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/work/cotterell/liam/master-thesis/checkpoints/language_model/standard/checkpoint_best_standard_64_1.0000E-04.pt
2022-05-13 00:56:41 | INFO | fairseq.checkpoint_utils | Saved checkpoint checkpoints/language_model/standard/checkpoint_best_standard_64_1.0000E-04.pt (epoch 20 @ 780 updates, score 9.354) (writing took 1.1550518870353699 seconds)
2022-05-13 00:56:41 | INFO | fairseq_cli.train | end of epoch 20 (average epoch stats below)
2022-05-13 00:56:41 | INFO | train | epoch 020 | loss 9.481 | ppl 714.67 | wps 26717.3 | ups 0.2 | wpb 130930 | bsz 255.7 | num_updates 780 | lr 1.95805e-05 | gnorm 0.431 | train_wall 177 | gb_free 7.9 | wall 4111
2022-05-13 00:56:41 | INFO | fairseq.data.iterators | grouped total_num_itrs = 39
2022-05-13 00:56:41 | INFO | fairseq.trainer | begin training epoch 21
2022-05-13 00:56:41 | INFO | fairseq_cli.train | Start iterating over samples
2022-05-13 00:58:20 | INFO | train_inner | epoch 021:     20 / 39 loss=9.517, ppl=732.51, wps=25718.5, ups=0.2, wpb=130916, bsz=255.7, num_updates=800, lr=2.008e-05, gnorm=0.434, train_wall=466, gb_free=7.9, wall=4209
2022-05-13 00:59:51 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-05-13 00:59:55 | INFO | valid | epoch 021 | valid on 'valid' subset | loss 9.244 | ppl 606.14 | wps 76817.3 | wpb 2041.1 | bsz 4 | num_updates 819 | best_loss 9.244
2022-05-13 00:59:55 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 21 @ 819 updates
2022-05-13 00:59:55 | INFO | fairseq.trainer | Saving checkpoint to /cluster/work/cotterell/liam/master-thesis/checkpoints/language_model/standard/checkpoint_best_standard_64_1.0000E-04.pt
2022-05-13 00:59:56 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/work/cotterell/liam/master-thesis/checkpoints/language_model/standard/checkpoint_best_standard_64_1.0000E-04.pt
2022-05-13 00:59:56 | INFO | fairseq.checkpoint_utils | Saved checkpoint checkpoints/language_model/standard/checkpoint_best_standard_64_1.0000E-04.pt (epoch 21 @ 819 updates, score 9.244) (writing took 1.2624011812731624 seconds)
2022-05-13 00:59:56 | INFO | fairseq_cli.train | end of epoch 21 (average epoch stats below)
2022-05-13 00:59:56 | INFO | train | epoch 021 | loss 9.366 | ppl 659.76 | wps 26182.6 | ups 0.2 | wpb 130930 | bsz 255.7 | num_updates 819 | lr 2.05545e-05 | gnorm 0.466 | train_wall 180 | gb_free 7.9 | wall 4306
2022-05-13 00:59:56 | INFO | fairseq.data.iterators | grouped total_num_itrs = 39
2022-05-13 00:59:56 | INFO | fairseq.trainer | begin training epoch 22
2022-05-13 00:59:56 | INFO | fairseq_cli.train | Start iterating over samples
2022-05-13 01:03:15 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-05-13 01:03:19 | INFO | valid | epoch 022 | valid on 'valid' subset | loss 9.141 | ppl 564.59 | wps 76727.7 | wpb 2041.1 | bsz 4 | num_updates 858 | best_loss 9.141
2022-05-13 01:03:19 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 22 @ 858 updates
2022-05-13 01:03:19 | INFO | fairseq.trainer | Saving checkpoint to /cluster/work/cotterell/liam/master-thesis/checkpoints/language_model/standard/checkpoint_best_standard_64_1.0000E-04.pt
2022-05-13 01:03:20 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/work/cotterell/liam/master-thesis/checkpoints/language_model/standard/checkpoint_best_standard_64_1.0000E-04.pt
2022-05-13 01:03:20 | INFO | fairseq.checkpoint_utils | Saved checkpoint checkpoints/language_model/standard/checkpoint_best_standard_64_1.0000E-04.pt (epoch 22 @ 858 updates, score 9.141) (writing took 1.0796768860891461 seconds)
2022-05-13 01:03:20 | INFO | fairseq_cli.train | end of epoch 22 (average epoch stats below)
2022-05-13 01:03:20 | INFO | train | epoch 022 | loss 9.26 | ppl 612.98 | wps 25067.9 | ups 0.19 | wpb 130930 | bsz 255.7 | num_updates 858 | lr 2.15285e-05 | gnorm 0.385 | train_wall 183 | gb_free 7.9 | wall 4510
2022-05-13 01:03:20 | INFO | fairseq.data.iterators | grouped total_num_itrs = 39
2022-05-13 01:03:20 | INFO | fairseq.trainer | begin training epoch 23
2022-05-13 01:03:20 | INFO | fairseq_cli.train | Start iterating over samples
2022-05-13 01:06:30 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-05-13 01:06:34 | INFO | valid | epoch 023 | valid on 'valid' subset | loss 9.048 | ppl 529.19 | wps 76668.3 | wpb 2041.1 | bsz 4 | num_updates 897 | best_loss 9.048
2022-05-13 01:06:34 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 23 @ 897 updates
2022-05-13 01:06:34 | INFO | fairseq.trainer | Saving checkpoint to /cluster/work/cotterell/liam/master-thesis/checkpoints/language_model/standard/checkpoint_best_standard_64_1.0000E-04.pt
2022-05-13 01:06:35 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/work/cotterell/liam/master-thesis/checkpoints/language_model/standard/checkpoint_best_standard_64_1.0000E-04.pt
2022-05-13 01:06:35 | INFO | fairseq.checkpoint_utils | Saved checkpoint checkpoints/language_model/standard/checkpoint_best_standard_64_1.0000E-04.pt (epoch 23 @ 897 updates, score 9.048) (writing took 1.0678083789534867 seconds)
2022-05-13 01:06:35 | INFO | fairseq_cli.train | end of epoch 23 (average epoch stats below)
2022-05-13 01:06:35 | INFO | train | epoch 023 | loss 9.16 | ppl 572.22 | wps 26149.3 | ups 0.2 | wpb 130930 | bsz 255.7 | num_updates 897 | lr 2.25026e-05 | gnorm 0.472 | train_wall 180 | gb_free 7.9 | wall 4705
2022-05-13 01:06:35 | INFO | fairseq.data.iterators | grouped total_num_itrs = 39
2022-05-13 01:06:35 | INFO | fairseq.trainer | begin training epoch 24
2022-05-13 01:06:35 | INFO | fairseq_cli.train | Start iterating over samples
2022-05-13 01:06:50 | INFO | train_inner | epoch 024:      3 / 39 loss=9.232, ppl=601.47, wps=25657.5, ups=0.2, wpb=130899, bsz=255.7, num_updates=900, lr=2.25775e-05, gnorm=0.432, train_wall=464, gb_free=7.9, wall=4720
2022-05-13 01:09:43 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-05-13 01:09:46 | INFO | valid | epoch 024 | valid on 'valid' subset | loss 8.951 | ppl 495.02 | wps 76610.7 | wpb 2041.1 | bsz 4 | num_updates 936 | best_loss 8.951
2022-05-13 01:09:46 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 24 @ 936 updates
2022-05-13 01:09:46 | INFO | fairseq.trainer | Saving checkpoint to /cluster/work/cotterell/liam/master-thesis/checkpoints/language_model/standard/checkpoint_best_standard_64_1.0000E-04.pt
2022-05-13 01:09:48 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/work/cotterell/liam/master-thesis/checkpoints/language_model/standard/checkpoint_best_standard_64_1.0000E-04.pt
2022-05-13 01:09:48 | INFO | fairseq.checkpoint_utils | Saved checkpoint checkpoints/language_model/standard/checkpoint_best_standard_64_1.0000E-04.pt (epoch 24 @ 936 updates, score 8.951) (writing took 1.3107070671394467 seconds)
2022-05-13 01:09:48 | INFO | fairseq_cli.train | end of epoch 24 (average epoch stats below)
2022-05-13 01:09:48 | INFO | train | epoch 024 | loss 9.066 | ppl 535.96 | wps 26516.6 | ups 0.2 | wpb 130930 | bsz 255.7 | num_updates 936 | lr 2.34766e-05 | gnorm 0.436 | train_wall 178 | gb_free 7.9 | wall 4898
2022-05-13 01:09:48 | INFO | fairseq.data.iterators | grouped total_num_itrs = 39
2022-05-13 01:09:48 | INFO | fairseq.trainer | begin training epoch 25
2022-05-13 01:09:48 | INFO | fairseq_cli.train | Start iterating over samples
2022-05-13 01:12:59 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-05-13 01:13:03 | INFO | valid | epoch 025 | valid on 'valid' subset | loss 8.862 | ppl 465.34 | wps 76723.4 | wpb 2041.1 | bsz 4 | num_updates 975 | best_loss 8.862
2022-05-13 01:13:03 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 25 @ 975 updates
2022-05-13 01:13:03 | INFO | fairseq.trainer | Saving checkpoint to /cluster/work/cotterell/liam/master-thesis/checkpoints/language_model/standard/checkpoint_best_standard_64_1.0000E-04.pt
2022-05-13 01:13:04 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/work/cotterell/liam/master-thesis/checkpoints/language_model/standard/checkpoint_best_standard_64_1.0000E-04.pt
2022-05-13 01:13:04 | INFO | fairseq.checkpoint_utils | Saved checkpoint checkpoints/language_model/standard/checkpoint_best_standard_64_1.0000E-04.pt (epoch 25 @ 975 updates, score 8.862) (writing took 1.092978022992611 seconds)
2022-05-13 01:13:04 | INFO | fairseq_cli.train | end of epoch 25 (average epoch stats below)
2022-05-13 01:13:04 | INFO | train | epoch 025 | loss 8.975 | ppl 503.31 | wps 25982.3 | ups 0.2 | wpb 130930 | bsz 255.7 | num_updates 975 | lr 2.44506e-05 | gnorm 0.47 | train_wall 181 | gb_free 7.9 | wall 5094
2022-05-13 01:13:04 | INFO | fairseq.data.iterators | grouped total_num_itrs = 39
2022-05-13 01:13:04 | INFO | fairseq.trainer | begin training epoch 26
2022-05-13 01:13:04 | INFO | fairseq_cli.train | Start iterating over samples
2022-05-13 01:15:10 | INFO | train_inner | epoch 026:     25 / 39 loss=8.988, ppl=507.81, wps=26172.4, ups=0.2, wpb=130967, bsz=255.8, num_updates=1000, lr=2.5075e-05, gnorm=0.497, train_wall=463, gb_free=7.9, wall=5220
2022-05-13 01:16:18 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-05-13 01:16:25 | INFO | valid | epoch 026 | valid on 'valid' subset | loss 8.777 | ppl 438.8 | wps 39870.6 | wpb 2041.1 | bsz 4 | num_updates 1014 | best_loss 8.777
2022-05-13 01:16:25 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 26 @ 1014 updates
2022-05-13 01:16:25 | INFO | fairseq.trainer | Saving checkpoint to /cluster/work/cotterell/liam/master-thesis/checkpoints/language_model/standard/checkpoint_best_standard_64_1.0000E-04.pt
2022-05-13 01:16:26 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/work/cotterell/liam/master-thesis/checkpoints/language_model/standard/checkpoint_best_standard_64_1.0000E-04.pt
2022-05-13 01:16:26 | INFO | fairseq.checkpoint_utils | Saved checkpoint checkpoints/language_model/standard/checkpoint_best_standard_64_1.0000E-04.pt (epoch 26 @ 1014 updates, score 8.777) (writing took 1.2845528102479875 seconds)
2022-05-13 01:16:26 | INFO | fairseq_cli.train | end of epoch 26 (average epoch stats below)
2022-05-13 01:16:26 | INFO | train | epoch 026 | loss 8.888 | ppl 473.68 | wps 25266.9 | ups 0.19 | wpb 130930 | bsz 255.7 | num_updates 1014 | lr 2.54247e-05 | gnorm 0.594 | train_wall 181 | gb_free 7.9 | wall 5296
2022-05-13 01:16:26 | INFO | fairseq.data.iterators | grouped total_num_itrs = 39
2022-05-13 01:16:26 | INFO | fairseq.trainer | begin training epoch 27
2022-05-13 01:16:26 | INFO | fairseq_cli.train | Start iterating over samples
2022-05-13 01:19:47 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-05-13 01:19:51 | INFO | valid | epoch 027 | valid on 'valid' subset | loss 8.698 | ppl 415.18 | wps 76649.9 | wpb 2041.1 | bsz 4 | num_updates 1053 | best_loss 8.698
2022-05-13 01:19:51 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 27 @ 1053 updates
2022-05-13 01:19:51 | INFO | fairseq.trainer | Saving checkpoint to /cluster/work/cotterell/liam/master-thesis/checkpoints/language_model/standard/checkpoint_best_standard_64_1.0000E-04.pt
2022-05-13 01:19:52 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/work/cotterell/liam/master-thesis/checkpoints/language_model/standard/checkpoint_best_standard_64_1.0000E-04.pt
2022-05-13 01:19:52 | INFO | fairseq.checkpoint_utils | Saved checkpoint checkpoints/language_model/standard/checkpoint_best_standard_64_1.0000E-04.pt (epoch 27 @ 1053 updates, score 8.698) (writing took 1.249459533020854 seconds)
2022-05-13 01:19:52 | INFO | fairseq_cli.train | end of epoch 27 (average epoch stats below)
2022-05-13 01:19:52 | INFO | train | epoch 027 | loss 8.803 | ppl 446.56 | wps 24798 | ups 0.19 | wpb 130930 | bsz 255.7 | num_updates 1053 | lr 2.63987e-05 | gnorm 0.492 | train_wall 185 | gb_free 7.9 | wall 5502
2022-05-13 01:19:52 | INFO | fairseq.data.iterators | grouped total_num_itrs = 39
2022-05-13 01:19:52 | INFO | fairseq.trainer | begin training epoch 28
2022-05-13 01:19:52 | INFO | fairseq_cli.train | Start iterating over samples
2022-05-13 01:23:07 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-05-13 01:23:11 | INFO | valid | epoch 028 | valid on 'valid' subset | loss 8.616 | ppl 392.4 | wps 76766.5 | wpb 2041.1 | bsz 4 | num_updates 1092 | best_loss 8.616
2022-05-13 01:23:11 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 28 @ 1092 updates
2022-05-13 01:23:11 | INFO | fairseq.trainer | Saving checkpoint to /cluster/work/cotterell/liam/master-thesis/checkpoints/language_model/standard/checkpoint_best_standard_64_1.0000E-04.pt
2022-05-13 01:23:12 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/work/cotterell/liam/master-thesis/checkpoints/language_model/standard/checkpoint_best_standard_64_1.0000E-04.pt
2022-05-13 01:23:12 | INFO | fairseq.checkpoint_utils | Saved checkpoint checkpoints/language_model/standard/checkpoint_best_standard_64_1.0000E-04.pt (epoch 28 @ 1092 updates, score 8.616) (writing took 1.100507149938494 seconds)
2022-05-13 01:23:12 | INFO | fairseq_cli.train | end of epoch 28 (average epoch stats below)
2022-05-13 01:23:12 | INFO | train | epoch 028 | loss 8.721 | ppl 422.03 | wps 25557.1 | ups 0.2 | wpb 130930 | bsz 255.7 | num_updates 1092 | lr 2.73727e-05 | gnorm 0.618 | train_wall 183 | gb_free 7.9 | wall 5702
2022-05-13 01:23:12 | INFO | fairseq.data.iterators | grouped total_num_itrs = 39
2022-05-13 01:23:12 | INFO | fairseq.trainer | begin training epoch 29
2022-05-13 01:23:12 | INFO | fairseq_cli.train | Start iterating over samples
2022-05-13 01:23:50 | INFO | train_inner | epoch 029:      8 / 39 loss=8.769, ppl=436.36, wps=25160.6, ups=0.19, wpb=130910, bsz=255.7, num_updates=1100, lr=2.75725e-05, gnorm=0.561, train_wall=468, gb_free=7.9, wall=5740
2022-05-13 01:26:19 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-05-13 01:26:22 | INFO | valid | epoch 029 | valid on 'valid' subset | loss 8.542 | ppl 372.76 | wps 76577.2 | wpb 2041.1 | bsz 4 | num_updates 1131 | best_loss 8.542
2022-05-13 01:26:22 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 29 @ 1131 updates
2022-05-13 01:26:22 | INFO | fairseq.trainer | Saving checkpoint to /cluster/work/cotterell/liam/master-thesis/checkpoints/language_model/standard/checkpoint_best_standard_64_1.0000E-04.pt
2022-05-13 01:26:23 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/work/cotterell/liam/master-thesis/checkpoints/language_model/standard/checkpoint_best_standard_64_1.0000E-04.pt
2022-05-13 01:26:23 | INFO | fairseq.checkpoint_utils | Saved checkpoint checkpoints/language_model/standard/checkpoint_best_standard_64_1.0000E-04.pt (epoch 29 @ 1131 updates, score 8.542) (writing took 1.0594011629000306 seconds)
2022-05-13 01:26:23 | INFO | fairseq_cli.train | end of epoch 29 (average epoch stats below)
2022-05-13 01:26:23 | INFO | train | epoch 029 | loss 8.642 | ppl 399.55 | wps 26698.4 | ups 0.2 | wpb 130930 | bsz 255.7 | num_updates 1131 | lr 2.83467e-05 | gnorm 0.668 | train_wall 178 | gb_free 7.9 | wall 5893
2022-05-13 01:26:23 | INFO | fairseq.data.iterators | grouped total_num_itrs = 39
2022-05-13 01:26:23 | INFO | fairseq.trainer | begin training epoch 30
2022-05-13 01:26:23 | INFO | fairseq_cli.train | Start iterating over samples
2022-05-13 01:29:31 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-05-13 01:29:35 | INFO | valid | epoch 030 | valid on 'valid' subset | loss 8.469 | ppl 354.22 | wps 76546.4 | wpb 2041.1 | bsz 4 | num_updates 1170 | best_loss 8.469
2022-05-13 01:29:35 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 30 @ 1170 updates
2022-05-13 01:29:35 | INFO | fairseq.trainer | Saving checkpoint to /cluster/work/cotterell/liam/master-thesis/checkpoints/language_model/standard/checkpoint_best_standard_64_1.0000E-04.pt
2022-05-13 01:29:36 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/work/cotterell/liam/master-thesis/checkpoints/language_model/standard/checkpoint_best_standard_64_1.0000E-04.pt
2022-05-13 01:29:36 | INFO | fairseq.checkpoint_utils | Saved checkpoint checkpoints/language_model/standard/checkpoint_best_standard_64_1.0000E-04.pt (epoch 30 @ 1170 updates, score 8.469) (writing took 1.1488106539472938 seconds)
2022-05-13 01:29:36 | INFO | fairseq_cli.train | end of epoch 30 (average epoch stats below)
2022-05-13 01:29:36 | INFO | train | epoch 030 | loss 8.565 | ppl 378.78 | wps 26505.7 | ups 0.2 | wpb 130930 | bsz 255.7 | num_updates 1170 | lr 2.93207e-05 | gnorm 0.597 | train_wall 178 | gb_free 7.9 | wall 6086
2022-05-13 01:29:36 | INFO | fairseq.data.iterators | grouped total_num_itrs = 39
2022-05-13 01:29:36 | INFO | fairseq.trainer | begin training epoch 31
2022-05-13 01:29:36 | INFO | fairseq_cli.train | Start iterating over samples
2022-05-13 01:32:11 | INFO | train_inner | epoch 031:     30 / 39 loss=8.567, ppl=379.27, wps=26167.4, ups=0.2, wpb=130951, bsz=255.8, num_updates=1200, lr=3.007e-05, gnorm=0.636, train_wall=463, gb_free=7.9, wall=6241
2022-05-13 01:32:54 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-05-13 01:32:58 | INFO | valid | epoch 031 | valid on 'valid' subset | loss 8.391 | ppl 335.76 | wps 76704.7 | wpb 2041.1 | bsz 4 | num_updates 1209 | best_loss 8.391
2022-05-13 01:32:58 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 31 @ 1209 updates
2022-05-13 01:32:58 | INFO | fairseq.trainer | Saving checkpoint to /cluster/work/cotterell/liam/master-thesis/checkpoints/language_model/standard/checkpoint_best_standard_64_1.0000E-04.pt
2022-05-13 01:32:59 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/work/cotterell/liam/master-thesis/checkpoints/language_model/standard/checkpoint_best_standard_64_1.0000E-04.pt
2022-05-13 01:32:59 | INFO | fairseq.checkpoint_utils | Saved checkpoint checkpoints/language_model/standard/checkpoint_best_standard_64_1.0000E-04.pt (epoch 31 @ 1209 updates, score 8.391) (writing took 1.1444196300581098 seconds)
2022-05-13 01:32:59 | INFO | fairseq_cli.train | end of epoch 31 (average epoch stats below)
2022-05-13 01:32:59 | INFO | train | epoch 031 | loss 8.492 | ppl 359.97 | wps 25119.6 | ups 0.19 | wpb 130930 | bsz 255.7 | num_updates 1209 | lr 3.02948e-05 | gnorm 0.661 | train_wall 185 | gb_free 7.9 | wall 6289
2022-05-13 01:32:59 | INFO | fairseq.data.iterators | grouped total_num_itrs = 39
2022-05-13 01:32:59 | INFO | fairseq.trainer | begin training epoch 32
2022-05-13 01:32:59 | INFO | fairseq_cli.train | Start iterating over samples
2022-05-13 01:36:10 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-05-13 01:36:14 | INFO | valid | epoch 032 | valid on 'valid' subset | loss 8.325 | ppl 320.74 | wps 76553.9 | wpb 2041.1 | bsz 4 | num_updates 1248 | best_loss 8.325
2022-05-13 01:36:14 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 32 @ 1248 updates
2022-05-13 01:36:14 | INFO | fairseq.trainer | Saving checkpoint to /cluster/work/cotterell/liam/master-thesis/checkpoints/language_model/standard/checkpoint_best_standard_64_1.0000E-04.pt
2022-05-13 01:36:15 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/work/cotterell/liam/master-thesis/checkpoints/language_model/standard/checkpoint_best_standard_64_1.0000E-04.pt
2022-05-13 01:36:15 | INFO | fairseq.checkpoint_utils | Saved checkpoint checkpoints/language_model/standard/checkpoint_best_standard_64_1.0000E-04.pt (epoch 32 @ 1248 updates, score 8.325) (writing took 1.2797289011068642 seconds)
2022-05-13 01:36:15 | INFO | fairseq_cli.train | end of epoch 32 (average epoch stats below)
2022-05-13 01:36:15 | INFO | train | epoch 032 | loss 8.419 | ppl 342.34 | wps 26078.9 | ups 0.2 | wpb 130930 | bsz 255.7 | num_updates 1248 | lr 3.12688e-05 | gnorm 0.478 | train_wall 181 | gb_free 7.9 | wall 6485
2022-05-13 01:36:15 | INFO | fairseq.data.iterators | grouped total_num_itrs = 39
2022-05-13 01:36:15 | INFO | fairseq.trainer | begin training epoch 33
2022-05-13 01:36:15 | INFO | fairseq_cli.train | Start iterating over samples
2022-05-13 01:39:27 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-05-13 01:39:31 | INFO | valid | epoch 033 | valid on 'valid' subset | loss 8.259 | ppl 306.45 | wps 76583.9 | wpb 2041.1 | bsz 4 | num_updates 1287 | best_loss 8.259
2022-05-13 01:39:31 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 33 @ 1287 updates
2022-05-13 01:39:31 | INFO | fairseq.trainer | Saving checkpoint to /cluster/work/cotterell/liam/master-thesis/checkpoints/language_model/standard/checkpoint_best_standard_64_1.0000E-04.pt
2022-05-13 01:39:32 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/work/cotterell/liam/master-thesis/checkpoints/language_model/standard/checkpoint_best_standard_64_1.0000E-04.pt
2022-05-13 01:39:32 | INFO | fairseq.checkpoint_utils | Saved checkpoint checkpoints/language_model/standard/checkpoint_best_standard_64_1.0000E-04.pt (epoch 33 @ 1287 updates, score 8.259) (writing took 1.2074855887331069 seconds)
2022-05-13 01:39:32 | INFO | fairseq_cli.train | end of epoch 33 (average epoch stats below)
2022-05-13 01:39:32 | INFO | train | epoch 033 | loss 8.351 | ppl 326.4 | wps 25898.9 | ups 0.2 | wpb 130930 | bsz 255.7 | num_updates 1287 | lr 3.22428e-05 | gnorm 0.659 | train_wall 180 | gb_free 7.9 | wall 6682
2022-05-13 01:39:32 | INFO | fairseq.data.iterators | grouped total_num_itrs = 39
2022-05-13 01:39:32 | INFO | fairseq.trainer | begin training epoch 34
2022-05-13 01:39:32 | INFO | fairseq_cli.train | Start iterating over samples
2022-05-13 01:40:35 | INFO | train_inner | epoch 034:     13 / 39 loss=8.381, ppl=333.48, wps=25988, ups=0.2, wpb=130916, bsz=255.7, num_updates=1300, lr=3.25675e-05, gnorm=0.569, train_wall=462, gb_free=7.9, wall=6744
2022-05-13 01:42:39 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-05-13 01:42:43 | INFO | valid | epoch 034 | valid on 'valid' subset | loss 8.197 | ppl 293.52 | wps 76510.7 | wpb 2041.1 | bsz 4 | num_updates 1326 | best_loss 8.197
2022-05-13 01:42:43 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 34 @ 1326 updates
2022-05-13 01:42:43 | INFO | fairseq.trainer | Saving checkpoint to /cluster/work/cotterell/liam/master-thesis/checkpoints/language_model/standard/checkpoint_best_standard_64_1.0000E-04.pt
2022-05-13 01:42:44 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/work/cotterell/liam/master-thesis/checkpoints/language_model/standard/checkpoint_best_standard_64_1.0000E-04.pt
2022-05-13 01:42:44 | INFO | fairseq.checkpoint_utils | Saved checkpoint checkpoints/language_model/standard/checkpoint_best_standard_64_1.0000E-04.pt (epoch 34 @ 1326 updates, score 8.197) (writing took 1.1103118020109832 seconds)
2022-05-13 01:42:44 | INFO | fairseq_cli.train | end of epoch 34 (average epoch stats below)
2022-05-13 01:42:44 | INFO | train | epoch 034 | loss 8.284 | ppl 311.78 | wps 26646.7 | ups 0.2 | wpb 130930 | bsz 255.7 | num_updates 1326 | lr 3.32169e-05 | gnorm 0.556 | train_wall 178 | gb_free 7.9 | wall 6874
2022-05-13 01:42:44 | INFO | fairseq.data.iterators | grouped total_num_itrs = 39
2022-05-13 01:42:44 | INFO | fairseq.trainer | begin training epoch 35
2022-05-13 01:42:44 | INFO | fairseq_cli.train | Start iterating over samples
2022-05-13 01:45:54 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-05-13 01:45:58 | INFO | valid | epoch 035 | valid on 'valid' subset | loss 8.139 | ppl 281.87 | wps 76609.9 | wpb 2041.1 | bsz 4 | num_updates 1365 | best_loss 8.139
2022-05-13 01:45:58 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 35 @ 1365 updates
2022-05-13 01:45:58 | INFO | fairseq.trainer | Saving checkpoint to /cluster/work/cotterell/liam/master-thesis/checkpoints/language_model/standard/checkpoint_best_standard_64_1.0000E-04.pt
2022-05-13 01:45:59 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/work/cotterell/liam/master-thesis/checkpoints/language_model/standard/checkpoint_best_standard_64_1.0000E-04.pt
2022-05-13 01:45:59 | INFO | fairseq.checkpoint_utils | Saved checkpoint checkpoints/language_model/standard/checkpoint_best_standard_64_1.0000E-04.pt (epoch 35 @ 1365 updates, score 8.139) (writing took 1.166524349246174 seconds)
2022-05-13 01:45:59 | INFO | fairseq_cli.train | end of epoch 35 (average epoch stats below)
2022-05-13 01:45:59 | INFO | train | epoch 035 | loss 8.221 | ppl 298.32 | wps 26142.7 | ups 0.2 | wpb 130930 | bsz 255.7 | num_updates 1365 | lr 3.41909e-05 | gnorm 0.523 | train_wall 181 | gb_free 7.9 | wall 7069
2022-05-13 01:45:59 | INFO | fairseq.data.iterators | grouped total_num_itrs = 39
2022-05-13 01:45:59 | INFO | fairseq.trainer | begin training epoch 36
2022-05-13 01:45:59 | INFO | fairseq_cli.train | Start iterating over samples
2022-05-13 01:48:47 | INFO | train_inner | epoch 036:     35 / 39 loss=8.214, ppl=297.01, wps=26603.1, ups=0.2, wpb=130946, bsz=255.8, num_updates=1400, lr=3.5065e-05, gnorm=0.589, train_wall=459, gb_free=7.9, wall=7237
2022-05-13 01:49:11 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-05-13 01:49:14 | INFO | valid | epoch 036 | valid on 'valid' subset | loss 8.08 | ppl 270.63 | wps 76598.7 | wpb 2041.1 | bsz 4 | num_updates 1404 | best_loss 8.08
2022-05-13 01:49:14 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 36 @ 1404 updates
2022-05-13 01:49:14 | INFO | fairseq.trainer | Saving checkpoint to /cluster/work/cotterell/liam/master-thesis/checkpoints/language_model/standard/checkpoint_best_standard_64_1.0000E-04.pt
2022-05-13 01:49:16 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/work/cotterell/liam/master-thesis/checkpoints/language_model/standard/checkpoint_best_standard_64_1.0000E-04.pt
2022-05-13 01:49:16 | INFO | fairseq.checkpoint_utils | Saved checkpoint checkpoints/language_model/standard/checkpoint_best_standard_64_1.0000E-04.pt (epoch 36 @ 1404 updates, score 8.08) (writing took 1.3751599509269 seconds)
2022-05-13 01:49:16 | INFO | fairseq_cli.train | end of epoch 36 (average epoch stats below)
2022-05-13 01:49:16 | INFO | train | epoch 036 | loss 8.16 | ppl 286.08 | wps 25969.2 | ups 0.2 | wpb 130930 | bsz 255.7 | num_updates 1404 | lr 3.51649e-05 | gnorm 0.665 | train_wall 180 | gb_free 7.9 | wall 7266
2022-05-13 01:49:16 | INFO | fairseq.data.iterators | grouped total_num_itrs = 39
2022-05-13 01:49:16 | INFO | fairseq.trainer | begin training epoch 37
2022-05-13 01:49:16 | INFO | fairseq_cli.train | Start iterating over samples
2022-05-13 01:52:23 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-05-13 01:52:27 | INFO | valid | epoch 037 | valid on 'valid' subset | loss 8.027 | ppl 260.78 | wps 76593.2 | wpb 2041.1 | bsz 4 | num_updates 1443 | best_loss 8.027
2022-05-13 01:52:27 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 37 @ 1443 updates
2022-05-13 01:52:27 | INFO | fairseq.trainer | Saving checkpoint to /cluster/work/cotterell/liam/master-thesis/checkpoints/language_model/standard/checkpoint_best_standard_64_1.0000E-04.pt
2022-05-13 01:52:28 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/work/cotterell/liam/master-thesis/checkpoints/language_model/standard/checkpoint_best_standard_64_1.0000E-04.pt
2022-05-13 01:52:28 | INFO | fairseq.checkpoint_utils | Saved checkpoint checkpoints/language_model/standard/checkpoint_best_standard_64_1.0000E-04.pt (epoch 37 @ 1443 updates, score 8.027) (writing took 1.0828637261874974 seconds)
2022-05-13 01:52:28 | INFO | fairseq_cli.train | end of epoch 37 (average epoch stats below)
2022-05-13 01:52:28 | INFO | train | epoch 037 | loss 8.101 | ppl 274.53 | wps 26546.3 | ups 0.2 | wpb 130930 | bsz 255.7 | num_updates 1443 | lr 3.61389e-05 | gnorm 0.568 | train_wall 178 | gb_free 7.9 | wall 7458
2022-05-13 01:52:28 | INFO | fairseq.data.iterators | grouped total_num_itrs = 39
2022-05-13 01:52:28 | INFO | fairseq.trainer | begin training epoch 38
2022-05-13 01:52:28 | INFO | fairseq_cli.train | Start iterating over samples
2022-05-13 01:55:38 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-05-13 01:55:42 | INFO | valid | epoch 038 | valid on 'valid' subset | loss 7.975 | ppl 251.6 | wps 76640.5 | wpb 2041.1 | bsz 4 | num_updates 1482 | best_loss 7.975
2022-05-13 01:55:42 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 38 @ 1482 updates
2022-05-13 01:55:42 | INFO | fairseq.trainer | Saving checkpoint to /cluster/work/cotterell/liam/master-thesis/checkpoints/language_model/standard/checkpoint_best_standard_64_1.0000E-04.pt
2022-05-13 01:55:43 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/work/cotterell/liam/master-thesis/checkpoints/language_model/standard/checkpoint_best_standard_64_1.0000E-04.pt
2022-05-13 01:55:43 | INFO | fairseq.checkpoint_utils | Saved checkpoint checkpoints/language_model/standard/checkpoint_best_standard_64_1.0000E-04.pt (epoch 38 @ 1482 updates, score 7.975) (writing took 1.11167311668396 seconds)
2022-05-13 01:55:43 | INFO | fairseq_cli.train | end of epoch 38 (average epoch stats below)
2022-05-13 01:55:43 | INFO | train | epoch 038 | loss 8.044 | ppl 263.92 | wps 26252.6 | ups 0.2 | wpb 130930 | bsz 255.7 | num_updates 1482 | lr 3.7113e-05 | gnorm 0.597 | train_wall 180 | gb_free 7.9 | wall 7652
2022-05-13 01:55:43 | INFO | fairseq.data.iterators | grouped total_num_itrs = 39
2022-05-13 01:55:43 | INFO | fairseq.trainer | begin training epoch 39
2022-05-13 01:55:43 | INFO | fairseq_cli.train | Start iterating over samples
2022-05-13 01:57:10 | INFO | train_inner | epoch 039:     18 / 39 loss=8.062, ppl=267.28, wps=26015.7, ups=0.2, wpb=130916, bsz=255.7, num_updates=1500, lr=3.75625e-05, gnorm=0.566, train_wall=461, gb_free=7.9, wall=7740
2022-05-13 01:58:50 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-05-13 01:58:54 | INFO | valid | epoch 039 | valid on 'valid' subset | loss 7.921 | ppl 242.44 | wps 76545 | wpb 2041.1 | bsz 4 | num_updates 1521 | best_loss 7.921
2022-05-13 01:58:54 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 39 @ 1521 updates
2022-05-13 01:58:54 | INFO | fairseq.trainer | Saving checkpoint to /cluster/work/cotterell/liam/master-thesis/checkpoints/language_model/standard/checkpoint_best_standard_64_1.0000E-04.pt
2022-05-13 01:58:55 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/work/cotterell/liam/master-thesis/checkpoints/language_model/standard/checkpoint_best_standard_64_1.0000E-04.pt
2022-05-13 01:58:55 | INFO | fairseq.checkpoint_utils | Saved checkpoint checkpoints/language_model/standard/checkpoint_best_standard_64_1.0000E-04.pt (epoch 39 @ 1521 updates, score 7.921) (writing took 1.2854262189939618 seconds)
2022-05-13 01:58:55 | INFO | fairseq_cli.train | end of epoch 39 (average epoch stats below)
2022-05-13 01:58:55 | INFO | train | epoch 039 | loss 7.989 | ppl 254.09 | wps 26486.4 | ups 0.2 | wpb 130930 | bsz 255.7 | num_updates 1521 | lr 3.8087e-05 | gnorm 0.555 | train_wall 179 | gb_free 7.9 | wall 7845
2022-05-13 01:58:55 | INFO | fairseq.data.iterators | grouped total_num_itrs = 39
2022-05-13 01:58:55 | INFO | fairseq.trainer | begin training epoch 40
2022-05-13 01:58:55 | INFO | fairseq_cli.train | Start iterating over samples
2022-05-13 02:02:07 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-05-13 02:02:11 | INFO | valid | epoch 040 | valid on 'valid' subset | loss 7.872 | ppl 234.25 | wps 76692.7 | wpb 2041.1 | bsz 4 | num_updates 1560 | best_loss 7.872
2022-05-13 02:02:11 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 40 @ 1560 updates
2022-05-13 02:02:11 | INFO | fairseq.trainer | Saving checkpoint to /cluster/work/cotterell/liam/master-thesis/checkpoints/language_model/standard/checkpoint_best_standard_64_1.0000E-04.pt
2022-05-13 02:02:13 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/work/cotterell/liam/master-thesis/checkpoints/language_model/standard/checkpoint_best_standard_64_1.0000E-04.pt
2022-05-13 02:02:13 | INFO | fairseq.checkpoint_utils | Saved checkpoint checkpoints/language_model/standard/checkpoint_best_standard_64_1.0000E-04.pt (epoch 40 @ 1560 updates, score 7.872) (writing took 1.2006070096977055 seconds)
2022-05-13 02:02:13 | INFO | fairseq_cli.train | end of epoch 40 (average epoch stats below)
2022-05-13 02:02:13 | INFO | train | epoch 040 | loss 7.937 | ppl 245.01 | wps 25907.9 | ups 0.2 | wpb 130930 | bsz 255.7 | num_updates 1560 | lr 3.9061e-05 | gnorm 0.64 | train_wall 180 | gb_free 7.9 | wall 8042
2022-05-13 02:02:13 | INFO | fairseq.data.iterators | grouped total_num_itrs = 39
2022-05-13 02:02:13 | INFO | fairseq.trainer | begin training epoch 41
2022-05-13 02:02:13 | INFO | fairseq_cli.train | Start iterating over samples
2022-05-13 02:05:21 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-05-13 02:05:25 | INFO | valid | epoch 041 | valid on 'valid' subset | loss 7.826 | ppl 226.98 | wps 76646.2 | wpb 2041.1 | bsz 4 | num_updates 1599 | best_loss 7.826
2022-05-13 02:05:25 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 41 @ 1599 updates
2022-05-13 02:05:25 | INFO | fairseq.trainer | Saving checkpoint to /cluster/work/cotterell/liam/master-thesis/checkpoints/language_model/standard/checkpoint_best_standard_64_1.0000E-04.pt
2022-05-13 02:05:26 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/work/cotterell/liam/master-thesis/checkpoints/language_model/standard/checkpoint_best_standard_64_1.0000E-04.pt
2022-05-13 02:05:26 | INFO | fairseq.checkpoint_utils | Saved checkpoint checkpoints/language_model/standard/checkpoint_best_standard_64_1.0000E-04.pt (epoch 41 @ 1599 updates, score 7.826) (writing took 1.1876991940662265 seconds)
2022-05-13 02:05:26 | INFO | fairseq_cli.train | end of epoch 41 (average epoch stats below)
2022-05-13 02:05:26 | INFO | train | epoch 041 | loss 7.885 | ppl 236.34 | wps 26399.9 | ups 0.2 | wpb 130930 | bsz 255.7 | num_updates 1599 | lr 4.0035e-05 | gnorm 0.532 | train_wall 179 | gb_free 7.9 | wall 8236
2022-05-13 02:05:26 | INFO | fairseq.data.iterators | grouped total_num_itrs = 39
2022-05-13 02:05:26 | INFO | fairseq.trainer | begin training epoch 42
2022-05-13 02:05:26 | INFO | fairseq_cli.train | Start iterating over samples
2022-05-13 02:05:31 | INFO | train_inner | epoch 042:      1 / 39 loss=7.924, ppl=242.9, wps=26139.4, ups=0.2, wpb=130899, bsz=255.7, num_updates=1600, lr=4.006e-05, gnorm=0.585, train_wall=460, gb_free=7.9, wall=8241
2022-05-13 02:08:33 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-05-13 02:08:37 | INFO | valid | epoch 042 | valid on 'valid' subset | loss 7.779 | ppl 219.65 | wps 76720.3 | wpb 2041.1 | bsz 4 | num_updates 1638 | best_loss 7.779
2022-05-13 02:08:37 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 42 @ 1638 updates
2022-05-13 02:08:37 | INFO | fairseq.trainer | Saving checkpoint to /cluster/work/cotterell/liam/master-thesis/checkpoints/language_model/standard/checkpoint_best_standard_64_1.0000E-04.pt
2022-05-13 02:08:38 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/work/cotterell/liam/master-thesis/checkpoints/language_model/standard/checkpoint_best_standard_64_1.0000E-04.pt
2022-05-13 02:08:38 | INFO | fairseq.checkpoint_utils | Saved checkpoint checkpoints/language_model/standard/checkpoint_best_standard_64_1.0000E-04.pt (epoch 42 @ 1638 updates, score 7.779) (writing took 1.0986158070154488 seconds)
2022-05-13 02:08:38 | INFO | fairseq_cli.train | end of epoch 42 (average epoch stats below)
2022-05-13 02:08:38 | INFO | train | epoch 042 | loss 7.835 | ppl 228.27 | wps 26555.1 | ups 0.2 | wpb 130930 | bsz 255.7 | num_updates 1638 | lr 4.10091e-05 | gnorm 0.672 | train_wall 178 | gb_free 7.9 | wall 8428
2022-05-13 02:08:38 | INFO | fairseq.data.iterators | grouped total_num_itrs = 39
2022-05-13 02:08:38 | INFO | fairseq.trainer | begin training epoch 43
2022-05-13 02:08:38 | INFO | fairseq_cli.train | Start iterating over samples
2022-05-13 02:11:46 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-05-13 02:11:50 | INFO | valid | epoch 043 | valid on 'valid' subset | loss 7.727 | ppl 211.91 | wps 76652.5 | wpb 2041.1 | bsz 4 | num_updates 1677 | best_loss 7.727
2022-05-13 02:11:50 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 43 @ 1677 updates
2022-05-13 02:11:50 | INFO | fairseq.trainer | Saving checkpoint to /cluster/work/cotterell/liam/master-thesis/checkpoints/language_model/standard/checkpoint_best_standard_64_1.0000E-04.pt
2022-05-13 02:11:51 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/work/cotterell/liam/master-thesis/checkpoints/language_model/standard/checkpoint_best_standard_64_1.0000E-04.pt
2022-05-13 02:11:51 | INFO | fairseq.checkpoint_utils | Saved checkpoint checkpoints/language_model/standard/checkpoint_best_standard_64_1.0000E-04.pt (epoch 43 @ 1677 updates, score 7.727) (writing took 1.2923311782069504 seconds)
2022-05-13 02:11:51 | INFO | fairseq_cli.train | end of epoch 43 (average epoch stats below)
2022-05-13 02:11:51 | INFO | train | epoch 043 | loss 7.784 | ppl 220.37 | wps 26459 | ups 0.2 | wpb 130930 | bsz 255.7 | num_updates 1677 | lr 4.19831e-05 | gnorm 0.566 | train_wall 178 | gb_free 7.9 | wall 8621
2022-05-13 02:11:51 | INFO | fairseq.data.iterators | grouped total_num_itrs = 39
2022-05-13 02:11:51 | INFO | fairseq.trainer | begin training epoch 44
2022-05-13 02:11:51 | INFO | fairseq_cli.train | Start iterating over samples
2022-05-13 02:13:47 | INFO | train_inner | epoch 044:     23 / 39 loss=7.794, ppl=221.87, wps=26369.3, ups=0.2, wpb=130956, bsz=255.8, num_updates=1700, lr=4.25575e-05, gnorm=0.597, train_wall=459, gb_free=7.9, wall=8737
2022-05-13 02:15:04 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-05-13 02:15:08 | INFO | valid | epoch 044 | valid on 'valid' subset | loss 7.682 | ppl 205.3 | wps 76584 | wpb 2041.1 | bsz 4 | num_updates 1716 | best_loss 7.682
2022-05-13 02:15:08 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 44 @ 1716 updates
2022-05-13 02:15:08 | INFO | fairseq.trainer | Saving checkpoint to /cluster/work/cotterell/liam/master-thesis/checkpoints/language_model/standard/checkpoint_best_standard_64_1.0000E-04.pt
2022-05-13 02:15:10 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/work/cotterell/liam/master-thesis/checkpoints/language_model/standard/checkpoint_best_standard_64_1.0000E-04.pt
2022-05-13 02:15:10 | INFO | fairseq.checkpoint_utils | Saved checkpoint checkpoints/language_model/standard/checkpoint_best_standard_64_1.0000E-04.pt (epoch 44 @ 1716 updates, score 7.682) (writing took 1.309454187285155 seconds)
2022-05-13 02:15:10 | INFO | fairseq_cli.train | end of epoch 44 (average epoch stats below)
2022-05-13 02:15:10 | INFO | train | epoch 044 | loss 7.734 | ppl 212.89 | wps 25742.7 | ups 0.2 | wpb 130930 | bsz 255.7 | num_updates 1716 | lr 4.29571e-05 | gnorm 0.511 | train_wall 181 | gb_free 7.9 | wall 8819
2022-05-13 02:15:10 | INFO | fairseq.data.iterators | grouped total_num_itrs = 39
2022-05-13 02:15:10 | INFO | fairseq.trainer | begin training epoch 45
2022-05-13 02:15:10 | INFO | fairseq_cli.train | Start iterating over samples
2022-05-13 02:18:17 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-05-13 02:18:21 | INFO | valid | epoch 045 | valid on 'valid' subset | loss 7.632 | ppl 198.4 | wps 76725.7 | wpb 2041.1 | bsz 4 | num_updates 1755 | best_loss 7.632
2022-05-13 02:18:21 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 45 @ 1755 updates
2022-05-13 02:18:21 | INFO | fairseq.trainer | Saving checkpoint to /cluster/work/cotterell/liam/master-thesis/checkpoints/language_model/standard/checkpoint_best_standard_64_1.0000E-04.pt
2022-05-13 02:18:22 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/work/cotterell/liam/master-thesis/checkpoints/language_model/standard/checkpoint_best_standard_64_1.0000E-04.pt
2022-05-13 02:18:22 | INFO | fairseq.checkpoint_utils | Saved checkpoint checkpoints/language_model/standard/checkpoint_best_standard_64_1.0000E-04.pt (epoch 45 @ 1755 updates, score 7.632) (writing took 1.1881223381496966 seconds)
2022-05-13 02:18:22 | INFO | fairseq_cli.train | end of epoch 45 (average epoch stats below)
2022-05-13 02:18:22 | INFO | train | epoch 045 | loss 7.684 | ppl 205.7 | wps 26550.4 | ups 0.2 | wpb 130930 | bsz 255.7 | num_updates 1755 | lr 4.39311e-05 | gnorm 0.575 | train_wall 178 | gb_free 7.9 | wall 9012
2022-05-13 02:18:22 | INFO | fairseq.data.iterators | grouped total_num_itrs = 39
2022-05-13 02:18:22 | INFO | fairseq.trainer | begin training epoch 46
2022-05-13 02:18:22 | INFO | fairseq_cli.train | Start iterating over samples
2022-05-13 02:21:29 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-05-13 02:21:33 | INFO | valid | epoch 046 | valid on 'valid' subset | loss 7.584 | ppl 191.86 | wps 76598.9 | wpb 2041.1 | bsz 4 | num_updates 1794 | best_loss 7.584
2022-05-13 02:21:33 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 46 @ 1794 updates
2022-05-13 02:21:33 | INFO | fairseq.trainer | Saving checkpoint to /cluster/work/cotterell/liam/master-thesis/checkpoints/language_model/standard/checkpoint_best_standard_64_1.0000E-04.pt
2022-05-13 02:21:34 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/work/cotterell/liam/master-thesis/checkpoints/language_model/standard/checkpoint_best_standard_64_1.0000E-04.pt
2022-05-13 02:21:34 | INFO | fairseq.checkpoint_utils | Saved checkpoint checkpoints/language_model/standard/checkpoint_best_standard_64_1.0000E-04.pt (epoch 46 @ 1794 updates, score 7.584) (writing took 1.0235551772639155 seconds)
2022-05-13 02:21:34 | INFO | fairseq_cli.train | end of epoch 46 (average epoch stats below)
2022-05-13 02:21:34 | INFO | train | epoch 046 | loss 7.634 | ppl 198.58 | wps 26577.9 | ups 0.2 | wpb 130930 | bsz 255.7 | num_updates 1794 | lr 4.49052e-05 | gnorm 0.535 | train_wall 178 | gb_free 7.9 | wall 9204
2022-05-13 02:21:34 | INFO | fairseq.data.iterators | grouped total_num_itrs = 39
2022-05-13 02:21:34 | INFO | fairseq.trainer | begin training epoch 47
2022-05-13 02:21:34 | INFO | fairseq_cli.train | Start iterating over samples
2022-05-13 02:22:04 | INFO | train_inner | epoch 047:      6 / 39 loss=7.665, ppl=203, wps=26369.1, ups=0.2, wpb=130920, bsz=255.7, num_updates=1800, lr=4.5055e-05, gnorm=0.555, train_wall=457, gb_free=7.9, wall=9234
2022-05-13 02:24:48 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-05-13 02:24:52 | INFO | valid | epoch 047 | valid on 'valid' subset | loss 7.537 | ppl 185.7 | wps 76882 | wpb 2041.1 | bsz 4 | num_updates 1833 | best_loss 7.537
2022-05-13 02:24:52 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 47 @ 1833 updates
2022-05-13 02:24:52 | INFO | fairseq.trainer | Saving checkpoint to /cluster/work/cotterell/liam/master-thesis/checkpoints/language_model/standard/checkpoint_best_standard_64_1.0000E-04.pt
2022-05-13 02:24:54 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/work/cotterell/liam/master-thesis/checkpoints/language_model/standard/checkpoint_best_standard_64_1.0000E-04.pt
2022-05-13 02:24:54 | INFO | fairseq.checkpoint_utils | Saved checkpoint checkpoints/language_model/standard/checkpoint_best_standard_64_1.0000E-04.pt (epoch 47 @ 1833 updates, score 7.537) (writing took 1.309608242008835 seconds)
2022-05-13 02:24:54 | INFO | fairseq_cli.train | end of epoch 47 (average epoch stats below)
2022-05-13 02:24:54 | INFO | train | epoch 047 | loss 7.584 | ppl 191.87 | wps 25576.8 | ups 0.2 | wpb 130930 | bsz 255.7 | num_updates 1833 | lr 4.58792e-05 | gnorm 0.588 | train_wall 183 | gb_free 7.9 | wall 9404
2022-05-13 02:24:54 | INFO | fairseq.data.iterators | grouped total_num_itrs = 39
2022-05-13 02:24:54 | INFO | fairseq.trainer | begin training epoch 48
2022-05-13 02:24:54 | INFO | fairseq_cli.train | Start iterating over samples
2022-05-13 02:28:01 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-05-13 02:28:04 | INFO | valid | epoch 048 | valid on 'valid' subset | loss 7.489 | ppl 179.61 | wps 76627.6 | wpb 2041.1 | bsz 4 | num_updates 1872 | best_loss 7.489
2022-05-13 02:28:04 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 48 @ 1872 updates
2022-05-13 02:28:04 | INFO | fairseq.trainer | Saving checkpoint to /cluster/work/cotterell/liam/master-thesis/checkpoints/language_model/standard/checkpoint_best_standard_64_1.0000E-04.pt
2022-05-13 02:28:05 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/work/cotterell/liam/master-thesis/checkpoints/language_model/standard/checkpoint_best_standard_64_1.0000E-04.pt
2022-05-13 02:28:05 | INFO | fairseq.checkpoint_utils | Saved checkpoint checkpoints/language_model/standard/checkpoint_best_standard_64_1.0000E-04.pt (epoch 48 @ 1872 updates, score 7.489) (writing took 1.0617297338321805 seconds)
2022-05-13 02:28:05 | INFO | fairseq_cli.train | end of epoch 48 (average epoch stats below)
2022-05-13 02:28:05 | INFO | train | epoch 048 | loss 7.534 | ppl 185.35 | wps 26629.3 | ups 0.2 | wpb 130930 | bsz 255.7 | num_updates 1872 | lr 4.68532e-05 | gnorm 0.663 | train_wall 178 | gb_free 7.9 | wall 9595
2022-05-13 02:28:05 | INFO | fairseq.data.iterators | grouped total_num_itrs = 39
2022-05-13 02:28:05 | INFO | fairseq.trainer | begin training epoch 49
2022-05-13 02:28:05 | INFO | fairseq_cli.train | Start iterating over samples
2022-05-13 02:30:20 | INFO | train_inner | epoch 049:     28 / 39 loss=7.538, ppl=185.8, wps=26383.6, ups=0.2, wpb=130951, bsz=255.8, num_updates=1900, lr=4.75525e-05, gnorm=0.622, train_wall=461, gb_free=7.9, wall=9730
2022-05-13 02:31:13 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-05-13 02:31:17 | INFO | valid | epoch 049 | valid on 'valid' subset | loss 7.442 | ppl 173.89 | wps 76508.4 | wpb 2041.1 | bsz 4 | num_updates 1911 | best_loss 7.442
2022-05-13 02:31:17 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 49 @ 1911 updates
2022-05-13 02:31:17 | INFO | fairseq.trainer | Saving checkpoint to /cluster/work/cotterell/liam/master-thesis/checkpoints/language_model/standard/checkpoint_best_standard_64_1.0000E-04.pt
2022-05-13 02:31:18 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/work/cotterell/liam/master-thesis/checkpoints/language_model/standard/checkpoint_best_standard_64_1.0000E-04.pt
2022-05-13 02:31:18 | INFO | fairseq.checkpoint_utils | Saved checkpoint checkpoints/language_model/standard/checkpoint_best_standard_64_1.0000E-04.pt (epoch 49 @ 1911 updates, score 7.442) (writing took 1.2389889303594828 seconds)
2022-05-13 02:31:18 | INFO | fairseq_cli.train | end of epoch 49 (average epoch stats below)
2022-05-13 02:31:18 | INFO | train | epoch 049 | loss 7.485 | ppl 179.09 | wps 26479 | ups 0.2 | wpb 130930 | bsz 255.7 | num_updates 1911 | lr 4.78272e-05 | gnorm 0.626 | train_wall 178 | gb_free 7.9 | wall 9788
2022-05-13 02:31:18 | INFO | fairseq.data.iterators | grouped total_num_itrs = 39
2022-05-13 02:31:18 | INFO | fairseq.trainer | begin training epoch 50
2022-05-13 02:31:18 | INFO | fairseq_cli.train | Start iterating over samples
2022-05-13 02:34:25 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-05-13 02:34:29 | INFO | valid | epoch 050 | valid on 'valid' subset | loss 7.399 | ppl 168.74 | wps 76611.6 | wpb 2041.1 | bsz 4 | num_updates 1950 | best_loss 7.399
2022-05-13 02:34:29 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 50 @ 1950 updates
2022-05-13 02:34:29 | INFO | fairseq.trainer | Saving checkpoint to /cluster/work/cotterell/liam/master-thesis/checkpoints/language_model/standard/checkpoint_best_standard_64_1.0000E-04.pt
2022-05-13 02:34:30 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/work/cotterell/liam/master-thesis/checkpoints/language_model/standard/checkpoint_best_standard_64_1.0000E-04.pt
2022-05-13 02:34:30 | INFO | fairseq.checkpoint_utils | Saved checkpoint checkpoints/language_model/standard/checkpoint_best_standard_64_1.0000E-04.pt (epoch 50 @ 1950 updates, score 7.399) (writing took 1.2011287068016827 seconds)
2022-05-13 02:34:30 | INFO | fairseq_cli.train | end of epoch 50 (average epoch stats below)
2022-05-13 02:34:30 | INFO | train | epoch 050 | loss 7.436 | ppl 173.12 | wps 26642.9 | ups 0.2 | wpb 130930 | bsz 255.7 | num_updates 1950 | lr 4.88012e-05 | gnorm 0.627 | train_wall 178 | gb_free 7.9 | wall 9980
2022-05-13 02:34:30 | INFO | fairseq.data.iterators | grouped total_num_itrs = 39
2022-05-13 02:34:30 | INFO | fairseq.trainer | begin training epoch 51
2022-05-13 02:34:30 | INFO | fairseq_cli.train | Start iterating over samples
2022-05-13 02:37:45 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-05-13 02:37:49 | INFO | valid | epoch 051 | valid on 'valid' subset | loss 7.353 | ppl 163.43 | wps 76701.3 | wpb 2041.1 | bsz 4 | num_updates 1989 | best_loss 7.353
2022-05-13 02:37:49 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 51 @ 1989 updates
2022-05-13 02:37:49 | INFO | fairseq.trainer | Saving checkpoint to /cluster/work/cotterell/liam/master-thesis/checkpoints/language_model/standard/checkpoint_best_standard_64_1.0000E-04.pt
2022-05-13 02:37:50 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/work/cotterell/liam/master-thesis/checkpoints/language_model/standard/checkpoint_best_standard_64_1.0000E-04.pt
2022-05-13 02:37:50 | INFO | fairseq.checkpoint_utils | Saved checkpoint checkpoints/language_model/standard/checkpoint_best_standard_64_1.0000E-04.pt (epoch 51 @ 1989 updates, score 7.353) (writing took 1.1383370491676033 seconds)
2022-05-13 02:37:50 | INFO | fairseq_cli.train | end of epoch 51 (average epoch stats below)
2022-05-13 02:37:50 | INFO | train | epoch 051 | loss 7.387 | ppl 167.43 | wps 25499.5 | ups 0.19 | wpb 130930 | bsz 255.7 | num_updates 1989 | lr 4.97753e-05 | gnorm 0.693 | train_wall 184 | gb_free 7.9 | wall 10180
2022-05-13 02:37:50 | INFO | fairseq.data.iterators | grouped total_num_itrs = 39
2022-05-13 02:37:50 | INFO | fairseq.trainer | begin training epoch 52
2022-05-13 02:37:50 | INFO | fairseq_cli.train | Start iterating over samples
2022-05-13 02:38:43 | INFO | train_inner | epoch 052:     11 / 39 loss=7.411, ppl=170.2, wps=26038.3, ups=0.2, wpb=130916, bsz=255.7, num_updates=2000, lr=5.005e-05, gnorm=0.656, train_wall=462, gb_free=7.9, wall=10233
2022-05-13 02:40:58 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-05-13 02:41:01 | INFO | valid | epoch 052 | valid on 'valid' subset | loss 7.306 | ppl 158.29 | wps 76711.8 | wpb 2041.1 | bsz 4 | num_updates 2028 | best_loss 7.306
2022-05-13 02:41:01 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 52 @ 2028 updates
2022-05-13 02:41:01 | INFO | fairseq.trainer | Saving checkpoint to /cluster/work/cotterell/liam/master-thesis/checkpoints/language_model/standard/checkpoint_best_standard_64_1.0000E-04.pt
2022-05-13 02:41:03 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/work/cotterell/liam/master-thesis/checkpoints/language_model/standard/checkpoint_best_standard_64_1.0000E-04.pt
2022-05-13 02:41:03 | INFO | fairseq.checkpoint_utils | Saved checkpoint checkpoints/language_model/standard/checkpoint_best_standard_64_1.0000E-04.pt (epoch 52 @ 2028 updates, score 7.306) (writing took 1.1883955979719758 seconds)
2022-05-13 02:41:03 | INFO | fairseq_cli.train | end of epoch 52 (average epoch stats below)
2022-05-13 02:41:03 | INFO | train | epoch 052 | loss 7.34 | ppl 161.99 | wps 26539.8 | ups 0.2 | wpb 130930 | bsz 255.7 | num_updates 2028 | lr 5.07493e-05 | gnorm 0.646 | train_wall 178 | gb_free 7.9 | wall 10372
2022-05-13 02:41:03 | INFO | fairseq.data.iterators | grouped total_num_itrs = 39
2022-05-13 02:41:03 | INFO | fairseq.trainer | begin training epoch 53
2022-05-13 02:41:03 | INFO | fairseq_cli.train | Start iterating over samples
2022-05-13 02:44:10 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-05-13 02:44:14 | INFO | valid | epoch 053 | valid on 'valid' subset | loss 7.267 | ppl 153.97 | wps 76672.3 | wpb 2041.1 | bsz 4 | num_updates 2067 | best_loss 7.267
2022-05-13 02:44:14 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 53 @ 2067 updates
2022-05-13 02:44:14 | INFO | fairseq.trainer | Saving checkpoint to /cluster/work/cotterell/liam/master-thesis/checkpoints/language_model/standard/checkpoint_best_standard_64_1.0000E-04.pt
2022-05-13 02:44:15 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/work/cotterell/liam/master-thesis/checkpoints/language_model/standard/checkpoint_best_standard_64_1.0000E-04.pt
2022-05-13 02:44:15 | INFO | fairseq.checkpoint_utils | Saved checkpoint checkpoints/language_model/standard/checkpoint_best_standard_64_1.0000E-04.pt (epoch 53 @ 2067 updates, score 7.267) (writing took 1.153172919061035 seconds)
2022-05-13 02:44:15 | INFO | fairseq_cli.train | end of epoch 53 (average epoch stats below)
2022-05-13 02:44:15 | INFO | train | epoch 053 | loss 7.293 | ppl 156.87 | wps 26547.7 | ups 0.2 | wpb 130930 | bsz 255.7 | num_updates 2067 | lr 5.17233e-05 | gnorm 0.646 | train_wall 178 | gb_free 7.9 | wall 10565
2022-05-13 02:44:15 | INFO | fairseq.data.iterators | grouped total_num_itrs = 39
2022-05-13 02:44:15 | INFO | fairseq.trainer | begin training epoch 54
2022-05-13 02:44:15 | INFO | fairseq_cli.train | Start iterating over samples
2022-05-13 02:47:00 | INFO | train_inner | epoch 054:     33 / 39 loss=7.29, ppl=156.54, wps=26366.1, ups=0.2, wpb=130946, bsz=255.8, num_updates=2100, lr=5.25475e-05, gnorm=0.642, train_wall=460, gb_free=7.9, wall=10730
2022-05-13 02:47:28 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-05-13 02:47:32 | INFO | valid | epoch 054 | valid on 'valid' subset | loss 7.226 | ppl 149.74 | wps 76741.1 | wpb 2041.1 | bsz 4 | num_updates 2106 | best_loss 7.226
2022-05-13 02:47:32 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 54 @ 2106 updates
2022-05-13 02:47:32 | INFO | fairseq.trainer | Saving checkpoint to /cluster/work/cotterell/liam/master-thesis/checkpoints/language_model/standard/checkpoint_best_standard_64_1.0000E-04.pt
2022-05-13 02:47:34 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/work/cotterell/liam/master-thesis/checkpoints/language_model/standard/checkpoint_best_standard_64_1.0000E-04.pt
2022-05-13 02:47:34 | INFO | fairseq.checkpoint_utils | Saved checkpoint checkpoints/language_model/standard/checkpoint_best_standard_64_1.0000E-04.pt (epoch 54 @ 2106 updates, score 7.226) (writing took 1.270643908996135 seconds)
2022-05-13 02:47:34 | INFO | fairseq_cli.train | end of epoch 54 (average epoch stats below)
2022-05-13 02:47:34 | INFO | train | epoch 054 | loss 7.248 | ppl 151.99 | wps 25696.5 | ups 0.2 | wpb 130930 | bsz 255.7 | num_updates 2106 | lr 5.26974e-05 | gnorm 0.674 | train_wall 182 | gb_free 7.9 | wall 10763
2022-05-13 02:47:34 | INFO | fairseq.data.iterators | grouped total_num_itrs = 39
2022-05-13 02:47:34 | INFO | fairseq.trainer | begin training epoch 55
2022-05-13 02:47:34 | INFO | fairseq_cli.train | Start iterating over samples
2022-05-13 02:50:41 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-05-13 02:50:45 | INFO | valid | epoch 055 | valid on 'valid' subset | loss 7.184 | ppl 145.37 | wps 76493.4 | wpb 2041.1 | bsz 4 | num_updates 2145 | best_loss 7.184
2022-05-13 02:50:45 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 55 @ 2145 updates
2022-05-13 02:50:45 | INFO | fairseq.trainer | Saving checkpoint to /cluster/work/cotterell/liam/master-thesis/checkpoints/language_model/standard/checkpoint_best_standard_64_1.0000E-04.pt
2022-05-13 02:50:46 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/work/cotterell/liam/master-thesis/checkpoints/language_model/standard/checkpoint_best_standard_64_1.0000E-04.pt
2022-05-13 02:50:46 | INFO | fairseq.checkpoint_utils | Saved checkpoint checkpoints/language_model/standard/checkpoint_best_standard_64_1.0000E-04.pt (epoch 55 @ 2145 updates, score 7.184) (writing took 1.1246329238638282 seconds)
2022-05-13 02:50:46 | INFO | fairseq_cli.train | end of epoch 55 (average epoch stats below)
2022-05-13 02:50:46 | INFO | train | epoch 055 | loss 7.202 | ppl 147.26 | wps 26496.9 | ups 0.2 | wpb 130930 | bsz 255.7 | num_updates 2145 | lr 5.36714e-05 | gnorm 0.633 | train_wall 179 | gb_free 7.9 | wall 10956
2022-05-13 02:50:46 | INFO | fairseq.data.iterators | grouped total_num_itrs = 39
2022-05-13 02:50:46 | INFO | fairseq.trainer | begin training epoch 56
2022-05-13 02:50:46 | INFO | fairseq_cli.train | Start iterating over samples
2022-05-13 02:53:54 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-05-13 02:53:57 | INFO | valid | epoch 056 | valid on 'valid' subset | loss 7.141 | ppl 141.14 | wps 76496.2 | wpb 2041.1 | bsz 4 | num_updates 2184 | best_loss 7.141
2022-05-13 02:53:57 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 56 @ 2184 updates
2022-05-13 02:53:57 | INFO | fairseq.trainer | Saving checkpoint to /cluster/work/cotterell/liam/master-thesis/checkpoints/language_model/standard/checkpoint_best_standard_64_1.0000E-04.pt
2022-05-13 02:53:58 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/work/cotterell/liam/master-thesis/checkpoints/language_model/standard/checkpoint_best_standard_64_1.0000E-04.pt
2022-05-13 02:53:58 | INFO | fairseq.checkpoint_utils | Saved checkpoint checkpoints/language_model/standard/checkpoint_best_standard_64_1.0000E-04.pt (epoch 56 @ 2184 updates, score 7.141) (writing took 1.1171252578496933 seconds)
2022-05-13 02:53:58 | INFO | fairseq_cli.train | end of epoch 56 (average epoch stats below)
2022-05-13 02:53:58 | INFO | train | epoch 056 | loss 7.158 | ppl 142.78 | wps 26580.7 | ups 0.2 | wpb 130930 | bsz 255.7 | num_updates 2184 | lr 5.46454e-05 | gnorm 0.632 | train_wall 178 | gb_free 7.9 | wall 11148
2022-05-13 02:53:58 | INFO | fairseq.data.iterators | grouped total_num_itrs = 39
2022-05-13 02:53:58 | INFO | fairseq.trainer | begin training epoch 57
2022-05-13 02:53:58 | INFO | fairseq_cli.train | Start iterating over samples
2022-05-13 02:55:17 | INFO | train_inner | epoch 057:     16 / 39 loss=7.175, ppl=144.48, wps=26303, ups=0.2, wpb=130916, bsz=255.7, num_updates=2200, lr=5.5045e-05, gnorm=0.643, train_wall=458, gb_free=7.9, wall=11227
2022-05-13 02:57:07 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-05-13 02:57:11 | INFO | valid | epoch 057 | valid on 'valid' subset | loss 7.102 | ppl 137.41 | wps 76567.5 | wpb 2041.1 | bsz 4 | num_updates 2223 | best_loss 7.102
2022-05-13 02:57:11 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 57 @ 2223 updates
2022-05-13 02:57:11 | INFO | fairseq.trainer | Saving checkpoint to /cluster/work/cotterell/liam/master-thesis/checkpoints/language_model/standard/checkpoint_best_standard_64_1.0000E-04.pt
2022-05-13 02:57:12 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/work/cotterell/liam/master-thesis/checkpoints/language_model/standard/checkpoint_best_standard_64_1.0000E-04.pt
2022-05-13 02:57:12 | INFO | fairseq.checkpoint_utils | Saved checkpoint checkpoints/language_model/standard/checkpoint_best_standard_64_1.0000E-04.pt (epoch 57 @ 2223 updates, score 7.102) (writing took 1.0905622970312834 seconds)
2022-05-13 02:57:12 | INFO | fairseq_cli.train | end of epoch 57 (average epoch stats below)
2022-05-13 02:57:12 | INFO | train | epoch 057 | loss 7.114 | ppl 138.5 | wps 26319.6 | ups 0.2 | wpb 130930 | bsz 255.7 | num_updates 2223 | lr 5.56194e-05 | gnorm 0.677 | train_wall 179 | gb_free 7.9 | wall 11342
2022-05-13 02:57:12 | INFO | fairseq.data.iterators | grouped total_num_itrs = 39
2022-05-13 02:57:13 | INFO | fairseq.trainer | begin training epoch 58
2022-05-13 02:57:13 | INFO | fairseq_cli.train | Start iterating over samples
2022-05-13 03:00:22 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-05-13 03:00:25 | INFO | valid | epoch 058 | valid on 'valid' subset | loss 7.064 | ppl 133.78 | wps 76698.5 | wpb 2041.1 | bsz 4 | num_updates 2262 | best_loss 7.064
2022-05-13 03:00:25 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 58 @ 2262 updates
2022-05-13 03:00:25 | INFO | fairseq.trainer | Saving checkpoint to /cluster/work/cotterell/liam/master-thesis/checkpoints/language_model/standard/checkpoint_best_standard_64_1.0000E-04.pt
2022-05-13 03:00:26 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/work/cotterell/liam/master-thesis/checkpoints/language_model/standard/checkpoint_best_standard_64_1.0000E-04.pt
2022-05-13 03:00:26 | INFO | fairseq.checkpoint_utils | Saved checkpoint checkpoints/language_model/standard/checkpoint_best_standard_64_1.0000E-04.pt (epoch 58 @ 2262 updates, score 7.064) (writing took 1.066394176799804 seconds)
2022-05-13 03:00:26 | INFO | fairseq_cli.train | end of epoch 58 (average epoch stats below)
2022-05-13 03:00:26 | INFO | train | epoch 058 | loss 7.07 | ppl 134.37 | wps 26340.1 | ups 0.2 | wpb 130930 | bsz 255.7 | num_updates 2262 | lr 5.65935e-05 | gnorm 0.681 | train_wall 178 | gb_free 7.9 | wall 11536
2022-05-13 03:00:26 | INFO | fairseq.data.iterators | grouped total_num_itrs = 39
2022-05-13 03:00:26 | INFO | fairseq.trainer | begin training epoch 59
2022-05-13 03:00:26 | INFO | fairseq_cli.train | Start iterating over samples
2022-05-13 03:03:30 | INFO | train_inner | epoch 059:     38 / 39 loss=7.062, ppl=133.66, wps=26587.2, ups=0.2, wpb=130951, bsz=255.8, num_updates=2300, lr=5.75425e-05, gnorm=0.699, train_wall=457, gb_free=7.9, wall=11720
2022-05-13 03:03:35 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-05-13 03:03:38 | INFO | valid | epoch 059 | valid on 'valid' subset | loss 7.021 | ppl 129.87 | wps 76399.6 | wpb 2041.1 | bsz 4 | num_updates 2301 | best_loss 7.021
2022-05-13 03:03:38 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 59 @ 2301 updates
2022-05-13 03:03:38 | INFO | fairseq.trainer | Saving checkpoint to /cluster/work/cotterell/liam/master-thesis/checkpoints/language_model/standard/checkpoint_best_standard_64_1.0000E-04.pt
2022-05-13 03:03:40 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/work/cotterell/liam/master-thesis/checkpoints/language_model/standard/checkpoint_best_standard_64_1.0000E-04.pt
2022-05-13 03:03:40 | INFO | fairseq.checkpoint_utils | Saved checkpoint checkpoints/language_model/standard/checkpoint_best_standard_64_1.0000E-04.pt (epoch 59 @ 2301 updates, score 7.021) (writing took 1.2576135960407555 seconds)
2022-05-13 03:03:40 | INFO | fairseq_cli.train | end of epoch 59 (average epoch stats below)
2022-05-13 03:03:40 | INFO | train | epoch 059 | loss 7.027 | ppl 130.44 | wps 26402.5 | ups 0.2 | wpb 130930 | bsz 255.7 | num_updates 2301 | lr 5.75675e-05 | gnorm 0.704 | train_wall 179 | gb_free 7.9 | wall 11730
2022-05-13 03:03:40 | INFO | fairseq.data.iterators | grouped total_num_itrs = 39
2022-05-13 03:03:40 | INFO | fairseq.trainer | begin training epoch 60
2022-05-13 03:03:40 | INFO | fairseq_cli.train | Start iterating over samples
2022-05-13 03:06:48 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-05-13 03:06:52 | INFO | valid | epoch 060 | valid on 'valid' subset | loss 6.984 | ppl 126.63 | wps 76441 | wpb 2041.1 | bsz 4 | num_updates 2340 | best_loss 6.984
2022-05-13 03:06:52 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 60 @ 2340 updates
2022-05-13 03:06:52 | INFO | fairseq.trainer | Saving checkpoint to /cluster/work/cotterell/liam/master-thesis/checkpoints/language_model/standard/checkpoint_best_standard_64_1.0000E-04.pt
2022-05-13 03:06:53 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/work/cotterell/liam/master-thesis/checkpoints/language_model/standard/checkpoint_best_standard_64_1.0000E-04.pt
2022-05-13 03:06:53 | INFO | fairseq.checkpoint_utils | Saved checkpoint checkpoints/language_model/standard/checkpoint_best_standard_64_1.0000E-04.pt (epoch 60 @ 2340 updates, score 6.984) (writing took 1.2546068052761257 seconds)
2022-05-13 03:06:53 | INFO | fairseq_cli.train | end of epoch 60 (average epoch stats below)
2022-05-13 03:06:53 | INFO | train | epoch 060 | loss 6.985 | ppl 126.63 | wps 26383.5 | ups 0.2 | wpb 130930 | bsz 255.7 | num_updates 2340 | lr 5.85415e-05 | gnorm 0.667 | train_wall 180 | gb_free 7.9 | wall 11923
2022-05-13 03:06:53 | INFO | fairseq.data.iterators | grouped total_num_itrs = 39
2022-05-13 03:06:53 | INFO | fairseq.trainer | begin training epoch 61
2022-05-13 03:06:53 | INFO | fairseq_cli.train | Start iterating over samples
2022-05-13 03:10:06 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-05-13 03:10:10 | INFO | valid | epoch 061 | valid on 'valid' subset | loss 6.945 | ppl 123.25 | wps 76413.8 | wpb 2041.1 | bsz 4 | num_updates 2379 | best_loss 6.945
2022-05-13 03:10:10 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 61 @ 2379 updates
2022-05-13 03:10:10 | INFO | fairseq.trainer | Saving checkpoint to /cluster/work/cotterell/liam/master-thesis/checkpoints/language_model/standard/checkpoint_best_standard_64_1.0000E-04.pt
2022-05-13 03:10:11 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/work/cotterell/liam/master-thesis/checkpoints/language_model/standard/checkpoint_best_standard_64_1.0000E-04.pt
2022-05-13 03:10:11 | INFO | fairseq.checkpoint_utils | Saved checkpoint checkpoints/language_model/standard/checkpoint_best_standard_64_1.0000E-04.pt (epoch 61 @ 2379 updates, score 6.945) (writing took 1.242784284055233 seconds)
2022-05-13 03:10:11 | INFO | fairseq_cli.train | end of epoch 61 (average epoch stats below)
2022-05-13 03:10:11 | INFO | train | epoch 061 | loss 6.943 | ppl 123.07 | wps 25838.2 | ups 0.2 | wpb 130930 | bsz 255.7 | num_updates 2379 | lr 5.95155e-05 | gnorm 0.752 | train_wall 181 | gb_free 7.9 | wall 12121
2022-05-13 03:10:11 | INFO | fairseq.data.iterators | grouped total_num_itrs = 39
2022-05-13 03:10:11 | INFO | fairseq.trainer | begin training epoch 62
2022-05-13 03:10:11 | INFO | fairseq_cli.train | Start iterating over samples
2022-05-13 03:11:52 | INFO | train_inner | epoch 062:     21 / 39 loss=6.952, ppl=123.85, wps=26066.6, ups=0.2, wpb=130905, bsz=255.7, num_updates=2400, lr=6.004e-05, gnorm=0.715, train_wall=460, gb_free=7.9, wall=12222
2022-05-13 03:13:18 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-05-13 03:13:22 | INFO | valid | epoch 062 | valid on 'valid' subset | loss 6.91 | ppl 120.25 | wps 76613 | wpb 2041.1 | bsz 4 | num_updates 2418 | best_loss 6.91
2022-05-13 03:13:22 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 62 @ 2418 updates
2022-05-13 03:13:22 | INFO | fairseq.trainer | Saving checkpoint to /cluster/work/cotterell/liam/master-thesis/checkpoints/language_model/standard/checkpoint_best_standard_64_1.0000E-04.pt
2022-05-13 03:13:23 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/work/cotterell/liam/master-thesis/checkpoints/language_model/standard/checkpoint_best_standard_64_1.0000E-04.pt
2022-05-13 03:13:23 | INFO | fairseq.checkpoint_utils | Saved checkpoint checkpoints/language_model/standard/checkpoint_best_standard_64_1.0000E-04.pt (epoch 62 @ 2418 updates, score 6.91) (writing took 1.0682114283554256 seconds)
2022-05-13 03:13:23 | INFO | fairseq_cli.train | end of epoch 62 (average epoch stats below)
2022-05-13 03:13:23 | INFO | train | epoch 062 | loss 6.901 | ppl 119.53 | wps 26552 | ups 0.2 | wpb 130930 | bsz 255.7 | num_updates 2418 | lr 6.04896e-05 | gnorm 0.687 | train_wall 178 | gb_free 7.9 | wall 12313
2022-05-13 03:13:23 | INFO | fairseq.data.iterators | grouped total_num_itrs = 39
2022-05-13 03:13:23 | INFO | fairseq.trainer | begin training epoch 63
2022-05-13 03:13:23 | INFO | fairseq_cli.train | Start iterating over samples
2022-05-13 03:16:33 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-05-13 03:16:37 | INFO | valid | epoch 063 | valid on 'valid' subset | loss 6.877 | ppl 117.5 | wps 76475.4 | wpb 2041.1 | bsz 4 | num_updates 2457 | best_loss 6.877
2022-05-13 03:16:37 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 63 @ 2457 updates
2022-05-13 03:16:37 | INFO | fairseq.trainer | Saving checkpoint to /cluster/work/cotterell/liam/master-thesis/checkpoints/language_model/standard/checkpoint_best_standard_64_1.0000E-04.pt
2022-05-13 03:16:38 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/work/cotterell/liam/master-thesis/checkpoints/language_model/standard/checkpoint_best_standard_64_1.0000E-04.pt
2022-05-13 03:16:38 | INFO | fairseq.checkpoint_utils | Saved checkpoint checkpoints/language_model/standard/checkpoint_best_standard_64_1.0000E-04.pt (epoch 63 @ 2457 updates, score 6.877) (writing took 1.213938640896231 seconds)
2022-05-13 03:16:38 | INFO | fairseq_cli.train | end of epoch 63 (average epoch stats below)
2022-05-13 03:16:38 | INFO | train | epoch 063 | loss 6.861 | ppl 116.27 | wps 26234.8 | ups 0.2 | wpb 130930 | bsz 255.7 | num_updates 2457 | lr 6.14636e-05 | gnorm 0.764 | train_wall 181 | gb_free 7.9 | wall 12508
2022-05-13 03:16:38 | INFO | fairseq.data.iterators | grouped total_num_itrs = 39
2022-05-13 03:16:38 | INFO | fairseq.trainer | begin training epoch 64
2022-05-13 03:16:38 | INFO | fairseq_cli.train | Start iterating over samples
2022-05-13 03:19:44 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-05-13 03:19:48 | INFO | valid | epoch 064 | valid on 'valid' subset | loss 6.841 | ppl 114.63 | wps 76492.3 | wpb 2041.1 | bsz 4 | num_updates 2496 | best_loss 6.841
2022-05-13 03:19:48 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 64 @ 2496 updates
2022-05-13 03:19:48 | INFO | fairseq.trainer | Saving checkpoint to /cluster/work/cotterell/liam/master-thesis/checkpoints/language_model/standard/checkpoint_best_standard_64_1.0000E-04.pt
2022-05-13 03:19:49 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/work/cotterell/liam/master-thesis/checkpoints/language_model/standard/checkpoint_best_standard_64_1.0000E-04.pt
2022-05-13 03:19:49 | INFO | fairseq.checkpoint_utils | Saved checkpoint checkpoints/language_model/standard/checkpoint_best_standard_64_1.0000E-04.pt (epoch 64 @ 2496 updates, score 6.841) (writing took 1.2168764499947429 seconds)
2022-05-13 03:19:49 | INFO | fairseq_cli.train | end of epoch 64 (average epoch stats below)
2022-05-13 03:19:49 | INFO | train | epoch 064 | loss 6.821 | ppl 113.09 | wps 26654.2 | ups 0.2 | wpb 130930 | bsz 255.7 | num_updates 2496 | lr 6.24376e-05 | gnorm 0.797 | train_wall 178 | gb_free 7.9 | wall 12699
2022-05-13 03:19:49 | INFO | fairseq.data.iterators | grouped total_num_itrs = 39
2022-05-13 03:19:49 | INFO | fairseq.trainer | begin training epoch 65
2022-05-13 03:19:49 | INFO | fairseq_cli.train | Start iterating over samples
2022-05-13 03:20:09 | INFO | train_inner | epoch 065:      4 / 39 loss=6.849, ppl=115.26, wps=26367.5, ups=0.2, wpb=130920, bsz=255.7, num_updates=2500, lr=6.25375e-05, gnorm=0.746, train_wall=459, gb_free=7.9, wall=12719
2022-05-13 03:22:58 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-05-13 03:23:02 | INFO | valid | epoch 065 | valid on 'valid' subset | loss 6.803 | ppl 111.68 | wps 76697.5 | wpb 2041.1 | bsz 4 | num_updates 2535 | best_loss 6.803
2022-05-13 03:23:02 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 65 @ 2535 updates
2022-05-13 03:23:02 | INFO | fairseq.trainer | Saving checkpoint to /cluster/work/cotterell/liam/master-thesis/checkpoints/language_model/standard/checkpoint_best_standard_64_1.0000E-04.pt
2022-05-13 03:23:03 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/work/cotterell/liam/master-thesis/checkpoints/language_model/standard/checkpoint_best_standard_64_1.0000E-04.pt
2022-05-13 03:23:03 | INFO | fairseq.checkpoint_utils | Saved checkpoint checkpoints/language_model/standard/checkpoint_best_standard_64_1.0000E-04.pt (epoch 65 @ 2535 updates, score 6.803) (writing took 1.0952438842505217 seconds)
2022-05-13 03:23:03 | INFO | fairseq_cli.train | end of epoch 65 (average epoch stats below)
2022-05-13 03:23:03 | INFO | train | epoch 065 | loss 6.781 | ppl 109.97 | wps 26378.8 | ups 0.2 | wpb 130930 | bsz 255.7 | num_updates 2535 | lr 6.34116e-05 | gnorm 0.706 | train_wall 178 | gb_free 7.9 | wall 12893
2022-05-13 03:23:03 | INFO | fairseq.data.iterators | grouped total_num_itrs = 39
2022-05-13 03:23:03 | INFO | fairseq.trainer | begin training epoch 66
2022-05-13 03:23:03 | INFO | fairseq_cli.train | Start iterating over samples
2022-05-13 03:26:13 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-05-13 03:26:16 | INFO | valid | epoch 066 | valid on 'valid' subset | loss 6.773 | ppl 109.36 | wps 76746.2 | wpb 2041.1 | bsz 4 | num_updates 2574 | best_loss 6.773
2022-05-13 03:26:16 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 66 @ 2574 updates
2022-05-13 03:26:16 | INFO | fairseq.trainer | Saving checkpoint to /cluster/work/cotterell/liam/master-thesis/checkpoints/language_model/standard/checkpoint_best_standard_64_1.0000E-04.pt
2022-05-13 03:26:18 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/work/cotterell/liam/master-thesis/checkpoints/language_model/standard/checkpoint_best_standard_64_1.0000E-04.pt
2022-05-13 03:26:18 | INFO | fairseq.checkpoint_utils | Saved checkpoint checkpoints/language_model/standard/checkpoint_best_standard_64_1.0000E-04.pt (epoch 66 @ 2574 updates, score 6.773) (writing took 1.3996095871552825 seconds)
2022-05-13 03:26:18 | INFO | fairseq_cli.train | end of epoch 66 (average epoch stats below)
2022-05-13 03:26:18 | INFO | train | epoch 066 | loss 6.742 | ppl 107.04 | wps 26208.9 | ups 0.2 | wpb 130930 | bsz 255.7 | num_updates 2574 | lr 6.43857e-05 | gnorm 0.725 | train_wall 180 | gb_free 7.9 | wall 13088
2022-05-13 03:26:18 | INFO | fairseq.data.iterators | grouped total_num_itrs = 39
2022-05-13 03:26:18 | INFO | fairseq.trainer | begin training epoch 67
2022-05-13 03:26:18 | INFO | fairseq_cli.train | Start iterating over samples
2022-05-13 03:28:23 | INFO | train_inner | epoch 067:     26 / 39 loss=6.747, ppl=107.38, wps=26520.1, ups=0.2, wpb=130956, bsz=255.8, num_updates=2600, lr=6.5035e-05, gnorm=0.746, train_wall=459, gb_free=7.9, wall=13212
2022-05-13 03:29:25 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-05-13 03:29:28 | INFO | valid | epoch 067 | valid on 'valid' subset | loss 6.738 | ppl 106.73 | wps 76596.1 | wpb 2041.1 | bsz 4 | num_updates 2613 | best_loss 6.738
2022-05-13 03:29:28 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 67 @ 2613 updates
2022-05-13 03:29:28 | INFO | fairseq.trainer | Saving checkpoint to /cluster/work/cotterell/liam/master-thesis/checkpoints/language_model/standard/checkpoint_best_standard_64_1.0000E-04.pt
2022-05-13 03:29:30 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/work/cotterell/liam/master-thesis/checkpoints/language_model/standard/checkpoint_best_standard_64_1.0000E-04.pt
2022-05-13 03:29:30 | INFO | fairseq.checkpoint_utils | Saved checkpoint checkpoints/language_model/standard/checkpoint_best_standard_64_1.0000E-04.pt (epoch 67 @ 2613 updates, score 6.738) (writing took 1.2057589651085436 seconds)
2022-05-13 03:29:30 | INFO | fairseq_cli.train | end of epoch 67 (average epoch stats below)
2022-05-13 03:29:30 | INFO | train | epoch 067 | loss 6.704 | ppl 104.28 | wps 26624.5 | ups 0.2 | wpb 130930 | bsz 255.7 | num_updates 2613 | lr 6.53597e-05 | gnorm 0.781 | train_wall 178 | gb_free 7.9 | wall 13279
2022-05-13 03:29:30 | INFO | fairseq.data.iterators | grouped total_num_itrs = 39
2022-05-13 03:29:30 | INFO | fairseq.trainer | begin training epoch 68
2022-05-13 03:29:30 | INFO | fairseq_cli.train | Start iterating over samples
2022-05-13 03:32:42 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-05-13 03:32:46 | INFO | valid | epoch 068 | valid on 'valid' subset | loss 6.705 | ppl 104.31 | wps 76665.1 | wpb 2041.1 | bsz 4 | num_updates 2652 | best_loss 6.705
2022-05-13 03:32:46 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 68 @ 2652 updates
2022-05-13 03:32:46 | INFO | fairseq.trainer | Saving checkpoint to /cluster/work/cotterell/liam/master-thesis/checkpoints/language_model/standard/checkpoint_best_standard_64_1.0000E-04.pt
2022-05-13 03:32:47 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/work/cotterell/liam/master-thesis/checkpoints/language_model/standard/checkpoint_best_standard_64_1.0000E-04.pt
2022-05-13 03:32:47 | INFO | fairseq.checkpoint_utils | Saved checkpoint checkpoints/language_model/standard/checkpoint_best_standard_64_1.0000E-04.pt (epoch 68 @ 2652 updates, score 6.705) (writing took 1.148007552139461 seconds)
2022-05-13 03:32:47 | INFO | fairseq_cli.train | end of epoch 68 (average epoch stats below)
2022-05-13 03:32:47 | INFO | train | epoch 068 | loss 6.666 | ppl 101.55 | wps 25817.7 | ups 0.2 | wpb 130930 | bsz 255.7 | num_updates 2652 | lr 6.63337e-05 | gnorm 0.754 | train_wall 181 | gb_free 7.9 | wall 13477
2022-05-13 03:32:47 | INFO | fairseq.data.iterators | grouped total_num_itrs = 39
2022-05-13 03:32:47 | INFO | fairseq.trainer | begin training epoch 69
2022-05-13 03:32:47 | INFO | fairseq_cli.train | Start iterating over samples
2022-05-13 03:35:55 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-05-13 03:35:59 | INFO | valid | epoch 069 | valid on 'valid' subset | loss 6.674 | ppl 102.09 | wps 76570.1 | wpb 2041.1 | bsz 4 | num_updates 2691 | best_loss 6.674
2022-05-13 03:35:59 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 69 @ 2691 updates
2022-05-13 03:35:59 | INFO | fairseq.trainer | Saving checkpoint to /cluster/work/cotterell/liam/master-thesis/checkpoints/language_model/standard/checkpoint_best_standard_64_1.0000E-04.pt
2022-05-13 03:36:00 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/work/cotterell/liam/master-thesis/checkpoints/language_model/standard/checkpoint_best_standard_64_1.0000E-04.pt
2022-05-13 03:36:00 | INFO | fairseq.checkpoint_utils | Saved checkpoint checkpoints/language_model/standard/checkpoint_best_standard_64_1.0000E-04.pt (epoch 69 @ 2691 updates, score 6.674) (writing took 1.2005449933931231 seconds)
2022-05-13 03:36:00 | INFO | fairseq_cli.train | end of epoch 69 (average epoch stats below)
2022-05-13 03:36:00 | INFO | train | epoch 069 | loss 6.627 | ppl 98.87 | wps 26524.2 | ups 0.2 | wpb 130930 | bsz 255.7 | num_updates 2691 | lr 6.73077e-05 | gnorm 0.769 | train_wall 178 | gb_free 7.9 | wall 13670
2022-05-13 03:36:00 | INFO | fairseq.data.iterators | grouped total_num_itrs = 39
2022-05-13 03:36:00 | INFO | fairseq.trainer | begin training epoch 70
2022-05-13 03:36:00 | INFO | fairseq_cli.train | Start iterating over samples
2022-05-13 03:36:44 | INFO | train_inner | epoch 070:      9 / 39 loss=6.649, ppl=100.35, wps=26090.3, ups=0.2, wpb=130910, bsz=255.7, num_updates=2700, lr=6.75325e-05, gnorm=0.751, train_wall=460, gb_free=7.9, wall=13714
2022-05-13 03:39:08 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-05-13 03:39:12 | INFO | valid | epoch 070 | valid on 'valid' subset | loss 6.643 | ppl 99.94 | wps 76527.6 | wpb 2041.1 | bsz 4 | num_updates 2730 | best_loss 6.643
2022-05-13 03:39:12 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 70 @ 2730 updates
2022-05-13 03:39:12 | INFO | fairseq.trainer | Saving checkpoint to /cluster/work/cotterell/liam/master-thesis/checkpoints/language_model/standard/checkpoint_best_standard_64_1.0000E-04.pt
2022-05-13 03:39:13 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/work/cotterell/liam/master-thesis/checkpoints/language_model/standard/checkpoint_best_standard_64_1.0000E-04.pt
2022-05-13 03:39:13 | INFO | fairseq.checkpoint_utils | Saved checkpoint checkpoints/language_model/standard/checkpoint_best_standard_64_1.0000E-04.pt (epoch 70 @ 2730 updates, score 6.643) (writing took 1.14941899292171 seconds)
2022-05-13 03:39:13 | INFO | fairseq_cli.train | end of epoch 70 (average epoch stats below)
2022-05-13 03:39:13 | INFO | train | epoch 070 | loss 6.59 | ppl 96.37 | wps 26485.7 | ups 0.2 | wpb 130930 | bsz 255.7 | num_updates 2730 | lr 6.82818e-05 | gnorm 0.784 | train_wall 179 | gb_free 7.9 | wall 13863
2022-05-13 03:39:13 | INFO | fairseq.data.iterators | grouped total_num_itrs = 39
2022-05-13 03:39:13 | INFO | fairseq.trainer | begin training epoch 71
2022-05-13 03:39:13 | INFO | fairseq_cli.train | Start iterating over samples
2022-05-13 03:42:20 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-05-13 03:42:23 | INFO | valid | epoch 071 | valid on 'valid' subset | loss 6.612 | ppl 97.8 | wps 76400.3 | wpb 2041.1 | bsz 4 | num_updates 2769 | best_loss 6.612
2022-05-13 03:42:23 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 71 @ 2769 updates
2022-05-13 03:42:23 | INFO | fairseq.trainer | Saving checkpoint to /cluster/work/cotterell/liam/master-thesis/checkpoints/language_model/standard/checkpoint_best_standard_64_1.0000E-04.pt
2022-05-13 03:42:25 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/work/cotterell/liam/master-thesis/checkpoints/language_model/standard/checkpoint_best_standard_64_1.0000E-04.pt
2022-05-13 03:42:25 | INFO | fairseq.checkpoint_utils | Saved checkpoint checkpoints/language_model/standard/checkpoint_best_standard_64_1.0000E-04.pt (epoch 71 @ 2769 updates, score 6.612) (writing took 1.2328970287926495 seconds)
2022-05-13 03:42:25 | INFO | fairseq_cli.train | end of epoch 71 (average epoch stats below)
2022-05-13 03:42:25 | INFO | train | epoch 071 | loss 6.554 | ppl 93.99 | wps 26607 | ups 0.2 | wpb 130930 | bsz 255.7 | num_updates 2769 | lr 6.92558e-05 | gnorm 0.812 | train_wall 178 | gb_free 7.9 | wall 14054
2022-05-13 03:42:25 | INFO | fairseq.data.iterators | grouped total_num_itrs = 39
2022-05-13 03:42:25 | INFO | fairseq.trainer | begin training epoch 72
2022-05-13 03:42:25 | INFO | fairseq_cli.train | Start iterating over samples
2022-05-13 03:44:56 | INFO | train_inner | epoch 072:     31 / 39 loss=6.554, ppl=93.97, wps=26629.7, ups=0.2, wpb=130951, bsz=255.8, num_updates=2800, lr=7.003e-05, gnorm=0.809, train_wall=456, gb_free=7.9, wall=14206
2022-05-13 03:45:34 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-05-13 03:45:38 | INFO | valid | epoch 072 | valid on 'valid' subset | loss 6.583 | ppl 95.89 | wps 76678.1 | wpb 2041.1 | bsz 4 | num_updates 2808 | best_loss 6.583
2022-05-13 03:45:38 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 72 @ 2808 updates
2022-05-13 03:45:38 | INFO | fairseq.trainer | Saving checkpoint to /cluster/work/cotterell/liam/master-thesis/checkpoints/language_model/standard/checkpoint_best_standard_64_1.0000E-04.pt
2022-05-13 03:45:39 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/work/cotterell/liam/master-thesis/checkpoints/language_model/standard/checkpoint_best_standard_64_1.0000E-04.pt
2022-05-13 03:45:39 | INFO | fairseq.checkpoint_utils | Saved checkpoint checkpoints/language_model/standard/checkpoint_best_standard_64_1.0000E-04.pt (epoch 72 @ 2808 updates, score 6.583) (writing took 1.2231595418415964 seconds)
2022-05-13 03:45:39 | INFO | fairseq_cli.train | end of epoch 72 (average epoch stats below)
2022-05-13 03:45:39 | INFO | train | epoch 072 | loss 6.518 | ppl 91.67 | wps 26224.1 | ups 0.2 | wpb 130930 | bsz 255.7 | num_updates 2808 | lr 7.02298e-05 | gnorm 0.838 | train_wall 178 | gb_free 7.9 | wall 14249
2022-05-13 03:45:39 | INFO | fairseq.data.iterators | grouped total_num_itrs = 39
2022-05-13 03:45:39 | INFO | fairseq.trainer | begin training epoch 73
2022-05-13 03:45:39 | INFO | fairseq_cli.train | Start iterating over samples
2022-05-13 03:48:48 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-05-13 03:48:52 | INFO | valid | epoch 073 | valid on 'valid' subset | loss 6.553 | ppl 93.87 | wps 76541.1 | wpb 2041.1 | bsz 4 | num_updates 2847 | best_loss 6.553
2022-05-13 03:48:52 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 73 @ 2847 updates
2022-05-13 03:48:52 | INFO | fairseq.trainer | Saving checkpoint to /cluster/work/cotterell/liam/master-thesis/checkpoints/language_model/standard/checkpoint_best_standard_64_1.0000E-04.pt
2022-05-13 03:48:53 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/work/cotterell/liam/master-thesis/checkpoints/language_model/standard/checkpoint_best_standard_64_1.0000E-04.pt
2022-05-13 03:48:53 | INFO | fairseq.checkpoint_utils | Saved checkpoint checkpoints/language_model/standard/checkpoint_best_standard_64_1.0000E-04.pt (epoch 73 @ 2847 updates, score 6.553) (writing took 1.3803794458508492 seconds)
2022-05-13 03:48:53 | INFO | fairseq_cli.train | end of epoch 73 (average epoch stats below)
2022-05-13 03:48:54 | INFO | train | epoch 073 | loss 6.482 | ppl 89.4 | wps 26292.7 | ups 0.2 | wpb 130930 | bsz 255.7 | num_updates 2847 | lr 7.12038e-05 | gnorm 0.792 | train_wall 179 | gb_free 7.9 | wall 14443
2022-05-13 03:48:54 | INFO | fairseq.data.iterators | grouped total_num_itrs = 39
2022-05-13 03:48:54 | INFO | fairseq.trainer | begin training epoch 74
2022-05-13 03:48:54 | INFO | fairseq_cli.train | Start iterating over samples
2022-05-13 03:52:03 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-05-13 03:52:07 | INFO | valid | epoch 074 | valid on 'valid' subset | loss 6.524 | ppl 92.02 | wps 76606.2 | wpb 2041.1 | bsz 4 | num_updates 2886 | best_loss 6.524
2022-05-13 03:52:07 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 74 @ 2886 updates
2022-05-13 03:52:07 | INFO | fairseq.trainer | Saving checkpoint to /cluster/work/cotterell/liam/master-thesis/checkpoints/language_model/standard/checkpoint_best_standard_64_1.0000E-04.pt
2022-05-13 03:52:08 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/work/cotterell/liam/master-thesis/checkpoints/language_model/standard/checkpoint_best_standard_64_1.0000E-04.pt
2022-05-13 03:52:08 | INFO | fairseq.checkpoint_utils | Saved checkpoint checkpoints/language_model/standard/checkpoint_best_standard_64_1.0000E-04.pt (epoch 74 @ 2886 updates, score 6.524) (writing took 1.1831989563070238 seconds)
2022-05-13 03:52:08 | INFO | fairseq_cli.train | end of epoch 74 (average epoch stats below)
2022-05-13 03:52:08 | INFO | train | epoch 074 | loss 6.447 | ppl 87.27 | wps 26238.3 | ups 0.2 | wpb 130930 | bsz 255.7 | num_updates 2886 | lr 7.21779e-05 | gnorm 0.799 | train_wall 180 | gb_free 7.9 | wall 14638
2022-05-13 03:52:08 | INFO | fairseq.data.iterators | grouped total_num_itrs = 39
2022-05-13 03:52:08 | INFO | fairseq.trainer | begin training epoch 75
2022-05-13 03:52:08 | INFO | fairseq_cli.train | Start iterating over samples
2022-05-13 03:53:16 | INFO | train_inner | epoch 075:     14 / 39 loss=6.462, ppl=88.14, wps=26205.6, ups=0.2, wpb=130911, bsz=255.7, num_updates=2900, lr=7.25275e-05, gnorm=0.809, train_wall=459, gb_free=7.9, wall=14705
2022-05-13 03:55:15 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-05-13 03:55:19 | INFO | valid | epoch 075 | valid on 'valid' subset | loss 6.499 | ppl 90.43 | wps 76425.6 | wpb 2041.1 | bsz 4 | num_updates 2925 | best_loss 6.499
2022-05-13 03:55:19 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 75 @ 2925 updates
2022-05-13 03:55:19 | INFO | fairseq.trainer | Saving checkpoint to /cluster/work/cotterell/liam/master-thesis/checkpoints/language_model/standard/checkpoint_best_standard_64_1.0000E-04.pt
2022-05-13 03:55:20 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/work/cotterell/liam/master-thesis/checkpoints/language_model/standard/checkpoint_best_standard_64_1.0000E-04.pt
2022-05-13 03:55:20 | INFO | fairseq.checkpoint_utils | Saved checkpoint checkpoints/language_model/standard/checkpoint_best_standard_64_1.0000E-04.pt (epoch 75 @ 2925 updates, score 6.499) (writing took 1.0864201341755688 seconds)
2022-05-13 03:55:20 | INFO | fairseq_cli.train | end of epoch 75 (average epoch stats below)
2022-05-13 03:55:20 | INFO | train | epoch 075 | loss 6.413 | ppl 85.21 | wps 26616.3 | ups 0.2 | wpb 130930 | bsz 255.7 | num_updates 2925 | lr 7.31519e-05 | gnorm 0.828 | train_wall 178 | gb_free 7.9 | wall 14830
2022-05-13 03:55:20 | INFO | fairseq.data.iterators | grouped total_num_itrs = 39
2022-05-13 03:55:20 | INFO | fairseq.trainer | begin training epoch 76
2022-05-13 03:55:20 | INFO | fairseq_cli.train | Start iterating over samples
2022-05-13 03:58:34 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-05-13 03:58:38 | INFO | valid | epoch 076 | valid on 'valid' subset | loss 6.476 | ppl 89.01 | wps 76560.1 | wpb 2041.1 | bsz 4 | num_updates 2964 | best_loss 6.476
2022-05-13 03:58:38 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 76 @ 2964 updates
2022-05-13 03:58:38 | INFO | fairseq.trainer | Saving checkpoint to /cluster/work/cotterell/liam/master-thesis/checkpoints/language_model/standard/checkpoint_best_standard_64_1.0000E-04.pt
2022-05-13 03:58:39 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/work/cotterell/liam/master-thesis/checkpoints/language_model/standard/checkpoint_best_standard_64_1.0000E-04.pt
2022-05-13 03:58:40 | INFO | fairseq.checkpoint_utils | Saved checkpoint checkpoints/language_model/standard/checkpoint_best_standard_64_1.0000E-04.pt (epoch 76 @ 2964 updates, score 6.476) (writing took 1.3249777690507472 seconds)
2022-05-13 03:58:40 | INFO | fairseq_cli.train | end of epoch 76 (average epoch stats below)
2022-05-13 03:58:40 | INFO | train | epoch 076 | loss 6.378 | ppl 83.18 | wps 25590 | ups 0.2 | wpb 130930 | bsz 255.7 | num_updates 2964 | lr 7.41259e-05 | gnorm 0.793 | train_wall 182 | gb_free 7.9 | wall 15029
2022-05-13 03:58:40 | INFO | fairseq.data.iterators | grouped total_num_itrs = 39
2022-05-13 03:58:40 | INFO | fairseq.trainer | begin training epoch 77
2022-05-13 03:58:40 | INFO | fairseq_cli.train | Start iterating over samples
2022-05-13 04:01:33 | INFO | train_inner | epoch 077:     36 / 39 loss=6.374, ppl=82.96, wps=26322.7, ups=0.2, wpb=130951, bsz=255.8, num_updates=3000, lr=7.5025e-05, gnorm=0.813, train_wall=461, gb_free=7.9, wall=15203
2022-05-13 04:01:47 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-05-13 04:01:51 | INFO | valid | epoch 077 | valid on 'valid' subset | loss 6.444 | ppl 87.07 | wps 76740.8 | wpb 2041.1 | bsz 4 | num_updates 3003 | best_loss 6.444
2022-05-13 04:01:51 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 77 @ 3003 updates
2022-05-13 04:01:51 | INFO | fairseq.trainer | Saving checkpoint to /cluster/work/cotterell/liam/master-thesis/checkpoints/language_model/standard/checkpoint_best_standard_64_1.0000E-04.pt
2022-05-13 04:01:53 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/work/cotterell/liam/master-thesis/checkpoints/language_model/standard/checkpoint_best_standard_64_1.0000E-04.pt
2022-05-13 04:01:53 | INFO | fairseq.checkpoint_utils | Saved checkpoint checkpoints/language_model/standard/checkpoint_best_standard_64_1.0000E-04.pt (epoch 77 @ 3003 updates, score 6.444) (writing took 1.506895957980305 seconds)
2022-05-13 04:01:53 | INFO | fairseq_cli.train | end of epoch 77 (average epoch stats below)
2022-05-13 04:01:53 | INFO | train | epoch 077 | loss 6.345 | ppl 81.29 | wps 26453.5 | ups 0.2 | wpb 130930 | bsz 255.7 | num_updates 3003 | lr 7.50999e-05 | gnorm 0.823 | train_wall 179 | gb_free 7.9 | wall 15222
2022-05-13 04:01:53 | INFO | fairseq.data.iterators | grouped total_num_itrs = 39
2022-05-13 04:01:53 | INFO | fairseq.trainer | begin training epoch 78
2022-05-13 04:01:53 | INFO | fairseq_cli.train | Start iterating over samples
2022-05-13 04:04:59 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-05-13 04:05:03 | INFO | valid | epoch 078 | valid on 'valid' subset | loss 6.418 | ppl 85.48 | wps 76640.1 | wpb 2041.1 | bsz 4 | num_updates 3042 | best_loss 6.418
2022-05-13 04:05:03 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 78 @ 3042 updates
2022-05-13 04:05:03 | INFO | fairseq.trainer | Saving checkpoint to /cluster/work/cotterell/liam/master-thesis/checkpoints/language_model/standard/checkpoint_best_standard_64_1.0000E-04.pt
2022-05-13 04:05:04 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/work/cotterell/liam/master-thesis/checkpoints/language_model/standard/checkpoint_best_standard_64_1.0000E-04.pt
2022-05-13 04:05:04 | INFO | fairseq.checkpoint_utils | Saved checkpoint checkpoints/language_model/standard/checkpoint_best_standard_64_1.0000E-04.pt (epoch 78 @ 3042 updates, score 6.418) (writing took 1.182529326993972 seconds)
2022-05-13 04:05:04 | INFO | fairseq_cli.train | end of epoch 78 (average epoch stats below)
2022-05-13 04:05:04 | INFO | train | epoch 078 | loss 6.312 | ppl 79.43 | wps 26613 | ups 0.2 | wpb 130930 | bsz 255.7 | num_updates 3042 | lr 7.6074e-05 | gnorm 0.837 | train_wall 178 | gb_free 7.9 | wall 15414
2022-05-13 04:05:04 | INFO | fairseq.data.iterators | grouped total_num_itrs = 39
2022-05-13 04:05:04 | INFO | fairseq.trainer | begin training epoch 79
2022-05-13 04:05:04 | INFO | fairseq_cli.train | Start iterating over samples
2022-05-13 04:08:13 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-05-13 04:08:17 | INFO | valid | epoch 079 | valid on 'valid' subset | loss 6.391 | ppl 83.9 | wps 76648.4 | wpb 2041.1 | bsz 4 | num_updates 3081 | best_loss 6.391
2022-05-13 04:08:17 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 79 @ 3081 updates
2022-05-13 04:08:17 | INFO | fairseq.trainer | Saving checkpoint to /cluster/work/cotterell/liam/master-thesis/checkpoints/language_model/standard/checkpoint_best_standard_64_1.0000E-04.pt
2022-05-13 04:08:18 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/work/cotterell/liam/master-thesis/checkpoints/language_model/standard/checkpoint_best_standard_64_1.0000E-04.pt
2022-05-13 04:08:18 | INFO | fairseq.checkpoint_utils | Saved checkpoint checkpoints/language_model/standard/checkpoint_best_standard_64_1.0000E-04.pt (epoch 79 @ 3081 updates, score 6.391) (writing took 1.1818353198468685 seconds)
2022-05-13 04:08:18 | INFO | fairseq_cli.train | end of epoch 79 (average epoch stats below)
2022-05-13 04:08:18 | INFO | train | epoch 079 | loss 6.28 | ppl 77.7 | wps 26399.7 | ups 0.2 | wpb 130930 | bsz 255.7 | num_updates 3081 | lr 7.7048e-05 | gnorm 0.851 | train_wall 180 | gb_free 7.9 | wall 15608
2022-05-13 04:08:18 | INFO | fairseq.data.iterators | grouped total_num_itrs = 39
2022-05-13 04:08:18 | INFO | fairseq.trainer | begin training epoch 80
2022-05-13 04:08:18 | INFO | fairseq_cli.train | Start iterating over samples
2022-05-13 04:09:57 | INFO | train_inner | epoch 080:     19 / 39 loss=6.289, ppl=78.19, wps=25970.6, ups=0.2, wpb=130920, bsz=255.7, num_updates=3100, lr=7.75225e-05, gnorm=0.841, train_wall=463, gb_free=7.9, wall=15707
2022-05-13 04:11:34 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-05-13 04:11:38 | INFO | valid | epoch 080 | valid on 'valid' subset | loss 6.364 | ppl 82.35 | wps 76582.9 | wpb 2041.1 | bsz 4 | num_updates 3120 | best_loss 6.364
2022-05-13 04:11:38 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 80 @ 3120 updates
2022-05-13 04:11:38 | INFO | fairseq.trainer | Saving checkpoint to /cluster/work/cotterell/liam/master-thesis/checkpoints/language_model/standard/checkpoint_best_standard_64_1.0000E-04.pt
2022-05-13 04:11:39 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/work/cotterell/liam/master-thesis/checkpoints/language_model/standard/checkpoint_best_standard_64_1.0000E-04.pt
2022-05-13 04:11:39 | INFO | fairseq.checkpoint_utils | Saved checkpoint checkpoints/language_model/standard/checkpoint_best_standard_64_1.0000E-04.pt (epoch 80 @ 3120 updates, score 6.364) (writing took 1.383368771057576 seconds)
2022-05-13 04:11:39 | INFO | fairseq_cli.train | end of epoch 80 (average epoch stats below)
2022-05-13 04:11:39 | INFO | train | epoch 080 | loss 6.247 | ppl 75.93 | wps 25331.1 | ups 0.19 | wpb 130930 | bsz 255.7 | num_updates 3120 | lr 7.8022e-05 | gnorm 0.842 | train_wall 184 | gb_free 7.9 | wall 15809
2022-05-13 04:11:39 | INFO | fairseq.data.iterators | grouped total_num_itrs = 39
2022-05-13 04:11:39 | INFO | fairseq.trainer | begin training epoch 81
2022-05-13 04:11:39 | INFO | fairseq_cli.train | Start iterating over samples
2022-05-13 04:14:49 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-05-13 04:14:53 | INFO | valid | epoch 081 | valid on 'valid' subset | loss 6.348 | ppl 81.48 | wps 76470.8 | wpb 2041.1 | bsz 4 | num_updates 3159 | best_loss 6.348
2022-05-13 04:14:53 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 81 @ 3159 updates
2022-05-13 04:14:53 | INFO | fairseq.trainer | Saving checkpoint to /cluster/work/cotterell/liam/master-thesis/checkpoints/language_model/standard/checkpoint_best_standard_64_1.0000E-04.pt
2022-05-13 04:14:54 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/work/cotterell/liam/master-thesis/checkpoints/language_model/standard/checkpoint_best_standard_64_1.0000E-04.pt
2022-05-13 04:14:54 | INFO | fairseq.checkpoint_utils | Saved checkpoint checkpoints/language_model/standard/checkpoint_best_standard_64_1.0000E-04.pt (epoch 81 @ 3159 updates, score 6.348) (writing took 1.1787702846340835 seconds)
2022-05-13 04:14:54 | INFO | fairseq_cli.train | end of epoch 81 (average epoch stats below)
2022-05-13 04:14:54 | INFO | train | epoch 081 | loss 6.214 | ppl 74.26 | wps 26286.7 | ups 0.2 | wpb 130930 | bsz 255.7 | num_updates 3159 | lr 7.8996e-05 | gnorm 0.791 | train_wall 180 | gb_free 7.9 | wall 16004
2022-05-13 04:14:54 | INFO | fairseq.data.iterators | grouped total_num_itrs = 39
2022-05-13 04:14:54 | INFO | fairseq.trainer | begin training epoch 82
2022-05-13 04:14:54 | INFO | fairseq_cli.train | Start iterating over samples
2022-05-13 04:18:01 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-05-13 04:18:05 | INFO | valid | epoch 082 | valid on 'valid' subset | loss 6.318 | ppl 79.81 | wps 76680.6 | wpb 2041.1 | bsz 4 | num_updates 3198 | best_loss 6.318
2022-05-13 04:18:05 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 82 @ 3198 updates
2022-05-13 04:18:05 | INFO | fairseq.trainer | Saving checkpoint to /cluster/work/cotterell/liam/master-thesis/checkpoints/language_model/standard/checkpoint_best_standard_64_1.0000E-04.pt
2022-05-13 04:18:06 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/work/cotterell/liam/master-thesis/checkpoints/language_model/standard/checkpoint_best_standard_64_1.0000E-04.pt
2022-05-13 04:18:06 | INFO | fairseq.checkpoint_utils | Saved checkpoint checkpoints/language_model/standard/checkpoint_best_standard_64_1.0000E-04.pt (epoch 82 @ 3198 updates, score 6.318) (writing took 1.2023089649155736 seconds)
2022-05-13 04:18:06 | INFO | fairseq_cli.train | end of epoch 82 (average epoch stats below)
2022-05-13 04:18:06 | INFO | train | epoch 082 | loss 6.183 | ppl 72.68 | wps 26520.3 | ups 0.2 | wpb 130930 | bsz 255.7 | num_updates 3198 | lr 7.997e-05 | gnorm 0.839 | train_wall 178 | gb_free 7.9 | wall 16196
2022-05-13 04:18:06 | INFO | fairseq.data.iterators | grouped total_num_itrs = 39
2022-05-13 04:18:06 | INFO | fairseq.trainer | begin training epoch 83
2022-05-13 04:18:06 | INFO | fairseq_cli.train | Start iterating over samples
2022-05-13 04:18:16 | INFO | train_inner | epoch 083:      2 / 39 loss=6.206, ppl=73.84, wps=26245.6, ups=0.2, wpb=130905, bsz=255.7, num_updates=3200, lr=8.002e-05, gnorm=0.827, train_wall=459, gb_free=7.9, wall=16206
2022-05-13 04:21:18 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-05-13 04:21:22 | INFO | valid | epoch 083 | valid on 'valid' subset | loss 6.298 | ppl 78.7 | wps 76538.7 | wpb 2041.1 | bsz 4 | num_updates 3237 | best_loss 6.298
2022-05-13 04:21:22 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 83 @ 3237 updates
2022-05-13 04:21:22 | INFO | fairseq.trainer | Saving checkpoint to /cluster/work/cotterell/liam/master-thesis/checkpoints/language_model/standard/checkpoint_best_standard_64_1.0000E-04.pt
2022-05-13 04:21:23 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/work/cotterell/liam/master-thesis/checkpoints/language_model/standard/checkpoint_best_standard_64_1.0000E-04.pt
2022-05-13 04:21:24 | INFO | fairseq.checkpoint_utils | Saved checkpoint checkpoints/language_model/standard/checkpoint_best_standard_64_1.0000E-04.pt (epoch 83 @ 3237 updates, score 6.298) (writing took 1.2212278093211353 seconds)
2022-05-13 04:21:24 | INFO | fairseq_cli.train | end of epoch 83 (average epoch stats below)
2022-05-13 04:21:24 | INFO | train | epoch 083 | loss 6.154 | ppl 71.22 | wps 25884.7 | ups 0.2 | wpb 130930 | bsz 255.7 | num_updates 3237 | lr 8.09441e-05 | gnorm 0.947 | train_wall 181 | gb_free 7.9 | wall 16393
2022-05-13 04:21:24 | INFO | fairseq.data.iterators | grouped total_num_itrs = 39
2022-05-13 04:21:24 | INFO | fairseq.trainer | begin training epoch 84
2022-05-13 04:21:24 | INFO | fairseq_cli.train | Start iterating over samples
2022-05-13 04:24:32 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-05-13 04:24:36 | INFO | valid | epoch 084 | valid on 'valid' subset | loss 6.273 | ppl 77.32 | wps 76555.8 | wpb 2041.1 | bsz 4 | num_updates 3276 | best_loss 6.273
2022-05-13 04:24:36 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 84 @ 3276 updates
2022-05-13 04:24:36 | INFO | fairseq.trainer | Saving checkpoint to /cluster/work/cotterell/liam/master-thesis/checkpoints/language_model/standard/checkpoint_best_standard_64_1.0000E-04.pt
2022-05-13 04:24:37 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/work/cotterell/liam/master-thesis/checkpoints/language_model/standard/checkpoint_best_standard_64_1.0000E-04.pt
2022-05-13 04:24:37 | INFO | fairseq.checkpoint_utils | Saved checkpoint checkpoints/language_model/standard/checkpoint_best_standard_64_1.0000E-04.pt (epoch 84 @ 3276 updates, score 6.273) (writing took 1.372089697048068 seconds)
2022-05-13 04:24:37 | INFO | fairseq_cli.train | end of epoch 84 (average epoch stats below)
2022-05-13 04:24:37 | INFO | train | epoch 084 | loss 6.122 | ppl 69.67 | wps 26351.9 | ups 0.2 | wpb 130930 | bsz 255.7 | num_updates 3276 | lr 8.19181e-05 | gnorm 0.815 | train_wall 179 | gb_free 7.9 | wall 16587
2022-05-13 04:24:37 | INFO | fairseq.data.iterators | grouped total_num_itrs = 39
2022-05-13 04:24:37 | INFO | fairseq.trainer | begin training epoch 85
2022-05-13 04:24:37 | INFO | fairseq_cli.train | Start iterating over samples
2022-05-13 04:26:33 | INFO | train_inner | epoch 085:     24 / 39 loss=6.127, ppl=69.9, wps=26352.9, ups=0.2, wpb=130956, bsz=255.8, num_updates=3300, lr=8.25175e-05, gnorm=0.89, train_wall=460, gb_free=7.9, wall=16703
2022-05-13 04:27:45 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-05-13 04:27:48 | INFO | valid | epoch 085 | valid on 'valid' subset | loss 6.251 | ppl 76.19 | wps 76686.2 | wpb 2041.1 | bsz 4 | num_updates 3315 | best_loss 6.251
2022-05-13 04:27:48 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 85 @ 3315 updates
2022-05-13 04:27:48 | INFO | fairseq.trainer | Saving checkpoint to /cluster/work/cotterell/liam/master-thesis/checkpoints/language_model/standard/checkpoint_best_standard_64_1.0000E-04.pt
2022-05-13 04:27:50 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/work/cotterell/liam/master-thesis/checkpoints/language_model/standard/checkpoint_best_standard_64_1.0000E-04.pt
2022-05-13 04:27:50 | INFO | fairseq.checkpoint_utils | Saved checkpoint checkpoints/language_model/standard/checkpoint_best_standard_64_1.0000E-04.pt (epoch 85 @ 3315 updates, score 6.251) (writing took 1.1759314108639956 seconds)
2022-05-13 04:27:50 | INFO | fairseq_cli.train | end of epoch 85 (average epoch stats below)
2022-05-13 04:27:50 | INFO | train | epoch 085 | loss 6.093 | ppl 68.26 | wps 26552.3 | ups 0.2 | wpb 130930 | bsz 255.7 | num_updates 3315 | lr 8.28921e-05 | gnorm 0.911 | train_wall 178 | gb_free 7.9 | wall 16779
2022-05-13 04:27:50 | INFO | fairseq.data.iterators | grouped total_num_itrs = 39
2022-05-13 04:27:50 | INFO | fairseq.trainer | begin training epoch 86
2022-05-13 04:27:50 | INFO | fairseq_cli.train | Start iterating over samples
2022-05-13 04:30:56 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-05-13 04:31:00 | INFO | valid | epoch 086 | valid on 'valid' subset | loss 6.228 | ppl 74.95 | wps 76496.7 | wpb 2041.1 | bsz 4 | num_updates 3354 | best_loss 6.228
2022-05-13 04:31:00 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 86 @ 3354 updates
2022-05-13 04:31:00 | INFO | fairseq.trainer | Saving checkpoint to /cluster/work/cotterell/liam/master-thesis/checkpoints/language_model/standard/checkpoint_best_standard_64_1.0000E-04.pt
2022-05-13 04:31:01 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/work/cotterell/liam/master-thesis/checkpoints/language_model/standard/checkpoint_best_standard_64_1.0000E-04.pt
2022-05-13 04:31:02 | INFO | fairseq.checkpoint_utils | Saved checkpoint checkpoints/language_model/standard/checkpoint_best_standard_64_1.0000E-04.pt (epoch 86 @ 3354 updates, score 6.228) (writing took 1.247444196138531 seconds)
2022-05-13 04:31:02 | INFO | fairseq_cli.train | end of epoch 86 (average epoch stats below)
2022-05-13 04:31:02 | INFO | train | epoch 086 | loss 6.062 | ppl 66.8 | wps 26604.7 | ups 0.2 | wpb 130930 | bsz 255.7 | num_updates 3354 | lr 8.38662e-05 | gnorm 0.766 | train_wall 178 | gb_free 7.9 | wall 16971
2022-05-13 04:31:02 | INFO | fairseq.data.iterators | grouped total_num_itrs = 39
2022-05-13 04:31:02 | INFO | fairseq.trainer | begin training epoch 87
2022-05-13 04:31:02 | INFO | fairseq_cli.train | Start iterating over samples
2022-05-13 04:34:15 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-05-13 04:34:19 | INFO | valid | epoch 087 | valid on 'valid' subset | loss 6.205 | ppl 73.75 | wps 76722.8 | wpb 2041.1 | bsz 4 | num_updates 3393 | best_loss 6.205
2022-05-13 04:34:19 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 87 @ 3393 updates
2022-05-13 04:34:19 | INFO | fairseq.trainer | Saving checkpoint to /cluster/work/cotterell/liam/master-thesis/checkpoints/language_model/standard/checkpoint_best_standard_64_1.0000E-04.pt
2022-05-13 04:34:20 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/work/cotterell/liam/master-thesis/checkpoints/language_model/standard/checkpoint_best_standard_64_1.0000E-04.pt
2022-05-13 04:34:20 | INFO | fairseq.checkpoint_utils | Saved checkpoint checkpoints/language_model/standard/checkpoint_best_standard_64_1.0000E-04.pt (epoch 87 @ 3393 updates, score 6.205) (writing took 1.2591598057188094 seconds)
2022-05-13 04:34:20 | INFO | fairseq_cli.train | end of epoch 87 (average epoch stats below)
2022-05-13 04:34:20 | INFO | train | epoch 087 | loss 6.034 | ppl 65.53 | wps 25711.5 | ups 0.2 | wpb 130930 | bsz 255.7 | num_updates 3393 | lr 8.48402e-05 | gnorm 0.903 | train_wall 182 | gb_free 7.9 | wall 17170
2022-05-13 04:34:20 | INFO | fairseq.data.iterators | grouped total_num_itrs = 39
2022-05-13 04:34:20 | INFO | fairseq.trainer | begin training epoch 88
2022-05-13 04:34:20 | INFO | fairseq_cli.train | Start iterating over samples
2022-05-13 04:34:54 | INFO | train_inner | epoch 088:      7 / 39 loss=6.051, ppl=66.31, wps=26107.8, ups=0.2, wpb=130910, bsz=255.7, num_updates=3400, lr=8.5015e-05, gnorm=0.833, train_wall=461, gb_free=7.9, wall=17204
2022-05-13 04:37:28 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-05-13 04:37:32 | INFO | valid | epoch 088 | valid on 'valid' subset | loss 6.191 | ppl 73.06 | wps 76619.6 | wpb 2041.1 | bsz 4 | num_updates 3432 | best_loss 6.191
2022-05-13 04:37:32 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 88 @ 3432 updates
2022-05-13 04:37:32 | INFO | fairseq.trainer | Saving checkpoint to /cluster/work/cotterell/liam/master-thesis/checkpoints/language_model/standard/checkpoint_best_standard_64_1.0000E-04.pt
2022-05-13 04:37:33 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/work/cotterell/liam/master-thesis/checkpoints/language_model/standard/checkpoint_best_standard_64_1.0000E-04.pt
2022-05-13 04:37:33 | INFO | fairseq.checkpoint_utils | Saved checkpoint checkpoints/language_model/standard/checkpoint_best_standard_64_1.0000E-04.pt (epoch 88 @ 3432 updates, score 6.191) (writing took 1.2515152008272707 seconds)
2022-05-13 04:37:33 | INFO | fairseq_cli.train | end of epoch 88 (average epoch stats below)
2022-05-13 04:37:33 | INFO | train | epoch 088 | loss 6.005 | ppl 64.21 | wps 26457.7 | ups 0.2 | wpb 130930 | bsz 255.7 | num_updates 3432 | lr 8.58142e-05 | gnorm 0.858 | train_wall 178 | gb_free 7.9 | wall 17363
2022-05-13 04:37:33 | INFO | fairseq.data.iterators | grouped total_num_itrs = 39
2022-05-13 04:37:33 | INFO | fairseq.trainer | begin training epoch 89
2022-05-13 04:37:33 | INFO | fairseq_cli.train | Start iterating over samples
2022-05-13 04:40:41 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-05-13 04:40:45 | INFO | valid | epoch 089 | valid on 'valid' subset | loss 6.167 | ppl 71.86 | wps 76720.9 | wpb 2041.1 | bsz 4 | num_updates 3471 | best_loss 6.167
2022-05-13 04:40:45 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 89 @ 3471 updates
2022-05-13 04:40:45 | INFO | fairseq.trainer | Saving checkpoint to /cluster/work/cotterell/liam/master-thesis/checkpoints/language_model/standard/checkpoint_best_standard_64_1.0000E-04.pt
2022-05-13 04:40:46 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/work/cotterell/liam/master-thesis/checkpoints/language_model/standard/checkpoint_best_standard_64_1.0000E-04.pt
2022-05-13 04:40:46 | INFO | fairseq.checkpoint_utils | Saved checkpoint checkpoints/language_model/standard/checkpoint_best_standard_64_1.0000E-04.pt (epoch 89 @ 3471 updates, score 6.167) (writing took 1.262176473159343 seconds)
2022-05-13 04:40:46 | INFO | fairseq_cli.train | end of epoch 89 (average epoch stats below)
2022-05-13 04:40:46 | INFO | train | epoch 089 | loss 5.976 | ppl 62.94 | wps 26405.5 | ups 0.2 | wpb 130930 | bsz 255.7 | num_updates 3471 | lr 8.67882e-05 | gnorm 0.836 | train_wall 179 | gb_free 7.9 | wall 17556
2022-05-13 04:40:46 | INFO | fairseq.data.iterators | grouped total_num_itrs = 39
2022-05-13 04:40:47 | INFO | fairseq.trainer | begin training epoch 90
2022-05-13 04:40:47 | INFO | fairseq_cli.train | Start iterating over samples
2022-05-13 04:43:05 | INFO | train_inner | epoch 090:     29 / 39 loss=5.978, ppl=63.02, wps=26660.9, ups=0.2, wpb=130956, bsz=255.8, num_updates=3500, lr=8.75125e-05, gnorm=0.859, train_wall=458, gb_free=7.9, wall=17695
2022-05-13 04:43:54 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-05-13 04:43:58 | INFO | valid | epoch 090 | valid on 'valid' subset | loss 6.15 | ppl 71.03 | wps 76643.9 | wpb 2041.1 | bsz 4 | num_updates 3510 | best_loss 6.15
2022-05-13 04:43:58 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 90 @ 3510 updates
2022-05-13 04:43:58 | INFO | fairseq.trainer | Saving checkpoint to /cluster/work/cotterell/liam/master-thesis/checkpoints/language_model/standard/checkpoint_best_standard_64_1.0000E-04.pt
2022-05-13 04:43:59 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/work/cotterell/liam/master-thesis/checkpoints/language_model/standard/checkpoint_best_standard_64_1.0000E-04.pt
2022-05-13 04:43:59 | INFO | fairseq.checkpoint_utils | Saved checkpoint checkpoints/language_model/standard/checkpoint_best_standard_64_1.0000E-04.pt (epoch 90 @ 3510 updates, score 6.15) (writing took 1.0772416610270739 seconds)
2022-05-13 04:43:59 | INFO | fairseq_cli.train | end of epoch 90 (average epoch stats below)
2022-05-13 04:43:59 | INFO | train | epoch 090 | loss 5.948 | ppl 61.74 | wps 26482.5 | ups 0.2 | wpb 130930 | bsz 255.7 | num_updates 3510 | lr 8.77623e-05 | gnorm 0.856 | train_wall 178 | gb_free 7.9 | wall 17749
2022-05-13 04:43:59 | INFO | fairseq.data.iterators | grouped total_num_itrs = 39
2022-05-13 04:43:59 | INFO | fairseq.trainer | begin training epoch 91
2022-05-13 04:43:59 | INFO | fairseq_cli.train | Start iterating over samples
2022-05-13 04:47:08 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-05-13 04:47:12 | INFO | valid | epoch 091 | valid on 'valid' subset | loss 6.132 | ppl 70.14 | wps 76475.8 | wpb 2041.1 | bsz 4 | num_updates 3549 | best_loss 6.132
2022-05-13 04:47:12 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 91 @ 3549 updates
2022-05-13 04:47:12 | INFO | fairseq.trainer | Saving checkpoint to /cluster/work/cotterell/liam/master-thesis/checkpoints/language_model/standard/checkpoint_best_standard_64_1.0000E-04.pt
2022-05-13 04:47:13 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/work/cotterell/liam/master-thesis/checkpoints/language_model/standard/checkpoint_best_standard_64_1.0000E-04.pt
2022-05-13 04:47:13 | INFO | fairseq.checkpoint_utils | Saved checkpoint checkpoints/language_model/standard/checkpoint_best_standard_64_1.0000E-04.pt (epoch 91 @ 3549 updates, score 6.132) (writing took 1.3246216322295368 seconds)
2022-05-13 04:47:13 | INFO | fairseq_cli.train | end of epoch 91 (average epoch stats below)
2022-05-13 04:47:13 | INFO | train | epoch 091 | loss 5.92 | ppl 60.54 | wps 26299.9 | ups 0.2 | wpb 130930 | bsz 255.7 | num_updates 3549 | lr 8.87363e-05 | gnorm 0.833 | train_wall 179 | gb_free 7.9 | wall 17943
2022-05-13 04:47:13 | INFO | fairseq.data.iterators | grouped total_num_itrs = 39
2022-05-13 04:47:13 | INFO | fairseq.trainer | begin training epoch 92
2022-05-13 04:47:13 | INFO | fairseq_cli.train | Start iterating over samples
2022-05-13 04:50:21 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-05-13 04:50:25 | INFO | valid | epoch 092 | valid on 'valid' subset | loss 6.105 | ppl 68.84 | wps 76735.3 | wpb 2041.1 | bsz 4 | num_updates 3588 | best_loss 6.105
2022-05-13 04:50:25 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 92 @ 3588 updates
2022-05-13 04:50:25 | INFO | fairseq.trainer | Saving checkpoint to /cluster/work/cotterell/liam/master-thesis/checkpoints/language_model/standard/checkpoint_best_standard_64_1.0000E-04.pt
2022-05-13 04:50:27 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/work/cotterell/liam/master-thesis/checkpoints/language_model/standard/checkpoint_best_standard_64_1.0000E-04.pt
2022-05-13 04:50:27 | INFO | fairseq.checkpoint_utils | Saved checkpoint checkpoints/language_model/standard/checkpoint_best_standard_64_1.0000E-04.pt (epoch 92 @ 3588 updates, score 6.105) (writing took 1.1687743971124291 seconds)
2022-05-13 04:50:27 | INFO | fairseq_cli.train | end of epoch 92 (average epoch stats below)
2022-05-13 04:50:27 | INFO | train | epoch 092 | loss 5.892 | ppl 59.37 | wps 26443.2 | ups 0.2 | wpb 130930 | bsz 255.7 | num_updates 3588 | lr 8.97103e-05 | gnorm 0.854 | train_wall 179 | gb_free 7.9 | wall 18136
2022-05-13 04:50:27 | INFO | fairseq.data.iterators | grouped total_num_itrs = 39
2022-05-13 04:50:27 | INFO | fairseq.trainer | begin training epoch 93
2022-05-13 04:50:27 | INFO | fairseq_cli.train | Start iterating over samples
2022-05-13 04:51:24 | INFO | train_inner | epoch 093:     12 / 39 loss=5.905, ppl=59.93, wps=26254.6, ups=0.2, wpb=130910, bsz=255.7, num_updates=3600, lr=9.001e-05, gnorm=0.845, train_wall=457, gb_free=7.9, wall=18194
2022-05-13 04:53:33 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-05-13 04:53:37 | INFO | valid | epoch 093 | valid on 'valid' subset | loss 6.093 | ppl 68.25 | wps 76634.4 | wpb 2041.1 | bsz 4 | num_updates 3627 | best_loss 6.093
2022-05-13 04:53:37 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 93 @ 3627 updates
2022-05-13 04:53:37 | INFO | fairseq.trainer | Saving checkpoint to /cluster/work/cotterell/liam/master-thesis/checkpoints/language_model/standard/checkpoint_best_standard_64_1.0000E-04.pt
2022-05-13 04:53:38 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/work/cotterell/liam/master-thesis/checkpoints/language_model/standard/checkpoint_best_standard_64_1.0000E-04.pt
2022-05-13 04:53:38 | INFO | fairseq.checkpoint_utils | Saved checkpoint checkpoints/language_model/standard/checkpoint_best_standard_64_1.0000E-04.pt (epoch 93 @ 3627 updates, score 6.093) (writing took 1.1223058458417654 seconds)
2022-05-13 04:53:38 | INFO | fairseq_cli.train | end of epoch 93 (average epoch stats below)
2022-05-13 04:53:38 | INFO | train | epoch 093 | loss 5.865 | ppl 58.27 | wps 26639.1 | ups 0.2 | wpb 130930 | bsz 255.7 | num_updates 3627 | lr 9.06843e-05 | gnorm 0.903 | train_wall 178 | gb_free 7.9 | wall 18328
2022-05-13 04:53:38 | INFO | fairseq.data.iterators | grouped total_num_itrs = 39
2022-05-13 04:53:38 | INFO | fairseq.trainer | begin training epoch 94
2022-05-13 04:53:38 | INFO | fairseq_cli.train | Start iterating over samples
2022-05-13 04:56:51 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-05-13 04:56:55 | INFO | valid | epoch 094 | valid on 'valid' subset | loss 6.077 | ppl 67.53 | wps 76513 | wpb 2041.1 | bsz 4 | num_updates 3666 | best_loss 6.077
2022-05-13 04:56:55 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 94 @ 3666 updates
2022-05-13 04:56:55 | INFO | fairseq.trainer | Saving checkpoint to /cluster/work/cotterell/liam/master-thesis/checkpoints/language_model/standard/checkpoint_best_standard_64_1.0000E-04.pt
2022-05-13 04:56:56 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/work/cotterell/liam/master-thesis/checkpoints/language_model/standard/checkpoint_best_standard_64_1.0000E-04.pt
2022-05-13 04:56:56 | INFO | fairseq.checkpoint_utils | Saved checkpoint checkpoints/language_model/standard/checkpoint_best_standard_64_1.0000E-04.pt (epoch 94 @ 3666 updates, score 6.077) (writing took 1.3119214670732617 seconds)
2022-05-13 04:56:56 | INFO | fairseq_cli.train | end of epoch 94 (average epoch stats below)
2022-05-13 04:56:56 | INFO | train | epoch 094 | loss 5.837 | ppl 57.18 | wps 25808.1 | ups 0.2 | wpb 130930 | bsz 255.7 | num_updates 3666 | lr 9.16584e-05 | gnorm 0.839 | train_wall 181 | gb_free 7.9 | wall 18526
2022-05-13 04:56:56 | INFO | fairseq.data.iterators | grouped total_num_itrs = 39
2022-05-13 04:56:56 | INFO | fairseq.trainer | begin training epoch 95
2022-05-13 04:56:56 | INFO | fairseq_cli.train | Start iterating over samples
2022-05-13 04:59:42 | INFO | train_inner | epoch 095:     34 / 39 loss=5.835, ppl=57.09, wps=26279.5, ups=0.2, wpb=130956, bsz=255.8, num_updates=3700, lr=9.25075e-05, gnorm=0.881, train_wall=462, gb_free=7.9, wall=18692
2022-05-13 05:00:06 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-05-13 05:00:10 | INFO | valid | epoch 095 | valid on 'valid' subset | loss 6.053 | ppl 66.39 | wps 76650.7 | wpb 2041.1 | bsz 4 | num_updates 3705 | best_loss 6.053
2022-05-13 05:00:10 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 95 @ 3705 updates
2022-05-13 05:00:10 | INFO | fairseq.trainer | Saving checkpoint to /cluster/work/cotterell/liam/master-thesis/checkpoints/language_model/standard/checkpoint_best_standard_64_1.0000E-04.pt
2022-05-13 05:00:11 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/work/cotterell/liam/master-thesis/checkpoints/language_model/standard/checkpoint_best_standard_64_1.0000E-04.pt
2022-05-13 05:00:11 | INFO | fairseq.checkpoint_utils | Saved checkpoint checkpoints/language_model/standard/checkpoint_best_standard_64_1.0000E-04.pt (epoch 95 @ 3705 updates, score 6.053) (writing took 1.1982681769877672 seconds)
2022-05-13 05:00:11 | INFO | fairseq_cli.train | end of epoch 95 (average epoch stats below)
2022-05-13 05:00:11 | INFO | train | epoch 095 | loss 5.812 | ppl 56.17 | wps 26140.5 | ups 0.2 | wpb 130930 | bsz 255.7 | num_updates 3705 | lr 9.26324e-05 | gnorm 0.899 | train_wall 180 | gb_free 7.9 | wall 18721
2022-05-13 05:00:12 | INFO | fairseq.data.iterators | grouped total_num_itrs = 39
2022-05-13 05:00:12 | INFO | fairseq.trainer | begin training epoch 96
2022-05-13 05:00:12 | INFO | fairseq_cli.train | Start iterating over samples
2022-05-13 05:03:18 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-05-13 05:03:22 | INFO | valid | epoch 096 | valid on 'valid' subset | loss 6.043 | ppl 65.94 | wps 76541.4 | wpb 2041.1 | bsz 4 | num_updates 3744 | best_loss 6.043
2022-05-13 05:03:22 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 96 @ 3744 updates
2022-05-13 05:03:22 | INFO | fairseq.trainer | Saving checkpoint to /cluster/work/cotterell/liam/master-thesis/checkpoints/language_model/standard/checkpoint_best_standard_64_1.0000E-04.pt
2022-05-13 05:03:23 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/work/cotterell/liam/master-thesis/checkpoints/language_model/standard/checkpoint_best_standard_64_1.0000E-04.pt
2022-05-13 05:03:23 | INFO | fairseq.checkpoint_utils | Saved checkpoint checkpoints/language_model/standard/checkpoint_best_standard_64_1.0000E-04.pt (epoch 96 @ 3744 updates, score 6.043) (writing took 1.1506361700594425 seconds)
2022-05-13 05:03:23 | INFO | fairseq_cli.train | end of epoch 96 (average epoch stats below)
2022-05-13 05:03:23 | INFO | train | epoch 096 | loss 5.785 | ppl 55.14 | wps 26634.9 | ups 0.2 | wpb 130930 | bsz 255.7 | num_updates 3744 | lr 9.36064e-05 | gnorm 0.881 | train_wall 178 | gb_free 7.9 | wall 18913
2022-05-13 05:03:23 | INFO | fairseq.data.iterators | grouped total_num_itrs = 39
2022-05-13 05:03:23 | INFO | fairseq.trainer | begin training epoch 97
2022-05-13 05:03:23 | INFO | fairseq_cli.train | Start iterating over samples
2022-05-13 05:06:35 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-05-13 05:06:39 | INFO | valid | epoch 097 | valid on 'valid' subset | loss 6.02 | ppl 64.88 | wps 76590.6 | wpb 2041.1 | bsz 4 | num_updates 3783 | best_loss 6.02
2022-05-13 05:06:39 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 97 @ 3783 updates
2022-05-13 05:06:39 | INFO | fairseq.trainer | Saving checkpoint to /cluster/work/cotterell/liam/master-thesis/checkpoints/language_model/standard/checkpoint_best_standard_64_1.0000E-04.pt
2022-05-13 05:06:40 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/work/cotterell/liam/master-thesis/checkpoints/language_model/standard/checkpoint_best_standard_64_1.0000E-04.pt
2022-05-13 05:06:40 | INFO | fairseq.checkpoint_utils | Saved checkpoint checkpoints/language_model/standard/checkpoint_best_standard_64_1.0000E-04.pt (epoch 97 @ 3783 updates, score 6.02) (writing took 1.2173019866459072 seconds)
2022-05-13 05:06:40 | INFO | fairseq_cli.train | end of epoch 97 (average epoch stats below)
2022-05-13 05:06:40 | INFO | train | epoch 097 | loss 5.759 | ppl 54.15 | wps 25966.7 | ups 0.2 | wpb 130930 | bsz 255.7 | num_updates 3783 | lr 9.45804e-05 | gnorm 0.829 | train_wall 181 | gb_free 7.9 | wall 19110
2022-05-13 05:06:40 | INFO | fairseq.data.iterators | grouped total_num_itrs = 39
2022-05-13 05:06:40 | INFO | fairseq.trainer | begin training epoch 98
2022-05-13 05:06:40 | INFO | fairseq_cli.train | Start iterating over samples
2022-05-13 05:08:06 | INFO | train_inner | epoch 098:     17 / 39 loss=5.767, ppl=54.45, wps=26010.1, ups=0.2, wpb=130899, bsz=255.7, num_updates=3800, lr=9.5005e-05, gnorm=0.867, train_wall=462, gb_free=7.9, wall=19195
2022-05-13 05:09:51 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-05-13 05:09:55 | INFO | valid | epoch 098 | valid on 'valid' subset | loss 6.013 | ppl 64.56 | wps 76703.9 | wpb 2041.1 | bsz 4 | num_updates 3822 | best_loss 6.013
2022-05-13 05:09:55 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 98 @ 3822 updates
2022-05-13 05:09:55 | INFO | fairseq.trainer | Saving checkpoint to /cluster/work/cotterell/liam/master-thesis/checkpoints/language_model/standard/checkpoint_best_standard_64_1.0000E-04.pt
2022-05-13 05:09:56 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/work/cotterell/liam/master-thesis/checkpoints/language_model/standard/checkpoint_best_standard_64_1.0000E-04.pt
2022-05-13 05:09:56 | INFO | fairseq.checkpoint_utils | Saved checkpoint checkpoints/language_model/standard/checkpoint_best_standard_64_1.0000E-04.pt (epoch 98 @ 3822 updates, score 6.013) (writing took 1.226470806170255 seconds)
2022-05-13 05:09:56 | INFO | fairseq_cli.train | end of epoch 98 (average epoch stats below)
2022-05-13 05:09:56 | INFO | train | epoch 098 | loss 5.733 | ppl 53.18 | wps 26025.3 | ups 0.2 | wpb 130930 | bsz 255.7 | num_updates 3822 | lr 9.55545e-05 | gnorm 0.845 | train_wall 181 | gb_free 7.9 | wall 19306
2022-05-13 05:09:56 | INFO | fairseq.data.iterators | grouped total_num_itrs = 39
2022-05-13 05:09:56 | INFO | fairseq.trainer | begin training epoch 99
2022-05-13 05:09:56 | INFO | fairseq_cli.train | Start iterating over samples
2022-05-13 05:13:04 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-05-13 05:13:08 | INFO | valid | epoch 099 | valid on 'valid' subset | loss 5.989 | ppl 63.53 | wps 76520.8 | wpb 2041.1 | bsz 4 | num_updates 3861 | best_loss 5.989
2022-05-13 05:13:08 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 99 @ 3861 updates
2022-05-13 05:13:08 | INFO | fairseq.trainer | Saving checkpoint to /cluster/work/cotterell/liam/master-thesis/checkpoints/language_model/standard/checkpoint_best_standard_64_1.0000E-04.pt
2022-05-13 05:13:10 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/work/cotterell/liam/master-thesis/checkpoints/language_model/standard/checkpoint_best_standard_64_1.0000E-04.pt
2022-05-13 05:13:10 | INFO | fairseq.checkpoint_utils | Saved checkpoint checkpoints/language_model/standard/checkpoint_best_standard_64_1.0000E-04.pt (epoch 99 @ 3861 updates, score 5.989) (writing took 1.2364065619185567 seconds)
2022-05-13 05:13:10 | INFO | fairseq_cli.train | end of epoch 99 (average epoch stats below)
2022-05-13 05:13:10 | INFO | train | epoch 099 | loss 5.709 | ppl 52.32 | wps 26369.3 | ups 0.2 | wpb 130930 | bsz 255.7 | num_updates 3861 | lr 9.65285e-05 | gnorm 0.932 | train_wall 179 | gb_free 7.9 | wall 19499
2022-05-13 05:13:10 | INFO | fairseq.data.iterators | grouped total_num_itrs = 39
2022-05-13 05:13:10 | INFO | fairseq.trainer | begin training epoch 100
2022-05-13 05:13:10 | INFO | fairseq_cli.train | Start iterating over samples
2022-05-13 05:16:17 | INFO | train_inner | epoch 100:     39 / 39 loss=5.705, ppl=52.16, wps=26668.8, ups=0.2, wpb=130916, bsz=255.7, num_updates=3900, lr=9.75025e-05, gnorm=0.887, train_wall=457, gb_free=7.9, wall=19686
2022-05-13 05:16:17 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-05-13 05:16:20 | INFO | valid | epoch 100 | valid on 'valid' subset | loss 5.972 | ppl 62.75 | wps 76649.7 | wpb 2041.1 | bsz 4 | num_updates 3900 | best_loss 5.972
2022-05-13 05:16:20 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 100 @ 3900 updates
2022-05-13 05:16:20 | INFO | fairseq.trainer | Saving checkpoint to /cluster/work/cotterell/liam/master-thesis/checkpoints/language_model/standard/checkpoint_best_standard_64_1.0000E-04.pt
2022-05-13 05:16:21 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/work/cotterell/liam/master-thesis/checkpoints/language_model/standard/checkpoint_best_standard_64_1.0000E-04.pt
2022-05-13 05:16:22 | INFO | fairseq.checkpoint_utils | Saved checkpoint checkpoints/language_model/standard/checkpoint_best_standard_64_1.0000E-04.pt (epoch 100 @ 3900 updates, score 5.972) (writing took 1.191790015436709 seconds)
2022-05-13 05:16:22 | INFO | fairseq_cli.train | end of epoch 100 (average epoch stats below)
2022-05-13 05:16:22 | INFO | train | epoch 100 | loss 5.683 | ppl 51.36 | wps 26609.9 | ups 0.2 | wpb 130930 | bsz 255.7 | num_updates 3900 | lr 9.75025e-05 | gnorm 0.886 | train_wall 178 | gb_free 7.9 | wall 19691
2022-05-13 05:16:22 | INFO | fairseq.data.iterators | grouped total_num_itrs = 39
2022-05-13 05:16:22 | INFO | fairseq.trainer | begin training epoch 101
2022-05-13 05:16:22 | INFO | fairseq_cli.train | Start iterating over samples
2022-05-13 05:19:32 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-05-13 05:19:36 | INFO | valid | epoch 101 | valid on 'valid' subset | loss 5.96 | ppl 62.24 | wps 76465.3 | wpb 2041.1 | bsz 4 | num_updates 3939 | best_loss 5.96
2022-05-13 05:19:36 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 101 @ 3939 updates
2022-05-13 05:19:36 | INFO | fairseq.trainer | Saving checkpoint to /cluster/work/cotterell/liam/master-thesis/checkpoints/language_model/standard/checkpoint_best_standard_64_1.0000E-04.pt
2022-05-13 05:19:37 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/work/cotterell/liam/master-thesis/checkpoints/language_model/standard/checkpoint_best_standard_64_1.0000E-04.pt
2022-05-13 05:19:37 | INFO | fairseq.checkpoint_utils | Saved checkpoint checkpoints/language_model/standard/checkpoint_best_standard_64_1.0000E-04.pt (epoch 101 @ 3939 updates, score 5.96) (writing took 1.1322349081747234 seconds)
2022-05-13 05:19:37 | INFO | fairseq_cli.train | end of epoch 101 (average epoch stats below)
2022-05-13 05:19:37 | INFO | train | epoch 101 | loss 5.658 | ppl 50.49 | wps 26078.9 | ups 0.2 | wpb 130930 | bsz 255.7 | num_updates 3939 | lr 9.84765e-05 | gnorm 0.894 | train_wall 180 | gb_free 7.9 | wall 19887
2022-05-13 05:19:37 | INFO | fairseq.data.iterators | grouped total_num_itrs = 39
2022-05-13 05:19:37 | INFO | fairseq.trainer | begin training epoch 102
2022-05-13 05:19:37 | INFO | fairseq_cli.train | Start iterating over samples
2022-05-13 05:22:48 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-05-13 05:22:52 | INFO | valid | epoch 102 | valid on 'valid' subset | loss 5.944 | ppl 61.56 | wps 76541.7 | wpb 2041.1 | bsz 4 | num_updates 3978 | best_loss 5.944
2022-05-13 05:22:52 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 102 @ 3978 updates
2022-05-13 05:22:52 | INFO | fairseq.trainer | Saving checkpoint to /cluster/work/cotterell/liam/master-thesis/checkpoints/language_model/standard/checkpoint_best_standard_64_1.0000E-04.pt
2022-05-13 05:22:53 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/work/cotterell/liam/master-thesis/checkpoints/language_model/standard/checkpoint_best_standard_64_1.0000E-04.pt
2022-05-13 05:22:53 | INFO | fairseq.checkpoint_utils | Saved checkpoint checkpoints/language_model/standard/checkpoint_best_standard_64_1.0000E-04.pt (epoch 102 @ 3978 updates, score 5.944) (writing took 1.251295241061598 seconds)
2022-05-13 05:22:53 | INFO | fairseq_cli.train | end of epoch 102 (average epoch stats below)
2022-05-13 05:22:53 | INFO | train | epoch 102 | loss 5.633 | ppl 49.63 | wps 26084.8 | ups 0.2 | wpb 130930 | bsz 255.7 | num_updates 3978 | lr 9.94506e-05 | gnorm 0.848 | train_wall 181 | gb_free 7.9 | wall 20083
2022-05-13 05:22:53 | INFO | fairseq.data.iterators | grouped total_num_itrs = 39
2022-05-13 05:22:53 | INFO | fairseq.trainer | begin training epoch 103
2022-05-13 05:22:53 | INFO | fairseq_cli.train | Start iterating over samples
2022-05-13 05:24:40 | INFO | train_inner | epoch 103:     22 / 39 loss=5.638, ppl=49.79, wps=26034, ups=0.2, wpb=130961, bsz=255.8, num_updates=4000, lr=0.0001, gnorm=0.881, train_wall=462, gb_free=7.9, wall=20189
2022-05-13 05:26:01 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-05-13 05:26:05 | INFO | valid | epoch 103 | valid on 'valid' subset | loss 5.931 | ppl 61.02 | wps 76561.3 | wpb 2041.1 | bsz 4 | num_updates 4017 | best_loss 5.931
2022-05-13 05:26:05 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 103 @ 4017 updates
2022-05-13 05:26:05 | INFO | fairseq.trainer | Saving checkpoint to /cluster/work/cotterell/liam/master-thesis/checkpoints/language_model/standard/checkpoint_best_standard_64_1.0000E-04.pt
2022-05-13 05:26:06 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/work/cotterell/liam/master-thesis/checkpoints/language_model/standard/checkpoint_best_standard_64_1.0000E-04.pt
2022-05-13 05:26:06 | INFO | fairseq.checkpoint_utils | Saved checkpoint checkpoints/language_model/standard/checkpoint_best_standard_64_1.0000E-04.pt (epoch 103 @ 4017 updates, score 5.931) (writing took 1.2317645768634975 seconds)
2022-05-13 05:26:06 | INFO | fairseq_cli.train | end of epoch 103 (average epoch stats below)
2022-05-13 05:26:06 | INFO | train | epoch 103 | loss 5.609 | ppl 48.81 | wps 26472.9 | ups 0.2 | wpb 130930 | bsz 255.7 | num_updates 4017 | lr 9.97882e-05 | gnorm 0.886 | train_wall 179 | gb_free 7.9 | wall 20276
2022-05-13 05:26:06 | INFO | fairseq.data.iterators | grouped total_num_itrs = 39
2022-05-13 05:26:06 | INFO | fairseq.trainer | begin training epoch 104
2022-05-13 05:26:06 | INFO | fairseq_cli.train | Start iterating over samples
2022-05-13 05:29:13 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-05-13 05:29:17 | INFO | valid | epoch 104 | valid on 'valid' subset | loss 5.919 | ppl 60.5 | wps 76377.1 | wpb 2041.1 | bsz 4 | num_updates 4056 | best_loss 5.919
2022-05-13 05:29:17 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 104 @ 4056 updates
2022-05-13 05:29:17 | INFO | fairseq.trainer | Saving checkpoint to /cluster/work/cotterell/liam/master-thesis/checkpoints/language_model/standard/checkpoint_best_standard_64_1.0000E-04.pt
2022-05-13 05:29:18 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/work/cotterell/liam/master-thesis/checkpoints/language_model/standard/checkpoint_best_standard_64_1.0000E-04.pt
2022-05-13 05:29:18 | INFO | fairseq.checkpoint_utils | Saved checkpoint checkpoints/language_model/standard/checkpoint_best_standard_64_1.0000E-04.pt (epoch 104 @ 4056 updates, score 5.919) (writing took 1.2113948171027005 seconds)
2022-05-13 05:29:18 | INFO | fairseq_cli.train | end of epoch 104 (average epoch stats below)
2022-05-13 05:29:18 | INFO | train | epoch 104 | loss 5.584 | ppl 47.98 | wps 26632.5 | ups 0.2 | wpb 130930 | bsz 255.7 | num_updates 4056 | lr 9.93073e-05 | gnorm 0.871 | train_wall 178 | gb_free 7.9 | wall 20468
2022-05-13 05:29:18 | INFO | fairseq.data.iterators | grouped total_num_itrs = 39
2022-05-13 05:29:18 | INFO | fairseq.trainer | begin training epoch 105
2022-05-13 05:29:18 | INFO | fairseq_cli.train | Start iterating over samples
2022-05-13 05:32:32 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-05-13 05:32:36 | INFO | valid | epoch 105 | valid on 'valid' subset | loss 5.906 | ppl 59.96 | wps 76694.7 | wpb 2041.1 | bsz 4 | num_updates 4095 | best_loss 5.906
2022-05-13 05:32:36 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 105 @ 4095 updates
2022-05-13 05:32:36 | INFO | fairseq.trainer | Saving checkpoint to /cluster/work/cotterell/liam/master-thesis/checkpoints/language_model/standard/checkpoint_best_standard_64_1.0000E-04.pt
2022-05-13 05:32:37 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/work/cotterell/liam/master-thesis/checkpoints/language_model/standard/checkpoint_best_standard_64_1.0000E-04.pt
2022-05-13 05:32:37 | INFO | fairseq.checkpoint_utils | Saved checkpoint checkpoints/language_model/standard/checkpoint_best_standard_64_1.0000E-04.pt (epoch 105 @ 4095 updates, score 5.906) (writing took 1.3050351678393781 seconds)
2022-05-13 05:32:37 | INFO | fairseq_cli.train | end of epoch 105 (average epoch stats below)
2022-05-13 05:32:37 | INFO | train | epoch 105 | loss 5.561 | ppl 47.22 | wps 25633.9 | ups 0.2 | wpb 130930 | bsz 255.7 | num_updates 4095 | lr 9.88332e-05 | gnorm 0.885 | train_wall 182 | gb_free 7.9 | wall 20667
2022-05-13 05:32:37 | INFO | fairseq.data.iterators | grouped total_num_itrs = 39
2022-05-13 05:32:37 | INFO | fairseq.trainer | begin training epoch 106
2022-05-13 05:32:37 | INFO | fairseq_cli.train | Start iterating over samples
2022-05-13 05:33:01 | INFO | train_inner | epoch 106:      5 / 39 loss=5.577, ppl=47.73, wps=26111.2, ups=0.2, wpb=130905, bsz=255.7, num_updates=4100, lr=9.8773e-05, gnorm=0.866, train_wall=461, gb_free=7.9, wall=20691
2022-05-13 05:35:44 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-05-13 05:35:48 | INFO | valid | epoch 106 | valid on 'valid' subset | loss 5.896 | ppl 59.57 | wps 76503.2 | wpb 2041.1 | bsz 4 | num_updates 4134 | best_loss 5.896
2022-05-13 05:35:48 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 106 @ 4134 updates
2022-05-13 05:35:48 | INFO | fairseq.trainer | Saving checkpoint to /cluster/work/cotterell/liam/master-thesis/checkpoints/language_model/standard/checkpoint_best_standard_64_1.0000E-04.pt
2022-05-13 05:35:49 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/work/cotterell/liam/master-thesis/checkpoints/language_model/standard/checkpoint_best_standard_64_1.0000E-04.pt
2022-05-13 05:35:49 | INFO | fairseq.checkpoint_utils | Saved checkpoint checkpoints/language_model/standard/checkpoint_best_standard_64_1.0000E-04.pt (epoch 106 @ 4134 updates, score 5.896) (writing took 1.1369576631113887 seconds)
2022-05-13 05:35:49 | INFO | fairseq_cli.train | end of epoch 106 (average epoch stats below)
2022-05-13 05:35:49 | INFO | train | epoch 106 | loss 5.536 | ppl 46.39 | wps 26539.5 | ups 0.2 | wpb 130930 | bsz 255.7 | num_updates 4134 | lr 9.83659e-05 | gnorm 0.794 | train_wall 178 | gb_free 7.9 | wall 20859
2022-05-13 05:35:49 | INFO | fairseq.data.iterators | grouped total_num_itrs = 39
2022-05-13 05:35:49 | INFO | fairseq.trainer | begin training epoch 107
2022-05-13 05:35:49 | INFO | fairseq_cli.train | Start iterating over samples
2022-05-13 05:38:57 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-05-13 05:39:01 | INFO | valid | epoch 107 | valid on 'valid' subset | loss 5.878 | ppl 58.82 | wps 76501.5 | wpb 2041.1 | bsz 4 | num_updates 4173 | best_loss 5.878
2022-05-13 05:39:01 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 107 @ 4173 updates
2022-05-13 05:39:01 | INFO | fairseq.trainer | Saving checkpoint to /cluster/work/cotterell/liam/master-thesis/checkpoints/language_model/standard/checkpoint_best_standard_64_1.0000E-04.pt
2022-05-13 05:39:02 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/work/cotterell/liam/master-thesis/checkpoints/language_model/standard/checkpoint_best_standard_64_1.0000E-04.pt
2022-05-13 05:39:02 | INFO | fairseq.checkpoint_utils | Saved checkpoint checkpoints/language_model/standard/checkpoint_best_standard_64_1.0000E-04.pt (epoch 107 @ 4173 updates, score 5.878) (writing took 1.2105302112177014 seconds)
2022-05-13 05:39:02 | INFO | fairseq_cli.train | end of epoch 107 (average epoch stats below)
2022-05-13 05:39:02 | INFO | train | epoch 107 | loss 5.516 | ppl 45.75 | wps 26472.3 | ups 0.2 | wpb 130930 | bsz 255.7 | num_updates 4173 | lr 9.79052e-05 | gnorm 0.922 | train_wall 179 | gb_free 7.9 | wall 21052
2022-05-13 05:39:02 | INFO | fairseq.data.iterators | grouped total_num_itrs = 39
2022-05-13 05:39:02 | INFO | fairseq.trainer | begin training epoch 108
2022-05-13 05:39:02 | INFO | fairseq_cli.train | Start iterating over samples
2022-05-13 05:41:12 | INFO | train_inner | epoch 108:     27 / 39 loss=5.516, ppl=45.75, wps=26681.8, ups=0.2, wpb=130961, bsz=255.8, num_updates=4200, lr=9.759e-05, gnorm=0.846, train_wall=457, gb_free=7.9, wall=21182
2022-05-13 05:42:09 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-05-13 05:42:13 | INFO | valid | epoch 108 | valid on 'valid' subset | loss 5.867 | ppl 58.36 | wps 76531.9 | wpb 2041.1 | bsz 4 | num_updates 4212 | best_loss 5.867
2022-05-13 05:42:13 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 108 @ 4212 updates
2022-05-13 05:42:13 | INFO | fairseq.trainer | Saving checkpoint to /cluster/work/cotterell/liam/master-thesis/checkpoints/language_model/standard/checkpoint_best_standard_64_1.0000E-04.pt
2022-05-13 05:42:14 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/work/cotterell/liam/master-thesis/checkpoints/language_model/standard/checkpoint_best_standard_64_1.0000E-04.pt
2022-05-13 05:42:14 | INFO | fairseq.checkpoint_utils | Saved checkpoint checkpoints/language_model/standard/checkpoint_best_standard_64_1.0000E-04.pt (epoch 108 @ 4212 updates, score 5.867) (writing took 1.1464081578888 seconds)
2022-05-13 05:42:14 | INFO | fairseq_cli.train | end of epoch 108 (average epoch stats below)
2022-05-13 05:42:14 | INFO | train | epoch 108 | loss 5.491 | ppl 44.97 | wps 26617.4 | ups 0.2 | wpb 130930 | bsz 255.7 | num_updates 4212 | lr 9.74509e-05 | gnorm 0.849 | train_wall 178 | gb_free 7.9 | wall 21244
2022-05-13 05:42:14 | INFO | fairseq.data.iterators | grouped total_num_itrs = 39
2022-05-13 05:42:14 | INFO | fairseq.trainer | begin training epoch 109
2022-05-13 05:42:14 | INFO | fairseq_cli.train | Start iterating over samples
2022-05-13 05:45:25 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-05-13 05:45:29 | INFO | valid | epoch 109 | valid on 'valid' subset | loss 5.858 | ppl 58 | wps 76527.8 | wpb 2041.1 | bsz 4 | num_updates 4251 | best_loss 5.858
2022-05-13 05:45:29 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 109 @ 4251 updates
2022-05-13 05:45:29 | INFO | fairseq.trainer | Saving checkpoint to /cluster/work/cotterell/liam/master-thesis/checkpoints/language_model/standard/checkpoint_best_standard_64_1.0000E-04.pt
2022-05-13 05:45:31 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/work/cotterell/liam/master-thesis/checkpoints/language_model/standard/checkpoint_best_standard_64_1.0000E-04.pt
2022-05-13 05:45:31 | INFO | fairseq.checkpoint_utils | Saved checkpoint checkpoints/language_model/standard/checkpoint_best_standard_64_1.0000E-04.pt (epoch 109 @ 4251 updates, score 5.858) (writing took 1.1537545933388174 seconds)
2022-05-13 05:45:31 | INFO | fairseq_cli.train | end of epoch 109 (average epoch stats below)
2022-05-13 05:45:31 | INFO | train | epoch 109 | loss 5.469 | ppl 44.28 | wps 25972.6 | ups 0.2 | wpb 130930 | bsz 255.7 | num_updates 4251 | lr 9.70028e-05 | gnorm 0.826 | train_wall 180 | gb_free 7.9 | wall 21440
2022-05-13 05:45:31 | INFO | fairseq.data.iterators | grouped total_num_itrs = 39
2022-05-13 05:45:31 | INFO | fairseq.trainer | begin training epoch 110
2022-05-13 05:45:31 | INFO | fairseq_cli.train | Start iterating over samples
2022-05-13 05:48:40 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-05-13 05:48:44 | INFO | valid | epoch 110 | valid on 'valid' subset | loss 5.845 | ppl 57.47 | wps 76563.1 | wpb 2041.1 | bsz 4 | num_updates 4290 | best_loss 5.845
2022-05-13 05:48:44 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 110 @ 4290 updates
2022-05-13 05:48:44 | INFO | fairseq.trainer | Saving checkpoint to /cluster/work/cotterell/liam/master-thesis/checkpoints/language_model/standard/checkpoint_best_standard_64_1.0000E-04.pt
2022-05-13 05:48:45 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/work/cotterell/liam/master-thesis/checkpoints/language_model/standard/checkpoint_best_standard_64_1.0000E-04.pt
2022-05-13 05:48:45 | INFO | fairseq.checkpoint_utils | Saved checkpoint checkpoints/language_model/standard/checkpoint_best_standard_64_1.0000E-04.pt (epoch 110 @ 4290 updates, score 5.845) (writing took 1.233879059087485 seconds)
2022-05-13 05:48:45 | INFO | fairseq_cli.train | end of epoch 110 (average epoch stats below)
2022-05-13 05:48:45 | INFO | train | epoch 110 | loss 5.446 | ppl 43.59 | wps 26256.3 | ups 0.2 | wpb 130930 | bsz 255.7 | num_updates 4290 | lr 9.65609e-05 | gnorm 0.815 | train_wall 180 | gb_free 7.9 | wall 21635
2022-05-13 05:48:45 | INFO | fairseq.data.iterators | grouped total_num_itrs = 39
2022-05-13 05:48:45 | INFO | fairseq.trainer | begin training epoch 111
2022-05-13 05:48:45 | INFO | fairseq_cli.train | Start iterating over samples
2022-05-13 05:49:37 | INFO | train_inner | epoch 111:     10 / 39 loss=5.459, ppl=43.97, wps=25912.1, ups=0.2, wpb=130895, bsz=255.7, num_updates=4300, lr=9.64486e-05, gnorm=0.853, train_wall=464, gb_free=7.9, wall=21687
2022-05-13 05:52:02 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-05-13 05:52:06 | INFO | valid | epoch 111 | valid on 'valid' subset | loss 5.832 | ppl 56.95 | wps 76556.1 | wpb 2041.1 | bsz 4 | num_updates 4329 | best_loss 5.832
2022-05-13 05:52:06 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 111 @ 4329 updates
2022-05-13 05:52:06 | INFO | fairseq.trainer | Saving checkpoint to /cluster/work/cotterell/liam/master-thesis/checkpoints/language_model/standard/checkpoint_best_standard_64_1.0000E-04.pt
2022-05-13 05:52:07 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/work/cotterell/liam/master-thesis/checkpoints/language_model/standard/checkpoint_best_standard_64_1.0000E-04.pt
2022-05-13 05:52:07 | INFO | fairseq.checkpoint_utils | Saved checkpoint checkpoints/language_model/standard/checkpoint_best_standard_64_1.0000E-04.pt (epoch 111 @ 4329 updates, score 5.832) (writing took 1.1636517099104822 seconds)
2022-05-13 05:52:07 | INFO | fairseq_cli.train | end of epoch 111 (average epoch stats below)
2022-05-13 05:52:07 | INFO | train | epoch 111 | loss 5.427 | ppl 43.03 | wps 25292.3 | ups 0.19 | wpb 130930 | bsz 255.7 | num_updates 4329 | lr 9.6125e-05 | gnorm 0.925 | train_wall 185 | gb_free 7.9 | wall 21837
2022-05-13 05:52:07 | INFO | fairseq.data.iterators | grouped total_num_itrs = 39
2022-05-13 05:52:07 | INFO | fairseq.trainer | begin training epoch 112
2022-05-13 05:52:07 | INFO | fairseq_cli.train | Start iterating over samples
2022-05-13 05:55:14 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-05-13 05:55:18 | INFO | valid | epoch 112 | valid on 'valid' subset | loss 5.832 | ppl 56.95 | wps 76716.7 | wpb 2041.1 | bsz 4 | num_updates 4368 | best_loss 5.832
2022-05-13 05:55:18 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 112 @ 4368 updates
2022-05-13 05:55:18 | INFO | fairseq.trainer | Saving checkpoint to /cluster/work/cotterell/liam/master-thesis/checkpoints/language_model/standard/checkpoint_best_standard_64_1.0000E-04.pt
2022-05-13 05:55:19 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/work/cotterell/liam/master-thesis/checkpoints/language_model/standard/checkpoint_best_standard_64_1.0000E-04.pt
2022-05-13 05:55:19 | INFO | fairseq.checkpoint_utils | Saved checkpoint checkpoints/language_model/standard/checkpoint_best_standard_64_1.0000E-04.pt (epoch 112 @ 4368 updates, score 5.832) (writing took 1.229549144860357 seconds)
2022-05-13 05:55:19 | INFO | fairseq_cli.train | end of epoch 112 (average epoch stats below)
2022-05-13 05:55:19 | INFO | train | epoch 112 | loss 5.404 | ppl 42.35 | wps 26554.6 | ups 0.2 | wpb 130930 | bsz 255.7 | num_updates 4368 | lr 9.56949e-05 | gnorm 0.807 | train_wall 178 | gb_free 7.9 | wall 22029
2022-05-13 05:55:19 | INFO | fairseq.data.iterators | grouped total_num_itrs = 39
2022-05-13 05:55:19 | INFO | fairseq.trainer | begin training epoch 113
2022-05-13 05:55:19 | INFO | fairseq_cli.train | Start iterating over samples
2022-05-13 05:57:53 | INFO | train_inner | epoch 113:     32 / 39 loss=5.404, ppl=42.34, wps=26389.2, ups=0.2, wpb=130961, bsz=255.8, num_updates=4400, lr=9.53463e-05, gnorm=0.851, train_wall=460, gb_free=7.9, wall=22183
2022-05-13 05:58:31 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-05-13 05:58:35 | INFO | valid | epoch 113 | valid on 'valid' subset | loss 5.82 | ppl 56.48 | wps 76688.5 | wpb 2041.1 | bsz 4 | num_updates 4407 | best_loss 5.82
2022-05-13 05:58:35 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 113 @ 4407 updates
2022-05-13 05:58:35 | INFO | fairseq.trainer | Saving checkpoint to /cluster/work/cotterell/liam/master-thesis/checkpoints/language_model/standard/checkpoint_best_standard_64_1.0000E-04.pt
2022-05-13 05:58:36 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/work/cotterell/liam/master-thesis/checkpoints/language_model/standard/checkpoint_best_standard_64_1.0000E-04.pt
2022-05-13 05:58:36 | INFO | fairseq.checkpoint_utils | Saved checkpoint checkpoints/language_model/standard/checkpoint_best_standard_64_1.0000E-04.pt (epoch 113 @ 4407 updates, score 5.82) (writing took 1.1732723386958241 seconds)
2022-05-13 05:58:36 | INFO | fairseq_cli.train | end of epoch 113 (average epoch stats below)
2022-05-13 05:58:36 | INFO | train | epoch 113 | loss 5.385 | ppl 41.78 | wps 25987.2 | ups 0.2 | wpb 130930 | bsz 255.7 | num_updates 4407 | lr 9.52705e-05 | gnorm 0.832 | train_wall 181 | gb_free 7.9 | wall 22226
2022-05-13 05:58:36 | INFO | fairseq.data.iterators | grouped total_num_itrs = 39
2022-05-13 05:58:36 | INFO | fairseq.trainer | begin training epoch 114
2022-05-13 05:58:36 | INFO | fairseq_cli.train | Start iterating over samples
2022-05-13 06:01:46 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-05-13 06:01:50 | INFO | valid | epoch 114 | valid on 'valid' subset | loss 5.805 | ppl 55.92 | wps 76537.5 | wpb 2041.1 | bsz 4 | num_updates 4446 | best_loss 5.805
2022-05-13 06:01:50 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 114 @ 4446 updates
2022-05-13 06:01:50 | INFO | fairseq.trainer | Saving checkpoint to /cluster/work/cotterell/liam/master-thesis/checkpoints/language_model/standard/checkpoint_best_standard_64_1.0000E-04.pt
2022-05-13 06:01:51 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/work/cotterell/liam/master-thesis/checkpoints/language_model/standard/checkpoint_best_standard_64_1.0000E-04.pt
2022-05-13 06:01:51 | INFO | fairseq.checkpoint_utils | Saved checkpoint checkpoints/language_model/standard/checkpoint_best_standard_64_1.0000E-04.pt (epoch 114 @ 4446 updates, score 5.805) (writing took 1.0714934803545475 seconds)
2022-05-13 06:01:51 | INFO | fairseq_cli.train | end of epoch 114 (average epoch stats below)
2022-05-13 06:01:51 | INFO | train | epoch 114 | loss 5.365 | ppl 41.22 | wps 26192.7 | ups 0.2 | wpb 130930 | bsz 255.7 | num_updates 4446 | lr 9.48517e-05 | gnorm 0.872 | train_wall 180 | gb_free 7.9 | wall 22421
2022-05-13 06:01:51 | INFO | fairseq.data.iterators | grouped total_num_itrs = 39
2022-05-13 06:01:51 | INFO | fairseq.trainer | begin training epoch 115
2022-05-13 06:01:51 | INFO | fairseq_cli.train | Start iterating over samples
2022-05-13 06:04:59 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-05-13 06:05:03 | INFO | valid | epoch 115 | valid on 'valid' subset | loss 5.801 | ppl 55.75 | wps 76452 | wpb 2041.1 | bsz 4 | num_updates 4485 | best_loss 5.801
2022-05-13 06:05:03 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 115 @ 4485 updates
2022-05-13 06:05:03 | INFO | fairseq.trainer | Saving checkpoint to /cluster/work/cotterell/liam/master-thesis/checkpoints/language_model/standard/checkpoint_best_standard_64_1.0000E-04.pt
2022-05-13 06:05:04 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/work/cotterell/liam/master-thesis/checkpoints/language_model/standard/checkpoint_best_standard_64_1.0000E-04.pt
2022-05-13 06:05:04 | INFO | fairseq.checkpoint_utils | Saved checkpoint checkpoints/language_model/standard/checkpoint_best_standard_64_1.0000E-04.pt (epoch 115 @ 4485 updates, score 5.801) (writing took 1.2762601668946445 seconds)
2022-05-13 06:05:04 | INFO | fairseq_cli.train | end of epoch 115 (average epoch stats below)
2022-05-13 06:05:04 | INFO | train | epoch 115 | loss 5.344 | ppl 40.62 | wps 26379.5 | ups 0.2 | wpb 130930 | bsz 255.7 | num_updates 4485 | lr 9.44384e-05 | gnorm 0.848 | train_wall 179 | gb_free 7.9 | wall 22614
2022-05-13 06:05:04 | INFO | fairseq.data.iterators | grouped total_num_itrs = 39
2022-05-13 06:05:04 | INFO | fairseq.trainer | begin training epoch 116
2022-05-13 06:05:04 | INFO | fairseq_cli.train | Start iterating over samples
2022-05-13 06:06:16 | INFO | train_inner | epoch 116:     15 / 39 loss=5.352, ppl=40.83, wps=26020.1, ups=0.2, wpb=130916, bsz=255.7, num_updates=4500, lr=9.42809e-05, gnorm=0.854, train_wall=462, gb_free=7.9, wall=22686
2022-05-13 06:08:12 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-05-13 06:08:16 | INFO | valid | epoch 116 | valid on 'valid' subset | loss 5.786 | ppl 55.18 | wps 76631.6 | wpb 2041.1 | bsz 4 | num_updates 4524 | best_loss 5.786
2022-05-13 06:08:16 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 116 @ 4524 updates
2022-05-13 06:08:16 | INFO | fairseq.trainer | Saving checkpoint to /cluster/work/cotterell/liam/master-thesis/checkpoints/language_model/standard/checkpoint_best_standard_64_1.0000E-04.pt
2022-05-13 06:08:17 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/work/cotterell/liam/master-thesis/checkpoints/language_model/standard/checkpoint_best_standard_64_1.0000E-04.pt
2022-05-13 06:08:17 | INFO | fairseq.checkpoint_utils | Saved checkpoint checkpoints/language_model/standard/checkpoint_best_standard_64_1.0000E-04.pt (epoch 116 @ 4524 updates, score 5.786) (writing took 1.0449526319280267 seconds)
2022-05-13 06:08:17 | INFO | fairseq_cli.train | end of epoch 116 (average epoch stats below)
2022-05-13 06:08:17 | INFO | train | epoch 116 | loss 5.324 | ppl 40.06 | wps 26533.3 | ups 0.2 | wpb 130930 | bsz 255.7 | num_updates 4524 | lr 9.40305e-05 | gnorm 0.797 | train_wall 178 | gb_free 7.9 | wall 22807
2022-05-13 06:08:17 | INFO | fairseq.data.iterators | grouped total_num_itrs = 39
2022-05-13 06:08:17 | INFO | fairseq.trainer | begin training epoch 117
2022-05-13 06:08:17 | INFO | fairseq_cli.train | Start iterating over samples
2022-05-13 06:11:29 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-05-13 06:11:32 | INFO | valid | epoch 117 | valid on 'valid' subset | loss 5.78 | ppl 54.96 | wps 75931.9 | wpb 2041.1 | bsz 4 | num_updates 4563 | best_loss 5.78
2022-05-13 06:11:32 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 117 @ 4563 updates
2022-05-13 06:11:32 | INFO | fairseq.trainer | Saving checkpoint to /cluster/work/cotterell/liam/master-thesis/checkpoints/language_model/standard/checkpoint_best_standard_64_1.0000E-04.pt
2022-05-13 06:11:34 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/work/cotterell/liam/master-thesis/checkpoints/language_model/standard/checkpoint_best_standard_64_1.0000E-04.pt
2022-05-13 06:11:34 | INFO | fairseq.checkpoint_utils | Saved checkpoint checkpoints/language_model/standard/checkpoint_best_standard_64_1.0000E-04.pt (epoch 117 @ 4563 updates, score 5.78) (writing took 1.0986802279949188 seconds)
2022-05-13 06:11:34 | INFO | fairseq_cli.train | end of epoch 117 (average epoch stats below)
2022-05-13 06:11:34 | INFO | train | epoch 117 | loss 5.304 | ppl 39.51 | wps 25948.3 | ups 0.2 | wpb 130930 | bsz 255.7 | num_updates 4563 | lr 9.36278e-05 | gnorm 0.796 | train_wall 180 | gb_free 7.9 | wall 23003
2022-05-13 06:11:34 | INFO | fairseq.data.iterators | grouped total_num_itrs = 39
2022-05-13 06:11:34 | INFO | fairseq.trainer | begin training epoch 118
2022-05-13 06:11:34 | INFO | fairseq_cli.train | Start iterating over samples
2022-05-13 06:14:33 | INFO | train_inner | epoch 118:     37 / 39 loss=5.303, ppl=39.49, wps=26385, ups=0.2, wpb=130946, bsz=255.8, num_updates=4600, lr=9.32505e-05, gnorm=0.8, train_wall=459, gb_free=7.9, wall=23182
2022-05-13 06:14:42 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-05-13 06:14:46 | INFO | valid | epoch 118 | valid on 'valid' subset | loss 5.77 | ppl 54.55 | wps 76564.9 | wpb 2041.1 | bsz 4 | num_updates 4602 | best_loss 5.77
2022-05-13 06:14:46 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 118 @ 4602 updates
2022-05-13 06:14:46 | INFO | fairseq.trainer | Saving checkpoint to /cluster/work/cotterell/liam/master-thesis/checkpoints/language_model/standard/checkpoint_best_standard_64_1.0000E-04.pt
2022-05-13 06:14:47 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/work/cotterell/liam/master-thesis/checkpoints/language_model/standard/checkpoint_best_standard_64_1.0000E-04.pt
2022-05-13 06:14:47 | INFO | fairseq.checkpoint_utils | Saved checkpoint checkpoints/language_model/standard/checkpoint_best_standard_64_1.0000E-04.pt (epoch 118 @ 4602 updates, score 5.77) (writing took 1.3059823070652783 seconds)
2022-05-13 06:14:47 | INFO | fairseq_cli.train | end of epoch 118 (average epoch stats below)
2022-05-13 06:14:47 | INFO | train | epoch 118 | loss 5.287 | ppl 39.03 | wps 26364.8 | ups 0.2 | wpb 130930 | bsz 255.7 | num_updates 4602 | lr 9.32302e-05 | gnorm 0.835 | train_wall 179 | gb_free 7.9 | wall 23197
2022-05-13 06:14:47 | INFO | fairseq.data.iterators | grouped total_num_itrs = 39
2022-05-13 06:14:47 | INFO | fairseq.trainer | begin training epoch 119
2022-05-13 06:14:47 | INFO | fairseq_cli.train | Start iterating over samples
2022-05-13 06:17:54 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-05-13 06:17:58 | INFO | valid | epoch 119 | valid on 'valid' subset | loss 5.766 | ppl 54.41 | wps 76438 | wpb 2041.1 | bsz 4 | num_updates 4641 | best_loss 5.766
2022-05-13 06:17:58 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 119 @ 4641 updates
2022-05-13 06:17:58 | INFO | fairseq.trainer | Saving checkpoint to /cluster/work/cotterell/liam/master-thesis/checkpoints/language_model/standard/checkpoint_best_standard_64_1.0000E-04.pt
2022-05-13 06:17:59 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/work/cotterell/liam/master-thesis/checkpoints/language_model/standard/checkpoint_best_standard_64_1.0000E-04.pt
2022-05-13 06:17:59 | INFO | fairseq.checkpoint_utils | Saved checkpoint checkpoints/language_model/standard/checkpoint_best_standard_64_1.0000E-04.pt (epoch 119 @ 4641 updates, score 5.766) (writing took 1.1617619479075074 seconds)
2022-05-13 06:17:59 | INFO | fairseq_cli.train | end of epoch 119 (average epoch stats below)
2022-05-13 06:17:59 | INFO | train | epoch 119 | loss 5.267 | ppl 38.51 | wps 26635.9 | ups 0.2 | wpb 130930 | bsz 255.7 | num_updates 4641 | lr 9.28377e-05 | gnorm 0.827 | train_wall 178 | gb_free 7.9 | wall 23389
2022-05-13 06:17:59 | INFO | fairseq.data.iterators | grouped total_num_itrs = 39
2022-05-13 06:17:59 | INFO | fairseq.trainer | begin training epoch 120
2022-05-13 06:17:59 | INFO | fairseq_cli.train | Start iterating over samples
2022-05-13 06:21:13 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-05-13 06:21:17 | INFO | valid | epoch 120 | valid on 'valid' subset | loss 5.759 | ppl 54.17 | wps 76631 | wpb 2041.1 | bsz 4 | num_updates 4680 | best_loss 5.759
2022-05-13 06:21:17 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 120 @ 4680 updates
2022-05-13 06:21:17 | INFO | fairseq.trainer | Saving checkpoint to /cluster/work/cotterell/liam/master-thesis/checkpoints/language_model/standard/checkpoint_best_standard_64_1.0000E-04.pt
2022-05-13 06:21:18 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/work/cotterell/liam/master-thesis/checkpoints/language_model/standard/checkpoint_best_standard_64_1.0000E-04.pt
2022-05-13 06:21:18 | INFO | fairseq.checkpoint_utils | Saved checkpoint checkpoints/language_model/standard/checkpoint_best_standard_64_1.0000E-04.pt (epoch 120 @ 4680 updates, score 5.759) (writing took 1.3607250032946467 seconds)
2022-05-13 06:21:18 | INFO | fairseq_cli.train | end of epoch 120 (average epoch stats below)
2022-05-13 06:21:18 | INFO | train | epoch 120 | loss 5.248 | ppl 38.01 | wps 25618.6 | ups 0.2 | wpb 130930 | bsz 255.7 | num_updates 4680 | lr 9.245e-05 | gnorm 0.813 | train_wall 182 | gb_free 7.9 | wall 23588
2022-05-13 06:21:18 | INFO | fairseq.data.iterators | grouped total_num_itrs = 39
2022-05-13 06:21:18 | INFO | fairseq.trainer | begin training epoch 121
2022-05-13 06:21:18 | INFO | fairseq_cli.train | Start iterating over samples
2022-05-13 06:22:54 | INFO | train_inner | epoch 121:     20 / 39 loss=5.253, ppl=38.13, wps=26095.2, ups=0.2, wpb=130910, bsz=255.7, num_updates=4700, lr=9.22531e-05, gnorm=0.818, train_wall=461, gb_free=7.9, wall=23684
2022-05-13 06:24:26 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-05-13 06:24:29 | INFO | valid | epoch 121 | valid on 'valid' subset | loss 5.75 | ppl 53.81 | wps 76535.1 | wpb 2041.1 | bsz 4 | num_updates 4719 | best_loss 5.75
2022-05-13 06:24:29 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 121 @ 4719 updates
2022-05-13 06:24:29 | INFO | fairseq.trainer | Saving checkpoint to /cluster/work/cotterell/liam/master-thesis/checkpoints/language_model/standard/checkpoint_best_standard_64_1.0000E-04.pt
2022-05-13 06:24:31 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/work/cotterell/liam/master-thesis/checkpoints/language_model/standard/checkpoint_best_standard_64_1.0000E-04.pt
2022-05-13 06:24:31 | INFO | fairseq.checkpoint_utils | Saved checkpoint checkpoints/language_model/standard/checkpoint_best_standard_64_1.0000E-04.pt (epoch 121 @ 4719 updates, score 5.75) (writing took 1.6188294561579823 seconds)
2022-05-13 06:24:31 | INFO | fairseq_cli.train | end of epoch 121 (average epoch stats below)
2022-05-13 06:24:32 | INFO | train | epoch 121 | loss 5.231 | ppl 37.56 | wps 26383.9 | ups 0.2 | wpb 130930 | bsz 255.7 | num_updates 4719 | lr 9.20672e-05 | gnorm 0.818 | train_wall 178 | gb_free 7.9 | wall 23782
2022-05-13 06:24:33 | INFO | fairseq.data.iterators | grouped total_num_itrs = 39
2022-05-13 06:24:33 | INFO | fairseq.trainer | begin training epoch 122
2022-05-13 06:24:33 | INFO | fairseq_cli.train | Start iterating over samples
2022-05-13 06:27:41 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-05-13 06:27:45 | INFO | valid | epoch 122 | valid on 'valid' subset | loss 5.744 | ppl 53.58 | wps 76631.4 | wpb 2041.1 | bsz 4 | num_updates 4758 | best_loss 5.744
2022-05-13 06:27:45 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 122 @ 4758 updates
2022-05-13 06:27:45 | INFO | fairseq.trainer | Saving checkpoint to /cluster/work/cotterell/liam/master-thesis/checkpoints/language_model/standard/checkpoint_best_standard_64_1.0000E-04.pt
2022-05-13 06:27:46 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/work/cotterell/liam/master-thesis/checkpoints/language_model/standard/checkpoint_best_standard_64_1.0000E-04.pt
2022-05-13 06:27:46 | INFO | fairseq.checkpoint_utils | Saved checkpoint checkpoints/language_model/standard/checkpoint_best_standard_64_1.0000E-04.pt (epoch 122 @ 4758 updates, score 5.744) (writing took 1.169790812768042 seconds)
2022-05-13 06:27:46 | INFO | fairseq_cli.train | end of epoch 122 (average epoch stats below)
2022-05-13 06:27:46 | INFO | train | epoch 122 | loss 5.213 | ppl 37.09 | wps 26271.1 | ups 0.2 | wpb 130930 | bsz 255.7 | num_updates 4758 | lr 9.16891e-05 | gnorm 0.841 | train_wall 179 | gb_free 7.9 | wall 23976
2022-05-13 06:27:46 | INFO | fairseq.data.iterators | grouped total_num_itrs = 39
2022-05-13 06:27:46 | INFO | fairseq.trainer | begin training epoch 123
2022-05-13 06:27:46 | INFO | fairseq_cli.train | Start iterating over samples
2022-05-13 06:30:53 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-05-13 06:30:57 | INFO | valid | epoch 123 | valid on 'valid' subset | loss 5.735 | ppl 53.27 | wps 76429.3 | wpb 2041.1 | bsz 4 | num_updates 4797 | best_loss 5.735
2022-05-13 06:30:57 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 123 @ 4797 updates
2022-05-13 06:30:57 | INFO | fairseq.trainer | Saving checkpoint to /cluster/work/cotterell/liam/master-thesis/checkpoints/language_model/standard/checkpoint_best_standard_64_1.0000E-04.pt
2022-05-13 06:30:58 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/work/cotterell/liam/master-thesis/checkpoints/language_model/standard/checkpoint_best_standard_64_1.0000E-04.pt
2022-05-13 06:30:58 | INFO | fairseq.checkpoint_utils | Saved checkpoint checkpoints/language_model/standard/checkpoint_best_standard_64_1.0000E-04.pt (epoch 123 @ 4797 updates, score 5.735) (writing took 1.2018732731230557 seconds)
2022-05-13 06:30:58 | INFO | fairseq_cli.train | end of epoch 123 (average epoch stats below)
2022-05-13 06:30:58 | INFO | train | epoch 123 | loss 5.195 | ppl 36.64 | wps 26626.2 | ups 0.2 | wpb 130930 | bsz 255.7 | num_updates 4797 | lr 9.13156e-05 | gnorm 0.815 | train_wall 178 | gb_free 7.9 | wall 24168
2022-05-13 06:30:58 | INFO | fairseq.data.iterators | grouped total_num_itrs = 39
2022-05-13 06:30:58 | INFO | fairseq.trainer | begin training epoch 124
2022-05-13 06:30:58 | INFO | fairseq_cli.train | Start iterating over samples
2022-05-13 06:31:12 | INFO | train_inner | epoch 124:      3 / 39 loss=5.208, ppl=36.97, wps=26283, ups=0.2, wpb=130916, bsz=255.7, num_updates=4800, lr=9.12871e-05, gnorm=0.822, train_wall=458, gb_free=7.9, wall=24182
2022-05-13 06:34:12 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-05-13 06:34:16 | INFO | valid | epoch 124 | valid on 'valid' subset | loss 5.73 | ppl 53.06 | wps 76548.5 | wpb 2041.1 | bsz 4 | num_updates 4836 | best_loss 5.73
2022-05-13 06:34:16 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 124 @ 4836 updates
2022-05-13 06:34:16 | INFO | fairseq.trainer | Saving checkpoint to /cluster/work/cotterell/liam/master-thesis/checkpoints/language_model/standard/checkpoint_best_standard_64_1.0000E-04.pt
2022-05-13 06:34:17 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/work/cotterell/liam/master-thesis/checkpoints/language_model/standard/checkpoint_best_standard_64_1.0000E-04.pt
2022-05-13 06:34:17 | INFO | fairseq.checkpoint_utils | Saved checkpoint checkpoints/language_model/standard/checkpoint_best_standard_64_1.0000E-04.pt (epoch 124 @ 4836 updates, score 5.73) (writing took 1.2743879747577012 seconds)
2022-05-13 06:34:17 | INFO | fairseq_cli.train | end of epoch 124 (average epoch stats below)
2022-05-13 06:34:17 | INFO | train | epoch 124 | loss 5.179 | ppl 36.22 | wps 25605.4 | ups 0.2 | wpb 130930 | bsz 255.7 | num_updates 4836 | lr 9.09467e-05 | gnorm 0.868 | train_wall 182 | gb_free 7.9 | wall 24367
2022-05-13 06:34:17 | INFO | fairseq.data.iterators | grouped total_num_itrs = 39
2022-05-13 06:34:17 | INFO | fairseq.trainer | begin training epoch 125
2022-05-13 06:34:17 | INFO | fairseq_cli.train | Start iterating over samples
2022-05-13 06:37:24 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-05-13 06:37:28 | INFO | valid | epoch 125 | valid on 'valid' subset | loss 5.724 | ppl 52.86 | wps 76536.4 | wpb 2041.1 | bsz 4 | num_updates 4875 | best_loss 5.724
2022-05-13 06:37:28 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 125 @ 4875 updates
2022-05-13 06:37:28 | INFO | fairseq.trainer | Saving checkpoint to /cluster/work/cotterell/liam/master-thesis/checkpoints/language_model/standard/checkpoint_best_standard_64_1.0000E-04.pt
2022-05-13 06:37:29 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/work/cotterell/liam/master-thesis/checkpoints/language_model/standard/checkpoint_best_standard_64_1.0000E-04.pt
2022-05-13 06:37:29 | INFO | fairseq.checkpoint_utils | Saved checkpoint checkpoints/language_model/standard/checkpoint_best_standard_64_1.0000E-04.pt (epoch 125 @ 4875 updates, score 5.724) (writing took 1.094749164301902 seconds)
2022-05-13 06:37:29 | INFO | fairseq_cli.train | end of epoch 125 (average epoch stats below)
2022-05-13 06:37:29 | INFO | train | epoch 125 | loss 5.16 | ppl 35.77 | wps 26651.6 | ups 0.2 | wpb 130930 | bsz 255.7 | num_updates 4875 | lr 9.05822e-05 | gnorm 0.797 | train_wall 178 | gb_free 7.9 | wall 24559
2022-05-13 06:37:29 | INFO | fairseq.data.iterators | grouped total_num_itrs = 39
2022-05-13 06:37:29 | INFO | fairseq.trainer | begin training epoch 126
2022-05-13 06:37:29 | INFO | fairseq_cli.train | Start iterating over samples
2022-05-13 06:39:30 | INFO | train_inner | epoch 126:     25 / 39 loss=5.163, ppl=35.84, wps=26322.1, ups=0.2, wpb=130946, bsz=255.8, num_updates=4900, lr=9.03508e-05, gnorm=0.836, train_wall=460, gb_free=7.9, wall=24680
2022-05-13 06:40:37 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-05-13 06:40:41 | INFO | valid | epoch 126 | valid on 'valid' subset | loss 5.721 | ppl 52.75 | wps 76413.8 | wpb 2041.1 | bsz 4 | num_updates 4914 | best_loss 5.721
2022-05-13 06:40:41 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 126 @ 4914 updates
2022-05-13 06:40:41 | INFO | fairseq.trainer | Saving checkpoint to /cluster/work/cotterell/liam/master-thesis/checkpoints/language_model/standard/checkpoint_best_standard_64_1.0000E-04.pt
2022-05-13 06:40:42 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/work/cotterell/liam/master-thesis/checkpoints/language_model/standard/checkpoint_best_standard_64_1.0000E-04.pt
2022-05-13 06:40:42 | INFO | fairseq.checkpoint_utils | Saved checkpoint checkpoints/language_model/standard/checkpoint_best_standard_64_1.0000E-04.pt (epoch 126 @ 4914 updates, score 5.721) (writing took 1.136191762983799 seconds)
2022-05-13 06:40:42 | INFO | fairseq_cli.train | end of epoch 126 (average epoch stats below)
2022-05-13 06:40:42 | INFO | train | epoch 126 | loss 5.143 | ppl 35.35 | wps 26489.5 | ups 0.2 | wpb 130930 | bsz 255.7 | num_updates 4914 | lr 9.0222e-05 | gnorm 0.813 | train_wall 178 | gb_free 7.9 | wall 24752
2022-05-13 06:40:42 | INFO | fairseq.data.iterators | grouped total_num_itrs = 39
2022-05-13 06:40:42 | INFO | fairseq.trainer | begin training epoch 127
2022-05-13 06:40:42 | INFO | fairseq_cli.train | Start iterating over samples
2022-05-13 06:43:49 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-05-13 06:43:53 | INFO | valid | epoch 127 | valid on 'valid' subset | loss 5.714 | ppl 52.48 | wps 76550 | wpb 2041.1 | bsz 4 | num_updates 4953 | best_loss 5.714
2022-05-13 06:43:53 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 127 @ 4953 updates
2022-05-13 06:43:53 | INFO | fairseq.trainer | Saving checkpoint to /cluster/work/cotterell/liam/master-thesis/checkpoints/language_model/standard/checkpoint_best_standard_64_1.0000E-04.pt
2022-05-13 06:43:54 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/work/cotterell/liam/master-thesis/checkpoints/language_model/standard/checkpoint_best_standard_64_1.0000E-04.pt
2022-05-13 06:43:54 | INFO | fairseq.checkpoint_utils | Saved checkpoint checkpoints/language_model/standard/checkpoint_best_standard_64_1.0000E-04.pt (epoch 127 @ 4953 updates, score 5.714) (writing took 1.1430103811435401 seconds)
2022-05-13 06:43:54 | INFO | fairseq_cli.train | end of epoch 127 (average epoch stats below)
2022-05-13 06:43:54 | INFO | train | epoch 127 | loss 5.127 | ppl 34.93 | wps 26578.4 | ups 0.2 | wpb 130930 | bsz 255.7 | num_updates 4953 | lr 8.98661e-05 | gnorm 0.815 | train_wall 178 | gb_free 7.9 | wall 24944
2022-05-13 06:43:54 | INFO | fairseq.data.iterators | grouped total_num_itrs = 39
2022-05-13 06:43:54 | INFO | fairseq.trainer | begin training epoch 128
2022-05-13 06:43:54 | INFO | fairseq_cli.train | Start iterating over samples
2022-05-13 06:47:05 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-05-13 06:47:10 | INFO | valid | epoch 128 | valid on 'valid' subset | loss 5.708 | ppl 52.28 | wps 76675.6 | wpb 2041.1 | bsz 4 | num_updates 4992 | best_loss 5.708
2022-05-13 06:47:10 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 128 @ 4992 updates
2022-05-13 06:47:10 | INFO | fairseq.trainer | Saving checkpoint to /cluster/work/cotterell/liam/master-thesis/checkpoints/language_model/standard/checkpoint_best_standard_64_1.0000E-04.pt
2022-05-13 06:47:11 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/work/cotterell/liam/master-thesis/checkpoints/language_model/standard/checkpoint_best_standard_64_1.0000E-04.pt
2022-05-13 06:47:11 | INFO | fairseq.checkpoint_utils | Saved checkpoint checkpoints/language_model/standard/checkpoint_best_standard_64_1.0000E-04.pt (epoch 128 @ 4992 updates, score 5.708) (writing took 1.2479292508214712 seconds)
2022-05-13 06:47:11 | INFO | fairseq_cli.train | end of epoch 128 (average epoch stats below)
2022-05-13 06:47:11 | INFO | train | epoch 128 | loss 5.11 | ppl 34.54 | wps 25926 | ups 0.2 | wpb 130930 | bsz 255.7 | num_updates 4992 | lr 8.95144e-05 | gnorm 0.802 | train_wall 180 | gb_free 7.9 | wall 25141
2022-05-13 06:47:11 | INFO | fairseq.data.iterators | grouped total_num_itrs = 39
2022-05-13 06:47:11 | INFO | fairseq.trainer | begin training epoch 129
2022-05-13 06:47:11 | INFO | fairseq_cli.train | Start iterating over samples
2022-05-13 06:47:49 | INFO | train_inner | epoch 129:      8 / 39 loss=5.119, ppl=34.76, wps=26219.2, ups=0.2, wpb=130920, bsz=255.7, num_updates=5000, lr=8.94427e-05, gnorm=0.8, train_wall=458, gb_free=7.9, wall=25179
2022-05-13 06:50:18 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-05-13 06:50:21 | INFO | valid | epoch 129 | valid on 'valid' subset | loss 5.703 | ppl 52.08 | wps 76518.7 | wpb 2041.1 | bsz 4 | num_updates 5031 | best_loss 5.703
2022-05-13 06:50:21 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 129 @ 5031 updates
2022-05-13 06:50:21 | INFO | fairseq.trainer | Saving checkpoint to /cluster/work/cotterell/liam/master-thesis/checkpoints/language_model/standard/checkpoint_best_standard_64_1.0000E-04.pt
2022-05-13 06:50:22 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/work/cotterell/liam/master-thesis/checkpoints/language_model/standard/checkpoint_best_standard_64_1.0000E-04.pt
2022-05-13 06:50:22 | INFO | fairseq.checkpoint_utils | Saved checkpoint checkpoints/language_model/standard/checkpoint_best_standard_64_1.0000E-04.pt (epoch 129 @ 5031 updates, score 5.703) (writing took 1.0762408711016178 seconds)
2022-05-13 06:50:22 | INFO | fairseq_cli.train | end of epoch 129 (average epoch stats below)
2022-05-13 06:50:22 | INFO | train | epoch 129 | loss 5.095 | ppl 34.17 | wps 26652.1 | ups 0.2 | wpb 130930 | bsz 255.7 | num_updates 5031 | lr 8.91667e-05 | gnorm 0.824 | train_wall 178 | gb_free 7.9 | wall 25332
2022-05-13 06:50:22 | INFO | fairseq.data.iterators | grouped total_num_itrs = 39
2022-05-13 06:50:22 | INFO | fairseq.trainer | begin training epoch 130
2022-05-13 06:50:22 | INFO | fairseq_cli.train | Start iterating over samples
2022-05-13 06:53:32 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-05-13 06:53:35 | INFO | valid | epoch 130 | valid on 'valid' subset | loss 5.699 | ppl 51.95 | wps 76464.7 | wpb 2041.1 | bsz 4 | num_updates 5070 | best_loss 5.699
2022-05-13 06:53:35 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 130 @ 5070 updates
2022-05-13 06:53:35 | INFO | fairseq.trainer | Saving checkpoint to /cluster/work/cotterell/liam/master-thesis/checkpoints/language_model/standard/checkpoint_best_standard_64_1.0000E-04.pt
2022-05-13 06:53:37 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/work/cotterell/liam/master-thesis/checkpoints/language_model/standard/checkpoint_best_standard_64_1.0000E-04.pt
2022-05-13 06:53:37 | INFO | fairseq.checkpoint_utils | Saved checkpoint checkpoints/language_model/standard/checkpoint_best_standard_64_1.0000E-04.pt (epoch 130 @ 5070 updates, score 5.699) (writing took 1.1949179479852319 seconds)
2022-05-13 06:53:37 | INFO | fairseq_cli.train | end of epoch 130 (average epoch stats below)
2022-05-13 06:53:37 | INFO | train | epoch 130 | loss 5.078 | ppl 33.79 | wps 26295.1 | ups 0.2 | wpb 130930 | bsz 255.7 | num_updates 5070 | lr 8.88231e-05 | gnorm 0.805 | train_wall 180 | gb_free 7.9 | wall 25526
2022-05-13 06:53:37 | INFO | fairseq.data.iterators | grouped total_num_itrs = 39
2022-05-13 06:53:37 | INFO | fairseq.trainer | begin training epoch 131
2022-05-13 06:53:37 | INFO | fairseq_cli.train | Start iterating over samples
2022-05-13 06:56:01 | INFO | train_inner | epoch 131:     30 / 39 loss=5.079, ppl=33.79, wps=26638.5, ups=0.2, wpb=130946, bsz=255.8, num_updates=5100, lr=8.85615e-05, gnorm=0.828, train_wall=458, gb_free=7.9, wall=25671
2022-05-13 06:56:49 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-05-13 06:56:53 | INFO | valid | epoch 131 | valid on 'valid' subset | loss 5.693 | ppl 51.74 | wps 76686.6 | wpb 2041.1 | bsz 4 | num_updates 5109 | best_loss 5.693
2022-05-13 06:56:53 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 131 @ 5109 updates
2022-05-13 06:56:53 | INFO | fairseq.trainer | Saving checkpoint to /cluster/work/cotterell/liam/master-thesis/checkpoints/language_model/standard/checkpoint_best_standard_64_1.0000E-04.pt
2022-05-13 06:56:54 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/work/cotterell/liam/master-thesis/checkpoints/language_model/standard/checkpoint_best_standard_64_1.0000E-04.pt
2022-05-13 06:56:54 | INFO | fairseq.checkpoint_utils | Saved checkpoint checkpoints/language_model/standard/checkpoint_best_standard_64_1.0000E-04.pt (epoch 131 @ 5109 updates, score 5.693) (writing took 1.2149651902727783 seconds)
2022-05-13 06:56:54 | INFO | fairseq_cli.train | end of epoch 131 (average epoch stats below)
2022-05-13 06:56:54 | INFO | train | epoch 131 | loss 5.063 | ppl 33.42 | wps 25807 | ups 0.2 | wpb 130930 | bsz 255.7 | num_updates 5109 | lr 8.84834e-05 | gnorm 0.848 | train_wall 181 | gb_free 7.9 | wall 25724
2022-05-13 06:56:54 | INFO | fairseq.data.iterators | grouped total_num_itrs = 39
2022-05-13 06:56:54 | INFO | fairseq.trainer | begin training epoch 132
2022-05-13 06:56:54 | INFO | fairseq_cli.train | Start iterating over samples
2022-05-13 07:00:02 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-05-13 07:00:06 | INFO | valid | epoch 132 | valid on 'valid' subset | loss 5.686 | ppl 51.48 | wps 76611.1 | wpb 2041.1 | bsz 4 | num_updates 5148 | best_loss 5.686
2022-05-13 07:00:06 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 132 @ 5148 updates
2022-05-13 07:00:06 | INFO | fairseq.trainer | Saving checkpoint to /cluster/work/cotterell/liam/master-thesis/checkpoints/language_model/standard/checkpoint_best_standard_64_1.0000E-04.pt
2022-05-13 07:00:07 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/work/cotterell/liam/master-thesis/checkpoints/language_model/standard/checkpoint_best_standard_64_1.0000E-04.pt
2022-05-13 07:00:07 | INFO | fairseq.checkpoint_utils | Saved checkpoint checkpoints/language_model/standard/checkpoint_best_standard_64_1.0000E-04.pt (epoch 132 @ 5148 updates, score 5.686) (writing took 1.1503584911115468 seconds)
2022-05-13 07:00:07 | INFO | fairseq_cli.train | end of epoch 132 (average epoch stats below)
2022-05-13 07:00:07 | INFO | train | epoch 132 | loss 5.046 | ppl 33.04 | wps 26499.4 | ups 0.2 | wpb 130930 | bsz 255.7 | num_updates 5148 | lr 8.81476e-05 | gnorm 0.798 | train_wall 178 | gb_free 7.9 | wall 25917
2022-05-13 07:00:07 | INFO | fairseq.data.iterators | grouped total_num_itrs = 39
2022-05-13 07:00:07 | INFO | fairseq.trainer | begin training epoch 133
2022-05-13 07:00:07 | INFO | fairseq_cli.train | Start iterating over samples
2022-05-13 07:03:18 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-05-13 07:03:21 | INFO | valid | epoch 133 | valid on 'valid' subset | loss 5.687 | ppl 51.5 | wps 76596 | wpb 2041.1 | bsz 4 | num_updates 5187 | best_loss 5.686
2022-05-13 07:03:21 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 133 @ 5187 updates
2022-05-13 07:03:21 | INFO | fairseq_cli.train | end of epoch 133 (average epoch stats below)
2022-05-13 07:03:21 | INFO | train | epoch 133 | loss 5.031 | ppl 32.69 | wps 26278.7 | ups 0.2 | wpb 130930 | bsz 255.7 | num_updates 5187 | lr 8.78156e-05 | gnorm 0.772 | train_wall 181 | gb_free 7.9 | wall 26111
2022-05-13 07:03:21 | INFO | fairseq.data.iterators | grouped total_num_itrs = 39
2022-05-13 07:03:21 | INFO | fairseq.trainer | begin training epoch 134
2022-05-13 07:03:21 | INFO | fairseq_cli.train | Start iterating over samples
2022-05-13 07:04:24 | INFO | train_inner | epoch 134:     13 / 39 loss=5.039, ppl=32.88, wps=26022.8, ups=0.2, wpb=130905, bsz=255.7, num_updates=5200, lr=8.77058e-05, gnorm=0.799, train_wall=462, gb_free=7.9, wall=26174
2022-05-13 07:06:32 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-05-13 07:06:36 | INFO | valid | epoch 134 | valid on 'valid' subset | loss 5.684 | ppl 51.4 | wps 76760.2 | wpb 2041.1 | bsz 4 | num_updates 5226 | best_loss 5.684
2022-05-13 07:06:36 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 134 @ 5226 updates
2022-05-13 07:06:36 | INFO | fairseq.trainer | Saving checkpoint to /cluster/work/cotterell/liam/master-thesis/checkpoints/language_model/standard/checkpoint_best_standard_64_1.0000E-04.pt
2022-05-13 07:06:37 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/work/cotterell/liam/master-thesis/checkpoints/language_model/standard/checkpoint_best_standard_64_1.0000E-04.pt
2022-05-13 07:06:38 | INFO | fairseq.checkpoint_utils | Saved checkpoint checkpoints/language_model/standard/checkpoint_best_standard_64_1.0000E-04.pt (epoch 134 @ 5226 updates, score 5.684) (writing took 1.320879276841879 seconds)
2022-05-13 07:06:38 | INFO | fairseq_cli.train | end of epoch 134 (average epoch stats below)
2022-05-13 07:06:38 | INFO | train | epoch 134 | loss 5.016 | ppl 32.35 | wps 26038.5 | ups 0.2 | wpb 130930 | bsz 255.7 | num_updates 5226 | lr 8.74874e-05 | gnorm 0.809 | train_wall 182 | gb_free 7.9 | wall 26307
2022-05-13 07:06:38 | INFO | fairseq.data.iterators | grouped total_num_itrs = 39
2022-05-13 07:06:38 | INFO | fairseq.trainer | begin training epoch 135
2022-05-13 07:06:38 | INFO | fairseq_cli.train | Start iterating over samples
2022-05-13 07:09:46 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-05-13 07:09:50 | INFO | valid | epoch 135 | valid on 'valid' subset | loss 5.68 | ppl 51.27 | wps 76632.5 | wpb 2041.1 | bsz 4 | num_updates 5265 | best_loss 5.68
2022-05-13 07:09:50 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 135 @ 5265 updates
2022-05-13 07:09:50 | INFO | fairseq.trainer | Saving checkpoint to /cluster/work/cotterell/liam/master-thesis/checkpoints/language_model/standard/checkpoint_best_standard_64_1.0000E-04.pt
2022-05-13 07:09:51 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/work/cotterell/liam/master-thesis/checkpoints/language_model/standard/checkpoint_best_standard_64_1.0000E-04.pt
2022-05-13 07:09:51 | INFO | fairseq.checkpoint_utils | Saved checkpoint checkpoints/language_model/standard/checkpoint_best_standard_64_1.0000E-04.pt (epoch 135 @ 5265 updates, score 5.68) (writing took 1.1679127612151206 seconds)
2022-05-13 07:09:51 | INFO | fairseq_cli.train | end of epoch 135 (average epoch stats below)
2022-05-13 07:09:51 | INFO | train | epoch 135 | loss 5 | ppl 32.01 | wps 26383.9 | ups 0.2 | wpb 130930 | bsz 255.7 | num_updates 5265 | lr 8.71627e-05 | gnorm 0.834 | train_wall 178 | gb_free 7.9 | wall 26501
2022-05-13 07:09:51 | INFO | fairseq.data.iterators | grouped total_num_itrs = 39
2022-05-13 07:09:51 | INFO | fairseq.trainer | begin training epoch 136
2022-05-13 07:09:51 | INFO | fairseq_cli.train | Start iterating over samples
2022-05-13 07:12:39 | INFO | train_inner | epoch 136:     35 / 39 loss=4.999, ppl=31.97, wps=26421.2, ups=0.2, wpb=130961, bsz=255.8, num_updates=5300, lr=8.68744e-05, gnorm=0.807, train_wall=461, gb_free=7.9, wall=26669
2022-05-13 07:12:58 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-05-13 07:13:02 | INFO | valid | epoch 136 | valid on 'valid' subset | loss 5.672 | ppl 50.99 | wps 76625.3 | wpb 2041.1 | bsz 4 | num_updates 5304 | best_loss 5.672
2022-05-13 07:13:02 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 136 @ 5304 updates
2022-05-13 07:13:02 | INFO | fairseq.trainer | Saving checkpoint to /cluster/work/cotterell/liam/master-thesis/checkpoints/language_model/standard/checkpoint_best_standard_64_1.0000E-04.pt
2022-05-13 07:13:03 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/work/cotterell/liam/master-thesis/checkpoints/language_model/standard/checkpoint_best_standard_64_1.0000E-04.pt
2022-05-13 07:13:03 | INFO | fairseq.checkpoint_utils | Saved checkpoint checkpoints/language_model/standard/checkpoint_best_standard_64_1.0000E-04.pt (epoch 136 @ 5304 updates, score 5.672) (writing took 1.0529253650456667 seconds)
2022-05-13 07:13:03 | INFO | fairseq_cli.train | end of epoch 136 (average epoch stats below)
2022-05-13 07:13:03 | INFO | train | epoch 136 | loss 4.986 | ppl 31.69 | wps 26537.1 | ups 0.2 | wpb 130930 | bsz 255.7 | num_updates 5304 | lr 8.68417e-05 | gnorm 0.793 | train_wall 178 | gb_free 7.9 | wall 26693
2022-05-13 07:13:04 | INFO | fairseq.data.iterators | grouped total_num_itrs = 39
2022-05-13 07:13:04 | INFO | fairseq.trainer | begin training epoch 137
2022-05-13 07:13:04 | INFO | fairseq_cli.train | Start iterating over samples
2022-05-13 07:16:16 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-05-13 07:16:20 | INFO | valid | epoch 137 | valid on 'valid' subset | loss 5.669 | ppl 50.87 | wps 76609.8 | wpb 2041.1 | bsz 4 | num_updates 5343 | best_loss 5.669
2022-05-13 07:16:20 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 137 @ 5343 updates
2022-05-13 07:16:20 | INFO | fairseq.trainer | Saving checkpoint to /cluster/work/cotterell/liam/master-thesis/checkpoints/language_model/standard/checkpoint_best_standard_64_1.0000E-04.pt
2022-05-13 07:16:22 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/work/cotterell/liam/master-thesis/checkpoints/language_model/standard/checkpoint_best_standard_64_1.0000E-04.pt
2022-05-13 07:16:22 | INFO | fairseq.checkpoint_utils | Saved checkpoint checkpoints/language_model/standard/checkpoint_best_standard_64_1.0000E-04.pt (epoch 137 @ 5343 updates, score 5.669) (writing took 1.356125006917864 seconds)
2022-05-13 07:16:22 | INFO | fairseq_cli.train | end of epoch 137 (average epoch stats below)
2022-05-13 07:16:22 | INFO | train | epoch 137 | loss 4.971 | ppl 31.37 | wps 25769.3 | ups 0.2 | wpb 130930 | bsz 255.7 | num_updates 5343 | lr 8.65242e-05 | gnorm 0.807 | train_wall 182 | gb_free 7.9 | wall 26891
2022-05-13 07:16:22 | INFO | fairseq.data.iterators | grouped total_num_itrs = 39
2022-05-13 07:16:22 | INFO | fairseq.trainer | begin training epoch 138
2022-05-13 07:16:22 | INFO | fairseq_cli.train | Start iterating over samples
2022-05-13 07:19:31 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-05-13 07:19:35 | INFO | valid | epoch 138 | valid on 'valid' subset | loss 5.666 | ppl 50.76 | wps 71455.5 | wpb 2041.1 | bsz 4 | num_updates 5382 | best_loss 5.666
2022-05-13 07:19:35 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 138 @ 5382 updates
2022-05-13 07:19:35 | INFO | fairseq.trainer | Saving checkpoint to /cluster/work/cotterell/liam/master-thesis/checkpoints/language_model/standard/checkpoint_best_standard_64_1.0000E-04.pt
2022-05-13 07:19:36 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/work/cotterell/liam/master-thesis/checkpoints/language_model/standard/checkpoint_best_standard_64_1.0000E-04.pt
2022-05-13 07:19:36 | INFO | fairseq.checkpoint_utils | Saved checkpoint checkpoints/language_model/standard/checkpoint_best_standard_64_1.0000E-04.pt (epoch 138 @ 5382 updates, score 5.666) (writing took 1.2394160372205079 seconds)
2022-05-13 07:19:36 | INFO | fairseq_cli.train | end of epoch 138 (average epoch stats below)
2022-05-13 07:19:36 | INFO | train | epoch 138 | loss 4.957 | ppl 31.06 | wps 26285.8 | ups 0.2 | wpb 130930 | bsz 255.7 | num_updates 5382 | lr 8.62101e-05 | gnorm 0.866 | train_wall 178 | gb_free 7.9 | wall 27086
2022-05-13 07:19:36 | INFO | fairseq.data.iterators | grouped total_num_itrs = 39
2022-05-13 07:19:36 | INFO | fairseq.trainer | begin training epoch 139
2022-05-13 07:19:36 | INFO | fairseq_cli.train | Start iterating over samples
2022-05-13 07:21:05 | INFO | train_inner | epoch 139:     18 / 39 loss=4.96, ppl=31.13, wps=25872.6, ups=0.2, wpb=130905, bsz=255.7, num_updates=5400, lr=8.60663e-05, gnorm=0.824, train_wall=463, gb_free=7.9, wall=27175
2022-05-13 07:22:46 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-05-13 07:22:50 | INFO | valid | epoch 139 | valid on 'valid' subset | loss 5.665 | ppl 50.73 | wps 76664.9 | wpb 2041.1 | bsz 4 | num_updates 5421 | best_loss 5.665
2022-05-13 07:22:50 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 139 @ 5421 updates
2022-05-13 07:22:50 | INFO | fairseq.trainer | Saving checkpoint to /cluster/work/cotterell/liam/master-thesis/checkpoints/language_model/standard/checkpoint_best_standard_64_1.0000E-04.pt
2022-05-13 07:22:51 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/work/cotterell/liam/master-thesis/checkpoints/language_model/standard/checkpoint_best_standard_64_1.0000E-04.pt
2022-05-13 07:22:51 | INFO | fairseq.checkpoint_utils | Saved checkpoint checkpoints/language_model/standard/checkpoint_best_standard_64_1.0000E-04.pt (epoch 139 @ 5421 updates, score 5.665) (writing took 1.1780326357111335 seconds)
2022-05-13 07:22:51 | INFO | fairseq_cli.train | end of epoch 139 (average epoch stats below)
2022-05-13 07:22:51 | INFO | train | epoch 139 | loss 4.94 | ppl 30.7 | wps 26160.6 | ups 0.2 | wpb 130930 | bsz 255.7 | num_updates 5421 | lr 8.58994e-05 | gnorm 0.757 | train_wall 181 | gb_free 7.9 | wall 27281
2022-05-13 07:22:51 | INFO | fairseq.data.iterators | grouped total_num_itrs = 39
2022-05-13 07:22:51 | INFO | fairseq.trainer | begin training epoch 140
2022-05-13 07:22:51 | INFO | fairseq_cli.train | Start iterating over samples
2022-05-13 07:26:06 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-05-13 07:26:10 | INFO | valid | epoch 140 | valid on 'valid' subset | loss 5.659 | ppl 50.54 | wps 76612.6 | wpb 2041.1 | bsz 4 | num_updates 5460 | best_loss 5.659
2022-05-13 07:26:10 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 140 @ 5460 updates
2022-05-13 07:26:10 | INFO | fairseq.trainer | Saving checkpoint to /cluster/work/cotterell/liam/master-thesis/checkpoints/language_model/standard/checkpoint_best_standard_64_1.0000E-04.pt
2022-05-13 07:26:12 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/work/cotterell/liam/master-thesis/checkpoints/language_model/standard/checkpoint_best_standard_64_1.0000E-04.pt
2022-05-13 07:26:12 | INFO | fairseq.checkpoint_utils | Saved checkpoint checkpoints/language_model/standard/checkpoint_best_standard_64_1.0000E-04.pt (epoch 140 @ 5460 updates, score 5.659) (writing took 1.2074505756609142 seconds)
2022-05-13 07:26:12 | INFO | fairseq_cli.train | end of epoch 140 (average epoch stats below)
2022-05-13 07:26:12 | INFO | train | epoch 140 | loss 4.928 | ppl 30.44 | wps 25458.8 | ups 0.19 | wpb 130930 | bsz 255.7 | num_updates 5460 | lr 8.55921e-05 | gnorm 0.834 | train_wall 183 | gb_free 7.9 | wall 27481
2022-05-13 07:26:12 | INFO | fairseq.data.iterators | grouped total_num_itrs = 39
2022-05-13 07:26:12 | INFO | fairseq.trainer | begin training epoch 141
2022-05-13 07:26:12 | INFO | fairseq_cli.train | Start iterating over samples
2022-05-13 07:29:33 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-05-13 07:29:37 | INFO | valid | epoch 141 | valid on 'valid' subset | loss 5.658 | ppl 50.5 | wps 76610.6 | wpb 2041.1 | bsz 4 | num_updates 5499 | best_loss 5.658
2022-05-13 07:29:37 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 141 @ 5499 updates
2022-05-13 07:29:37 | INFO | fairseq.trainer | Saving checkpoint to /cluster/work/cotterell/liam/master-thesis/checkpoints/language_model/standard/checkpoint_best_standard_64_1.0000E-04.pt
2022-05-13 07:29:38 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/work/cotterell/liam/master-thesis/checkpoints/language_model/standard/checkpoint_best_standard_64_1.0000E-04.pt
2022-05-13 07:29:38 | INFO | fairseq.checkpoint_utils | Saved checkpoint checkpoints/language_model/standard/checkpoint_best_standard_64_1.0000E-04.pt (epoch 141 @ 5499 updates, score 5.658) (writing took 1.317836157977581 seconds)
2022-05-13 07:29:38 | INFO | fairseq_cli.train | end of epoch 141 (average epoch stats below)
2022-05-13 07:29:38 | INFO | train | epoch 141 | loss 4.913 | ppl 30.13 | wps 24730.8 | ups 0.19 | wpb 130930 | bsz 255.7 | num_updates 5499 | lr 8.5288e-05 | gnorm 0.812 | train_wall 189 | gb_free 7.9 | wall 27688
2022-05-13 07:29:38 | INFO | fairseq.data.iterators | grouped total_num_itrs = 39
2022-05-13 07:29:38 | INFO | fairseq.trainer | begin training epoch 142
2022-05-13 07:29:38 | INFO | fairseq_cli.train | Start iterating over samples
2022-05-13 07:29:43 | INFO | train_inner | epoch 142:      1 / 39 loss=4.925, ppl=30.39, wps=25293.3, ups=0.19, wpb=130920, bsz=255.7, num_updates=5500, lr=8.52803e-05, gnorm=0.81, train_wall=472, gb_free=7.9, wall=27693
2022-05-13 07:32:50 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-05-13 07:32:54 | INFO | valid | epoch 142 | valid on 'valid' subset | loss 5.661 | ppl 50.59 | wps 76745.5 | wpb 2041.1 | bsz 4 | num_updates 5538 | best_loss 5.658
2022-05-13 07:32:54 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 142 @ 5538 updates
2022-05-13 07:32:54 | INFO | fairseq_cli.train | end of epoch 142 (average epoch stats below)
2022-05-13 07:32:54 | INFO | train | epoch 142 | loss 4.898 | ppl 29.81 | wps 26079.9 | ups 0.2 | wpb 130930 | bsz 255.7 | num_updates 5538 | lr 8.49872e-05 | gnorm 0.783 | train_wall 181 | gb_free 7.9 | wall 27884
2022-05-13 07:32:54 | INFO | fairseq.data.iterators | grouped total_num_itrs = 39
2022-05-13 07:32:54 | INFO | fairseq.trainer | begin training epoch 143
2022-05-13 07:32:54 | INFO | fairseq_cli.train | Start iterating over samples
2022-05-13 07:36:06 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-05-13 07:36:10 | INFO | valid | epoch 143 | valid on 'valid' subset | loss 5.654 | ppl 50.37 | wps 76272.9 | wpb 2041.1 | bsz 4 | num_updates 5577 | best_loss 5.654
2022-05-13 07:36:10 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 143 @ 5577 updates
2022-05-13 07:36:10 | INFO | fairseq.trainer | Saving checkpoint to /cluster/work/cotterell/liam/master-thesis/checkpoints/language_model/standard/checkpoint_best_standard_64_1.0000E-04.pt
2022-05-13 07:36:11 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/work/cotterell/liam/master-thesis/checkpoints/language_model/standard/checkpoint_best_standard_64_1.0000E-04.pt
2022-05-13 07:36:11 | INFO | fairseq.checkpoint_utils | Saved checkpoint checkpoints/language_model/standard/checkpoint_best_standard_64_1.0000E-04.pt (epoch 143 @ 5577 updates, score 5.654) (writing took 1.1107975449413061 seconds)
2022-05-13 07:36:11 | INFO | fairseq_cli.train | end of epoch 143 (average epoch stats below)
2022-05-13 07:36:11 | INFO | train | epoch 143 | loss 4.886 | ppl 29.56 | wps 25915.2 | ups 0.2 | wpb 130930 | bsz 255.7 | num_updates 5577 | lr 8.46895e-05 | gnorm 0.837 | train_wall 183 | gb_free 7.9 | wall 28081
2022-05-13 07:36:11 | INFO | fairseq.data.iterators | grouped total_num_itrs = 39
2022-05-13 07:36:11 | INFO | fairseq.trainer | begin training epoch 144
2022-05-13 07:36:11 | INFO | fairseq_cli.train | Start iterating over samples
2022-05-13 07:38:01 | INFO | train_inner | epoch 144:     23 / 39 loss=4.886, ppl=29.57, wps=26283, ups=0.2, wpb=130951, bsz=255.8, num_updates=5600, lr=8.45154e-05, gnorm=0.811, train_wall=464, gb_free=7.9, wall=28191
2022-05-13 07:39:20 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-05-13 07:39:23 | INFO | valid | epoch 144 | valid on 'valid' subset | loss 5.646 | ppl 50.09 | wps 76491.1 | wpb 2041.1 | bsz 4 | num_updates 5616 | best_loss 5.646
2022-05-13 07:39:23 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 144 @ 5616 updates
2022-05-13 07:39:23 | INFO | fairseq.trainer | Saving checkpoint to /cluster/work/cotterell/liam/master-thesis/checkpoints/language_model/standard/checkpoint_best_standard_64_1.0000E-04.pt
2022-05-13 07:39:24 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/work/cotterell/liam/master-thesis/checkpoints/language_model/standard/checkpoint_best_standard_64_1.0000E-04.pt
2022-05-13 07:39:24 | INFO | fairseq.checkpoint_utils | Saved checkpoint checkpoints/language_model/standard/checkpoint_best_standard_64_1.0000E-04.pt (epoch 144 @ 5616 updates, score 5.646) (writing took 1.1274295668117702 seconds)
2022-05-13 07:39:24 | INFO | fairseq_cli.train | end of epoch 144 (average epoch stats below)
2022-05-13 07:39:24 | INFO | train | epoch 144 | loss 4.871 | ppl 29.25 | wps 26387.6 | ups 0.2 | wpb 130930 | bsz 255.7 | num_updates 5616 | lr 8.43949e-05 | gnorm 0.801 | train_wall 180 | gb_free 7.9 | wall 28274
2022-05-13 07:39:24 | INFO | fairseq.data.iterators | grouped total_num_itrs = 39
2022-05-13 07:39:24 | INFO | fairseq.trainer | begin training epoch 145
2022-05-13 07:39:24 | INFO | fairseq_cli.train | Start iterating over samples
2022-05-13 07:42:33 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-05-13 07:42:37 | INFO | valid | epoch 145 | valid on 'valid' subset | loss 5.644 | ppl 50.02 | wps 76451.5 | wpb 2041.1 | bsz 4 | num_updates 5655 | best_loss 5.644
2022-05-13 07:42:37 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 145 @ 5655 updates
2022-05-13 07:42:37 | INFO | fairseq.trainer | Saving checkpoint to /cluster/work/cotterell/liam/master-thesis/checkpoints/language_model/standard/checkpoint_best_standard_64_1.0000E-04.pt
2022-05-13 07:42:38 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/work/cotterell/liam/master-thesis/checkpoints/language_model/standard/checkpoint_best_standard_64_1.0000E-04.pt
2022-05-13 07:42:38 | INFO | fairseq.checkpoint_utils | Saved checkpoint checkpoints/language_model/standard/checkpoint_best_standard_64_1.0000E-04.pt (epoch 145 @ 5655 updates, score 5.644) (writing took 1.1906607123091817 seconds)
2022-05-13 07:42:38 | INFO | fairseq_cli.train | end of epoch 145 (average epoch stats below)
2022-05-13 07:42:38 | INFO | train | epoch 145 | loss 4.857 | ppl 28.97 | wps 26373.8 | ups 0.2 | wpb 130930 | bsz 255.7 | num_updates 5655 | lr 8.41034e-05 | gnorm 0.778 | train_wall 179 | gb_free 7.9 | wall 28468
2022-05-13 07:42:38 | INFO | fairseq.data.iterators | grouped total_num_itrs = 39
2022-05-13 07:42:38 | INFO | fairseq.trainer | begin training epoch 146
2022-05-13 07:42:38 | INFO | fairseq_cli.train | Start iterating over samples
2022-05-13 07:45:56 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-05-13 07:46:00 | INFO | valid | epoch 146 | valid on 'valid' subset | loss 5.651 | ppl 50.24 | wps 76660.1 | wpb 2041.1 | bsz 4 | num_updates 5694 | best_loss 5.644
2022-05-13 07:46:00 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 146 @ 5694 updates
2022-05-13 07:46:00 | INFO | fairseq_cli.train | end of epoch 146 (average epoch stats below)
2022-05-13 07:46:00 | INFO | train | epoch 146 | loss 4.844 | ppl 28.71 | wps 25264.3 | ups 0.19 | wpb 130930 | bsz 255.7 | num_updates 5694 | lr 8.38149e-05 | gnorm 0.836 | train_wall 185 | gb_free 7.9 | wall 28670
2022-05-13 07:46:00 | INFO | fairseq.data.iterators | grouped total_num_itrs = 39
2022-05-13 07:46:00 | INFO | fairseq.trainer | begin training epoch 147
2022-05-13 07:46:00 | INFO | fairseq_cli.train | Start iterating over samples
2022-05-13 07:46:32 | INFO | train_inner | epoch 147:      6 / 39 loss=4.852, ppl=28.88, wps=25638.5, ups=0.2, wpb=130911, bsz=255.7, num_updates=5700, lr=8.37708e-05, gnorm=0.807, train_wall=468, gb_free=7.9, wall=28702
2022-05-13 07:49:13 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-05-13 07:49:17 | INFO | valid | epoch 147 | valid on 'valid' subset | loss 5.648 | ppl 50.13 | wps 76538.3 | wpb 2041.1 | bsz 4 | num_updates 5733 | best_loss 5.644
2022-05-13 07:49:17 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 147 @ 5733 updates
2022-05-13 07:49:17 | INFO | fairseq_cli.train | end of epoch 147 (average epoch stats below)
2022-05-13 07:49:17 | INFO | train | epoch 147 | loss 4.831 | ppl 28.46 | wps 25966.5 | ups 0.2 | wpb 130930 | bsz 255.7 | num_updates 5733 | lr 8.35293e-05 | gnorm 0.845 | train_wall 181 | gb_free 7.9 | wall 28867
2022-05-13 07:49:17 | INFO | fairseq.data.iterators | grouped total_num_itrs = 39
2022-05-13 07:49:17 | INFO | fairseq.trainer | begin training epoch 148
2022-05-13 07:49:17 | INFO | fairseq_cli.train | Start iterating over samples
2022-05-13 07:52:24 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-05-13 07:52:27 | INFO | valid | epoch 148 | valid on 'valid' subset | loss 5.642 | ppl 49.95 | wps 76606.4 | wpb 2041.1 | bsz 4 | num_updates 5772 | best_loss 5.642
2022-05-13 07:52:27 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 148 @ 5772 updates
2022-05-13 07:52:27 | INFO | fairseq.trainer | Saving checkpoint to /cluster/work/cotterell/liam/master-thesis/checkpoints/language_model/standard/checkpoint_best_standard_64_1.0000E-04.pt
2022-05-13 07:52:29 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/work/cotterell/liam/master-thesis/checkpoints/language_model/standard/checkpoint_best_standard_64_1.0000E-04.pt
2022-05-13 07:52:29 | INFO | fairseq.checkpoint_utils | Saved checkpoint checkpoints/language_model/standard/checkpoint_best_standard_64_1.0000E-04.pt (epoch 148 @ 5772 updates, score 5.642) (writing took 1.3296783152036369 seconds)
2022-05-13 07:52:29 | INFO | fairseq_cli.train | end of epoch 148 (average epoch stats below)
2022-05-13 07:52:29 | INFO | train | epoch 148 | loss 4.817 | ppl 28.19 | wps 26599.8 | ups 0.2 | wpb 130930 | bsz 255.7 | num_updates 5772 | lr 8.32467e-05 | gnorm 0.821 | train_wall 178 | gb_free 7.9 | wall 29059
2022-05-13 07:52:29 | INFO | fairseq.data.iterators | grouped total_num_itrs = 39
2022-05-13 07:52:29 | INFO | fairseq.trainer | begin training epoch 149
2022-05-13 07:52:29 | INFO | fairseq_cli.train | Start iterating over samples
2022-05-13 07:54:46 | INFO | train_inner | epoch 149:     28 / 39 loss=4.818, ppl=28.21, wps=26521.9, ups=0.2, wpb=130951, bsz=255.8, num_updates=5800, lr=8.30455e-05, gnorm=0.817, train_wall=459, gb_free=7.9, wall=29195
2022-05-13 07:55:38 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-05-13 07:55:42 | INFO | valid | epoch 149 | valid on 'valid' subset | loss 5.639 | ppl 49.83 | wps 76549.2 | wpb 2041.1 | bsz 4 | num_updates 5811 | best_loss 5.639
2022-05-13 07:55:42 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 149 @ 5811 updates
2022-05-13 07:55:42 | INFO | fairseq.trainer | Saving checkpoint to /cluster/work/cotterell/liam/master-thesis/checkpoints/language_model/standard/checkpoint_best_standard_64_1.0000E-04.pt
2022-05-13 07:55:44 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/work/cotterell/liam/master-thesis/checkpoints/language_model/standard/checkpoint_best_standard_64_1.0000E-04.pt
2022-05-13 07:55:44 | INFO | fairseq.checkpoint_utils | Saved checkpoint checkpoints/language_model/standard/checkpoint_best_standard_64_1.0000E-04.pt (epoch 149 @ 5811 updates, score 5.639) (writing took 1.359855911694467 seconds)
2022-05-13 07:55:44 | INFO | fairseq_cli.train | end of epoch 149 (average epoch stats below)
2022-05-13 07:55:44 | INFO | train | epoch 149 | loss 4.803 | ppl 27.91 | wps 26216.7 | ups 0.2 | wpb 130930 | bsz 255.7 | num_updates 5811 | lr 8.29668e-05 | gnorm 0.793 | train_wall 180 | gb_free 7.9 | wall 29253
2022-05-13 07:55:44 | INFO | fairseq.data.iterators | grouped total_num_itrs = 39
2022-05-13 07:55:44 | INFO | fairseq.trainer | begin training epoch 150
2022-05-13 07:55:44 | INFO | fairseq_cli.train | Start iterating over samples
2022-05-13 07:58:56 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-05-13 07:59:00 | INFO | valid | epoch 150 | valid on 'valid' subset | loss 5.642 | ppl 49.94 | wps 76710.7 | wpb 2041.1 | bsz 4 | num_updates 5850 | best_loss 5.639
2022-05-13 07:59:00 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 150 @ 5850 updates
2022-05-13 07:59:00 | INFO | fairseq_cli.train | end of epoch 150 (average epoch stats below)
2022-05-13 07:59:00 | INFO | train | epoch 150 | loss 4.791 | ppl 27.68 | wps 25981.6 | ups 0.2 | wpb 130930 | bsz 255.7 | num_updates 5850 | lr 8.26898e-05 | gnorm 0.807 | train_wall 181 | gb_free 7.9 | wall 29450
2022-05-13 07:59:00 | INFO | fairseq.data.iterators | grouped total_num_itrs = 39
2022-05-13 07:59:00 | INFO | fairseq.trainer | begin training epoch 151
2022-05-13 07:59:00 | INFO | fairseq_cli.train | Start iterating over samples
2022-05-13 08:02:07 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-05-13 08:02:11 | INFO | valid | epoch 151 | valid on 'valid' subset | loss 5.645 | ppl 50.05 | wps 76527.6 | wpb 2041.1 | bsz 4 | num_updates 5889 | best_loss 5.639
2022-05-13 08:02:11 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 151 @ 5889 updates
2022-05-13 08:02:11 | INFO | fairseq_cli.train | end of epoch 151 (average epoch stats below)
2022-05-13 08:02:11 | INFO | train | epoch 151 | loss 4.778 | ppl 27.44 | wps 26717.2 | ups 0.2 | wpb 130930 | bsz 255.7 | num_updates 5889 | lr 8.24156e-05 | gnorm 0.82 | train_wall 178 | gb_free 7.9 | wall 29641
2022-05-13 08:02:11 | INFO | fairseq.data.iterators | grouped total_num_itrs = 39
2022-05-13 08:02:11 | INFO | fairseq.trainer | begin training epoch 152
2022-05-13 08:02:11 | INFO | fairseq_cli.train | Start iterating over samples
2022-05-13 08:03:04 | INFO | train_inner | epoch 152:     11 / 39 loss=4.784, ppl=27.56, wps=26261.6, ups=0.2, wpb=130916, bsz=255.7, num_updates=5900, lr=8.23387e-05, gnorm=0.815, train_wall=459, gb_free=7.9, wall=29694
2022-05-13 08:05:20 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-05-13 08:05:24 | INFO | valid | epoch 152 | valid on 'valid' subset | loss 5.639 | ppl 49.82 | wps 76699.3 | wpb 2041.1 | bsz 4 | num_updates 5928 | best_loss 5.639
2022-05-13 08:05:24 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 152 @ 5928 updates
2022-05-13 08:05:24 | INFO | fairseq.trainer | Saving checkpoint to /cluster/work/cotterell/liam/master-thesis/checkpoints/language_model/standard/checkpoint_best_standard_64_1.0000E-04.pt
2022-05-13 08:05:25 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/work/cotterell/liam/master-thesis/checkpoints/language_model/standard/checkpoint_best_standard_64_1.0000E-04.pt
2022-05-13 08:05:25 | INFO | fairseq.checkpoint_utils | Saved checkpoint checkpoints/language_model/standard/checkpoint_best_standard_64_1.0000E-04.pt (epoch 152 @ 5928 updates, score 5.639) (writing took 1.1471111658029258 seconds)
2022-05-13 08:05:25 | INFO | fairseq_cli.train | end of epoch 152 (average epoch stats below)
2022-05-13 08:05:25 | INFO | train | epoch 152 | loss 4.765 | ppl 27.18 | wps 26380.8 | ups 0.2 | wpb 130930 | bsz 255.7 | num_updates 5928 | lr 8.2144e-05 | gnorm 0.793 | train_wall 180 | gb_free 7.9 | wall 29835
2022-05-13 08:05:25 | INFO | fairseq.data.iterators | grouped total_num_itrs = 39
2022-05-13 08:05:25 | INFO | fairseq.trainer | begin training epoch 153
2022-05-13 08:05:25 | INFO | fairseq_cli.train | Start iterating over samples
2022-05-13 08:08:34 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-05-13 08:08:38 | INFO | valid | epoch 153 | valid on 'valid' subset | loss 5.64 | ppl 49.87 | wps 76530.6 | wpb 2041.1 | bsz 4 | num_updates 5967 | best_loss 5.639
2022-05-13 08:08:38 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 153 @ 5967 updates
2022-05-13 08:08:38 | INFO | fairseq_cli.train | end of epoch 153 (average epoch stats below)
2022-05-13 08:08:38 | INFO | train | epoch 153 | loss 4.753 | ppl 26.97 | wps 26487 | ups 0.2 | wpb 130930 | bsz 255.7 | num_updates 5967 | lr 8.18751e-05 | gnorm 0.853 | train_wall 180 | gb_free 7.9 | wall 30027
2022-05-13 08:08:38 | INFO | fairseq.data.iterators | grouped total_num_itrs = 39
2022-05-13 08:08:38 | INFO | fairseq.trainer | begin training epoch 154
2022-05-13 08:08:38 | INFO | fairseq_cli.train | Start iterating over samples
2022-05-13 08:11:21 | INFO | train_inner | epoch 154:     33 / 39 loss=4.752, ppl=26.95, wps=26333.6, ups=0.2, wpb=130961, bsz=255.8, num_updates=6000, lr=8.16497e-05, gnorm=0.817, train_wall=463, gb_free=7.9, wall=30191
2022-05-13 08:11:50 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-05-13 08:11:54 | INFO | valid | epoch 154 | valid on 'valid' subset | loss 5.636 | ppl 49.73 | wps 76667.4 | wpb 2041.1 | bsz 4 | num_updates 6006 | best_loss 5.636
2022-05-13 08:11:54 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 154 @ 6006 updates
2022-05-13 08:11:54 | INFO | fairseq.trainer | Saving checkpoint to /cluster/work/cotterell/liam/master-thesis/checkpoints/language_model/standard/checkpoint_best_standard_64_1.0000E-04.pt
2022-05-13 08:11:55 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/work/cotterell/liam/master-thesis/checkpoints/language_model/standard/checkpoint_best_standard_64_1.0000E-04.pt
2022-05-13 08:11:55 | INFO | fairseq.checkpoint_utils | Saved checkpoint checkpoints/language_model/standard/checkpoint_best_standard_64_1.0000E-04.pt (epoch 154 @ 6006 updates, score 5.636) (writing took 1.3287767749279737 seconds)
2022-05-13 08:11:55 | INFO | fairseq_cli.train | end of epoch 154 (average epoch stats below)
2022-05-13 08:11:55 | INFO | train | epoch 154 | loss 4.739 | ppl 26.71 | wps 25809.2 | ups 0.2 | wpb 130930 | bsz 255.7 | num_updates 6006 | lr 8.16089e-05 | gnorm 0.807 | train_wall 181 | gb_free 7.9 | wall 30225
2022-05-13 08:11:55 | INFO | fairseq.data.iterators | grouped total_num_itrs = 39
2022-05-13 08:11:55 | INFO | fairseq.trainer | begin training epoch 155
2022-05-13 08:11:55 | INFO | fairseq_cli.train | Start iterating over samples
2022-05-13 08:15:16 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-05-13 08:15:20 | INFO | valid | epoch 155 | valid on 'valid' subset | loss 5.64 | ppl 49.85 | wps 76445.8 | wpb 2041.1 | bsz 4 | num_updates 6045 | best_loss 5.636
2022-05-13 08:15:20 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 155 @ 6045 updates
2022-05-13 08:15:20 | INFO | fairseq_cli.train | end of epoch 155 (average epoch stats below)
2022-05-13 08:15:20 | INFO | train | epoch 155 | loss 4.728 | ppl 26.5 | wps 24930.6 | ups 0.19 | wpb 130930 | bsz 255.7 | num_updates 6045 | lr 8.13452e-05 | gnorm 0.832 | train_wall 187 | gb_free 7.9 | wall 30430
2022-05-13 08:15:20 | INFO | fairseq.data.iterators | grouped total_num_itrs = 39
2022-05-13 08:15:20 | INFO | fairseq.trainer | begin training epoch 156
2022-05-13 08:15:20 | INFO | fairseq_cli.train | Start iterating over samples
2022-05-13 08:18:31 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-05-13 08:18:35 | INFO | valid | epoch 156 | valid on 'valid' subset | loss 5.64 | ppl 49.87 | wps 76658.5 | wpb 2041.1 | bsz 4 | num_updates 6084 | best_loss 5.636
2022-05-13 08:18:35 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 156 @ 6084 updates
2022-05-13 08:18:35 | INFO | fairseq_cli.train | end of epoch 156 (average epoch stats below)
2022-05-13 08:18:35 | INFO | train | epoch 156 | loss 4.715 | ppl 26.26 | wps 26282.5 | ups 0.2 | wpb 130930 | bsz 255.7 | num_updates 6084 | lr 8.1084e-05 | gnorm 0.806 | train_wall 181 | gb_free 7.9 | wall 30624
2022-05-13 08:18:35 | INFO | fairseq.data.iterators | grouped total_num_itrs = 39
2022-05-13 08:18:35 | INFO | fairseq.trainer | begin training epoch 157
2022-05-13 08:18:35 | INFO | fairseq_cli.train | Start iterating over samples
2022-05-13 08:19:51 | INFO | train_inner | epoch 157:     16 / 39 loss=4.718, ppl=26.31, wps=25677.9, ups=0.2, wpb=130910, bsz=255.7, num_updates=6100, lr=8.09776e-05, gnorm=0.809, train_wall=468, gb_free=7.9, wall=30701
2022-05-13 08:21:47 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-05-13 08:21:50 | INFO | valid | epoch 157 | valid on 'valid' subset | loss 5.641 | ppl 49.91 | wps 76598.3 | wpb 2041.1 | bsz 4 | num_updates 6123 | best_loss 5.636
2022-05-13 08:21:50 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 157 @ 6123 updates
2022-05-13 08:21:50 | INFO | fairseq_cli.train | end of epoch 157 (average epoch stats below)
2022-05-13 08:21:50 | INFO | train | epoch 157 | loss 4.702 | ppl 26.03 | wps 26060.6 | ups 0.2 | wpb 130930 | bsz 255.7 | num_updates 6123 | lr 8.08254e-05 | gnorm 0.787 | train_wall 182 | gb_free 7.9 | wall 30820
2022-05-13 08:21:50 | INFO | fairseq.data.iterators | grouped total_num_itrs = 39
2022-05-13 08:21:50 | INFO | fairseq.trainer | begin training epoch 158
2022-05-13 08:21:50 | INFO | fairseq_cli.train | Start iterating over samples
2022-05-13 08:24:58 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-05-13 08:25:02 | INFO | valid | epoch 158 | valid on 'valid' subset | loss 5.631 | ppl 49.57 | wps 76557 | wpb 2041.1 | bsz 4 | num_updates 6162 | best_loss 5.631
2022-05-13 08:25:02 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 158 @ 6162 updates
2022-05-13 08:25:02 | INFO | fairseq.trainer | Saving checkpoint to /cluster/work/cotterell/liam/master-thesis/checkpoints/language_model/standard/checkpoint_best_standard_64_1.0000E-04.pt
2022-05-13 08:25:03 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/work/cotterell/liam/master-thesis/checkpoints/language_model/standard/checkpoint_best_standard_64_1.0000E-04.pt
2022-05-13 08:25:03 | INFO | fairseq.checkpoint_utils | Saved checkpoint checkpoints/language_model/standard/checkpoint_best_standard_64_1.0000E-04.pt (epoch 158 @ 6162 updates, score 5.631) (writing took 1.3662049719132483 seconds)
2022-05-13 08:25:03 | INFO | fairseq_cli.train | end of epoch 158 (average epoch stats below)
2022-05-13 08:25:03 | INFO | train | epoch 158 | loss 4.69 | ppl 25.81 | wps 26503.1 | ups 0.2 | wpb 130930 | bsz 255.7 | num_updates 6162 | lr 8.05692e-05 | gnorm 0.822 | train_wall 178 | gb_free 7.9 | wall 31013
2022-05-13 08:25:03 | INFO | fairseq.data.iterators | grouped total_num_itrs = 39
2022-05-13 08:25:03 | INFO | fairseq.trainer | begin training epoch 159
2022-05-13 08:25:03 | INFO | fairseq_cli.train | Start iterating over samples
2022-05-13 08:28:06 | INFO | train_inner | epoch 159:     38 / 39 loss=4.69, ppl=25.8, wps=26477.3, ups=0.2, wpb=130946, bsz=255.8, num_updates=6200, lr=8.03219e-05, gnorm=0.821, train_wall=461, gb_free=7.9, wall=31196
2022-05-13 08:28:10 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-05-13 08:28:14 | INFO | valid | epoch 159 | valid on 'valid' subset | loss 5.637 | ppl 49.76 | wps 76358.5 | wpb 2041.1 | bsz 4 | num_updates 6201 | best_loss 5.631
2022-05-13 08:28:14 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 159 @ 6201 updates
2022-05-13 08:28:14 | INFO | fairseq_cli.train | end of epoch 159 (average epoch stats below)
2022-05-13 08:28:14 | INFO | train | epoch 159 | loss 4.678 | ppl 25.6 | wps 26711 | ups 0.2 | wpb 130930 | bsz 255.7 | num_updates 6201 | lr 8.03155e-05 | gnorm 0.831 | train_wall 178 | gb_free 7.9 | wall 31204
2022-05-13 08:28:14 | INFO | fairseq.data.iterators | grouped total_num_itrs = 39
2022-05-13 08:28:14 | INFO | fairseq.trainer | begin training epoch 160
2022-05-13 08:28:14 | INFO | fairseq_cli.train | Start iterating over samples
2022-05-13 08:31:28 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-05-13 08:31:32 | INFO | valid | epoch 160 | valid on 'valid' subset | loss 5.635 | ppl 49.68 | wps 76726.2 | wpb 2041.1 | bsz 4 | num_updates 6240 | best_loss 5.631
2022-05-13 08:31:32 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 160 @ 6240 updates
2022-05-13 08:31:32 | INFO | fairseq_cli.train | end of epoch 160 (average epoch stats below)
2022-05-13 08:31:32 | INFO | train | epoch 160 | loss 4.667 | ppl 25.4 | wps 25876.4 | ups 0.2 | wpb 130930 | bsz 255.7 | num_updates 6240 | lr 8.00641e-05 | gnorm 0.816 | train_wall 182 | gb_free 7.9 | wall 31401
2022-05-13 08:31:32 | INFO | fairseq.data.iterators | grouped total_num_itrs = 39
2022-05-13 08:31:32 | INFO | fairseq.trainer | begin training epoch 161
2022-05-13 08:31:32 | INFO | fairseq_cli.train | Start iterating over samples
2022-05-13 08:34:39 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-05-13 08:34:43 | INFO | valid | epoch 161 | valid on 'valid' subset | loss 5.631 | ppl 49.57 | wps 76646.7 | wpb 2041.1 | bsz 4 | num_updates 6279 | best_loss 5.631
2022-05-13 08:34:43 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 161 @ 6279 updates
2022-05-13 08:34:43 | INFO | fairseq.trainer | Saving checkpoint to /cluster/work/cotterell/liam/master-thesis/checkpoints/language_model/standard/checkpoint_best_standard_64_1.0000E-04.pt
2022-05-13 08:34:44 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/work/cotterell/liam/master-thesis/checkpoints/language_model/standard/checkpoint_best_standard_64_1.0000E-04.pt
2022-05-13 08:34:44 | INFO | fairseq.checkpoint_utils | Saved checkpoint checkpoints/language_model/standard/checkpoint_best_standard_64_1.0000E-04.pt (epoch 161 @ 6279 updates, score 5.631) (writing took 1.3439842220395803 seconds)
2022-05-13 08:34:44 | INFO | fairseq_cli.train | end of epoch 161 (average epoch stats below)
2022-05-13 08:34:44 | INFO | train | epoch 161 | loss 4.653 | ppl 25.17 | wps 26500.3 | ups 0.2 | wpb 130930 | bsz 255.7 | num_updates 6279 | lr 7.9815e-05 | gnorm 0.795 | train_wall 179 | gb_free 7.9 | wall 31594
2022-05-13 08:34:44 | INFO | fairseq.data.iterators | grouped total_num_itrs = 39
2022-05-13 08:34:44 | INFO | fairseq.trainer | begin training epoch 162
2022-05-13 08:34:44 | INFO | fairseq_cli.train | Start iterating over samples
2022-05-13 08:36:26 | INFO | train_inner | epoch 162:     21 / 39 loss=4.656, ppl=25.21, wps=26192.6, ups=0.2, wpb=130905, bsz=255.7, num_updates=6300, lr=7.96819e-05, gnorm=0.826, train_wall=461, gb_free=7.9, wall=31695
2022-05-13 08:37:52 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-05-13 08:37:56 | INFO | valid | epoch 162 | valid on 'valid' subset | loss 5.635 | ppl 49.7 | wps 76601.6 | wpb 2041.1 | bsz 4 | num_updates 6318 | best_loss 5.631
2022-05-13 08:37:56 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 162 @ 6318 updates
2022-05-13 08:37:56 | INFO | fairseq_cli.train | end of epoch 162 (average epoch stats below)
2022-05-13 08:37:56 | INFO | train | epoch 162 | loss 4.642 | ppl 24.97 | wps 26679.8 | ups 0.2 | wpb 130930 | bsz 255.7 | num_updates 6318 | lr 7.95683e-05 | gnorm 0.834 | train_wall 178 | gb_free 7.9 | wall 31786
2022-05-13 08:37:56 | INFO | fairseq.data.iterators | grouped total_num_itrs = 39
2022-05-13 08:37:56 | INFO | fairseq.trainer | begin training epoch 163
2022-05-13 08:37:56 | INFO | fairseq_cli.train | Start iterating over samples
2022-05-13 08:41:11 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-05-13 08:41:15 | INFO | valid | epoch 163 | valid on 'valid' subset | loss 5.636 | ppl 49.74 | wps 76573.2 | wpb 2041.1 | bsz 4 | num_updates 6357 | best_loss 5.631
2022-05-13 08:41:15 | INFO | fairseq_cli.train | early stop since valid performance hasn't improved for last 5 runs
2022-05-13 08:41:15 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 163 @ 6357 updates
2022-05-13 08:41:15 | INFO | fairseq_cli.train | end of epoch 163 (average epoch stats below)
2022-05-13 08:41:15 | INFO | train | epoch 163 | loss 4.631 | ppl 24.78 | wps 25637.4 | ups 0.2 | wpb 130930 | bsz 255.7 | num_updates 6357 | lr 7.93239e-05 | gnorm 0.848 | train_wall 183 | gb_free 7.9 | wall 31985
2022-05-13 08:41:15 | INFO | fairseq_cli.train | done training in 31972.6 seconds
