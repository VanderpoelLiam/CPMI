Sender: LSF System <lsfadmin@eu-g3-006>
Subject: Job 217553274: <train_lang_standard_16_5.0000E-04_full> in cluster <euler> Exited

Job <train_lang_standard_16_5.0000E-04_full> was submitted from host <eu-login-05> by user <euler_username> in cluster <euler> at Fri May  6 16:26:18 2022
Job was executed on host(s) <4*eu-g3-006>, in queue <gpu.24h>, as user <euler_username> in cluster <euler> at Fri May  6 16:26:42 2022
</cluster/home/euler_username> was used as the home directory.
</cluster/work/cotterell/liam/master-thesis> was used as the working directory.
Started at Fri May  6 16:26:42 2022
Terminated at Sat May  7 12:27:08 2022
Results reported at Sat May  7 12:27:08 2022

Your job looked like:

------------------------------------------------------------
# LSBATCH: User input
fairseq-train data/xsum-lang-full --save-dir checkpoints/language_model/standard --update-freq 16 --lr 0.0005 --checkpoint-suffix _standard_16_5.0000E-04_full --task language_modeling --arch transformer_lm --share-decoder-input-output-embed --dropout 0.1 --optimizer adam --adam-betas "(0.9, 0.98)" --weight-decay 0.01 --clip-norm 0.0 --lr-scheduler inverse_sqrt --warmup-updates 4000 --warmup-init-lr 1e-07 --tokens-per-sample 512 --sample-break-mode none --max-tokens 2048 --no-epoch-checkpoints --no-last-checkpoints --patience 5
------------------------------------------------------------

TERM_RUNLIMIT: job killed after reaching LSF run time limit.
Exited with exit code 140.

Resource usage summary:

    CPU time :                                   74108.00 sec.
    Max Memory :                                 4356 MB
    Average Memory :                             2882.01 MB
    Total Requested Memory :                     8192.00 MB
    Delta Memory :                               3836.00 MB
    Max Swap :                                   -
    Max Processes :                              4
    Max Threads :                                14
    Run time :                                   72025 sec.
    Turnaround time :                            72050 sec.

The output (if any) follows:

2022-05-06 16:28:22 | INFO | fairseq.tasks.text_to_speech | Please install tensorboardX: pip install tensorboardX
2022-05-06 16:28:26 | INFO | fairseq_cli.train | {'_name': None, 'common': {'_name': None, 'no_progress_bar': False, 'log_interval': 100, 'log_format': None, 'log_file': None, 'tensorboard_logdir': None, 'wandb_project': None, 'azureml_logging': False, 'seed': 1, 'cpu': False, 'tpu': False, 'bf16': False, 'memory_efficient_bf16': False, 'fp16': False, 'memory_efficient_fp16': False, 'fp16_no_flatten_grads': False, 'fp16_init_scale': 128, 'fp16_scale_window': None, 'fp16_scale_tolerance': 0.0, 'on_cpu_convert_precision': False, 'min_loss_scale': 0.0001, 'threshold_loss_scale': None, 'amp': False, 'amp_batch_retries': 2, 'amp_init_scale': 128, 'amp_scale_window': None, 'user_dir': None, 'empty_cache_freq': 0, 'all_gather_list_size': 16384, 'model_parallel_size': 1, 'quantization_config_path': None, 'profile': False, 'reset_logging': False, 'suppress_crashes': False, 'use_plasma_view': False, 'plasma_path': '/tmp/plasma'}, 'common_eval': {'_name': None, 'path': None, 'post_process': None, 'quiet': False, 'model_overrides': '{}', 'results_path': None, 'print_tokens': False}, 'distributed_training': {'_name': None, 'distributed_world_size': 1, 'distributed_num_procs': 1, 'distributed_rank': 0, 'distributed_backend': 'nccl', 'distributed_init_method': None, 'distributed_port': -1, 'device_id': 0, 'distributed_no_spawn': False, 'ddp_backend': 'pytorch_ddp', 'ddp_comm_hook': 'none', 'bucket_cap_mb': 25, 'fix_batches_to_gpus': False, 'find_unused_parameters': False, 'gradient_as_bucket_view': False, 'fast_stat_sync': False, 'heartbeat_timeout': -1, 'broadcast_buffers': False, 'slowmo_momentum': None, 'slowmo_base_algorithm': 'localsgd', 'localsgd_frequency': 3, 'nprocs_per_node': 1, 'pipeline_model_parallel': False, 'pipeline_balance': None, 'pipeline_devices': None, 'pipeline_chunks': 0, 'pipeline_encoder_balance': None, 'pipeline_encoder_devices': None, 'pipeline_decoder_balance': None, 'pipeline_decoder_devices': None, 'pipeline_checkpoint': 'never', 'zero_sharding': 'none', 'fp16': False, 'memory_efficient_fp16': False, 'tpu': False, 'no_reshard_after_forward': False, 'fp32_reduce_scatter': False, 'cpu_offload': False, 'use_sharded_state': False, 'not_fsdp_flatten_parameters': False}, 'dataset': {'_name': None, 'num_workers': 1, 'skip_invalid_size_inputs_valid_test': False, 'max_tokens': 2048, 'batch_size': None, 'required_batch_size_multiple': 8, 'required_seq_len_multiple': 1, 'dataset_impl': None, 'data_buffer_size': 10, 'train_subset': 'train', 'valid_subset': 'valid', 'combine_valid_subsets': None, 'ignore_unused_valid_subsets': False, 'validate_interval': 1, 'validate_interval_updates': 0, 'validate_after_updates': 0, 'fixed_validation_seed': None, 'disable_validation': False, 'max_tokens_valid': 2048, 'batch_size_valid': None, 'max_valid_steps': None, 'curriculum': 0, 'gen_subset': 'test', 'num_shards': 1, 'shard_id': 0, 'grouped_shuffling': False, 'update_epoch_batch_itr': False, 'update_ordered_indices_seed': False}, 'optimization': {'_name': None, 'max_epoch': 0, 'max_update': 0, 'stop_time_hours': 0.0, 'clip_norm': 0.0, 'sentence_avg': False, 'update_freq': [16], 'lr': [0.0005], 'stop_min_lr': -1.0, 'use_bmuf': False, 'skip_remainder_batch': False}, 'checkpoint': {'_name': None, 'save_dir': 'checkpoints/language_model/standard', 'restore_file': 'checkpoint_last.pt', 'continue_once': None, 'finetune_from_model': None, 'reset_dataloader': False, 'reset_lr_scheduler': False, 'reset_meters': False, 'reset_optimizer': False, 'optimizer_overrides': '{}', 'save_interval': 1, 'save_interval_updates': 0, 'keep_interval_updates': -1, 'keep_interval_updates_pattern': -1, 'keep_last_epochs': -1, 'keep_best_checkpoints': -1, 'no_save': False, 'no_epoch_checkpoints': True, 'no_last_checkpoints': True, 'no_save_optimizer_state': False, 'best_checkpoint_metric': 'loss', 'maximize_best_checkpoint_metric': False, 'patience': 5, 'checkpoint_suffix': '_standard_16_5.0000E-04_full', 'checkpoint_shard_count': 1, 'load_checkpoint_on_all_dp_ranks': False, 'write_checkpoints_asynchronously': False, 'model_parallel_size': 1}, 'bmuf': {'_name': None, 'block_lr': 1.0, 'block_momentum': 0.875, 'global_sync_iter': 50, 'warmup_iterations': 500, 'use_nbm': False, 'average_sync': False, 'distributed_world_size': 1}, 'generation': {'_name': None, 'beam': 5, 'nbest': 1, 'max_len_a': 0.0, 'max_len_b': 200, 'min_len': 1, 'match_source_len': False, 'unnormalized': False, 'no_early_stop': False, 'no_beamable_mm': False, 'lenpen': 1.0, 'unkpen': 0.0, 'replace_unk': None, 'sacrebleu': False, 'score_reference': False, 'prefix_size': 0, 'no_repeat_ngram_size': 0, 'sampling': False, 'sampling_topk': -1, 'sampling_topp': -1.0, 'constraints': None, 'temperature': 1.0, 'diverse_beam_groups': -1, 'diverse_beam_strength': 0.5, 'diversity_rate': -1.0, 'print_alignment': None, 'print_step': False, 'lm_path': None, 'ent_threshold': 0.0, 'lm_weight': 0.0, 'iter_decode_eos_penalty': 0.0, 'iter_decode_max_iter': 10, 'iter_decode_force_max_iter': False, 'iter_decode_with_beam': 1, 'iter_decode_with_external_reranker': False, 'retain_iter_history': False, 'retain_dropout': False, 'retain_dropout_modules': None, 'decoding_format': None, 'no_seed_provided': False}, 'eval_lm': {'_name': None, 'output_word_probs': False, 'output_word_stats': False, 'context_window': 0, 'softmax_batch': 9223372036854775807}, 'interactive': {'_name': None, 'buffer_size': 0, 'input': '-'}, 'model': {'_name': 'transformer_lm', 'activation_fn': relu, 'dropout': 0.1, 'attention_dropout': 0.0, 'activation_dropout': 0.0, 'relu_dropout': 0.0, 'decoder_embed_dim': 512, 'decoder_output_dim': 512, 'decoder_input_dim': 512, 'decoder_ffn_embed_dim': 2048, 'decoder_layers': 6, 'decoder_attention_heads': 8, 'decoder_normalize_before': False, 'no_decoder_final_norm': False, 'adaptive_softmax_cutoff': None, 'adaptive_softmax_dropout': 0.0, 'adaptive_softmax_factor': 4.0, 'no_token_positional_embeddings': False, 'share_decoder_input_output_embed': True, 'character_embeddings': False, 'character_filters': '[(1, 64), (2, 128), (3, 192), (4, 256), (5, 256), (6, 256), (7, 256)]', 'character_embedding_dim': 4, 'char_embedder_highway_layers': 2, 'adaptive_input': False, 'adaptive_input_factor': 4.0, 'adaptive_input_cutoff': None, 'tie_adaptive_weights': False, 'tie_adaptive_proj': False, 'decoder_learned_pos': False, 'layernorm_embedding': False, 'no_scale_embedding': False, 'checkpoint_activations': False, 'offload_activations': False, 'decoder_layerdrop': 0.0, 'decoder_layers_to_keep': None, 'quant_noise_pq': 0.0, 'quant_noise_pq_block_size': 8, 'quant_noise_scalar': 0.0, 'min_params_to_wrap': 100000000, 'base_layers': 0, 'base_sublayers': 1, 'base_shuffle': 1, 'scale_fc': False, 'scale_attn': False, 'scale_heads': False, 'scale_resids': False, 'add_bos_token': False, 'tokens_per_sample': 512, 'max_target_positions': None, 'tpu': False}, 'task': {'_name': 'language_modeling', 'data': 'data/xsum-lang-full', 'sample_break_mode': none, 'tokens_per_sample': 512, 'output_dictionary_size': -1, 'self_target': False, 'future_target': False, 'past_target': False, 'add_bos_token': False, 'max_target_positions': None, 'shorten_method': none, 'shorten_data_split_list': '', 'pad_to_fixed_length': False, 'pad_to_fixed_bsz': False, 'seed': 1, 'batch_size': None, 'batch_size_valid': None, 'dataset_impl': None, 'data_buffer_size': 10, 'tpu': False, 'use_plasma_view': False, 'plasma_path': '/tmp/plasma'}, 'criterion': {'_name': 'cross_entropy', 'sentence_avg': False}, 'optimizer': {'_name': 'adam', 'adam_betas': '(0.9, 0.98)', 'adam_eps': 1e-08, 'weight_decay': 0.01, 'use_old_adam': False, 'fp16_adam_stats': False, 'tpu': False, 'lr': [0.0005]}, 'lr_scheduler': {'_name': 'inverse_sqrt', 'warmup_updates': 4000, 'warmup_init_lr': 1e-07, 'lr': [0.0005]}, 'scoring': {'_name': 'bleu', 'pad': 1, 'eos': 2, 'unk': 3}, 'bpe': None, 'tokenizer': None, 'ema': {'_name': None, 'store_ema': False, 'ema_decay': 0.9999, 'ema_start_update': 0, 'ema_seed_model': None, 'ema_update_freq': 1, 'ema_fp32': False}, 'simul_type': None}
2022-05-06 16:28:26 | INFO | fairseq.tasks.language_modeling | dictionary: 49992 types
2022-05-06 16:28:28 | INFO | fairseq_cli.train | TransformerLanguageModel(
  (decoder): TransformerDecoder(
    (dropout_module): FairseqDropout()
    (embed_tokens): Embedding(49992, 512, padding_idx=1)
    (embed_positions): SinusoidalPositionalEmbedding()
    (layers): ModuleList(
      (0): TransformerDecoderLayerBase(
        (dropout_module): FairseqDropout()
        (self_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=512, out_features=512, bias=True)
          (v_proj): Linear(in_features=512, out_features=512, bias=True)
          (q_proj): Linear(in_features=512, out_features=512, bias=True)
          (out_proj): Linear(in_features=512, out_features=512, bias=True)
        )
        (activation_dropout_module): FairseqDropout()
        (self_attn_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
        (fc1): Linear(in_features=512, out_features=2048, bias=True)
        (fc2): Linear(in_features=2048, out_features=512, bias=True)
        (final_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
      )
      (1): TransformerDecoderLayerBase(
        (dropout_module): FairseqDropout()
        (self_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=512, out_features=512, bias=True)
          (v_proj): Linear(in_features=512, out_features=512, bias=True)
          (q_proj): Linear(in_features=512, out_features=512, bias=True)
          (out_proj): Linear(in_features=512, out_features=512, bias=True)
        )
        (activation_dropout_module): FairseqDropout()
        (self_attn_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
        (fc1): Linear(in_features=512, out_features=2048, bias=True)
        (fc2): Linear(in_features=2048, out_features=512, bias=True)
        (final_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
      )
      (2): TransformerDecoderLayerBase(
        (dropout_module): FairseqDropout()
        (self_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=512, out_features=512, bias=True)
          (v_proj): Linear(in_features=512, out_features=512, bias=True)
          (q_proj): Linear(in_features=512, out_features=512, bias=True)
          (out_proj): Linear(in_features=512, out_features=512, bias=True)
        )
        (activation_dropout_module): FairseqDropout()
        (self_attn_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
        (fc1): Linear(in_features=512, out_features=2048, bias=True)
        (fc2): Linear(in_features=2048, out_features=512, bias=True)
        (final_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
      )
      (3): TransformerDecoderLayerBase(
        (dropout_module): FairseqDropout()
        (self_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=512, out_features=512, bias=True)
          (v_proj): Linear(in_features=512, out_features=512, bias=True)
          (q_proj): Linear(in_features=512, out_features=512, bias=True)
          (out_proj): Linear(in_features=512, out_features=512, bias=True)
        )
        (activation_dropout_module): FairseqDropout()
        (self_attn_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
        (fc1): Linear(in_features=512, out_features=2048, bias=True)
        (fc2): Linear(in_features=2048, out_features=512, bias=True)
        (final_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
      )
      (4): TransformerDecoderLayerBase(
        (dropout_module): FairseqDropout()
        (self_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=512, out_features=512, bias=True)
          (v_proj): Linear(in_features=512, out_features=512, bias=True)
          (q_proj): Linear(in_features=512, out_features=512, bias=True)
          (out_proj): Linear(in_features=512, out_features=512, bias=True)
        )
        (activation_dropout_module): FairseqDropout()
        (self_attn_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
        (fc1): Linear(in_features=512, out_features=2048, bias=True)
        (fc2): Linear(in_features=2048, out_features=512, bias=True)
        (final_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
      )
      (5): TransformerDecoderLayerBase(
        (dropout_module): FairseqDropout()
        (self_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=512, out_features=512, bias=True)
          (v_proj): Linear(in_features=512, out_features=512, bias=True)
          (q_proj): Linear(in_features=512, out_features=512, bias=True)
          (out_proj): Linear(in_features=512, out_features=512, bias=True)
        )
        (activation_dropout_module): FairseqDropout()
        (self_attn_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
        (fc1): Linear(in_features=512, out_features=2048, bias=True)
        (fc2): Linear(in_features=2048, out_features=512, bias=True)
        (final_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
      )
    )
    (output_projection): Linear(in_features=512, out_features=49992, bias=False)
  )
)
2022-05-06 16:28:28 | INFO | fairseq_cli.train | task: LanguageModelingTask
2022-05-06 16:28:28 | INFO | fairseq_cli.train | model: TransformerLanguageModel
2022-05-06 16:28:28 | INFO | fairseq_cli.train | criterion: CrossEntropyCriterion
2022-05-06 16:28:28 | INFO | fairseq_cli.train | num. shared model params: 44,510,208 (num. trained: 44,510,208)
2022-05-06 16:28:28 | INFO | fairseq_cli.train | num. expert model params: 0 (num. trained: 0)
2022-05-06 16:28:28 | INFO | fairseq.data.data_utils | loaded 22,664 examples from: data/xsum-lang-full/valid
2022-05-06 16:29:13 | INFO | fairseq.trainer | detected shared parameter: decoder.embed_tokens.weight <- decoder.output_projection.weight
2022-05-06 16:29:13 | INFO | fairseq.utils | ***********************CUDA enviroments for all 1 workers***********************
2022-05-06 16:29:13 | INFO | fairseq.utils | rank   0: capabilities =  7.5  ; total memory = 10.761 GB ; name = NVIDIA GeForce RTX 2080 Ti
2022-05-06 16:29:13 | INFO | fairseq.utils | ***********************CUDA enviroments for all 1 workers***********************
2022-05-06 16:29:13 | INFO | fairseq_cli.train | training on 1 devices (GPUs/TPUs)
2022-05-06 16:29:13 | INFO | fairseq_cli.train | max tokens per device = 2048 and max sentences per device = None
2022-05-06 16:29:13 | INFO | fairseq.trainer | Preparing to load checkpoint checkpoints/language_model/standard/checkpoint_last_standard_16_5.0000E-04_full.pt
2022-05-06 16:29:13 | INFO | fairseq.trainer | No existing checkpoint found checkpoints/language_model/standard/checkpoint_last_standard_16_5.0000E-04_full.pt
2022-05-06 16:29:13 | INFO | fairseq.trainer | loading train data for epoch 1
2022-05-06 16:29:13 | INFO | fairseq.data.data_utils | loaded 408,090 examples from: data/xsum-lang-full/train
2022-05-06 16:29:14 | INFO | fairseq.trainer | NOTE: your device may support faster training with --fp16 or --amp
2022-05-06 16:29:14 | INFO | fairseq.data.iterators | grouped total_num_itrs = 2955
2022-05-06 16:29:14 | INFO | fairseq.trainer | begin training epoch 1
2022-05-06 16:29:14 | INFO | fairseq_cli.train | Start iterating over samples
2022-05-06 16:31:13 | INFO | train_inner | epoch 001:    100 / 2955 loss=14.712, ppl=26838.8, wps=29052.6, ups=0.89, wpb=32768, bsz=64, num_updates=100, lr=1.25975e-05, gnorm=3.227, train_wall=116, gb_free=7.9, wall=120
2022-05-06 16:33:07 | INFO | train_inner | epoch 001:    200 / 2955 loss=12.63, ppl=6340.22, wps=28815.1, ups=0.88, wpb=32763, bsz=64, num_updates=200, lr=2.5095e-05, gnorm=1.121, train_wall=112, gb_free=7.9, wall=234
2022-05-06 16:35:00 | INFO | train_inner | epoch 001:    300 / 2955 loss=11.212, ppl=2372.68, wps=28860.4, ups=0.88, wpb=32768, bsz=64, num_updates=300, lr=3.75925e-05, gnorm=0.733, train_wall=112, gb_free=7.9, wall=348
2022-05-06 16:36:54 | INFO | train_inner | epoch 001:    400 / 2955 loss=10.265, ppl=1230.1, wps=28854.4, ups=0.88, wpb=32768, bsz=64, num_updates=400, lr=5.009e-05, gnorm=0.575, train_wall=112, gb_free=7.9, wall=461
2022-05-06 16:38:50 | INFO | train_inner | epoch 001:    500 / 2955 loss=9.816, ppl=901.59, wps=28251.8, ups=0.86, wpb=32768, bsz=64, num_updates=500, lr=6.25875e-05, gnorm=0.611, train_wall=114, gb_free=7.9, wall=577
2022-05-06 16:41:07 | INFO | train_inner | epoch 001:    600 / 2955 loss=9.493, ppl=720.82, wps=23856.4, ups=0.73, wpb=32768, bsz=64, num_updates=600, lr=7.5085e-05, gnorm=0.589, train_wall=114, gb_free=7.9, wall=715
2022-05-06 16:43:22 | INFO | train_inner | epoch 001:    700 / 2955 loss=9.195, ppl=585.9, wps=24353.2, ups=0.74, wpb=32768, bsz=64, num_updates=700, lr=8.75825e-05, gnorm=0.672, train_wall=112, gb_free=7.9, wall=849
2022-05-06 16:45:32 | INFO | train_inner | epoch 001:    800 / 2955 loss=8.984, ppl=506.32, wps=25139.2, ups=0.77, wpb=32768, bsz=64, num_updates=800, lr=0.00010008, gnorm=0.659, train_wall=112, gb_free=7.9, wall=980
2022-05-06 16:47:36 | INFO | train_inner | epoch 001:    900 / 2955 loss=8.786, ppl=441.35, wps=26395.2, ups=0.81, wpb=32768, bsz=64, num_updates=900, lr=0.000112578, gnorm=0.705, train_wall=112, gb_free=7.9, wall=1104
2022-05-06 16:49:38 | INFO | train_inner | epoch 001:   1000 / 2955 loss=8.598, ppl=387.53, wps=26842.2, ups=0.82, wpb=32768, bsz=64, num_updates=1000, lr=0.000125075, gnorm=0.81, train_wall=114, gb_free=7.9, wall=1226
2022-05-06 16:51:40 | INFO | train_inner | epoch 001:   1100 / 2955 loss=8.442, ppl=347.81, wps=26940.1, ups=0.82, wpb=32768, bsz=64, num_updates=1100, lr=0.000137573, gnorm=0.828, train_wall=115, gb_free=7.9, wall=1347
2022-05-06 16:53:38 | INFO | train_inner | epoch 001:   1200 / 2955 loss=8.281, ppl=310.95, wps=27751.3, ups=0.85, wpb=32768, bsz=64, num_updates=1200, lr=0.00015007, gnorm=0.786, train_wall=112, gb_free=7.9, wall=1465
2022-05-06 16:55:38 | INFO | train_inner | epoch 001:   1300 / 2955 loss=8.144, ppl=282.86, wps=27322.3, ups=0.83, wpb=32757.8, bsz=64, num_updates=1300, lr=0.000162568, gnorm=0.838, train_wall=113, gb_free=7.9, wall=1585
2022-05-06 16:57:35 | INFO | train_inner | epoch 001:   1400 / 2955 loss=8.005, ppl=256.92, wps=28112.2, ups=0.86, wpb=32768, bsz=64, num_updates=1400, lr=0.000175065, gnorm=0.819, train_wall=112, gb_free=7.9, wall=1702
2022-05-06 16:59:31 | INFO | train_inner | epoch 001:   1500 / 2955 loss=7.879, ppl=235.48, wps=28169.1, ups=0.86, wpb=32768, bsz=64, num_updates=1500, lr=0.000187563, gnorm=0.835, train_wall=113, gb_free=7.9, wall=1818
2022-05-06 17:01:36 | INFO | train_inner | epoch 001:   1600 / 2955 loss=7.78, ppl=219.79, wps=26300.4, ups=0.8, wpb=32768, bsz=64, num_updates=1600, lr=0.00020006, gnorm=0.829, train_wall=120, gb_free=7.9, wall=1943
2022-05-06 17:03:30 | INFO | train_inner | epoch 001:   1700 / 2955 loss=7.67, ppl=203.64, wps=28645.8, ups=0.87, wpb=32768, bsz=64, num_updates=1700, lr=0.000212558, gnorm=0.815, train_wall=112, gb_free=7.9, wall=2057
2022-05-06 17:05:24 | INFO | train_inner | epoch 001:   1800 / 2955 loss=7.562, ppl=188.97, wps=28622.4, ups=0.87, wpb=32768, bsz=64, num_updates=1800, lr=0.000225055, gnorm=0.816, train_wall=112, gb_free=7.9, wall=2172
2022-05-06 17:07:19 | INFO | train_inner | epoch 001:   1900 / 2955 loss=7.471, ppl=177.37, wps=28689.1, ups=0.88, wpb=32768, bsz=64, num_updates=1900, lr=0.000237553, gnorm=0.815, train_wall=112, gb_free=7.9, wall=2286
2022-05-06 17:09:13 | INFO | train_inner | epoch 001:   2000 / 2955 loss=7.365, ppl=164.85, wps=28665.5, ups=0.87, wpb=32768, bsz=64, num_updates=2000, lr=0.00025005, gnorm=0.809, train_wall=112, gb_free=7.9, wall=2400
2022-05-06 17:11:12 | INFO | train_inner | epoch 001:   2100 / 2955 loss=7.267, ppl=154.07, wps=27631.2, ups=0.84, wpb=32768, bsz=64, num_updates=2100, lr=0.000262548, gnorm=0.813, train_wall=114, gb_free=7.9, wall=2519
2022-05-06 17:13:07 | INFO | train_inner | epoch 001:   2200 / 2955 loss=7.199, ppl=146.96, wps=28309.8, ups=0.86, wpb=32768, bsz=64, num_updates=2200, lr=0.000275045, gnorm=0.792, train_wall=112, gb_free=7.9, wall=2635
2022-05-06 17:15:12 | INFO | train_inner | epoch 001:   2300 / 2955 loss=7.114, ppl=138.5, wps=26324.7, ups=0.8, wpb=32768, bsz=64, num_updates=2300, lr=0.000287543, gnorm=0.775, train_wall=120, gb_free=7.9, wall=2759
2022-05-06 17:17:06 | INFO | train_inner | epoch 001:   2400 / 2955 loss=7.033, ppl=130.95, wps=28680, ups=0.88, wpb=32768, bsz=64, num_updates=2400, lr=0.00030004, gnorm=0.75, train_wall=112, gb_free=7.9, wall=2873
2022-05-06 17:19:00 | INFO | train_inner | epoch 001:   2500 / 2955 loss=6.96, ppl=124.49, wps=28723.3, ups=0.88, wpb=32768, bsz=64, num_updates=2500, lr=0.000312538, gnorm=0.748, train_wall=112, gb_free=7.9, wall=2987
2022-05-06 17:20:58 | INFO | train_inner | epoch 001:   2600 / 2955 loss=6.891, ppl=118.66, wps=27836.2, ups=0.85, wpb=32768, bsz=64, num_updates=2600, lr=0.000325035, gnorm=0.728, train_wall=115, gb_free=7.9, wall=3105
2022-05-06 17:22:52 | INFO | train_inner | epoch 001:   2700 / 2955 loss=6.819, ppl=112.89, wps=28607.2, ups=0.87, wpb=32768, bsz=64, num_updates=2700, lr=0.000337533, gnorm=0.72, train_wall=112, gb_free=7.9, wall=3220
2022-05-06 17:24:47 | INFO | train_inner | epoch 001:   2800 / 2955 loss=6.766, ppl=108.8, wps=28526.6, ups=0.87, wpb=32768, bsz=64, num_updates=2800, lr=0.00035003, gnorm=0.707, train_wall=113, gb_free=7.9, wall=3335
2022-05-06 17:26:41 | INFO | train_inner | epoch 001:   2900 / 2955 loss=6.71, ppl=104.72, wps=28773.9, ups=0.88, wpb=32768, bsz=64, num_updates=2900, lr=0.000362528, gnorm=0.706, train_wall=112, gb_free=7.9, wall=3448
2022-05-06 17:27:44 | INFO | fairseq_cli.train | begin validation on "valid" subset
/cluster/work/cotterell/liam/fairseq/fairseq/utils.py:374: UserWarning: amp_C fused kernels unavailable, disabling multi_tensor_l2norm; you may get better performance by installing NVIDIA's apex library
  warnings.warn(
2022-05-06 17:28:56 | INFO | valid | epoch 001 | valid on 'valid' subset | loss 6.514 | ppl 91.41 | wps 75056.5 | wpb 2047.4 | bsz 4 | num_updates 2955
2022-05-06 17:28:56 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 1 @ 2955 updates
2022-05-06 17:28:56 | INFO | fairseq.trainer | Saving checkpoint to /cluster/work/cotterell/liam/master-thesis/checkpoints/language_model/standard/checkpoint_best_standard_16_5.0000E-04_full.pt
2022-05-06 17:28:57 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/work/cotterell/liam/master-thesis/checkpoints/language_model/standard/checkpoint_best_standard_16_5.0000E-04_full.pt
2022-05-06 17:28:57 | INFO | fairseq.checkpoint_utils | Saved checkpoint checkpoints/language_model/standard/checkpoint_best_standard_16_5.0000E-04_full.pt (epoch 1 @ 2955 updates, score 6.514) (writing took 1.6431491211988032 seconds)
2022-05-06 17:28:57 | INFO | fairseq_cli.train | end of epoch 1 (average epoch stats below)
2022-05-06 17:28:57 | INFO | train | epoch 001 | loss 8.416 | ppl 341.63 | wps 27066.1 | ups 0.83 | wpb 32764.7 | bsz 64 | num_updates 2955 | lr 0.000369401 | gnorm 0.846 | train_wall 3353 | gb_free 7.9 | wall 3585
2022-05-06 17:28:58 | INFO | fairseq.data.iterators | grouped total_num_itrs = 2955
2022-05-06 17:28:58 | INFO | fairseq.trainer | begin training epoch 2
2022-05-06 17:28:58 | INFO | fairseq_cli.train | Start iterating over samples
2022-05-06 17:29:51 | INFO | train_inner | epoch 002:     45 / 2955 loss=6.63, ppl=99.06, wps=17246.7, ups=0.53, wpb=32686.1, bsz=63.8, num_updates=3000, lr=0.000375025, gnorm=0.691, train_wall=114, gb_free=7.9, wall=3638
2022-05-06 17:31:46 | INFO | train_inner | epoch 002:    145 / 2955 loss=6.58, ppl=95.66, wps=28410, ups=0.87, wpb=32768, bsz=64, num_updates=3100, lr=0.000387523, gnorm=0.666, train_wall=114, gb_free=7.9, wall=3753
2022-05-06 17:33:40 | INFO | train_inner | epoch 002:    245 / 2955 loss=6.515, ppl=91.47, wps=28764.3, ups=0.88, wpb=32768, bsz=64, num_updates=3200, lr=0.00040002, gnorm=0.67, train_wall=112, gb_free=7.9, wall=3867
2022-05-06 17:35:34 | INFO | train_inner | epoch 002:    345 / 2955 loss=6.474, ppl=88.91, wps=28766.5, ups=0.88, wpb=32768, bsz=64, num_updates=3300, lr=0.000412518, gnorm=0.67, train_wall=112, gb_free=7.9, wall=3981
2022-05-06 17:37:28 | INFO | train_inner | epoch 002:    445 / 2955 loss=6.462, ppl=88.13, wps=28772.8, ups=0.88, wpb=32768, bsz=64, num_updates=3400, lr=0.000425015, gnorm=0.657, train_wall=112, gb_free=7.9, wall=4095
2022-05-06 17:39:26 | INFO | train_inner | epoch 002:    545 / 2955 loss=6.405, ppl=84.72, wps=27581.5, ups=0.84, wpb=32768, bsz=64, num_updates=3500, lr=0.000437513, gnorm=0.638, train_wall=115, gb_free=7.9, wall=4214
2022-05-06 17:41:22 | INFO | train_inner | epoch 002:    645 / 2955 loss=6.349, ppl=81.53, wps=28379, ups=0.87, wpb=32768, bsz=64, num_updates=3600, lr=0.00045001, gnorm=0.628, train_wall=112, gb_free=7.9, wall=4329
2022-05-06 17:43:17 | INFO | train_inner | epoch 002:    745 / 2955 loss=6.314, ppl=79.54, wps=28593.4, ups=0.87, wpb=32768, bsz=64, num_updates=3700, lr=0.000462508, gnorm=0.623, train_wall=112, gb_free=7.9, wall=4444
2022-05-06 17:45:11 | INFO | train_inner | epoch 002:    845 / 2955 loss=6.281, ppl=77.74, wps=28564.1, ups=0.87, wpb=32763, bsz=64, num_updates=3800, lr=0.000475005, gnorm=0.605, train_wall=113, gb_free=7.9, wall=4559
2022-05-06 17:47:06 | INFO | train_inner | epoch 002:    945 / 2955 loss=6.251, ppl=76.17, wps=28663.3, ups=0.87, wpb=32768, bsz=64, num_updates=3900, lr=0.000487503, gnorm=0.6, train_wall=112, gb_free=7.9, wall=4673
2022-05-06 17:49:03 | INFO | train_inner | epoch 002:   1045 / 2955 loss=6.218, ppl=74.46, wps=27899.8, ups=0.85, wpb=32768, bsz=64, num_updates=4000, lr=0.0005, gnorm=0.602, train_wall=116, gb_free=7.9, wall=4790
2022-05-06 17:50:59 | INFO | train_inner | epoch 002:   1145 / 2955 loss=6.171, ppl=72.03, wps=28197.7, ups=0.86, wpb=32768, bsz=64, num_updates=4100, lr=0.000493865, gnorm=0.583, train_wall=115, gb_free=7.9, wall=4907
2022-05-06 17:52:56 | INFO | train_inner | epoch 002:   1245 / 2955 loss=6.144, ppl=70.73, wps=28016, ups=0.85, wpb=32768, bsz=64, num_updates=4200, lr=0.00048795, gnorm=0.564, train_wall=114, gb_free=7.9, wall=5024
2022-05-06 17:54:51 | INFO | train_inner | epoch 002:   1345 / 2955 loss=6.095, ppl=68.34, wps=28572.2, ups=0.87, wpb=32768, bsz=64, num_updates=4300, lr=0.000482243, gnorm=0.57, train_wall=112, gb_free=7.9, wall=5138
2022-05-06 17:56:45 | INFO | train_inner | epoch 002:   1445 / 2955 loss=6.082, ppl=67.75, wps=28632.7, ups=0.87, wpb=32768, bsz=64, num_updates=4400, lr=0.000476731, gnorm=0.559, train_wall=112, gb_free=7.9, wall=5253
2022-05-06 17:58:41 | INFO | train_inner | epoch 002:   1545 / 2955 loss=6.068, ppl=67.08, wps=28445.1, ups=0.87, wpb=32768, bsz=64, num_updates=4500, lr=0.000471405, gnorm=0.558, train_wall=112, gb_free=7.9, wall=5368
2022-05-06 18:00:35 | INFO | train_inner | epoch 002:   1645 / 2955 loss=6.007, ppl=64.29, wps=28623.5, ups=0.87, wpb=32768, bsz=64, num_updates=4600, lr=0.000466252, gnorm=0.542, train_wall=113, gb_free=7.9, wall=5482
2022-05-06 18:02:29 | INFO | train_inner | epoch 002:   1745 / 2955 loss=5.988, ppl=63.45, wps=28725.1, ups=0.88, wpb=32768, bsz=64, num_updates=4700, lr=0.000461266, gnorm=0.543, train_wall=112, gb_free=7.9, wall=5596
2022-05-06 18:04:24 | INFO | train_inner | epoch 002:   1845 / 2955 loss=5.95, ppl=61.83, wps=28470.6, ups=0.87, wpb=32768, bsz=64, num_updates=4800, lr=0.000456435, gnorm=0.541, train_wall=113, gb_free=7.9, wall=5711
2022-05-06 18:06:18 | INFO | train_inner | epoch 002:   1945 / 2955 loss=5.943, ppl=61.52, wps=28734.6, ups=0.88, wpb=32768, bsz=64, num_updates=4900, lr=0.000451754, gnorm=0.536, train_wall=112, gb_free=7.9, wall=5826
2022-05-06 18:08:13 | INFO | train_inner | epoch 002:   2045 / 2955 loss=5.907, ppl=60.01, wps=28635.5, ups=0.87, wpb=32768, bsz=64, num_updates=5000, lr=0.000447214, gnorm=0.536, train_wall=112, gb_free=7.9, wall=5940
2022-05-06 18:10:12 | INFO | train_inner | epoch 002:   2145 / 2955 loss=5.898, ppl=59.65, wps=27467.3, ups=0.84, wpb=32768, bsz=64, num_updates=5100, lr=0.000442807, gnorm=0.525, train_wall=116, gb_free=7.9, wall=6059
2022-05-06 18:12:06 | INFO | train_inner | epoch 002:   2245 / 2955 loss=5.848, ppl=57.62, wps=28634.8, ups=0.87, wpb=32768, bsz=64, num_updates=5200, lr=0.000438529, gnorm=0.522, train_wall=112, gb_free=7.9, wall=6174
2022-05-06 18:14:00 | INFO | train_inner | epoch 002:   2345 / 2955 loss=5.841, ppl=57.3, wps=28733.2, ups=0.88, wpb=32768, bsz=64, num_updates=5300, lr=0.000434372, gnorm=0.529, train_wall=112, gb_free=7.9, wall=6288
2022-05-06 18:15:56 | INFO | train_inner | epoch 002:   2445 / 2955 loss=5.825, ppl=56.68, wps=28441.3, ups=0.87, wpb=32768, bsz=64, num_updates=5400, lr=0.000430331, gnorm=0.52, train_wall=113, gb_free=7.9, wall=6403
2022-05-06 18:17:49 | INFO | train_inner | epoch 002:   2545 / 2955 loss=5.799, ppl=55.66, wps=28790.5, ups=0.88, wpb=32768, bsz=64, num_updates=5500, lr=0.000426401, gnorm=0.52, train_wall=112, gb_free=7.9, wall=6517
2022-05-06 18:19:47 | INFO | train_inner | epoch 002:   2645 / 2955 loss=5.782, ppl=55.03, wps=27766.8, ups=0.85, wpb=32768, bsz=64, num_updates=5600, lr=0.000422577, gnorm=0.525, train_wall=116, gb_free=7.9, wall=6635
2022-05-06 18:21:41 | INFO | train_inner | epoch 002:   2745 / 2955 loss=5.776, ppl=54.78, wps=28776.6, ups=0.88, wpb=32768, bsz=64, num_updates=5700, lr=0.000418854, gnorm=0.513, train_wall=112, gb_free=7.9, wall=6749
2022-05-06 18:23:35 | INFO | train_inner | epoch 002:   2845 / 2955 loss=5.758, ppl=54.11, wps=28785.2, ups=0.88, wpb=32757.8, bsz=64, num_updates=5800, lr=0.000415227, gnorm=0.509, train_wall=112, gb_free=7.9, wall=6862
2022-05-06 18:25:30 | INFO | train_inner | epoch 002:   2945 / 2955 loss=5.734, ppl=53.22, wps=28645.5, ups=0.87, wpb=32768, bsz=64, num_updates=5900, lr=0.000411693, gnorm=0.519, train_wall=112, gb_free=7.9, wall=6977
2022-05-06 18:25:41 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-05-06 18:26:52 | INFO | valid | epoch 002 | valid on 'valid' subset | loss 5.6 | ppl 48.51 | wps 75457.8 | wpb 2047.4 | bsz 4 | num_updates 5910 | best_loss 5.6
2022-05-06 18:26:52 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 2 @ 5910 updates
2022-05-06 18:26:52 | INFO | fairseq.trainer | Saving checkpoint to /cluster/work/cotterell/liam/master-thesis/checkpoints/language_model/standard/checkpoint_best_standard_16_5.0000E-04_full.pt
2022-05-06 18:26:54 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/work/cotterell/liam/master-thesis/checkpoints/language_model/standard/checkpoint_best_standard_16_5.0000E-04_full.pt
2022-05-06 18:26:54 | INFO | fairseq.checkpoint_utils | Saved checkpoint checkpoints/language_model/standard/checkpoint_best_standard_16_5.0000E-04_full.pt (epoch 2 @ 5910 updates, score 5.6) (writing took 1.7136425166390836 seconds)
2022-05-06 18:26:54 | INFO | fairseq_cli.train | end of epoch 2 (average epoch stats below)
2022-05-06 18:26:54 | INFO | train | epoch 002 | loss 6.092 | ppl 68.22 | wps 27847.8 | ups 0.85 | wpb 32764.7 | bsz 64 | num_updates 5910 | lr 0.000411345 | gnorm 0.573 | train_wall 3343 | gb_free 7.9 | wall 7062
2022-05-06 18:26:54 | INFO | fairseq.data.iterators | grouped total_num_itrs = 2955
2022-05-06 18:26:54 | INFO | fairseq.trainer | begin training epoch 3
2022-05-06 18:26:54 | INFO | fairseq_cli.train | Start iterating over samples
2022-05-06 18:28:37 | INFO | train_inner | epoch 003:     90 / 2955 loss=5.668, ppl=50.83, wps=17431.7, ups=0.53, wpb=32686.1, bsz=63.8, num_updates=6000, lr=0.000408248, gnorm=0.516, train_wall=112, gb_free=7.9, wall=7164
2022-05-06 18:30:31 | INFO | train_inner | epoch 003:    190 / 2955 loss=5.653, ppl=50.32, wps=28735.8, ups=0.88, wpb=32763, bsz=64, num_updates=6100, lr=0.000404888, gnorm=0.516, train_wall=113, gb_free=7.9, wall=7278
2022-05-06 18:32:25 | INFO | train_inner | epoch 003:    290 / 2955 loss=5.635, ppl=49.68, wps=28759, ups=0.88, wpb=32768, bsz=64, num_updates=6200, lr=0.00040161, gnorm=0.517, train_wall=112, gb_free=7.9, wall=7392
2022-05-06 18:34:19 | INFO | train_inner | epoch 003:    390 / 2955 loss=5.642, ppl=49.95, wps=28792.3, ups=0.88, wpb=32768, bsz=64, num_updates=6300, lr=0.00039841, gnorm=0.514, train_wall=112, gb_free=7.9, wall=7506
2022-05-06 18:36:16 | INFO | train_inner | epoch 003:    490 / 2955 loss=5.64, ppl=49.85, wps=27986.3, ups=0.85, wpb=32768, bsz=64, num_updates=6400, lr=0.000395285, gnorm=0.511, train_wall=112, gb_free=7.9, wall=7623
2022-05-06 18:38:25 | INFO | train_inner | epoch 003:    590 / 2955 loss=5.613, ppl=48.93, wps=25393.4, ups=0.77, wpb=32768, bsz=64, num_updates=6500, lr=0.000392232, gnorm=0.512, train_wall=116, gb_free=7.9, wall=7752
2022-05-06 18:40:28 | INFO | train_inner | epoch 003:    690 / 2955 loss=5.605, ppl=48.67, wps=26700.9, ups=0.81, wpb=32768, bsz=64, num_updates=6600, lr=0.000389249, gnorm=0.516, train_wall=114, gb_free=7.9, wall=7875
2022-05-06 18:42:27 | INFO | train_inner | epoch 003:    790 / 2955 loss=5.602, ppl=48.57, wps=27422.5, ups=0.84, wpb=32768, bsz=64, num_updates=6700, lr=0.000386334, gnorm=0.515, train_wall=112, gb_free=7.9, wall=7994
2022-05-06 18:44:25 | INFO | train_inner | epoch 003:    890 / 2955 loss=5.595, ppl=48.33, wps=27788.6, ups=0.85, wpb=32768, bsz=64, num_updates=6800, lr=0.000383482, gnorm=0.517, train_wall=112, gb_free=7.9, wall=8112
2022-05-06 18:46:22 | INFO | train_inner | epoch 003:    990 / 2955 loss=5.603, ppl=48.59, wps=28029.5, ups=0.86, wpb=32768, bsz=64, num_updates=6900, lr=0.000380693, gnorm=0.513, train_wall=112, gb_free=7.9, wall=8229
2022-05-06 18:48:24 | INFO | train_inner | epoch 003:   1090 / 2955 loss=5.571, ppl=47.55, wps=26890.1, ups=0.82, wpb=32768, bsz=64, num_updates=7000, lr=0.000377964, gnorm=0.509, train_wall=117, gb_free=7.9, wall=8351
2022-05-06 18:50:23 | INFO | train_inner | epoch 003:   1190 / 2955 loss=5.539, ppl=46.49, wps=27382.2, ups=0.84, wpb=32768, bsz=64, num_updates=7100, lr=0.000375293, gnorm=0.515, train_wall=116, gb_free=7.9, wall=8471
2022-05-06 18:52:19 | INFO | train_inner | epoch 003:   1290 / 2955 loss=5.557, ppl=47.09, wps=28436.1, ups=0.87, wpb=32768, bsz=64, num_updates=7200, lr=0.000372678, gnorm=0.513, train_wall=113, gb_free=7.9, wall=8586
2022-05-06 18:54:13 | INFO | train_inner | epoch 003:   1390 / 2955 loss=5.548, ppl=46.77, wps=28574.5, ups=0.87, wpb=32768, bsz=64, num_updates=7300, lr=0.000370117, gnorm=0.512, train_wall=112, gb_free=7.9, wall=8701
2022-05-06 18:56:08 | INFO | train_inner | epoch 003:   1490 / 2955 loss=5.523, ppl=45.99, wps=28594.1, ups=0.87, wpb=32768, bsz=64, num_updates=7400, lr=0.000367607, gnorm=0.512, train_wall=113, gb_free=7.9, wall=8815
2022-05-06 18:58:06 | INFO | train_inner | epoch 003:   1590 / 2955 loss=5.53, ppl=46.2, wps=27696.1, ups=0.85, wpb=32768, bsz=64, num_updates=7500, lr=0.000365148, gnorm=0.508, train_wall=116, gb_free=7.9, wall=8934
2022-05-06 19:00:03 | INFO | train_inner | epoch 003:   1690 / 2955 loss=5.523, ppl=45.97, wps=28005.6, ups=0.85, wpb=32768, bsz=64, num_updates=7600, lr=0.000362738, gnorm=0.509, train_wall=115, gb_free=7.9, wall=9051
2022-05-06 19:01:57 | INFO | train_inner | epoch 003:   1790 / 2955 loss=5.524, ppl=46.01, wps=28719.6, ups=0.88, wpb=32768, bsz=64, num_updates=7700, lr=0.000360375, gnorm=0.509, train_wall=113, gb_free=7.9, wall=9165
2022-05-06 19:03:51 | INFO | train_inner | epoch 003:   1890 / 2955 loss=5.491, ppl=44.98, wps=28789.3, ups=0.88, wpb=32757.8, bsz=64, num_updates=7800, lr=0.000358057, gnorm=0.513, train_wall=112, gb_free=7.9, wall=9279
2022-05-06 19:05:48 | INFO | train_inner | epoch 003:   1990 / 2955 loss=5.513, ppl=45.66, wps=28156.7, ups=0.86, wpb=32768, bsz=64, num_updates=7900, lr=0.000355784, gnorm=0.509, train_wall=113, gb_free=7.9, wall=9395
2022-05-06 19:07:46 | INFO | train_inner | epoch 003:   2090 / 2955 loss=5.491, ppl=44.98, wps=27757.8, ups=0.85, wpb=32768, bsz=64, num_updates=8000, lr=0.000353553, gnorm=0.508, train_wall=115, gb_free=7.9, wall=9513
2022-05-06 19:09:42 | INFO | train_inner | epoch 003:   2190 / 2955 loss=5.472, ppl=44.39, wps=28068.7, ups=0.86, wpb=32768, bsz=64, num_updates=8100, lr=0.000351364, gnorm=0.512, train_wall=115, gb_free=7.9, wall=9630
2022-05-06 19:11:36 | INFO | train_inner | epoch 003:   2290 / 2955 loss=5.481, ppl=44.67, wps=28762.1, ups=0.88, wpb=32768, bsz=64, num_updates=8200, lr=0.000349215, gnorm=0.508, train_wall=112, gb_free=7.9, wall=9744
2022-05-06 19:13:30 | INFO | train_inner | epoch 003:   2390 / 2955 loss=5.464, ppl=44.14, wps=28733.1, ups=0.88, wpb=32768, bsz=64, num_updates=8300, lr=0.000347105, gnorm=0.513, train_wall=113, gb_free=7.9, wall=9858
2022-05-06 19:15:24 | INFO | train_inner | epoch 003:   2490 / 2955 loss=5.468, ppl=44.27, wps=28748.4, ups=0.88, wpb=32768, bsz=64, num_updates=8400, lr=0.000345033, gnorm=0.511, train_wall=113, gb_free=7.9, wall=9972
2022-05-06 19:17:23 | INFO | train_inner | epoch 003:   2590 / 2955 loss=5.438, ppl=43.35, wps=27593.4, ups=0.84, wpb=32768, bsz=64, num_updates=8500, lr=0.000342997, gnorm=0.506, train_wall=116, gb_free=7.9, wall=10090
2022-05-06 19:19:17 | INFO | train_inner | epoch 003:   2690 / 2955 loss=5.45, ppl=43.7, wps=28659.7, ups=0.87, wpb=32768, bsz=64, num_updates=8600, lr=0.000340997, gnorm=0.51, train_wall=112, gb_free=7.9, wall=10205
2022-05-06 19:21:14 | INFO | train_inner | epoch 003:   2790 / 2955 loss=5.44, ppl=43.42, wps=27990.2, ups=0.85, wpb=32768, bsz=64, num_updates=8700, lr=0.000339032, gnorm=0.509, train_wall=115, gb_free=7.9, wall=10322
2022-05-06 19:23:10 | INFO | train_inner | epoch 003:   2890 / 2955 loss=5.438, ppl=43.35, wps=28372.5, ups=0.87, wpb=32768, bsz=64, num_updates=8800, lr=0.0003371, gnorm=0.509, train_wall=112, gb_free=7.9, wall=10437
2022-05-06 19:24:25 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-05-06 19:25:37 | INFO | valid | epoch 003 | valid on 'valid' subset | loss 5.32 | ppl 39.94 | wps 75817.2 | wpb 2047.4 | bsz 4 | num_updates 8865 | best_loss 5.32
2022-05-06 19:25:37 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 3 @ 8865 updates
2022-05-06 19:25:37 | INFO | fairseq.trainer | Saving checkpoint to /cluster/work/cotterell/liam/master-thesis/checkpoints/language_model/standard/checkpoint_best_standard_16_5.0000E-04_full.pt
2022-05-06 19:25:38 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/work/cotterell/liam/master-thesis/checkpoints/language_model/standard/checkpoint_best_standard_16_5.0000E-04_full.pt
2022-05-06 19:25:38 | INFO | fairseq.checkpoint_utils | Saved checkpoint checkpoints/language_model/standard/checkpoint_best_standard_16_5.0000E-04_full.pt (epoch 3 @ 8865 updates, score 5.32) (writing took 1.7493943460285664 seconds)
2022-05-06 19:25:38 | INFO | fairseq_cli.train | end of epoch 3 (average epoch stats below)
2022-05-06 19:25:38 | INFO | train | epoch 003 | loss 5.539 | ppl 46.48 | wps 27473.1 | ups 0.84 | wpb 32764.7 | bsz 64 | num_updates 8865 | lr 0.000335862 | gnorm 0.512 | train_wall 3355 | gb_free 7.9 | wall 10586
2022-05-06 19:25:38 | INFO | fairseq.data.iterators | grouped total_num_itrs = 2955
2022-05-06 19:25:39 | INFO | fairseq.trainer | begin training epoch 4
2022-05-06 19:25:39 | INFO | fairseq_cli.train | Start iterating over samples
2022-05-06 19:26:20 | INFO | train_inner | epoch 004:     35 / 2955 loss=5.403, ppl=42.31, wps=17227.4, ups=0.53, wpb=32686.1, bsz=63.8, num_updates=8900, lr=0.000335201, gnorm=0.508, train_wall=113, gb_free=7.9, wall=10627
2022-05-06 19:28:14 | INFO | train_inner | epoch 004:    135 / 2955 loss=5.345, ppl=40.64, wps=28635, ups=0.87, wpb=32768, bsz=64, num_updates=9000, lr=0.000333333, gnorm=0.515, train_wall=112, gb_free=7.9, wall=10741
2022-05-06 19:30:11 | INFO | train_inner | epoch 004:    235 / 2955 loss=5.342, ppl=40.56, wps=28019.4, ups=0.86, wpb=32768, bsz=64, num_updates=9100, lr=0.000331497, gnorm=0.512, train_wall=115, gb_free=7.9, wall=10858
2022-05-06 19:32:06 | INFO | train_inner | epoch 004:    335 / 2955 loss=5.329, ppl=40.2, wps=28479.5, ups=0.87, wpb=32768, bsz=64, num_updates=9200, lr=0.00032969, gnorm=0.519, train_wall=112, gb_free=7.9, wall=10973
2022-05-06 19:34:00 | INFO | train_inner | epoch 004:    435 / 2955 loss=5.356, ppl=40.96, wps=28698.2, ups=0.88, wpb=32768, bsz=64, num_updates=9300, lr=0.000327913, gnorm=0.517, train_wall=113, gb_free=7.9, wall=11088
2022-05-06 19:35:56 | INFO | train_inner | epoch 004:    535 / 2955 loss=5.336, ppl=40.4, wps=28454.8, ups=0.87, wpb=32768, bsz=64, num_updates=9400, lr=0.000326164, gnorm=0.517, train_wall=113, gb_free=7.9, wall=11203
2022-05-06 19:37:49 | INFO | train_inner | epoch 004:    635 / 2955 loss=5.343, ppl=40.59, wps=28756.9, ups=0.88, wpb=32768, bsz=64, num_updates=9500, lr=0.000324443, gnorm=0.517, train_wall=112, gb_free=7.9, wall=11317
2022-05-06 19:39:43 | INFO | train_inner | epoch 004:    735 / 2955 loss=5.336, ppl=40.39, wps=28753.1, ups=0.88, wpb=32768, bsz=64, num_updates=9600, lr=0.000322749, gnorm=0.514, train_wall=112, gb_free=7.9, wall=11431
2022-05-06 19:41:46 | INFO | train_inner | epoch 004:    835 / 2955 loss=5.343, ppl=40.59, wps=26821.3, ups=0.82, wpb=32768, bsz=64, num_updates=9700, lr=0.000321081, gnorm=0.516, train_wall=118, gb_free=7.9, wall=11553
2022-05-06 19:43:43 | INFO | train_inner | epoch 004:    935 / 2955 loss=5.324, ppl=40.07, wps=28016.2, ups=0.85, wpb=32768, bsz=64, num_updates=9800, lr=0.000319438, gnorm=0.517, train_wall=112, gb_free=7.9, wall=11670
2022-05-06 19:45:47 | INFO | train_inner | epoch 004:   1035 / 2955 loss=5.332, ppl=40.28, wps=26323.5, ups=0.8, wpb=32768, bsz=64, num_updates=9900, lr=0.000317821, gnorm=0.514, train_wall=113, gb_free=7.9, wall=11794
2022-05-06 19:47:48 | INFO | train_inner | epoch 004:   1135 / 2955 loss=5.342, ppl=40.57, wps=27194.9, ups=0.83, wpb=32768, bsz=64, num_updates=10000, lr=0.000316228, gnorm=0.515, train_wall=112, gb_free=7.9, wall=11915
2022-05-06 19:49:48 | INFO | train_inner | epoch 004:   1235 / 2955 loss=5.337, ppl=40.42, wps=27227.3, ups=0.83, wpb=32768, bsz=64, num_updates=10100, lr=0.000314658, gnorm=0.518, train_wall=114, gb_free=7.9, wall=12035
2022-05-06 19:51:51 | INFO | train_inner | epoch 004:   1335 / 2955 loss=5.318, ppl=39.9, wps=26614.2, ups=0.81, wpb=32768, bsz=64, num_updates=10200, lr=0.000313112, gnorm=0.516, train_wall=115, gb_free=7.9, wall=12158
2022-05-06 19:53:47 | INFO | train_inner | epoch 004:   1435 / 2955 loss=5.341, ppl=40.52, wps=28172.9, ups=0.86, wpb=32757.8, bsz=64, num_updates=10300, lr=0.000311588, gnorm=0.514, train_wall=112, gb_free=7.9, wall=12275
2022-05-06 19:55:43 | INFO | train_inner | epoch 004:   1535 / 2955 loss=5.316, ppl=39.84, wps=28251.3, ups=0.86, wpb=32768, bsz=64, num_updates=10400, lr=0.000310087, gnorm=0.513, train_wall=112, gb_free=7.9, wall=12391
2022-05-06 19:57:38 | INFO | train_inner | epoch 004:   1635 / 2955 loss=5.302, ppl=39.44, wps=28435, ups=0.87, wpb=32768, bsz=64, num_updates=10500, lr=0.000308607, gnorm=0.519, train_wall=112, gb_free=7.9, wall=12506
2022-05-06 19:59:39 | INFO | train_inner | epoch 004:   1735 / 2955 loss=5.311, ppl=39.69, wps=27223, ups=0.83, wpb=32768, bsz=64, num_updates=10600, lr=0.000307148, gnorm=0.514, train_wall=117, gb_free=7.9, wall=12626
2022-05-06 20:01:33 | INFO | train_inner | epoch 004:   1835 / 2955 loss=5.292, ppl=39.17, wps=28624.8, ups=0.87, wpb=32768, bsz=64, num_updates=10700, lr=0.000305709, gnorm=0.516, train_wall=112, gb_free=7.9, wall=12741
2022-05-06 20:03:28 | INFO | train_inner | epoch 004:   1935 / 2955 loss=5.32, ppl=39.96, wps=28689.6, ups=0.88, wpb=32768, bsz=64, num_updates=10800, lr=0.00030429, gnorm=0.521, train_wall=112, gb_free=7.9, wall=12855
2022-05-06 20:05:22 | INFO | train_inner | epoch 004:   2035 / 2955 loss=5.314, ppl=39.77, wps=28752.1, ups=0.88, wpb=32768, bsz=64, num_updates=10900, lr=0.000302891, gnorm=0.518, train_wall=112, gb_free=7.9, wall=12969
2022-05-06 20:07:36 | INFO | train_inner | epoch 004:   2135 / 2955 loss=5.27, ppl=38.58, wps=24332.6, ups=0.74, wpb=32763, bsz=64, num_updates=11000, lr=0.000301511, gnorm=0.516, train_wall=129, gb_free=7.9, wall=13103
2022-05-06 20:10:07 | INFO | train_inner | epoch 004:   2235 / 2955 loss=5.301, ppl=39.42, wps=21767.7, ups=0.66, wpb=32768, bsz=64, num_updates=11100, lr=0.00030015, gnorm=0.514, train_wall=139, gb_free=7.9, wall=13254
2022-05-06 20:12:02 | INFO | train_inner | epoch 004:   2335 / 2955 loss=5.286, ppl=39.02, wps=28446.8, ups=0.87, wpb=32768, bsz=64, num_updates=11200, lr=0.000298807, gnorm=0.517, train_wall=112, gb_free=7.9, wall=13369
2022-05-06 20:14:01 | INFO | train_inner | epoch 004:   2435 / 2955 loss=5.3, ppl=39.4, wps=27446.8, ups=0.84, wpb=32768, bsz=64, num_updates=11300, lr=0.000297482, gnorm=0.525, train_wall=117, gb_free=7.9, wall=13489
2022-05-06 20:15:56 | INFO | train_inner | epoch 004:   2535 / 2955 loss=5.275, ppl=38.71, wps=28620.4, ups=0.87, wpb=32768, bsz=64, num_updates=11400, lr=0.000296174, gnorm=0.52, train_wall=112, gb_free=7.9, wall=13603
2022-05-06 20:17:50 | INFO | train_inner | epoch 004:   2635 / 2955 loss=5.292, ppl=39.18, wps=28655.5, ups=0.87, wpb=32768, bsz=64, num_updates=11500, lr=0.000294884, gnorm=0.52, train_wall=112, gb_free=7.9, wall=13717
2022-05-06 20:19:44 | INFO | train_inner | epoch 004:   2735 / 2955 loss=5.286, ppl=39.01, wps=28731.9, ups=0.88, wpb=32768, bsz=64, num_updates=11600, lr=0.00029361, gnorm=0.516, train_wall=112, gb_free=7.9, wall=13831
2022-05-06 20:21:52 | INFO | train_inner | epoch 004:   2835 / 2955 loss=5.278, ppl=38.81, wps=25643.3, ups=0.78, wpb=32768, bsz=64, num_updates=11700, lr=0.000292353, gnorm=0.515, train_wall=118, gb_free=7.9, wall=13959
2022-05-06 20:23:52 | INFO | train_inner | epoch 004:   2935 / 2955 loss=5.253, ppl=38.13, wps=27224.2, ups=0.83, wpb=32768, bsz=64, num_updates=11800, lr=0.000291111, gnorm=0.517, train_wall=112, gb_free=7.9, wall=14080
2022-05-06 20:24:16 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-05-06 20:25:28 | INFO | valid | epoch 004 | valid on 'valid' subset | loss 5.177 | ppl 36.18 | wps 75233.7 | wpb 2047.4 | bsz 4 | num_updates 11820 | best_loss 5.177
2022-05-06 20:25:28 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 4 @ 11820 updates
2022-05-06 20:25:28 | INFO | fairseq.trainer | Saving checkpoint to /cluster/work/cotterell/liam/master-thesis/checkpoints/language_model/standard/checkpoint_best_standard_16_5.0000E-04_full.pt
2022-05-06 20:25:30 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/work/cotterell/liam/master-thesis/checkpoints/language_model/standard/checkpoint_best_standard_16_5.0000E-04_full.pt
2022-05-06 20:25:30 | INFO | fairseq.checkpoint_utils | Saved checkpoint checkpoints/language_model/standard/checkpoint_best_standard_16_5.0000E-04_full.pt (epoch 4 @ 11820 updates, score 5.177) (writing took 1.9968551797792315 seconds)
2022-05-06 20:25:30 | INFO | fairseq_cli.train | end of epoch 4 (average epoch stats below)
2022-05-06 20:25:30 | INFO | train | epoch 004 | loss 5.315 | ppl 39.81 | wps 26959.4 | ups 0.82 | wpb 32764.7 | bsz 64 | num_updates 11820 | lr 0.000290865 | gnorm 0.517 | train_wall 3392 | gb_free 7.9 | wall 14177
2022-05-06 20:25:30 | INFO | fairseq.data.iterators | grouped total_num_itrs = 2955
2022-05-06 20:25:30 | INFO | fairseq.trainer | begin training epoch 5
2022-05-06 20:25:30 | INFO | fairseq_cli.train | Start iterating over samples
2022-05-06 20:27:04 | INFO | train_inner | epoch 005:     80 / 2955 loss=5.214, ppl=37.13, wps=17028.7, ups=0.52, wpb=32686.1, bsz=63.8, num_updates=11900, lr=0.000289886, gnorm=0.522, train_wall=112, gb_free=7.9, wall=14272
2022-05-06 20:29:03 | INFO | train_inner | epoch 005:    180 / 2955 loss=5.172, ppl=36.04, wps=27633, ups=0.84, wpb=32768, bsz=64, num_updates=12000, lr=0.000288675, gnorm=0.519, train_wall=112, gb_free=7.9, wall=14390
2022-05-06 20:30:59 | INFO | train_inner | epoch 005:    280 / 2955 loss=5.19, ppl=36.52, wps=28130.1, ups=0.86, wpb=32768, bsz=64, num_updates=12100, lr=0.00028748, gnorm=0.521, train_wall=113, gb_free=7.9, wall=14507
2022-05-06 20:33:00 | INFO | train_inner | epoch 005:    380 / 2955 loss=5.194, ppl=36.61, wps=27248, ups=0.83, wpb=32768, bsz=64, num_updates=12200, lr=0.000286299, gnorm=0.529, train_wall=115, gb_free=7.9, wall=14627
2022-05-06 20:34:55 | INFO | train_inner | epoch 005:    480 / 2955 loss=5.188, ppl=36.46, wps=28314.2, ups=0.86, wpb=32768, bsz=64, num_updates=12300, lr=0.000285133, gnorm=0.522, train_wall=112, gb_free=7.9, wall=14743
2022-05-06 20:36:52 | INFO | train_inner | epoch 005:    580 / 2955 loss=5.207, ppl=36.93, wps=28130.1, ups=0.86, wpb=32768, bsz=64, num_updates=12400, lr=0.000283981, gnorm=0.525, train_wall=114, gb_free=7.9, wall=14859
2022-05-06 20:38:47 | INFO | train_inner | epoch 005:    680 / 2955 loss=5.189, ppl=36.47, wps=28542.2, ups=0.87, wpb=32768, bsz=64, num_updates=12500, lr=0.000282843, gnorm=0.529, train_wall=112, gb_free=7.9, wall=14974
2022-05-06 20:40:43 | INFO | train_inner | epoch 005:    780 / 2955 loss=5.196, ppl=36.66, wps=28257, ups=0.86, wpb=32768, bsz=64, num_updates=12600, lr=0.000281718, gnorm=0.528, train_wall=114, gb_free=7.9, wall=15090
2022-05-06 20:42:41 | INFO | train_inner | epoch 005:    880 / 2955 loss=5.199, ppl=36.74, wps=27629.4, ups=0.84, wpb=32768, bsz=64, num_updates=12700, lr=0.000280607, gnorm=0.53, train_wall=116, gb_free=7.9, wall=15209
2022-05-06 20:44:35 | INFO | train_inner | epoch 005:    980 / 2955 loss=5.216, ppl=37.18, wps=28744.8, ups=0.88, wpb=32768, bsz=64, num_updates=12800, lr=0.000279508, gnorm=0.531, train_wall=112, gb_free=7.9, wall=15323
2022-05-06 20:46:30 | INFO | train_inner | epoch 005:   1080 / 2955 loss=5.202, ppl=36.8, wps=28465.6, ups=0.87, wpb=32768, bsz=64, num_updates=12900, lr=0.000278423, gnorm=0.531, train_wall=113, gb_free=7.9, wall=15438
2022-05-06 20:48:24 | INFO | train_inner | epoch 005:   1180 / 2955 loss=5.197, ppl=36.68, wps=28771.6, ups=0.88, wpb=32768, bsz=64, num_updates=13000, lr=0.00027735, gnorm=0.532, train_wall=112, gb_free=7.9, wall=15552
2022-05-06 20:50:20 | INFO | train_inner | epoch 005:   1280 / 2955 loss=5.188, ppl=36.47, wps=28413.5, ups=0.87, wpb=32768, bsz=64, num_updates=13100, lr=0.000276289, gnorm=0.526, train_wall=113, gb_free=7.9, wall=15667
2022-05-06 20:52:14 | INFO | train_inner | epoch 005:   1380 / 2955 loss=5.205, ppl=36.88, wps=28545.8, ups=0.87, wpb=32757.8, bsz=64, num_updates=13200, lr=0.000275241, gnorm=0.527, train_wall=113, gb_free=7.9, wall=15782
2022-05-06 20:54:08 | INFO | train_inner | epoch 005:   1480 / 2955 loss=5.182, ppl=36.3, wps=28770.6, ups=0.88, wpb=32768, bsz=64, num_updates=13300, lr=0.000274204, gnorm=0.528, train_wall=112, gb_free=7.9, wall=15895
2022-05-06 20:56:02 | INFO | train_inner | epoch 005:   1580 / 2955 loss=5.186, ppl=36.4, wps=28778.8, ups=0.88, wpb=32768, bsz=64, num_updates=13400, lr=0.000273179, gnorm=0.53, train_wall=112, gb_free=7.9, wall=16009
2022-05-06 20:57:56 | INFO | train_inner | epoch 005:   1680 / 2955 loss=5.191, ppl=36.53, wps=28763, ups=0.88, wpb=32768, bsz=64, num_updates=13500, lr=0.000272166, gnorm=0.532, train_wall=112, gb_free=7.9, wall=16123
2022-05-06 20:59:52 | INFO | train_inner | epoch 005:   1780 / 2955 loss=5.183, ppl=36.32, wps=28276.5, ups=0.86, wpb=32768, bsz=64, num_updates=13600, lr=0.000271163, gnorm=0.534, train_wall=113, gb_free=7.9, wall=16239
2022-05-06 21:01:49 | INFO | train_inner | epoch 005:   1880 / 2955 loss=5.179, ppl=36.21, wps=28005.9, ups=0.85, wpb=32768, bsz=64, num_updates=13700, lr=0.000270172, gnorm=0.529, train_wall=114, gb_free=7.9, wall=16356
2022-05-06 21:03:46 | INFO | train_inner | epoch 005:   1980 / 2955 loss=5.167, ppl=35.92, wps=28063.7, ups=0.86, wpb=32768, bsz=64, num_updates=13800, lr=0.000269191, gnorm=0.529, train_wall=115, gb_free=7.9, wall=16473
2022-05-06 21:05:40 | INFO | train_inner | epoch 005:   2080 / 2955 loss=5.194, ppl=36.6, wps=28757.5, ups=0.88, wpb=32768, bsz=64, num_updates=13900, lr=0.000268221, gnorm=0.533, train_wall=112, gb_free=7.9, wall=16587
2022-05-06 21:07:33 | INFO | train_inner | epoch 005:   2180 / 2955 loss=5.183, ppl=36.34, wps=28769.7, ups=0.88, wpb=32768, bsz=64, num_updates=14000, lr=0.000267261, gnorm=0.53, train_wall=112, gb_free=7.9, wall=16701
2022-05-06 21:09:28 | INFO | train_inner | epoch 005:   2280 / 2955 loss=5.184, ppl=36.35, wps=28732.3, ups=0.88, wpb=32768, bsz=64, num_updates=14100, lr=0.000266312, gnorm=0.53, train_wall=113, gb_free=7.9, wall=16815
2022-05-06 21:11:22 | INFO | train_inner | epoch 005:   2380 / 2955 loss=5.19, ppl=36.49, wps=28736, ups=0.88, wpb=32768, bsz=64, num_updates=14200, lr=0.000265372, gnorm=0.533, train_wall=113, gb_free=7.9, wall=16929
2022-05-06 21:13:20 | INFO | train_inner | epoch 005:   2480 / 2955 loss=5.171, ppl=36.04, wps=27723.5, ups=0.85, wpb=32768, bsz=64, num_updates=14300, lr=0.000264443, gnorm=0.53, train_wall=116, gb_free=7.9, wall=17047
2022-05-06 21:15:14 | INFO | train_inner | epoch 005:   2580 / 2955 loss=5.157, ppl=35.68, wps=28695.2, ups=0.88, wpb=32763, bsz=64, num_updates=14400, lr=0.000263523, gnorm=0.533, train_wall=113, gb_free=7.9, wall=17161
2022-05-06 21:17:08 | INFO | train_inner | epoch 005:   2680 / 2955 loss=5.164, ppl=35.86, wps=28797.2, ups=0.88, wpb=32768, bsz=64, num_updates=14500, lr=0.000262613, gnorm=0.537, train_wall=112, gb_free=7.9, wall=17275
2022-05-06 21:19:03 | INFO | train_inner | epoch 005:   2780 / 2955 loss=5.173, ppl=36.07, wps=28503.3, ups=0.87, wpb=32768, bsz=64, num_updates=14600, lr=0.000261712, gnorm=0.532, train_wall=112, gb_free=7.9, wall=17390
2022-05-06 21:21:00 | INFO | train_inner | epoch 005:   2880 / 2955 loss=5.153, ppl=35.59, wps=27975.6, ups=0.85, wpb=32768, bsz=64, num_updates=14700, lr=0.00026082, gnorm=0.532, train_wall=115, gb_free=7.9, wall=17507
2022-05-06 21:22:26 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-05-06 21:23:36 | INFO | valid | epoch 005 | valid on 'valid' subset | loss 5.094 | ppl 34.16 | wps 76005.9 | wpb 2047.4 | bsz 4 | num_updates 14775 | best_loss 5.094
2022-05-06 21:23:36 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 5 @ 14775 updates
2022-05-06 21:23:36 | INFO | fairseq.trainer | Saving checkpoint to /cluster/work/cotterell/liam/master-thesis/checkpoints/language_model/standard/checkpoint_best_standard_16_5.0000E-04_full.pt
2022-05-06 21:23:38 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/work/cotterell/liam/master-thesis/checkpoints/language_model/standard/checkpoint_best_standard_16_5.0000E-04_full.pt
2022-05-06 21:23:38 | INFO | fairseq.checkpoint_utils | Saved checkpoint checkpoints/language_model/standard/checkpoint_best_standard_16_5.0000E-04_full.pt (epoch 5 @ 14775 updates, score 5.094) (writing took 1.573305734898895 seconds)
2022-05-06 21:23:38 | INFO | fairseq_cli.train | end of epoch 5 (average epoch stats below)
2022-05-06 21:23:38 | INFO | train | epoch 005 | loss 5.186 | ppl 36.39 | wps 27757.4 | ups 0.85 | wpb 32764.7 | bsz 64 | num_updates 14775 | lr 0.000260157 | gnorm 0.529 | train_wall 3345 | gb_free 7.9 | wall 17665
2022-05-06 21:23:38 | INFO | fairseq.data.iterators | grouped total_num_itrs = 2955
2022-05-06 21:23:38 | INFO | fairseq.trainer | begin training epoch 6
2022-05-06 21:23:38 | INFO | fairseq_cli.train | Start iterating over samples
2022-05-06 21:24:06 | INFO | train_inner | epoch 006:     25 / 2955 loss=5.153, ppl=35.58, wps=17516.3, ups=0.54, wpb=32686.1, bsz=63.8, num_updates=14800, lr=0.000259938, gnorm=0.53, train_wall=112, gb_free=7.9, wall=17694
2022-05-06 21:26:00 | INFO | train_inner | epoch 006:    125 / 2955 loss=5.084, ppl=33.92, wps=28782.4, ups=0.88, wpb=32757.8, bsz=64, num_updates=14900, lr=0.000259064, gnorm=0.537, train_wall=112, gb_free=7.9, wall=17808
2022-05-06 21:27:54 | INFO | train_inner | epoch 006:    225 / 2955 loss=5.089, ppl=34.04, wps=28805.1, ups=0.88, wpb=32768, bsz=64, num_updates=15000, lr=0.000258199, gnorm=0.54, train_wall=112, gb_free=7.9, wall=17921
2022-05-06 21:29:48 | INFO | train_inner | epoch 006:    325 / 2955 loss=5.106, ppl=34.45, wps=28788.6, ups=0.88, wpb=32768, bsz=64, num_updates=15100, lr=0.000257343, gnorm=0.539, train_wall=112, gb_free=7.9, wall=18035
2022-05-06 21:31:44 | INFO | train_inner | epoch 006:    425 / 2955 loss=5.079, ppl=33.81, wps=28316.5, ups=0.86, wpb=32768, bsz=64, num_updates=15200, lr=0.000256495, gnorm=0.539, train_wall=114, gb_free=7.9, wall=18151
2022-05-06 21:33:45 | INFO | train_inner | epoch 006:    525 / 2955 loss=5.088, ppl=34.01, wps=26985.2, ups=0.82, wpb=32768, bsz=64, num_updates=15300, lr=0.000255655, gnorm=0.537, train_wall=114, gb_free=7.9, wall=18272
2022-05-06 21:35:47 | INFO | train_inner | epoch 006:    625 / 2955 loss=5.102, ppl=34.34, wps=26889.5, ups=0.82, wpb=32768, bsz=64, num_updates=15400, lr=0.000254824, gnorm=0.541, train_wall=112, gb_free=7.9, wall=18394
2022-05-06 21:37:47 | INFO | train_inner | epoch 006:    725 / 2955 loss=5.099, ppl=34.28, wps=27345.6, ups=0.83, wpb=32768, bsz=64, num_updates=15500, lr=0.000254, gnorm=0.539, train_wall=112, gb_free=7.9, wall=18514
2022-05-06 21:39:49 | INFO | train_inner | epoch 006:    825 / 2955 loss=5.101, ppl=34.32, wps=26855.1, ups=0.82, wpb=32768, bsz=64, num_updates=15600, lr=0.000253185, gnorm=0.54, train_wall=115, gb_free=7.9, wall=18636
2022-05-06 21:41:48 | INFO | train_inner | epoch 006:    925 / 2955 loss=5.103, ppl=34.36, wps=27502.8, ups=0.84, wpb=32768, bsz=64, num_updates=15700, lr=0.000252377, gnorm=0.541, train_wall=114, gb_free=7.9, wall=18755
2022-05-06 21:43:44 | INFO | train_inner | epoch 006:   1025 / 2955 loss=5.084, ppl=33.92, wps=28093.9, ups=0.86, wpb=32768, bsz=64, num_updates=15800, lr=0.000251577, gnorm=0.54, train_wall=112, gb_free=7.9, wall=18872
2022-05-06 21:45:48 | INFO | train_inner | epoch 006:   1125 / 2955 loss=5.114, ppl=34.64, wps=26551.3, ups=0.81, wpb=32768, bsz=64, num_updates=15900, lr=0.000250785, gnorm=0.546, train_wall=119, gb_free=7.9, wall=18995
2022-05-06 21:47:43 | INFO | train_inner | epoch 006:   1225 / 2955 loss=5.098, ppl=34.25, wps=28448.2, ups=0.87, wpb=32768, bsz=64, num_updates=16000, lr=0.00025, gnorm=0.539, train_wall=113, gb_free=7.9, wall=19110
2022-05-06 21:49:38 | INFO | train_inner | epoch 006:   1325 / 2955 loss=5.122, ppl=34.81, wps=28578, ups=0.87, wpb=32768, bsz=64, num_updates=16100, lr=0.000249222, gnorm=0.552, train_wall=112, gb_free=7.9, wall=19225
2022-05-06 21:51:32 | INFO | train_inner | epoch 006:   1425 / 2955 loss=5.094, ppl=34.15, wps=28640.7, ups=0.87, wpb=32768, bsz=64, num_updates=16200, lr=0.000248452, gnorm=0.547, train_wall=112, gb_free=7.9, wall=19339
2022-05-06 21:53:26 | INFO | train_inner | epoch 006:   1525 / 2955 loss=5.108, ppl=34.5, wps=28700.4, ups=0.88, wpb=32768, bsz=64, num_updates=16300, lr=0.000247689, gnorm=0.544, train_wall=112, gb_free=7.9, wall=19454
2022-05-06 21:55:26 | INFO | train_inner | epoch 006:   1625 / 2955 loss=5.102, ppl=34.35, wps=27393.5, ups=0.84, wpb=32768, bsz=64, num_updates=16400, lr=0.000246932, gnorm=0.543, train_wall=117, gb_free=7.9, wall=19573
2022-05-06 21:57:26 | INFO | train_inner | epoch 006:   1725 / 2955 loss=5.103, ppl=34.37, wps=27190.4, ups=0.83, wpb=32768, bsz=64, num_updates=16500, lr=0.000246183, gnorm=0.544, train_wall=114, gb_free=7.9, wall=19694
2022-05-06 21:59:27 | INFO | train_inner | epoch 006:   1825 / 2955 loss=5.093, ppl=34.14, wps=27076.9, ups=0.83, wpb=32768, bsz=64, num_updates=16600, lr=0.00024544, gnorm=0.543, train_wall=112, gb_free=7.9, wall=19815
2022-05-06 22:01:26 | INFO | train_inner | epoch 006:   1925 / 2955 loss=5.097, ppl=34.21, wps=27538.4, ups=0.84, wpb=32768, bsz=64, num_updates=16700, lr=0.000244704, gnorm=0.545, train_wall=112, gb_free=7.9, wall=19934
2022-05-06 22:03:25 | INFO | train_inner | epoch 006:   2025 / 2955 loss=5.109, ppl=34.5, wps=27752.1, ups=0.85, wpb=32768, bsz=64, num_updates=16800, lr=0.000243975, gnorm=0.551, train_wall=112, gb_free=7.9, wall=20052
2022-05-06 22:05:27 | INFO | train_inner | epoch 006:   2125 / 2955 loss=5.107, ppl=34.48, wps=26741.3, ups=0.82, wpb=32768, bsz=64, num_updates=16900, lr=0.000243252, gnorm=0.545, train_wall=117, gb_free=7.9, wall=20174
2022-05-06 22:07:27 | INFO | train_inner | epoch 006:   2225 / 2955 loss=5.108, ppl=34.48, wps=27267.7, ups=0.83, wpb=32768, bsz=64, num_updates=17000, lr=0.000242536, gnorm=0.544, train_wall=112, gb_free=7.9, wall=20295
2022-05-06 22:09:28 | INFO | train_inner | epoch 006:   2325 / 2955 loss=5.089, ppl=34.04, wps=27146.7, ups=0.83, wpb=32768, bsz=64, num_updates=17100, lr=0.000241825, gnorm=0.55, train_wall=112, gb_free=7.9, wall=20415
2022-05-06 22:11:28 | INFO | train_inner | epoch 006:   2425 / 2955 loss=5.105, ppl=34.41, wps=27350.1, ups=0.83, wpb=32763, bsz=64, num_updates=17200, lr=0.000241121, gnorm=0.545, train_wall=113, gb_free=7.9, wall=20535
2022-05-06 22:13:26 | INFO | train_inner | epoch 006:   2525 / 2955 loss=5.088, ppl=34.02, wps=27727.8, ups=0.85, wpb=32768, bsz=64, num_updates=17300, lr=0.000240424, gnorm=0.549, train_wall=112, gb_free=7.9, wall=20653
2022-05-06 22:15:28 | INFO | train_inner | epoch 006:   2625 / 2955 loss=5.103, ppl=34.37, wps=26941, ups=0.82, wpb=32768, bsz=64, num_updates=17400, lr=0.000239732, gnorm=0.547, train_wall=116, gb_free=7.9, wall=20775
2022-05-06 22:17:24 | INFO | train_inner | epoch 006:   2725 / 2955 loss=5.097, ppl=34.22, wps=28166.9, ups=0.86, wpb=32768, bsz=64, num_updates=17500, lr=0.000239046, gnorm=0.545, train_wall=112, gb_free=7.9, wall=20891
2022-05-06 22:19:19 | INFO | train_inner | epoch 006:   2825 / 2955 loss=5.072, ppl=33.64, wps=28380.5, ups=0.87, wpb=32768, bsz=64, num_updates=17600, lr=0.000238366, gnorm=0.546, train_wall=112, gb_free=7.9, wall=21007
2022-05-06 22:21:15 | INFO | train_inner | epoch 006:   2925 / 2955 loss=5.099, ppl=34.27, wps=28419.3, ups=0.87, wpb=32768, bsz=64, num_updates=17700, lr=0.000237691, gnorm=0.551, train_wall=113, gb_free=7.9, wall=21122
2022-05-06 22:21:49 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-05-06 22:23:00 | INFO | valid | epoch 006 | valid on 'valid' subset | loss 5.035 | ppl 32.78 | wps 76042 | wpb 2047.4 | bsz 4 | num_updates 17730 | best_loss 5.035
2022-05-06 22:23:00 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 6 @ 17730 updates
2022-05-06 22:23:00 | INFO | fairseq.trainer | Saving checkpoint to /cluster/work/cotterell/liam/master-thesis/checkpoints/language_model/standard/checkpoint_best_standard_16_5.0000E-04_full.pt
2022-05-06 22:23:01 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/work/cotterell/liam/master-thesis/checkpoints/language_model/standard/checkpoint_best_standard_16_5.0000E-04_full.pt
2022-05-06 22:23:01 | INFO | fairseq.checkpoint_utils | Saved checkpoint checkpoints/language_model/standard/checkpoint_best_standard_16_5.0000E-04_full.pt (epoch 6 @ 17730 updates, score 5.035) (writing took 1.7312877201475203 seconds)
2022-05-06 22:23:01 | INFO | fairseq_cli.train | end of epoch 6 (average epoch stats below)
2022-05-06 22:23:01 | INFO | train | epoch 006 | loss 5.098 | ppl 34.25 | wps 27168.4 | ups 0.83 | wpb 32764.7 | bsz 64 | num_updates 17730 | lr 0.00023749 | gnorm 0.544 | train_wall 3350 | gb_free 7.9 | wall 21229
2022-05-06 22:23:02 | INFO | fairseq.data.iterators | grouped total_num_itrs = 2955
2022-05-06 22:23:02 | INFO | fairseq.trainer | begin training epoch 7
2022-05-06 22:23:02 | INFO | fairseq_cli.train | Start iterating over samples
2022-05-06 22:24:25 | INFO | train_inner | epoch 007:     70 / 2955 loss=5.042, ppl=32.94, wps=17188.9, ups=0.53, wpb=32686.1, bsz=63.8, num_updates=17800, lr=0.000237023, gnorm=0.55, train_wall=115, gb_free=7.9, wall=21312
2022-05-06 22:26:19 | INFO | train_inner | epoch 007:    170 / 2955 loss=5, ppl=31.99, wps=28685.5, ups=0.88, wpb=32768, bsz=64, num_updates=17900, lr=0.00023636, gnorm=0.552, train_wall=112, gb_free=7.9, wall=21426
2022-05-06 22:28:13 | INFO | train_inner | epoch 007:    270 / 2955 loss=5.01, ppl=32.23, wps=28646, ups=0.87, wpb=32768, bsz=64, num_updates=18000, lr=0.000235702, gnorm=0.55, train_wall=112, gb_free=7.9, wall=21541
2022-05-06 22:30:07 | INFO | train_inner | epoch 007:    370 / 2955 loss=5.021, ppl=32.46, wps=28725.6, ups=0.88, wpb=32768, bsz=64, num_updates=18100, lr=0.00023505, gnorm=0.552, train_wall=112, gb_free=7.9, wall=21655
2022-05-06 22:32:09 | INFO | train_inner | epoch 007:    470 / 2955 loss=5.023, ppl=32.52, wps=26861.3, ups=0.82, wpb=32768, bsz=64, num_updates=18200, lr=0.000234404, gnorm=0.551, train_wall=117, gb_free=7.9, wall=21777
2022-05-06 22:34:12 | INFO | train_inner | epoch 007:    570 / 2955 loss=5.034, ppl=32.77, wps=26843.4, ups=0.82, wpb=32768, bsz=64, num_updates=18300, lr=0.000233762, gnorm=0.555, train_wall=113, gb_free=7.9, wall=21899
2022-05-06 22:36:13 | INFO | train_inner | epoch 007:    670 / 2955 loss=5.036, ppl=32.81, wps=26995.5, ups=0.82, wpb=32768, bsz=64, num_updates=18400, lr=0.000233126, gnorm=0.55, train_wall=114, gb_free=7.9, wall=22020
2022-05-06 22:38:11 | INFO | train_inner | epoch 007:    770 / 2955 loss=5.034, ppl=32.77, wps=27762.4, ups=0.85, wpb=32768, bsz=64, num_updates=18500, lr=0.000232495, gnorm=0.555, train_wall=112, gb_free=7.9, wall=22138
2022-05-06 22:40:08 | INFO | train_inner | epoch 007:    870 / 2955 loss=5.048, ppl=33.09, wps=28010.9, ups=0.85, wpb=32768, bsz=64, num_updates=18600, lr=0.000231869, gnorm=0.561, train_wall=112, gb_free=7.9, wall=22255
2022-05-06 22:42:09 | INFO | train_inner | epoch 007:    970 / 2955 loss=5.048, ppl=33.09, wps=26982.2, ups=0.82, wpb=32768, bsz=64, num_updates=18700, lr=0.000231249, gnorm=0.553, train_wall=116, gb_free=7.9, wall=22377
2022-05-06 22:44:05 | INFO | train_inner | epoch 007:   1070 / 2955 loss=5.046, ppl=33.04, wps=28221.1, ups=0.86, wpb=32768, bsz=64, num_updates=18800, lr=0.000230633, gnorm=0.551, train_wall=113, gb_free=7.9, wall=22493
2022-05-06 22:46:01 | INFO | train_inner | epoch 007:   1170 / 2955 loss=5.025, ppl=32.57, wps=28447, ups=0.87, wpb=32768, bsz=64, num_updates=18900, lr=0.000230022, gnorm=0.562, train_wall=113, gb_free=7.9, wall=22608
2022-05-06 22:47:55 | INFO | train_inner | epoch 007:   1270 / 2955 loss=5.036, ppl=32.81, wps=28580.5, ups=0.87, wpb=32768, bsz=64, num_updates=19000, lr=0.000229416, gnorm=0.558, train_wall=112, gb_free=7.9, wall=22723
2022-05-06 22:49:50 | INFO | train_inner | epoch 007:   1370 / 2955 loss=5.039, ppl=32.87, wps=28616.7, ups=0.87, wpb=32768, bsz=64, num_updates=19100, lr=0.000228814, gnorm=0.557, train_wall=112, gb_free=7.9, wall=22837
2022-05-06 22:51:49 | INFO | train_inner | epoch 007:   1470 / 2955 loss=5.042, ppl=32.94, wps=27513.8, ups=0.84, wpb=32768, bsz=64, num_updates=19200, lr=0.000228218, gnorm=0.554, train_wall=115, gb_free=7.9, wall=22956
2022-05-06 22:53:45 | INFO | train_inner | epoch 007:   1570 / 2955 loss=5.046, ppl=33.03, wps=28327.9, ups=0.86, wpb=32768, bsz=64, num_updates=19300, lr=0.000227626, gnorm=0.557, train_wall=114, gb_free=7.9, wall=23072
2022-05-06 22:55:41 | INFO | train_inner | epoch 007:   1670 / 2955 loss=5.035, ppl=32.77, wps=28196, ups=0.86, wpb=32757.8, bsz=64, num_updates=19400, lr=0.000227038, gnorm=0.561, train_wall=114, gb_free=7.9, wall=23188
2022-05-06 22:57:35 | INFO | train_inner | epoch 007:   1770 / 2955 loss=5.038, ppl=32.85, wps=28675.5, ups=0.88, wpb=32763, bsz=64, num_updates=19500, lr=0.000226455, gnorm=0.561, train_wall=113, gb_free=7.9, wall=23302
2022-05-06 22:59:29 | INFO | train_inner | epoch 007:   1870 / 2955 loss=5.036, ppl=32.81, wps=28702, ups=0.88, wpb=32768, bsz=64, num_updates=19600, lr=0.000225877, gnorm=0.564, train_wall=113, gb_free=7.9, wall=23417
2022-05-06 23:01:23 | INFO | train_inner | epoch 007:   1970 / 2955 loss=5.037, ppl=32.83, wps=28726.8, ups=0.88, wpb=32768, bsz=64, num_updates=19700, lr=0.000225303, gnorm=0.555, train_wall=113, gb_free=7.9, wall=23531
2022-05-06 23:03:18 | INFO | train_inner | epoch 007:   2070 / 2955 loss=5.035, ppl=32.79, wps=28594.8, ups=0.87, wpb=32768, bsz=64, num_updates=19800, lr=0.000224733, gnorm=0.562, train_wall=113, gb_free=7.9, wall=23645
2022-05-06 23:05:14 | INFO | train_inner | epoch 007:   2170 / 2955 loss=5.033, ppl=32.73, wps=28124.3, ups=0.86, wpb=32768, bsz=64, num_updates=19900, lr=0.000224168, gnorm=0.559, train_wall=115, gb_free=7.9, wall=23762
2022-05-06 23:07:10 | INFO | train_inner | epoch 007:   2270 / 2955 loss=5.045, ppl=33.01, wps=28410.3, ups=0.87, wpb=32768, bsz=64, num_updates=20000, lr=0.000223607, gnorm=0.563, train_wall=113, gb_free=7.9, wall=23877
2022-05-06 23:09:04 | INFO | train_inner | epoch 007:   2370 / 2955 loss=5.04, ppl=32.89, wps=28759.3, ups=0.88, wpb=32768, bsz=64, num_updates=20100, lr=0.00022305, gnorm=0.563, train_wall=112, gb_free=7.9, wall=23991
2022-05-06 23:10:58 | INFO | train_inner | epoch 007:   2470 / 2955 loss=5.027, ppl=32.59, wps=28778.8, ups=0.88, wpb=32768, bsz=64, num_updates=20200, lr=0.000222497, gnorm=0.555, train_wall=113, gb_free=7.9, wall=24105
2022-05-06 23:12:52 | INFO | train_inner | epoch 007:   2570 / 2955 loss=5.062, ppl=33.4, wps=28650.8, ups=0.87, wpb=32768, bsz=64, num_updates=20300, lr=0.000221948, gnorm=0.56, train_wall=112, gb_free=7.9, wall=24219
2022-05-06 23:14:49 | INFO | train_inner | epoch 007:   2670 / 2955 loss=5.027, ppl=32.61, wps=27904.1, ups=0.85, wpb=32768, bsz=64, num_updates=20400, lr=0.000221404, gnorm=0.563, train_wall=116, gb_free=7.9, wall=24337
2022-05-06 23:16:43 | INFO | train_inner | epoch 007:   2770 / 2955 loss=5.025, ppl=32.55, wps=28731.5, ups=0.88, wpb=32768, bsz=64, num_updates=20500, lr=0.000220863, gnorm=0.564, train_wall=112, gb_free=7.9, wall=24451
2022-05-06 23:18:37 | INFO | train_inner | epoch 007:   2870 / 2955 loss=5.032, ppl=32.71, wps=28772.3, ups=0.88, wpb=32768, bsz=64, num_updates=20600, lr=0.000220326, gnorm=0.565, train_wall=112, gb_free=7.9, wall=24565
2022-05-06 23:20:14 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-05-06 23:21:26 | INFO | valid | epoch 007 | valid on 'valid' subset | loss 4.992 | ppl 31.82 | wps 75824.8 | wpb 2047.4 | bsz 4 | num_updates 20685 | best_loss 4.992
2022-05-06 23:21:26 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 7 @ 20685 updates
2022-05-06 23:21:26 | INFO | fairseq.trainer | Saving checkpoint to /cluster/work/cotterell/liam/master-thesis/checkpoints/language_model/standard/checkpoint_best_standard_16_5.0000E-04_full.pt
2022-05-06 23:21:28 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/work/cotterell/liam/master-thesis/checkpoints/language_model/standard/checkpoint_best_standard_16_5.0000E-04_full.pt
2022-05-06 23:21:28 | INFO | fairseq.checkpoint_utils | Saved checkpoint checkpoints/language_model/standard/checkpoint_best_standard_16_5.0000E-04_full.pt (epoch 7 @ 20685 updates, score 4.992) (writing took 1.7841669749468565 seconds)
2022-05-06 23:21:28 | INFO | fairseq_cli.train | end of epoch 7 (average epoch stats below)
2022-05-06 23:21:28 | INFO | train | epoch 007 | loss 5.034 | ppl 32.75 | wps 27613.9 | ups 0.84 | wpb 32764.7 | bsz 64 | num_updates 20685 | lr 0.000219873 | gnorm 0.558 | train_wall 3349 | gb_free 7.9 | wall 24735
2022-05-06 23:21:28 | INFO | fairseq.data.iterators | grouped total_num_itrs = 2955
2022-05-06 23:21:28 | INFO | fairseq.trainer | begin training epoch 8
2022-05-06 23:21:28 | INFO | fairseq_cli.train | Start iterating over samples
2022-05-06 23:21:45 | INFO | train_inner | epoch 008:     15 / 2955 loss=5.019, ppl=32.42, wps=17415.6, ups=0.53, wpb=32686.1, bsz=63.8, num_updates=20700, lr=0.000219793, gnorm=0.562, train_wall=113, gb_free=7.9, wall=24752
2022-05-06 23:23:39 | INFO | train_inner | epoch 008:    115 / 2955 loss=4.957, ppl=31.06, wps=28774.6, ups=0.88, wpb=32768, bsz=64, num_updates=20800, lr=0.000219265, gnorm=0.563, train_wall=112, gb_free=7.9, wall=24866
2022-05-06 23:25:34 | INFO | train_inner | epoch 008:    215 / 2955 loss=4.959, ppl=31.11, wps=28349.1, ups=0.87, wpb=32768, bsz=64, num_updates=20900, lr=0.000218739, gnorm=0.569, train_wall=112, gb_free=7.9, wall=24982
2022-05-06 23:27:28 | INFO | train_inner | epoch 008:    315 / 2955 loss=4.972, ppl=31.39, wps=28757.2, ups=0.88, wpb=32768, bsz=64, num_updates=21000, lr=0.000218218, gnorm=0.567, train_wall=113, gb_free=7.9, wall=25096
2022-05-06 23:29:22 | INFO | train_inner | epoch 008:    415 / 2955 loss=4.988, ppl=31.75, wps=28749.3, ups=0.88, wpb=32768, bsz=64, num_updates=21100, lr=0.0002177, gnorm=0.57, train_wall=113, gb_free=7.9, wall=25210
2022-05-06 23:31:22 | INFO | train_inner | epoch 008:    515 / 2955 loss=4.974, ppl=31.43, wps=27474.5, ups=0.84, wpb=32768, bsz=64, num_updates=21200, lr=0.000217186, gnorm=0.567, train_wall=118, gb_free=7.9, wall=25329
2022-05-06 23:33:17 | INFO | train_inner | epoch 008:    615 / 2955 loss=4.987, ppl=31.71, wps=28333.2, ups=0.86, wpb=32768, bsz=64, num_updates=21300, lr=0.000216676, gnorm=0.566, train_wall=113, gb_free=7.9, wall=25445
2022-05-06 23:35:19 | INFO | train_inner | epoch 008:    715 / 2955 loss=4.969, ppl=31.32, wps=26986.6, ups=0.82, wpb=32768, bsz=64, num_updates=21400, lr=0.000216169, gnorm=0.566, train_wall=112, gb_free=7.9, wall=25566
2022-05-06 23:37:20 | INFO | train_inner | epoch 008:    815 / 2955 loss=4.98, ppl=31.56, wps=27093.1, ups=0.83, wpb=32768, bsz=64, num_updates=21500, lr=0.000215666, gnorm=0.567, train_wall=112, gb_free=7.9, wall=25687
2022-05-06 23:39:25 | INFO | train_inner | epoch 008:    915 / 2955 loss=4.98, ppl=31.57, wps=26205.2, ups=0.8, wpb=32768, bsz=64, num_updates=21600, lr=0.000215166, gnorm=0.57, train_wall=117, gb_free=7.9, wall=25812
2022-05-06 23:41:28 | INFO | train_inner | epoch 008:   1015 / 2955 loss=4.973, ppl=31.42, wps=26671.2, ups=0.81, wpb=32768, bsz=64, num_updates=21700, lr=0.000214669, gnorm=0.577, train_wall=117, gb_free=7.9, wall=25935
2022-05-06 23:43:24 | INFO | train_inner | epoch 008:   1115 / 2955 loss=4.986, ppl=31.68, wps=28052.7, ups=0.86, wpb=32768, bsz=64, num_updates=21800, lr=0.000214176, gnorm=0.567, train_wall=112, gb_free=7.9, wall=26052
2022-05-06 23:45:20 | INFO | train_inner | epoch 008:   1215 / 2955 loss=4.983, ppl=31.63, wps=28252.8, ups=0.86, wpb=32768, bsz=64, num_updates=21900, lr=0.000213687, gnorm=0.569, train_wall=112, gb_free=7.9, wall=26168
2022-05-06 23:47:19 | INFO | train_inner | epoch 008:   1315 / 2955 loss=5.005, ppl=32.1, wps=27567.2, ups=0.84, wpb=32768, bsz=64, num_updates=22000, lr=0.000213201, gnorm=0.576, train_wall=116, gb_free=7.9, wall=26287
2022-05-06 23:49:14 | INFO | train_inner | epoch 008:   1415 / 2955 loss=4.994, ppl=31.86, wps=28547.6, ups=0.87, wpb=32768, bsz=64, num_updates=22100, lr=0.000212718, gnorm=0.572, train_wall=112, gb_free=7.9, wall=26401
2022-05-06 23:51:10 | INFO | train_inner | epoch 008:   1515 / 2955 loss=4.97, ppl=31.35, wps=28207.4, ups=0.86, wpb=32763, bsz=64, num_updates=22200, lr=0.000212238, gnorm=0.567, train_wall=113, gb_free=7.9, wall=26517
2022-05-06 23:53:04 | INFO | train_inner | epoch 008:   1615 / 2955 loss=4.989, ppl=31.76, wps=28675.7, ups=0.88, wpb=32768, bsz=64, num_updates=22300, lr=0.000211762, gnorm=0.573, train_wall=112, gb_free=7.9, wall=26632
2022-05-06 23:54:59 | INFO | train_inner | epoch 008:   1715 / 2955 loss=5.007, ppl=32.16, wps=28719.5, ups=0.88, wpb=32768, bsz=64, num_updates=22400, lr=0.000211289, gnorm=0.572, train_wall=112, gb_free=7.9, wall=26746
2022-05-06 23:56:53 | INFO | train_inner | epoch 008:   1815 / 2955 loss=4.997, ppl=31.92, wps=28503.8, ups=0.87, wpb=32768, bsz=64, num_updates=22500, lr=0.000210819, gnorm=0.571, train_wall=112, gb_free=7.9, wall=26861
2022-05-06 23:58:48 | INFO | train_inner | epoch 008:   1915 / 2955 loss=4.982, ppl=31.6, wps=28730.2, ups=0.88, wpb=32768, bsz=64, num_updates=22600, lr=0.000210352, gnorm=0.571, train_wall=113, gb_free=7.9, wall=26975
2022-05-07 00:00:42 | INFO | train_inner | epoch 008:   2015 / 2955 loss=4.978, ppl=31.51, wps=28517.9, ups=0.87, wpb=32768, bsz=64, num_updates=22700, lr=0.000209888, gnorm=0.578, train_wall=113, gb_free=7.9, wall=27090
2022-05-07 00:02:41 | INFO | train_inner | epoch 008:   2115 / 2955 loss=4.985, ppl=31.66, wps=27542.2, ups=0.84, wpb=32768, bsz=64, num_updates=22800, lr=0.000209427, gnorm=0.574, train_wall=117, gb_free=7.9, wall=27209
2022-05-07 00:04:35 | INFO | train_inner | epoch 008:   2215 / 2955 loss=4.99, ppl=31.78, wps=28734.8, ups=0.88, wpb=32768, bsz=64, num_updates=22900, lr=0.000208969, gnorm=0.575, train_wall=113, gb_free=7.9, wall=27323
2022-05-07 00:06:31 | INFO | train_inner | epoch 008:   2315 / 2955 loss=4.99, ppl=31.77, wps=28445.8, ups=0.87, wpb=32768, bsz=64, num_updates=23000, lr=0.000208514, gnorm=0.574, train_wall=114, gb_free=7.9, wall=27438
2022-05-07 00:08:24 | INFO | train_inner | epoch 008:   2415 / 2955 loss=4.999, ppl=31.97, wps=28778.7, ups=0.88, wpb=32768, bsz=64, num_updates=23100, lr=0.000208063, gnorm=0.574, train_wall=112, gb_free=7.9, wall=27552
2022-05-07 00:10:19 | INFO | train_inner | epoch 008:   2515 / 2955 loss=4.985, ppl=31.66, wps=28531, ups=0.87, wpb=32768, bsz=64, num_updates=23200, lr=0.000207614, gnorm=0.575, train_wall=113, gb_free=7.9, wall=27667
2022-05-07 00:12:17 | INFO | train_inner | epoch 008:   2615 / 2955 loss=4.986, ppl=31.68, wps=27908, ups=0.85, wpb=32768, bsz=64, num_updates=23300, lr=0.000207168, gnorm=0.575, train_wall=115, gb_free=7.9, wall=27784
2022-05-07 00:14:11 | INFO | train_inner | epoch 008:   2715 / 2955 loss=5.006, ppl=32.13, wps=28733.5, ups=0.88, wpb=32768, bsz=64, num_updates=23400, lr=0.000206725, gnorm=0.575, train_wall=113, gb_free=7.9, wall=27898
2022-05-07 00:16:11 | INFO | train_inner | epoch 008:   2815 / 2955 loss=4.981, ppl=31.59, wps=27218.3, ups=0.83, wpb=32757.8, bsz=64, num_updates=23500, lr=0.000206284, gnorm=0.571, train_wall=117, gb_free=7.9, wall=28018
2022-05-07 00:18:05 | INFO | train_inner | epoch 008:   2915 / 2955 loss=4.976, ppl=31.46, wps=28674.1, ups=0.88, wpb=32768, bsz=64, num_updates=23600, lr=0.000205847, gnorm=0.577, train_wall=112, gb_free=7.9, wall=28133
2022-05-07 00:18:51 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-05-07 00:20:02 | INFO | valid | epoch 008 | valid on 'valid' subset | loss 4.958 | ppl 31.07 | wps 76140.7 | wpb 2047.4 | bsz 4 | num_updates 23640 | best_loss 4.958
2022-05-07 00:20:02 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 8 @ 23640 updates
2022-05-07 00:20:02 | INFO | fairseq.trainer | Saving checkpoint to /cluster/work/cotterell/liam/master-thesis/checkpoints/language_model/standard/checkpoint_best_standard_16_5.0000E-04_full.pt
2022-05-07 00:20:03 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/work/cotterell/liam/master-thesis/checkpoints/language_model/standard/checkpoint_best_standard_16_5.0000E-04_full.pt
2022-05-07 00:20:03 | INFO | fairseq.checkpoint_utils | Saved checkpoint checkpoints/language_model/standard/checkpoint_best_standard_16_5.0000E-04_full.pt (epoch 8 @ 23640 updates, score 4.958) (writing took 1.5310349990613759 seconds)
2022-05-07 00:20:03 | INFO | fairseq_cli.train | end of epoch 8 (average epoch stats below)
2022-05-07 00:20:03 | INFO | train | epoch 008 | loss 4.984 | ppl 31.64 | wps 27539.4 | ups 0.84 | wpb 32764.7 | bsz 64 | num_updates 23640 | lr 0.000205673 | gnorm 0.571 | train_wall 3355 | gb_free 7.9 | wall 28251
2022-05-07 00:20:03 | INFO | fairseq.data.iterators | grouped total_num_itrs = 2955
2022-05-07 00:20:03 | INFO | fairseq.trainer | begin training epoch 9
2022-05-07 00:20:03 | INFO | fairseq_cli.train | Start iterating over samples
2022-05-07 00:21:12 | INFO | train_inner | epoch 009:     60 / 2955 loss=4.948, ppl=30.86, wps=17520.8, ups=0.54, wpb=32686.1, bsz=63.8, num_updates=23700, lr=0.000205412, gnorm=0.577, train_wall=112, gb_free=7.9, wall=28319
2022-05-07 00:23:06 | INFO | train_inner | epoch 009:    160 / 2955 loss=4.907, ppl=30, wps=28740.8, ups=0.88, wpb=32768, bsz=64, num_updates=23800, lr=0.00020498, gnorm=0.579, train_wall=112, gb_free=7.9, wall=28433
2022-05-07 00:25:00 | INFO | train_inner | epoch 009:    260 / 2955 loss=4.93, ppl=30.48, wps=28720.2, ups=0.88, wpb=32768, bsz=64, num_updates=23900, lr=0.000204551, gnorm=0.576, train_wall=113, gb_free=7.9, wall=28547
2022-05-07 00:26:54 | INFO | train_inner | epoch 009:    360 / 2955 loss=4.942, ppl=30.74, wps=28721.4, ups=0.88, wpb=32763, bsz=64, num_updates=24000, lr=0.000204124, gnorm=0.58, train_wall=113, gb_free=7.9, wall=28661
2022-05-07 00:28:48 | INFO | train_inner | epoch 009:    460 / 2955 loss=4.931, ppl=30.51, wps=28769.2, ups=0.88, wpb=32768, bsz=64, num_updates=24100, lr=0.0002037, gnorm=0.579, train_wall=112, gb_free=7.9, wall=28775
2022-05-07 00:30:56 | INFO | train_inner | epoch 009:    560 / 2955 loss=4.938, ppl=30.64, wps=25621.7, ups=0.78, wpb=32768, bsz=64, num_updates=24200, lr=0.000203279, gnorm=0.581, train_wall=116, gb_free=7.9, wall=28903
2022-05-07 00:32:57 | INFO | train_inner | epoch 009:    660 / 2955 loss=4.939, ppl=30.67, wps=27135.9, ups=0.83, wpb=32768, bsz=64, num_updates=24300, lr=0.00020286, gnorm=0.585, train_wall=112, gb_free=7.9, wall=29024
2022-05-07 00:34:56 | INFO | train_inner | epoch 009:    760 / 2955 loss=4.927, ppl=30.43, wps=27433.3, ups=0.84, wpb=32768, bsz=64, num_updates=24400, lr=0.000202444, gnorm=0.58, train_wall=112, gb_free=7.9, wall=29143
2022-05-07 00:36:54 | INFO | train_inner | epoch 009:    860 / 2955 loss=4.943, ppl=30.76, wps=27705.2, ups=0.85, wpb=32768, bsz=64, num_updates=24500, lr=0.000202031, gnorm=0.582, train_wall=113, gb_free=7.9, wall=29262
2022-05-07 00:38:51 | INFO | train_inner | epoch 009:    960 / 2955 loss=4.932, ppl=30.53, wps=28060.1, ups=0.86, wpb=32768, bsz=64, num_updates=24600, lr=0.000201619, gnorm=0.58, train_wall=112, gb_free=7.9, wall=29379
2022-05-07 00:40:52 | INFO | train_inner | epoch 009:   1060 / 2955 loss=4.943, ppl=30.76, wps=27201.8, ups=0.83, wpb=32768, bsz=64, num_updates=24700, lr=0.000201211, gnorm=0.581, train_wall=116, gb_free=7.9, wall=29499
2022-05-07 00:42:47 | INFO | train_inner | epoch 009:   1160 / 2955 loss=4.94, ppl=30.69, wps=28442.8, ups=0.87, wpb=32768, bsz=64, num_updates=24800, lr=0.000200805, gnorm=0.58, train_wall=112, gb_free=7.9, wall=29614
2022-05-07 00:44:42 | INFO | train_inner | epoch 009:   1260 / 2955 loss=4.95, ppl=30.92, wps=28527.6, ups=0.87, wpb=32768, bsz=64, num_updates=24900, lr=0.000200401, gnorm=0.579, train_wall=112, gb_free=7.9, wall=29729
2022-05-07 00:46:40 | INFO | train_inner | epoch 009:   1360 / 2955 loss=4.948, ppl=30.87, wps=27711.3, ups=0.85, wpb=32768, bsz=64, num_updates=25000, lr=0.0002, gnorm=0.585, train_wall=116, gb_free=7.9, wall=29847
2022-05-07 00:48:34 | INFO | train_inner | epoch 009:   1460 / 2955 loss=4.939, ppl=30.67, wps=28677, ups=0.88, wpb=32768, bsz=64, num_updates=25100, lr=0.000199601, gnorm=0.584, train_wall=113, gb_free=7.9, wall=29962
2022-05-07 00:50:35 | INFO | train_inner | epoch 009:   1560 / 2955 loss=4.946, ppl=30.83, wps=27120.5, ups=0.83, wpb=32768, bsz=64, num_updates=25200, lr=0.000199205, gnorm=0.589, train_wall=117, gb_free=7.9, wall=30082
2022-05-07 00:52:30 | INFO | train_inner | epoch 009:   1660 / 2955 loss=4.947, ppl=30.84, wps=28621.7, ups=0.87, wpb=32768, bsz=64, num_updates=25300, lr=0.000198811, gnorm=0.584, train_wall=112, gb_free=7.9, wall=30197
2022-05-07 00:54:24 | INFO | train_inner | epoch 009:   1760 / 2955 loss=4.949, ppl=30.89, wps=28636.2, ups=0.87, wpb=32768, bsz=64, num_updates=25400, lr=0.000198419, gnorm=0.581, train_wall=113, gb_free=7.9, wall=30311
2022-05-07 00:56:18 | INFO | train_inner | epoch 009:   1860 / 2955 loss=4.936, ppl=30.62, wps=28688.3, ups=0.88, wpb=32757.8, bsz=64, num_updates=25500, lr=0.00019803, gnorm=0.579, train_wall=113, gb_free=7.9, wall=30425
2022-05-07 00:58:12 | INFO | train_inner | epoch 009:   1960 / 2955 loss=4.942, ppl=30.73, wps=28729, ups=0.88, wpb=32768, bsz=64, num_updates=25600, lr=0.000197642, gnorm=0.59, train_wall=113, gb_free=7.9, wall=30540
2022-05-07 01:00:06 | INFO | train_inner | epoch 009:   2060 / 2955 loss=4.97, ppl=31.33, wps=28711.8, ups=0.88, wpb=32768, bsz=64, num_updates=25700, lr=0.000197257, gnorm=0.587, train_wall=113, gb_free=7.9, wall=30654
2022-05-07 01:02:02 | INFO | train_inner | epoch 009:   2160 / 2955 loss=4.953, ppl=30.97, wps=28336.2, ups=0.86, wpb=32768, bsz=64, num_updates=25800, lr=0.000196875, gnorm=0.585, train_wall=112, gb_free=7.9, wall=30769
2022-05-07 01:03:57 | INFO | train_inner | epoch 009:   2260 / 2955 loss=4.967, ppl=31.29, wps=28397.8, ups=0.87, wpb=32768, bsz=64, num_updates=25900, lr=0.000196494, gnorm=0.586, train_wall=113, gb_free=7.9, wall=30885
2022-05-07 01:05:55 | INFO | train_inner | epoch 009:   2360 / 2955 loss=4.953, ppl=30.97, wps=27750.2, ups=0.85, wpb=32768, bsz=64, num_updates=26000, lr=0.000196116, gnorm=0.586, train_wall=116, gb_free=7.9, wall=31003
2022-05-07 01:07:50 | INFO | train_inner | epoch 009:   2460 / 2955 loss=4.952, ppl=30.95, wps=28539, ups=0.87, wpb=32768, bsz=64, num_updates=26100, lr=0.00019574, gnorm=0.585, train_wall=113, gb_free=7.9, wall=31118
2022-05-07 01:09:45 | INFO | train_inner | epoch 009:   2560 / 2955 loss=4.954, ppl=30.99, wps=28631.2, ups=0.87, wpb=32768, bsz=64, num_updates=26200, lr=0.000195366, gnorm=0.589, train_wall=113, gb_free=7.9, wall=31232
2022-05-07 01:11:39 | INFO | train_inner | epoch 009:   2660 / 2955 loss=4.944, ppl=30.78, wps=28606.7, ups=0.87, wpb=32768, bsz=64, num_updates=26300, lr=0.000194994, gnorm=0.585, train_wall=112, gb_free=7.9, wall=31347
2022-05-07 01:13:34 | INFO | train_inner | epoch 009:   2760 / 2955 loss=4.945, ppl=30.8, wps=28661.4, ups=0.87, wpb=32768, bsz=64, num_updates=26400, lr=0.000194625, gnorm=0.585, train_wall=113, gb_free=7.9, wall=31461
2022-05-07 01:15:31 | INFO | train_inner | epoch 009:   2860 / 2955 loss=4.958, ppl=31.08, wps=27928, ups=0.85, wpb=32768, bsz=64, num_updates=26500, lr=0.000194257, gnorm=0.585, train_wall=115, gb_free=7.9, wall=31578
2022-05-07 01:17:21 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-05-07 01:18:32 | INFO | valid | epoch 009 | valid on 'valid' subset | loss 4.929 | ppl 30.47 | wps 76235.2 | wpb 2047.4 | bsz 4 | num_updates 26595 | best_loss 4.929
2022-05-07 01:18:32 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 9 @ 26595 updates
2022-05-07 01:18:32 | INFO | fairseq.trainer | Saving checkpoint to /cluster/work/cotterell/liam/master-thesis/checkpoints/language_model/standard/checkpoint_best_standard_16_5.0000E-04_full.pt
2022-05-07 01:18:34 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/work/cotterell/liam/master-thesis/checkpoints/language_model/standard/checkpoint_best_standard_16_5.0000E-04_full.pt
2022-05-07 01:18:34 | INFO | fairseq.checkpoint_utils | Saved checkpoint checkpoints/language_model/standard/checkpoint_best_standard_16_5.0000E-04_full.pt (epoch 9 @ 26595 updates, score 4.929) (writing took 1.7644144394434988 seconds)
2022-05-07 01:18:34 | INFO | fairseq_cli.train | end of epoch 9 (average epoch stats below)
2022-05-07 01:18:34 | INFO | train | epoch 009 | loss 4.944 | ppl 30.77 | wps 27580.3 | ups 0.84 | wpb 32764.7 | bsz 64 | num_updates 26595 | lr 0.00019391 | gnorm 0.583 | train_wall 3349 | gb_free 7.9 | wall 31761
2022-05-07 01:18:34 | INFO | fairseq.data.iterators | grouped total_num_itrs = 2955
2022-05-07 01:18:34 | INFO | fairseq.trainer | begin training epoch 10
2022-05-07 01:18:34 | INFO | fairseq_cli.train | Start iterating over samples
2022-05-07 01:18:41 | INFO | train_inner | epoch 010:      5 / 2955 loss=4.95, ppl=30.92, wps=17177.6, ups=0.53, wpb=32686.1, bsz=63.8, num_updates=26600, lr=0.000193892, gnorm=0.587, train_wall=114, gb_free=7.9, wall=31769
2022-05-07 01:20:36 | INFO | train_inner | epoch 010:    105 / 2955 loss=4.888, ppl=29.61, wps=28669, ups=0.87, wpb=32768, bsz=64, num_updates=26700, lr=0.000193528, gnorm=0.589, train_wall=112, gb_free=7.9, wall=31883
2022-05-07 01:22:30 | INFO | train_inner | epoch 010:    205 / 2955 loss=4.867, ppl=29.19, wps=28744.2, ups=0.88, wpb=32768, bsz=64, num_updates=26800, lr=0.000193167, gnorm=0.589, train_wall=113, gb_free=7.9, wall=31997
2022-05-07 01:24:23 | INFO | train_inner | epoch 010:    305 / 2955 loss=4.902, ppl=29.89, wps=28763.7, ups=0.88, wpb=32768, bsz=64, num_updates=26900, lr=0.000192807, gnorm=0.592, train_wall=113, gb_free=7.9, wall=32111
2022-05-07 01:26:18 | INFO | train_inner | epoch 010:    405 / 2955 loss=4.882, ppl=29.5, wps=28510.1, ups=0.87, wpb=32768, bsz=64, num_updates=27000, lr=0.00019245, gnorm=0.596, train_wall=113, gb_free=7.9, wall=32226
2022-05-07 01:28:17 | INFO | train_inner | epoch 010:    505 / 2955 loss=4.903, ppl=29.92, wps=27631.5, ups=0.84, wpb=32768, bsz=64, num_updates=27100, lr=0.000192095, gnorm=0.592, train_wall=113, gb_free=7.9, wall=32344
2022-05-07 01:30:19 | INFO | train_inner | epoch 010:    605 / 2955 loss=4.903, ppl=29.93, wps=26807.6, ups=0.82, wpb=32768, bsz=64, num_updates=27200, lr=0.000191741, gnorm=0.592, train_wall=113, gb_free=7.9, wall=32467
2022-05-07 01:32:19 | INFO | train_inner | epoch 010:    705 / 2955 loss=4.913, ppl=30.12, wps=27335.7, ups=0.83, wpb=32768, bsz=64, num_updates=27300, lr=0.00019139, gnorm=0.598, train_wall=112, gb_free=7.9, wall=32586
2022-05-07 01:34:18 | INFO | train_inner | epoch 010:    805 / 2955 loss=4.891, ppl=29.67, wps=27673, ups=0.84, wpb=32768, bsz=64, num_updates=27400, lr=0.00019104, gnorm=0.59, train_wall=112, gb_free=7.9, wall=32705
2022-05-07 01:36:19 | INFO | train_inner | epoch 010:    905 / 2955 loss=4.908, ppl=30.02, wps=27015.3, ups=0.82, wpb=32768, bsz=64, num_updates=27500, lr=0.000190693, gnorm=0.588, train_wall=116, gb_free=7.9, wall=32826
2022-05-07 01:38:15 | INFO | train_inner | epoch 010:   1005 / 2955 loss=4.902, ppl=29.9, wps=28082.9, ups=0.86, wpb=32768, bsz=64, num_updates=27600, lr=0.000190347, gnorm=0.592, train_wall=113, gb_free=7.9, wall=32943
2022-05-07 01:40:12 | INFO | train_inner | epoch 010:   1105 / 2955 loss=4.916, ppl=30.19, wps=28009.5, ups=0.86, wpb=32757.8, bsz=64, num_updates=27700, lr=0.000190003, gnorm=0.598, train_wall=114, gb_free=7.9, wall=33060
2022-05-07 01:42:08 | INFO | train_inner | epoch 010:   1205 / 2955 loss=4.912, ppl=30.1, wps=28295.2, ups=0.86, wpb=32768, bsz=64, num_updates=27800, lr=0.000189661, gnorm=0.596, train_wall=112, gb_free=7.9, wall=33176
2022-05-07 01:44:03 | INFO | train_inner | epoch 010:   1305 / 2955 loss=4.922, ppl=30.31, wps=28445, ups=0.87, wpb=32768, bsz=64, num_updates=27900, lr=0.000189321, gnorm=0.592, train_wall=112, gb_free=7.9, wall=33291
2022-05-07 01:46:03 | INFO | train_inner | epoch 010:   1405 / 2955 loss=4.898, ppl=29.81, wps=27523.7, ups=0.84, wpb=32768, bsz=64, num_updates=28000, lr=0.000188982, gnorm=0.593, train_wall=116, gb_free=7.9, wall=33410
2022-05-07 01:47:57 | INFO | train_inner | epoch 010:   1505 / 2955 loss=4.912, ppl=30.11, wps=28521.1, ups=0.87, wpb=32768, bsz=64, num_updates=28100, lr=0.000188646, gnorm=0.595, train_wall=113, gb_free=7.9, wall=33525
2022-05-07 01:49:55 | INFO | train_inner | epoch 010:   1605 / 2955 loss=4.902, ppl=29.91, wps=27949.4, ups=0.85, wpb=32768, bsz=64, num_updates=28200, lr=0.000188311, gnorm=0.591, train_wall=114, gb_free=7.9, wall=33642
2022-05-07 01:51:49 | INFO | train_inner | epoch 010:   1705 / 2955 loss=4.927, ppl=30.42, wps=28630.2, ups=0.87, wpb=32768, bsz=64, num_updates=28300, lr=0.000187978, gnorm=0.597, train_wall=113, gb_free=7.9, wall=33756
2022-05-07 01:53:44 | INFO | train_inner | epoch 010:   1805 / 2955 loss=4.911, ppl=30.08, wps=28597.5, ups=0.87, wpb=32768, bsz=64, num_updates=28400, lr=0.000187647, gnorm=0.6, train_wall=113, gb_free=7.9, wall=33871
2022-05-07 01:55:42 | INFO | train_inner | epoch 010:   1905 / 2955 loss=4.905, ppl=29.95, wps=27739.8, ups=0.85, wpb=32768, bsz=64, num_updates=28500, lr=0.000187317, gnorm=0.596, train_wall=116, gb_free=7.9, wall=33989
2022-05-07 01:57:36 | INFO | train_inner | epoch 010:   2005 / 2955 loss=4.919, ppl=30.25, wps=28604.8, ups=0.87, wpb=32768, bsz=64, num_updates=28600, lr=0.000186989, gnorm=0.596, train_wall=113, gb_free=7.9, wall=34104
2022-05-07 01:59:31 | INFO | train_inner | epoch 010:   2105 / 2955 loss=4.916, ppl=30.18, wps=28601.5, ups=0.87, wpb=32763, bsz=64, num_updates=28700, lr=0.000186663, gnorm=0.592, train_wall=113, gb_free=7.9, wall=34218
2022-05-07 02:01:35 | INFO | train_inner | epoch 010:   2205 / 2955 loss=4.914, ppl=30.16, wps=26467.2, ups=0.81, wpb=32768, bsz=64, num_updates=28800, lr=0.000186339, gnorm=0.6, train_wall=113, gb_free=7.9, wall=34342
2022-05-07 02:03:36 | INFO | train_inner | epoch 010:   2305 / 2955 loss=4.938, ppl=30.66, wps=27089.2, ups=0.83, wpb=32768, bsz=64, num_updates=28900, lr=0.000186016, gnorm=0.596, train_wall=112, gb_free=7.9, wall=34463
2022-05-07 02:05:36 | INFO | train_inner | epoch 010:   2405 / 2955 loss=4.915, ppl=30.18, wps=27142.1, ups=0.83, wpb=32768, bsz=64, num_updates=29000, lr=0.000185695, gnorm=0.599, train_wall=112, gb_free=7.9, wall=34584
2022-05-07 02:07:37 | INFO | train_inner | epoch 010:   2505 / 2955 loss=4.917, ppl=30.21, wps=27247.4, ups=0.83, wpb=32768, bsz=64, num_updates=29100, lr=0.000185376, gnorm=0.594, train_wall=114, gb_free=7.9, wall=34704
2022-05-07 02:09:37 | INFO | train_inner | epoch 010:   2605 / 2955 loss=4.923, ppl=30.33, wps=27126.4, ups=0.83, wpb=32768, bsz=64, num_updates=29200, lr=0.000185058, gnorm=0.597, train_wall=116, gb_free=7.9, wall=34825
2022-05-07 02:11:34 | INFO | train_inner | epoch 010:   2705 / 2955 loss=4.921, ppl=30.29, wps=28137.6, ups=0.86, wpb=32768, bsz=64, num_updates=29300, lr=0.000184742, gnorm=0.598, train_wall=113, gb_free=7.9, wall=34941
2022-05-07 02:13:30 | INFO | train_inner | epoch 010:   2805 / 2955 loss=4.909, ppl=30.05, wps=28286.5, ups=0.86, wpb=32768, bsz=64, num_updates=29400, lr=0.000184428, gnorm=0.594, train_wall=113, gb_free=7.9, wall=35057
2022-05-07 02:15:26 | INFO | train_inner | epoch 010:   2905 / 2955 loss=4.944, ppl=30.78, wps=28223.1, ups=0.86, wpb=32768, bsz=64, num_updates=29500, lr=0.000184115, gnorm=0.6, train_wall=113, gb_free=7.9, wall=35173
2022-05-07 02:16:23 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-05-07 02:17:36 | INFO | valid | epoch 010 | valid on 'valid' subset | loss 4.907 | ppl 29.99 | wps 73094.5 | wpb 2047.4 | bsz 4 | num_updates 29550 | best_loss 4.907
2022-05-07 02:17:36 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 10 @ 29550 updates
2022-05-07 02:17:36 | INFO | fairseq.trainer | Saving checkpoint to /cluster/work/cotterell/liam/master-thesis/checkpoints/language_model/standard/checkpoint_best_standard_16_5.0000E-04_full.pt
2022-05-07 02:17:38 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/work/cotterell/liam/master-thesis/checkpoints/language_model/standard/checkpoint_best_standard_16_5.0000E-04_full.pt
2022-05-07 02:17:38 | INFO | fairseq.checkpoint_utils | Saved checkpoint checkpoints/language_model/standard/checkpoint_best_standard_16_5.0000E-04_full.pt (epoch 10 @ 29550 updates, score 4.907) (writing took 1.3641712768003345 seconds)
2022-05-07 02:17:38 | INFO | fairseq_cli.train | end of epoch 10 (average epoch stats below)
2022-05-07 02:17:38 | INFO | train | epoch 010 | loss 4.91 | ppl 30.07 | wps 27320.2 | ups 0.83 | wpb 32764.7 | bsz 64 | num_updates 29550 | lr 0.000183959 | gnorm 0.595 | train_wall 3343 | gb_free 7.9 | wall 35305
2022-05-07 02:17:38 | INFO | fairseq.data.iterators | grouped total_num_itrs = 2955
2022-05-07 02:17:38 | INFO | fairseq.trainer | begin training epoch 11
2022-05-07 02:17:38 | INFO | fairseq_cli.train | Start iterating over samples
2022-05-07 02:18:36 | INFO | train_inner | epoch 011:     50 / 2955 loss=4.886, ppl=29.56, wps=17204.2, ups=0.53, wpb=32686.1, bsz=63.8, num_updates=29600, lr=0.000183804, gnorm=0.597, train_wall=113, gb_free=7.9, wall=35363
2022-05-07 02:20:31 | INFO | train_inner | epoch 011:    150 / 2955 loss=4.858, ppl=29, wps=28459.8, ups=0.87, wpb=32763, bsz=64, num_updates=29700, lr=0.000183494, gnorm=0.602, train_wall=113, gb_free=7.9, wall=35478
2022-05-07 02:22:27 | INFO | train_inner | epoch 011:    250 / 2955 loss=4.847, ppl=28.78, wps=28265.9, ups=0.86, wpb=32768, bsz=64, num_updates=29800, lr=0.000183186, gnorm=0.601, train_wall=113, gb_free=7.9, wall=35594
2022-05-07 02:24:22 | INFO | train_inner | epoch 011:    350 / 2955 loss=4.873, ppl=29.3, wps=28530.2, ups=0.87, wpb=32768, bsz=64, num_updates=29900, lr=0.000182879, gnorm=0.603, train_wall=113, gb_free=7.9, wall=35709
2022-05-07 02:26:16 | INFO | train_inner | epoch 011:    450 / 2955 loss=4.867, ppl=29.19, wps=28572.5, ups=0.87, wpb=32768, bsz=64, num_updates=30000, lr=0.000182574, gnorm=0.601, train_wall=113, gb_free=7.9, wall=35824
2022-05-07 02:28:15 | INFO | train_inner | epoch 011:    550 / 2955 loss=4.878, ppl=29.41, wps=27650.1, ups=0.84, wpb=32768, bsz=64, num_updates=30100, lr=0.000182271, gnorm=0.608, train_wall=115, gb_free=7.9, wall=35942
2022-05-07 02:30:13 | INFO | train_inner | epoch 011:    650 / 2955 loss=4.871, ppl=29.26, wps=27778.4, ups=0.85, wpb=32768, bsz=64, num_updates=30200, lr=0.000181969, gnorm=0.609, train_wall=116, gb_free=7.9, wall=36060
2022-05-07 02:32:07 | INFO | train_inner | epoch 011:    750 / 2955 loss=4.868, ppl=29.21, wps=28615.1, ups=0.87, wpb=32768, bsz=64, num_updates=30300, lr=0.000181668, gnorm=0.601, train_wall=113, gb_free=7.9, wall=36175
2022-05-07 02:34:02 | INFO | train_inner | epoch 011:    850 / 2955 loss=4.865, ppl=29.14, wps=28612.8, ups=0.87, wpb=32768, bsz=64, num_updates=30400, lr=0.000181369, gnorm=0.604, train_wall=113, gb_free=7.9, wall=36289
2022-05-07 02:35:56 | INFO | train_inner | epoch 011:    950 / 2955 loss=4.885, ppl=29.54, wps=28661.6, ups=0.87, wpb=32768, bsz=64, num_updates=30500, lr=0.000181071, gnorm=0.605, train_wall=113, gb_free=7.9, wall=36404
2022-05-07 02:37:53 | INFO | train_inner | epoch 011:   1050 / 2955 loss=4.878, ppl=29.41, wps=28029.5, ups=0.86, wpb=32768, bsz=64, num_updates=30600, lr=0.000180775, gnorm=0.602, train_wall=114, gb_free=7.9, wall=36521
2022-05-07 02:39:56 | INFO | train_inner | epoch 011:   1150 / 2955 loss=4.864, ppl=29.13, wps=26692.1, ups=0.81, wpb=32768, bsz=64, num_updates=30700, lr=0.000180481, gnorm=0.604, train_wall=116, gb_free=7.9, wall=36643
2022-05-07 02:41:55 | INFO | train_inner | epoch 011:   1250 / 2955 loss=4.907, ppl=30, wps=27638, ups=0.84, wpb=32768, bsz=64, num_updates=30800, lr=0.000180187, gnorm=0.611, train_wall=113, gb_free=7.9, wall=36762
2022-05-07 02:43:52 | INFO | train_inner | epoch 011:   1350 / 2955 loss=4.888, ppl=29.6, wps=27876.2, ups=0.85, wpb=32768, bsz=64, num_updates=30900, lr=0.000179896, gnorm=0.604, train_wall=113, gb_free=7.9, wall=36879
2022-05-07 02:45:49 | INFO | train_inner | epoch 011:   1450 / 2955 loss=4.882, ppl=29.49, wps=28040.5, ups=0.86, wpb=32768, bsz=64, num_updates=31000, lr=0.000179605, gnorm=0.605, train_wall=113, gb_free=7.9, wall=36996
2022-05-07 02:47:45 | INFO | train_inner | epoch 011:   1550 / 2955 loss=4.892, ppl=29.7, wps=28266.7, ups=0.86, wpb=32757.8, bsz=64, num_updates=31100, lr=0.000179316, gnorm=0.605, train_wall=113, gb_free=7.9, wall=37112
2022-05-07 02:49:44 | INFO | train_inner | epoch 011:   1650 / 2955 loss=4.878, ppl=29.41, wps=27429.5, ups=0.84, wpb=32768, bsz=64, num_updates=31200, lr=0.000179029, gnorm=0.604, train_wall=116, gb_free=7.9, wall=37232
2022-05-07 02:51:40 | INFO | train_inner | epoch 011:   1750 / 2955 loss=4.887, ppl=29.59, wps=28439.9, ups=0.87, wpb=32768, bsz=64, num_updates=31300, lr=0.000178743, gnorm=0.609, train_wall=113, gb_free=7.9, wall=37347
2022-05-07 02:53:34 | INFO | train_inner | epoch 011:   1850 / 2955 loss=4.899, ppl=29.83, wps=28576.1, ups=0.87, wpb=32768, bsz=64, num_updates=31400, lr=0.000178458, gnorm=0.609, train_wall=113, gb_free=7.9, wall=37461
2022-05-07 02:55:29 | INFO | train_inner | epoch 011:   1950 / 2955 loss=4.89, ppl=29.65, wps=28578.9, ups=0.87, wpb=32768, bsz=64, num_updates=31500, lr=0.000178174, gnorm=0.602, train_wall=113, gb_free=7.9, wall=37576
2022-05-07 02:57:25 | INFO | train_inner | epoch 011:   2050 / 2955 loss=4.889, ppl=29.64, wps=28319.5, ups=0.86, wpb=32768, bsz=64, num_updates=31600, lr=0.000177892, gnorm=0.61, train_wall=114, gb_free=7.9, wall=37692
2022-05-07 02:59:21 | INFO | train_inner | epoch 011:   2150 / 2955 loss=4.885, ppl=29.54, wps=28223.2, ups=0.86, wpb=32768, bsz=64, num_updates=31700, lr=0.000177611, gnorm=0.607, train_wall=113, gb_free=7.9, wall=37808
2022-05-07 03:01:18 | INFO | train_inner | epoch 011:   2250 / 2955 loss=4.895, ppl=29.75, wps=27825.8, ups=0.85, wpb=32768, bsz=64, num_updates=31800, lr=0.000177332, gnorm=0.61, train_wall=116, gb_free=7.9, wall=37926
2022-05-07 03:03:13 | INFO | train_inner | epoch 011:   2350 / 2955 loss=4.887, ppl=29.59, wps=28642.6, ups=0.87, wpb=32768, bsz=64, num_updates=31900, lr=0.000177054, gnorm=0.605, train_wall=113, gb_free=7.9, wall=38040
2022-05-07 03:05:07 | INFO | train_inner | epoch 011:   2450 / 2955 loss=4.889, ppl=29.63, wps=28634.3, ups=0.87, wpb=32768, bsz=64, num_updates=32000, lr=0.000176777, gnorm=0.604, train_wall=113, gb_free=7.9, wall=38155
2022-05-07 03:07:02 | INFO | train_inner | epoch 011:   2550 / 2955 loss=4.895, ppl=29.76, wps=28625.9, ups=0.87, wpb=32768, bsz=64, num_updates=32100, lr=0.000176501, gnorm=0.609, train_wall=113, gb_free=7.9, wall=38269
2022-05-07 03:08:57 | INFO | train_inner | epoch 011:   2650 / 2955 loss=4.898, ppl=29.81, wps=28413.1, ups=0.87, wpb=32768, bsz=64, num_updates=32200, lr=0.000176227, gnorm=0.603, train_wall=113, gb_free=7.9, wall=38384
2022-05-07 03:10:55 | INFO | train_inner | epoch 011:   2750 / 2955 loss=4.908, ppl=30.03, wps=27865.9, ups=0.85, wpb=32768, bsz=64, num_updates=32300, lr=0.000175954, gnorm=0.607, train_wall=116, gb_free=7.9, wall=38502
2022-05-07 03:12:49 | INFO | train_inner | epoch 011:   2850 / 2955 loss=4.892, ppl=29.68, wps=28650.7, ups=0.87, wpb=32768, bsz=64, num_updates=32400, lr=0.000175682, gnorm=0.607, train_wall=113, gb_free=7.9, wall=38616
2022-05-07 03:14:45 | INFO | train_inner | epoch 011:   2950 / 2955 loss=4.888, ppl=29.6, wps=28329.7, ups=0.86, wpb=32768, bsz=64, num_updates=32500, lr=0.000175412, gnorm=0.603, train_wall=113, gb_free=7.9, wall=38732
2022-05-07 03:14:50 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-05-07 03:16:03 | INFO | valid | epoch 011 | valid on 'valid' subset | loss 4.889 | ppl 29.64 | wps 74207.8 | wpb 2047.4 | bsz 4 | num_updates 32505 | best_loss 4.889
2022-05-07 03:16:03 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 11 @ 32505 updates
2022-05-07 03:16:03 | INFO | fairseq.trainer | Saving checkpoint to /cluster/work/cotterell/liam/master-thesis/checkpoints/language_model/standard/checkpoint_best_standard_16_5.0000E-04_full.pt
2022-05-07 03:16:05 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/work/cotterell/liam/master-thesis/checkpoints/language_model/standard/checkpoint_best_standard_16_5.0000E-04_full.pt
2022-05-07 03:16:05 | INFO | fairseq.checkpoint_utils | Saved checkpoint checkpoints/language_model/standard/checkpoint_best_standard_16_5.0000E-04_full.pt (epoch 11 @ 32505 updates, score 4.889) (writing took 1.7627037749625742 seconds)
2022-05-07 03:16:05 | INFO | fairseq_cli.train | end of epoch 11 (average epoch stats below)
2022-05-07 03:16:05 | INFO | train | epoch 011 | loss 4.882 | ppl 29.49 | wps 27606.3 | ups 0.84 | wpb 32764.7 | bsz 64 | num_updates 32505 | lr 0.000175398 | gnorm 0.605 | train_wall 3350 | gb_free 7.9 | wall 38812
2022-05-07 03:16:05 | INFO | fairseq.data.iterators | grouped total_num_itrs = 2955
2022-05-07 03:16:05 | INFO | fairseq.trainer | begin training epoch 12
2022-05-07 03:16:05 | INFO | fairseq_cli.train | Start iterating over samples
2022-05-07 03:17:54 | INFO | train_inner | epoch 012:     95 / 2955 loss=4.838, ppl=28.61, wps=17265.7, ups=0.53, wpb=32686.1, bsz=63.8, num_updates=32600, lr=0.000175142, gnorm=0.607, train_wall=112, gb_free=7.9, wall=38921
2022-05-07 03:19:48 | INFO | train_inner | epoch 012:    195 / 2955 loss=4.838, ppl=28.59, wps=28626.5, ups=0.87, wpb=32768, bsz=64, num_updates=32700, lr=0.000174874, gnorm=0.61, train_wall=113, gb_free=7.9, wall=39036
2022-05-07 03:21:47 | INFO | train_inner | epoch 012:    295 / 2955 loss=4.852, ppl=28.88, wps=27634, ups=0.84, wpb=32768, bsz=64, num_updates=32800, lr=0.000174608, gnorm=0.614, train_wall=115, gb_free=7.9, wall=39154
2022-05-07 03:23:42 | INFO | train_inner | epoch 012:    395 / 2955 loss=4.83, ppl=28.43, wps=28546.1, ups=0.87, wpb=32768, bsz=64, num_updates=32900, lr=0.000174342, gnorm=0.614, train_wall=113, gb_free=7.9, wall=39269
2022-05-07 03:25:37 | INFO | train_inner | epoch 012:    495 / 2955 loss=4.834, ppl=28.52, wps=28438.3, ups=0.87, wpb=32768, bsz=64, num_updates=33000, lr=0.000174078, gnorm=0.611, train_wall=113, gb_free=7.9, wall=39384
2022-05-07 03:27:33 | INFO | train_inner | epoch 012:    595 / 2955 loss=4.834, ppl=28.53, wps=28266.8, ups=0.86, wpb=32768, bsz=64, num_updates=33100, lr=0.000173814, gnorm=0.613, train_wall=114, gb_free=7.9, wall=39500
2022-05-07 03:29:29 | INFO | train_inner | epoch 012:    695 / 2955 loss=4.836, ppl=28.55, wps=28125.8, ups=0.86, wpb=32768, bsz=64, num_updates=33200, lr=0.000173553, gnorm=0.612, train_wall=115, gb_free=7.9, wall=39617
2022-05-07 03:31:25 | INFO | train_inner | epoch 012:    795 / 2955 loss=4.842, ppl=28.67, wps=28247.4, ups=0.86, wpb=32768, bsz=64, num_updates=33300, lr=0.000173292, gnorm=0.612, train_wall=113, gb_free=7.9, wall=39733
2022-05-07 03:33:20 | INFO | train_inner | epoch 012:    895 / 2955 loss=4.84, ppl=28.64, wps=28605, ups=0.87, wpb=32768, bsz=64, num_updates=33400, lr=0.000173032, gnorm=0.613, train_wall=113, gb_free=7.9, wall=39847
2022-05-07 03:35:16 | INFO | train_inner | epoch 012:    995 / 2955 loss=4.86, ppl=29.05, wps=28175.9, ups=0.86, wpb=32768, bsz=64, num_updates=33500, lr=0.000172774, gnorm=0.614, train_wall=113, gb_free=7.9, wall=39964
2022-05-07 03:37:15 | INFO | train_inner | epoch 012:   1095 / 2955 loss=4.862, ppl=29.09, wps=27700.6, ups=0.85, wpb=32768, bsz=64, num_updates=33600, lr=0.000172516, gnorm=0.616, train_wall=114, gb_free=7.9, wall=40082
2022-05-07 03:39:13 | INFO | train_inner | epoch 012:   1195 / 2955 loss=4.866, ppl=29.16, wps=27790.7, ups=0.85, wpb=32768, bsz=64, num_updates=33700, lr=0.00017226, gnorm=0.618, train_wall=114, gb_free=7.9, wall=40200
2022-05-07 03:41:09 | INFO | train_inner | epoch 012:   1295 / 2955 loss=4.857, ppl=28.97, wps=28152.4, ups=0.86, wpb=32768, bsz=64, num_updates=33800, lr=0.000172005, gnorm=0.619, train_wall=113, gb_free=7.9, wall=40316
2022-05-07 03:43:07 | INFO | train_inner | epoch 012:   1395 / 2955 loss=4.868, ppl=29.2, wps=27698.7, ups=0.85, wpb=32768, bsz=64, num_updates=33900, lr=0.000171751, gnorm=0.61, train_wall=116, gb_free=7.9, wall=40435
2022-05-07 03:45:03 | INFO | train_inner | epoch 012:   1495 / 2955 loss=4.862, ppl=29.08, wps=28409.4, ups=0.87, wpb=32768, bsz=64, num_updates=34000, lr=0.000171499, gnorm=0.618, train_wall=113, gb_free=7.9, wall=40550
2022-05-07 03:46:58 | INFO | train_inner | epoch 012:   1595 / 2955 loss=4.882, ppl=29.48, wps=28493, ups=0.87, wpb=32768, bsz=64, num_updates=34100, lr=0.000171247, gnorm=0.618, train_wall=113, gb_free=7.9, wall=40665
2022-05-07 03:48:54 | INFO | train_inner | epoch 012:   1695 / 2955 loss=4.855, ppl=28.93, wps=28139.4, ups=0.86, wpb=32768, bsz=64, num_updates=34200, lr=0.000170996, gnorm=0.613, train_wall=114, gb_free=7.9, wall=40781
2022-05-07 03:50:49 | INFO | train_inner | epoch 012:   1795 / 2955 loss=4.84, ppl=28.64, wps=28396.5, ups=0.87, wpb=32768, bsz=64, num_updates=34300, lr=0.000170747, gnorm=0.614, train_wall=113, gb_free=7.9, wall=40897
2022-05-07 03:52:47 | INFO | train_inner | epoch 012:   1895 / 2955 loss=4.861, ppl=29.06, wps=27787.8, ups=0.85, wpb=32768, bsz=64, num_updates=34400, lr=0.000170499, gnorm=0.616, train_wall=115, gb_free=7.9, wall=41015
2022-05-07 03:54:43 | INFO | train_inner | epoch 012:   1995 / 2955 loss=4.869, ppl=29.23, wps=28256.9, ups=0.86, wpb=32768, bsz=64, num_updates=34500, lr=0.000170251, gnorm=0.617, train_wall=114, gb_free=7.9, wall=41131
2022-05-07 03:56:38 | INFO | train_inner | epoch 012:   2095 / 2955 loss=4.869, ppl=29.23, wps=28620.8, ups=0.87, wpb=32768, bsz=64, num_updates=34600, lr=0.000170005, gnorm=0.617, train_wall=113, gb_free=7.9, wall=41245
2022-05-07 03:58:33 | INFO | train_inner | epoch 012:   2195 / 2955 loss=4.882, ppl=29.48, wps=28545, ups=0.87, wpb=32768, bsz=64, num_updates=34700, lr=0.00016976, gnorm=0.616, train_wall=113, gb_free=7.9, wall=41360
2022-05-07 04:00:27 | INFO | train_inner | epoch 012:   2295 / 2955 loss=4.865, ppl=29.13, wps=28681.1, ups=0.88, wpb=32768, bsz=64, num_updates=34800, lr=0.000169516, gnorm=0.618, train_wall=113, gb_free=7.9, wall=41474
2022-05-07 04:02:25 | INFO | train_inner | epoch 012:   2395 / 2955 loss=4.864, ppl=29.13, wps=27690.1, ups=0.85, wpb=32768, bsz=64, num_updates=34900, lr=0.000169273, gnorm=0.614, train_wall=116, gb_free=7.9, wall=41593
2022-05-07 04:04:19 | INFO | train_inner | epoch 012:   2495 / 2955 loss=4.865, ppl=29.15, wps=28671.2, ups=0.87, wpb=32768, bsz=64, num_updates=35000, lr=0.000169031, gnorm=0.619, train_wall=113, gb_free=7.9, wall=41707
2022-05-07 04:06:14 | INFO | train_inner | epoch 012:   2595 / 2955 loss=4.885, ppl=29.54, wps=28626.2, ups=0.87, wpb=32768, bsz=64, num_updates=35100, lr=0.00016879, gnorm=0.618, train_wall=113, gb_free=7.9, wall=41821
2022-05-07 04:08:09 | INFO | train_inner | epoch 012:   2695 / 2955 loss=4.874, ppl=29.32, wps=28582.1, ups=0.87, wpb=32763, bsz=64, num_updates=35200, lr=0.00016855, gnorm=0.613, train_wall=113, gb_free=7.9, wall=41936
2022-05-07 04:10:03 | INFO | train_inner | epoch 012:   2795 / 2955 loss=4.883, ppl=29.5, wps=28599.5, ups=0.87, wpb=32768, bsz=64, num_updates=35300, lr=0.000168311, gnorm=0.62, train_wall=113, gb_free=7.9, wall=42050
2022-05-07 04:11:59 | INFO | train_inner | epoch 012:   2895 / 2955 loss=4.865, ppl=29.14, wps=28200.6, ups=0.86, wpb=32757.8, bsz=64, num_updates=35400, lr=0.000168073, gnorm=0.617, train_wall=113, gb_free=7.9, wall=42167
2022-05-07 04:13:09 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-05-07 04:14:22 | INFO | valid | epoch 012 | valid on 'valid' subset | loss 4.874 | ppl 29.32 | wps 74039.4 | wpb 2047.4 | bsz 4 | num_updates 35460 | best_loss 4.874
2022-05-07 04:14:22 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 12 @ 35460 updates
2022-05-07 04:14:22 | INFO | fairseq.trainer | Saving checkpoint to /cluster/work/cotterell/liam/master-thesis/checkpoints/language_model/standard/checkpoint_best_standard_16_5.0000E-04_full.pt
2022-05-07 04:14:23 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/work/cotterell/liam/master-thesis/checkpoints/language_model/standard/checkpoint_best_standard_16_5.0000E-04_full.pt
2022-05-07 04:14:23 | INFO | fairseq.checkpoint_utils | Saved checkpoint checkpoints/language_model/standard/checkpoint_best_standard_16_5.0000E-04_full.pt (epoch 12 @ 35460 updates, score 4.874) (writing took 1.6424899231642485 seconds)
2022-05-07 04:14:23 | INFO | fairseq_cli.train | end of epoch 12 (average epoch stats below)
2022-05-07 04:14:23 | INFO | train | epoch 012 | loss 4.858 | ppl 29.01 | wps 27674 | ups 0.84 | wpb 32764.7 | bsz 64 | num_updates 35460 | lr 0.000167931 | gnorm 0.615 | train_wall 3351 | gb_free 7.9 | wall 42311
2022-05-07 04:14:24 | INFO | fairseq.data.iterators | grouped total_num_itrs = 2955
2022-05-07 04:14:24 | INFO | fairseq.trainer | begin training epoch 13
2022-05-07 04:14:24 | INFO | fairseq_cli.train | Start iterating over samples
2022-05-07 04:15:10 | INFO | train_inner | epoch 013:     40 / 2955 loss=4.856, ppl=28.97, wps=17158.7, ups=0.52, wpb=32686.1, bsz=63.8, num_updates=35500, lr=0.000167836, gnorm=0.621, train_wall=113, gb_free=7.9, wall=42357
2022-05-07 04:17:04 | INFO | train_inner | epoch 013:    140 / 2955 loss=4.805, ppl=27.95, wps=28575, ups=0.87, wpb=32768, bsz=64, num_updates=35600, lr=0.0001676, gnorm=0.617, train_wall=113, gb_free=7.9, wall=42472
2022-05-07 04:18:59 | INFO | train_inner | epoch 013:    240 / 2955 loss=4.81, ppl=28.06, wps=28545.5, ups=0.87, wpb=32768, bsz=64, num_updates=35700, lr=0.000167365, gnorm=0.623, train_wall=113, gb_free=7.9, wall=42587
2022-05-07 04:20:54 | INFO | train_inner | epoch 013:    340 / 2955 loss=4.814, ppl=28.13, wps=28649.5, ups=0.87, wpb=32768, bsz=64, num_updates=35800, lr=0.000167132, gnorm=0.622, train_wall=113, gb_free=7.9, wall=42701
2022-05-07 04:22:49 | INFO | train_inner | epoch 013:    440 / 2955 loss=4.807, ppl=27.99, wps=28485.7, ups=0.87, wpb=32768, bsz=64, num_updates=35900, lr=0.000166899, gnorm=0.629, train_wall=113, gb_free=7.9, wall=42816
2022-05-07 04:24:48 | INFO | train_inner | epoch 013:    540 / 2955 loss=4.822, ppl=28.29, wps=27520.3, ups=0.84, wpb=32768, bsz=64, num_updates=36000, lr=0.000166667, gnorm=0.624, train_wall=117, gb_free=7.9, wall=42935
2022-05-07 04:26:42 | INFO | train_inner | epoch 013:    640 / 2955 loss=4.818, ppl=28.22, wps=28660.3, ups=0.87, wpb=32768, bsz=64, num_updates=36100, lr=0.000166436, gnorm=0.62, train_wall=113, gb_free=7.9, wall=43049
2022-05-07 04:28:39 | INFO | train_inner | epoch 013:    740 / 2955 loss=4.823, ppl=28.31, wps=27941, ups=0.85, wpb=32768, bsz=64, num_updates=36200, lr=0.000166206, gnorm=0.619, train_wall=115, gb_free=7.9, wall=43167
2022-05-07 04:30:34 | INFO | train_inner | epoch 013:    840 / 2955 loss=4.834, ppl=28.53, wps=28661.7, ups=0.87, wpb=32768, bsz=64, num_updates=36300, lr=0.000165977, gnorm=0.624, train_wall=113, gb_free=7.9, wall=43281
2022-05-07 04:32:29 | INFO | train_inner | epoch 013:    940 / 2955 loss=4.829, ppl=28.42, wps=28526.9, ups=0.87, wpb=32768, bsz=64, num_updates=36400, lr=0.000165748, gnorm=0.626, train_wall=113, gb_free=7.9, wall=43396
2022-05-07 04:34:23 | INFO | train_inner | epoch 013:   1040 / 2955 loss=4.831, ppl=28.47, wps=28623.3, ups=0.87, wpb=32768, bsz=64, num_updates=36500, lr=0.000165521, gnorm=0.621, train_wall=113, gb_free=7.9, wall=43510
2022-05-07 04:36:22 | INFO | train_inner | epoch 013:   1140 / 2955 loss=4.842, ppl=28.69, wps=27645.4, ups=0.84, wpb=32757.8, bsz=64, num_updates=36600, lr=0.000165295, gnorm=0.629, train_wall=116, gb_free=7.9, wall=43629
2022-05-07 04:38:16 | INFO | train_inner | epoch 013:   1240 / 2955 loss=4.837, ppl=28.58, wps=28686, ups=0.88, wpb=32768, bsz=64, num_updates=36700, lr=0.00016507, gnorm=0.629, train_wall=113, gb_free=7.9, wall=43743
2022-05-07 04:40:12 | INFO | train_inner | epoch 013:   1340 / 2955 loss=4.85, ppl=28.85, wps=28159.8, ups=0.86, wpb=32763, bsz=64, num_updates=36800, lr=0.000164845, gnorm=0.623, train_wall=115, gb_free=7.9, wall=43859
2022-05-07 04:42:07 | INFO | train_inner | epoch 013:   1440 / 2955 loss=4.84, ppl=28.65, wps=28487, ups=0.87, wpb=32768, bsz=64, num_updates=36900, lr=0.000164622, gnorm=0.621, train_wall=113, gb_free=7.9, wall=43974
2022-05-07 04:44:01 | INFO | train_inner | epoch 013:   1540 / 2955 loss=4.848, ppl=28.79, wps=28678.7, ups=0.88, wpb=32768, bsz=64, num_updates=37000, lr=0.000164399, gnorm=0.626, train_wall=113, gb_free=7.9, wall=44089
2022-05-07 04:45:56 | INFO | train_inner | epoch 013:   1640 / 2955 loss=4.845, ppl=28.73, wps=28521.7, ups=0.87, wpb=32768, bsz=64, num_updates=37100, lr=0.000164177, gnorm=0.623, train_wall=113, gb_free=7.9, wall=44204
2022-05-07 04:47:51 | INFO | train_inner | epoch 013:   1740 / 2955 loss=4.85, ppl=28.84, wps=28605.2, ups=0.87, wpb=32768, bsz=64, num_updates=37200, lr=0.000163956, gnorm=0.623, train_wall=113, gb_free=7.9, wall=44318
2022-05-07 04:49:47 | INFO | train_inner | epoch 013:   1840 / 2955 loss=4.84, ppl=28.65, wps=28195.1, ups=0.86, wpb=32768, bsz=64, num_updates=37300, lr=0.000163737, gnorm=0.632, train_wall=115, gb_free=7.9, wall=44434
2022-05-07 04:51:52 | INFO | train_inner | epoch 013:   1940 / 2955 loss=4.848, ppl=28.8, wps=26168.4, ups=0.8, wpb=32768, bsz=64, num_updates=37400, lr=0.000163517, gnorm=0.627, train_wall=113, gb_free=7.9, wall=44560
2022-05-07 04:53:54 | INFO | train_inner | epoch 013:   2040 / 2955 loss=4.843, ppl=28.7, wps=26853.9, ups=0.82, wpb=32768, bsz=64, num_updates=37500, lr=0.000163299, gnorm=0.625, train_wall=112, gb_free=7.9, wall=44682
2022-05-07 04:55:54 | INFO | train_inner | epoch 013:   2140 / 2955 loss=4.854, ppl=28.93, wps=27309.6, ups=0.83, wpb=32768, bsz=64, num_updates=37600, lr=0.000163082, gnorm=0.626, train_wall=112, gb_free=7.9, wall=44802
2022-05-07 04:57:53 | INFO | train_inner | epoch 013:   2240 / 2955 loss=4.865, ppl=29.13, wps=27578.1, ups=0.84, wpb=32768, bsz=64, num_updates=37700, lr=0.000162866, gnorm=0.627, train_wall=113, gb_free=7.9, wall=44920
2022-05-07 04:59:55 | INFO | train_inner | epoch 013:   2340 / 2955 loss=4.857, ppl=28.98, wps=26813.6, ups=0.82, wpb=32768, bsz=64, num_updates=37800, lr=0.00016265, gnorm=0.625, train_wall=117, gb_free=7.9, wall=45043
2022-05-07 05:01:53 | INFO | train_inner | epoch 013:   2440 / 2955 loss=4.85, ppl=28.84, wps=27959.1, ups=0.85, wpb=32768, bsz=64, num_updates=37900, lr=0.000162435, gnorm=0.625, train_wall=113, gb_free=7.9, wall=45160
2022-05-07 05:03:48 | INFO | train_inner | epoch 013:   2540 / 2955 loss=4.845, ppl=28.75, wps=28279.2, ups=0.86, wpb=32768, bsz=64, num_updates=38000, lr=0.000162221, gnorm=0.626, train_wall=113, gb_free=7.9, wall=45276
2022-05-07 05:05:44 | INFO | train_inner | epoch 013:   2640 / 2955 loss=4.846, ppl=28.76, wps=28405.5, ups=0.87, wpb=32768, bsz=64, num_updates=38100, lr=0.000162008, gnorm=0.627, train_wall=113, gb_free=7.9, wall=45391
2022-05-07 05:07:39 | INFO | train_inner | epoch 013:   2740 / 2955 loss=4.85, ppl=28.85, wps=28468.4, ups=0.87, wpb=32768, bsz=64, num_updates=38200, lr=0.000161796, gnorm=0.622, train_wall=113, gb_free=7.9, wall=45506
2022-05-07 05:09:46 | INFO | train_inner | epoch 013:   2840 / 2955 loss=4.856, ppl=28.96, wps=25799.6, ups=0.79, wpb=32768, bsz=64, num_updates=38300, lr=0.000161585, gnorm=0.63, train_wall=119, gb_free=7.9, wall=45633
2022-05-07 05:11:48 | INFO | train_inner | epoch 013:   2940 / 2955 loss=4.832, ppl=28.48, wps=26748.6, ups=0.82, wpb=32768, bsz=64, num_updates=38400, lr=0.000161374, gnorm=0.624, train_wall=113, gb_free=7.9, wall=45756
2022-05-07 05:12:06 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-05-07 05:13:20 | INFO | valid | epoch 013 | valid on 'valid' subset | loss 4.86 | ppl 29.04 | wps 74219.3 | wpb 2047.4 | bsz 4 | num_updates 38415 | best_loss 4.86
2022-05-07 05:13:20 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 13 @ 38415 updates
2022-05-07 05:13:20 | INFO | fairseq.trainer | Saving checkpoint to /cluster/work/cotterell/liam/master-thesis/checkpoints/language_model/standard/checkpoint_best_standard_16_5.0000E-04_full.pt
2022-05-07 05:13:21 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/work/cotterell/liam/master-thesis/checkpoints/language_model/standard/checkpoint_best_standard_16_5.0000E-04_full.pt
2022-05-07 05:13:21 | INFO | fairseq.checkpoint_utils | Saved checkpoint checkpoints/language_model/standard/checkpoint_best_standard_16_5.0000E-04_full.pt (epoch 13 @ 38415 updates, score 4.86) (writing took 1.7550835879519582 seconds)
2022-05-07 05:13:21 | INFO | fairseq_cli.train | end of epoch 13 (average epoch stats below)
2022-05-07 05:13:21 | INFO | train | epoch 013 | loss 4.837 | ppl 28.59 | wps 27366.6 | ups 0.84 | wpb 32764.7 | bsz 64 | num_updates 38415 | lr 0.000161343 | gnorm 0.625 | train_wall 3352 | gb_free 7.9 | wall 45849
2022-05-07 05:13:21 | INFO | fairseq.data.iterators | grouped total_num_itrs = 2955
2022-05-07 05:13:21 | INFO | fairseq.trainer | begin training epoch 14
2022-05-07 05:13:21 | INFO | fairseq_cli.train | Start iterating over samples
2022-05-07 05:15:03 | INFO | train_inner | epoch 014:     85 / 2955 loss=4.8, ppl=27.86, wps=16761.7, ups=0.51, wpb=32686.1, bsz=63.8, num_updates=38500, lr=0.000161165, gnorm=0.633, train_wall=112, gb_free=7.9, wall=45951
2022-05-07 05:17:02 | INFO | train_inner | epoch 014:    185 / 2955 loss=4.792, ppl=27.71, wps=27531.5, ups=0.84, wpb=32768, bsz=64, num_updates=38600, lr=0.000160956, gnorm=0.626, train_wall=113, gb_free=7.9, wall=46070
2022-05-07 05:19:00 | INFO | train_inner | epoch 014:    285 / 2955 loss=4.799, ppl=27.83, wps=27896.3, ups=0.85, wpb=32768, bsz=64, num_updates=38700, lr=0.000160748, gnorm=0.629, train_wall=113, gb_free=7.9, wall=46187
2022-05-07 05:20:57 | INFO | train_inner | epoch 014:    385 / 2955 loss=4.794, ppl=27.74, wps=27983.6, ups=0.85, wpb=32768, bsz=64, num_updates=38800, lr=0.00016054, gnorm=0.631, train_wall=113, gb_free=7.9, wall=46304
2022-05-07 05:22:53 | INFO | train_inner | epoch 014:    485 / 2955 loss=4.794, ppl=27.75, wps=28211.1, ups=0.86, wpb=32768, bsz=64, num_updates=38900, lr=0.000160334, gnorm=0.631, train_wall=113, gb_free=7.9, wall=46420
2022-05-07 05:24:51 | INFO | train_inner | epoch 014:    585 / 2955 loss=4.814, ppl=28.13, wps=27895.9, ups=0.85, wpb=32768, bsz=64, num_updates=39000, lr=0.000160128, gnorm=0.628, train_wall=114, gb_free=7.9, wall=46538
2022-05-07 05:26:46 | INFO | train_inner | epoch 014:    685 / 2955 loss=4.816, ppl=28.16, wps=28416.4, ups=0.87, wpb=32768, bsz=64, num_updates=39100, lr=0.000159923, gnorm=0.63, train_wall=113, gb_free=7.9, wall=46653
2022-05-07 05:28:41 | INFO | train_inner | epoch 014:    785 / 2955 loss=4.814, ppl=28.13, wps=28514.1, ups=0.87, wpb=32768, bsz=64, num_updates=39200, lr=0.000159719, gnorm=0.63, train_wall=113, gb_free=7.9, wall=46768
2022-05-07 05:30:37 | INFO | train_inner | epoch 014:    885 / 2955 loss=4.825, ppl=28.34, wps=28128.8, ups=0.86, wpb=32768, bsz=64, num_updates=39300, lr=0.000159516, gnorm=0.632, train_wall=114, gb_free=7.9, wall=46885
2022-05-07 05:32:32 | INFO | train_inner | epoch 014:    985 / 2955 loss=4.817, ppl=28.18, wps=28453.5, ups=0.87, wpb=32768, bsz=64, num_updates=39400, lr=0.000159313, gnorm=0.629, train_wall=113, gb_free=7.9, wall=47000
2022-05-07 05:34:29 | INFO | train_inner | epoch 014:   1085 / 2955 loss=4.816, ppl=28.17, wps=28196.8, ups=0.86, wpb=32768, bsz=64, num_updates=39500, lr=0.000159111, gnorm=0.637, train_wall=114, gb_free=7.9, wall=47116
2022-05-07 05:36:23 | INFO | train_inner | epoch 014:   1185 / 2955 loss=4.838, ppl=28.59, wps=28617.2, ups=0.87, wpb=32768, bsz=64, num_updates=39600, lr=0.00015891, gnorm=0.634, train_wall=113, gb_free=7.9, wall=47231
2022-05-07 05:38:18 | INFO | train_inner | epoch 014:   1285 / 2955 loss=4.806, ppl=27.97, wps=28589.8, ups=0.87, wpb=32768, bsz=64, num_updates=39700, lr=0.00015871, gnorm=0.637, train_wall=113, gb_free=7.9, wall=47345
2022-05-07 05:40:14 | INFO | train_inner | epoch 014:   1385 / 2955 loss=4.82, ppl=28.25, wps=28210.6, ups=0.86, wpb=32768, bsz=64, num_updates=39800, lr=0.000158511, gnorm=0.636, train_wall=114, gb_free=7.9, wall=47461
2022-05-07 05:42:08 | INFO | train_inner | epoch 014:   1485 / 2955 loss=4.837, ppl=28.58, wps=28678.5, ups=0.88, wpb=32768, bsz=64, num_updates=39900, lr=0.000158312, gnorm=0.637, train_wall=113, gb_free=7.9, wall=47576
2022-05-07 05:44:06 | INFO | train_inner | epoch 014:   1585 / 2955 loss=4.822, ppl=28.28, wps=27799.2, ups=0.85, wpb=32768, bsz=64, num_updates=40000, lr=0.000158114, gnorm=0.632, train_wall=116, gb_free=7.9, wall=47693
2022-05-07 05:46:01 | INFO | train_inner | epoch 014:   1685 / 2955 loss=4.828, ppl=28.4, wps=28641.3, ups=0.87, wpb=32768, bsz=64, num_updates=40100, lr=0.000157917, gnorm=0.634, train_wall=113, gb_free=7.9, wall=47808
2022-05-07 05:47:58 | INFO | train_inner | epoch 014:   1785 / 2955 loss=4.829, ppl=28.42, wps=27836.7, ups=0.85, wpb=32757.8, bsz=64, num_updates=40200, lr=0.00015772, gnorm=0.631, train_wall=113, gb_free=7.9, wall=47926
2022-05-07 05:50:04 | INFO | train_inner | epoch 014:   1885 / 2955 loss=4.808, ppl=28.02, wps=26046.2, ups=0.79, wpb=32768, bsz=64, num_updates=40300, lr=0.000157524, gnorm=0.634, train_wall=114, gb_free=7.9, wall=48051
2022-05-07 05:52:05 | INFO | train_inner | epoch 014:   1985 / 2955 loss=4.83, ppl=28.43, wps=27031.9, ups=0.82, wpb=32768, bsz=64, num_updates=40400, lr=0.000157329, gnorm=0.632, train_wall=112, gb_free=7.9, wall=48173
2022-05-07 05:54:06 | INFO | train_inner | epoch 014:   2085 / 2955 loss=4.819, ppl=28.23, wps=27089.7, ups=0.83, wpb=32768, bsz=64, num_updates=40500, lr=0.000157135, gnorm=0.63, train_wall=113, gb_free=7.9, wall=48294
2022-05-07 05:56:04 | INFO | train_inner | epoch 014:   2185 / 2955 loss=4.83, ppl=28.44, wps=27889, ups=0.85, wpb=32768, bsz=64, num_updates=40600, lr=0.000156941, gnorm=0.632, train_wall=113, gb_free=7.9, wall=48411
2022-05-07 05:58:02 | INFO | train_inner | epoch 014:   2285 / 2955 loss=4.823, ppl=28.3, wps=27723.1, ups=0.85, wpb=32768, bsz=64, num_updates=40700, lr=0.000156748, gnorm=0.636, train_wall=114, gb_free=7.9, wall=48529
2022-05-07 06:00:00 | INFO | train_inner | epoch 014:   2385 / 2955 loss=4.829, ppl=28.41, wps=27765.7, ups=0.85, wpb=32768, bsz=64, num_updates=40800, lr=0.000156556, gnorm=0.634, train_wall=115, gb_free=7.9, wall=48647
2022-05-07 06:01:55 | INFO | train_inner | epoch 014:   2485 / 2955 loss=4.827, ppl=28.39, wps=28424.5, ups=0.87, wpb=32768, bsz=64, num_updates=40900, lr=0.000156365, gnorm=0.631, train_wall=112, gb_free=7.9, wall=48762
2022-05-07 06:03:54 | INFO | train_inner | epoch 014:   2585 / 2955 loss=4.821, ppl=28.27, wps=27576.3, ups=0.84, wpb=32768, bsz=64, num_updates=41000, lr=0.000156174, gnorm=0.634, train_wall=116, gb_free=7.9, wall=48881
2022-05-07 06:05:49 | INFO | train_inner | epoch 014:   2685 / 2955 loss=4.819, ppl=28.22, wps=28549.6, ups=0.87, wpb=32768, bsz=64, num_updates=41100, lr=0.000155984, gnorm=0.638, train_wall=113, gb_free=7.9, wall=48996
2022-05-07 06:07:44 | INFO | train_inner | epoch 014:   2785 / 2955 loss=4.832, ppl=28.49, wps=28506.1, ups=0.87, wpb=32763, bsz=64, num_updates=41200, lr=0.000155794, gnorm=0.634, train_wall=113, gb_free=7.9, wall=49111
2022-05-07 06:09:40 | INFO | train_inner | epoch 014:   2885 / 2955 loss=4.844, ppl=28.71, wps=28163.6, ups=0.86, wpb=32768, bsz=64, num_updates=41300, lr=0.000155606, gnorm=0.632, train_wall=114, gb_free=7.9, wall=49227
2022-05-07 06:11:00 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-05-07 06:12:12 | INFO | valid | epoch 014 | valid on 'valid' subset | loss 4.849 | ppl 28.81 | wps 74477 | wpb 2047.4 | bsz 4 | num_updates 41370 | best_loss 4.849
2022-05-07 06:12:12 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 14 @ 41370 updates
2022-05-07 06:12:12 | INFO | fairseq.trainer | Saving checkpoint to /cluster/work/cotterell/liam/master-thesis/checkpoints/language_model/standard/checkpoint_best_standard_16_5.0000E-04_full.pt
2022-05-07 06:12:14 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/work/cotterell/liam/master-thesis/checkpoints/language_model/standard/checkpoint_best_standard_16_5.0000E-04_full.pt
2022-05-07 06:12:14 | INFO | fairseq.checkpoint_utils | Saved checkpoint checkpoints/language_model/standard/checkpoint_best_standard_16_5.0000E-04_full.pt (epoch 14 @ 41370 updates, score 4.849) (writing took 1.671578569803387 seconds)
2022-05-07 06:12:14 | INFO | fairseq_cli.train | end of epoch 14 (average epoch stats below)
2022-05-07 06:12:14 | INFO | train | epoch 014 | loss 4.819 | ppl 28.22 | wps 27405.5 | ups 0.84 | wpb 32764.7 | bsz 64 | num_updates 41370 | lr 0.000155474 | gnorm 0.633 | train_wall 3347 | gb_free 7.9 | wall 49381
2022-05-07 06:12:14 | INFO | fairseq.data.iterators | grouped total_num_itrs = 2955
2022-05-07 06:12:14 | INFO | fairseq.trainer | begin training epoch 15
2022-05-07 06:12:14 | INFO | fairseq_cli.train | Start iterating over samples
2022-05-07 06:12:49 | INFO | train_inner | epoch 015:     30 / 2955 loss=4.807, ppl=28, wps=17327.8, ups=0.53, wpb=32686.1, bsz=63.8, num_updates=41400, lr=0.000155417, gnorm=0.632, train_wall=112, gb_free=7.9, wall=49416
2022-05-07 06:14:43 | INFO | train_inner | epoch 015:    130 / 2955 loss=4.764, ppl=27.16, wps=28687.6, ups=0.88, wpb=32768, bsz=64, num_updates=41500, lr=0.00015523, gnorm=0.635, train_wall=113, gb_free=7.9, wall=49530
2022-05-07 06:16:37 | INFO | train_inner | epoch 015:    230 / 2955 loss=4.761, ppl=27.12, wps=28709.7, ups=0.88, wpb=32768, bsz=64, num_updates=41600, lr=0.000155043, gnorm=0.638, train_wall=113, gb_free=7.9, wall=49644
2022-05-07 06:18:32 | INFO | train_inner | epoch 015:    330 / 2955 loss=4.77, ppl=27.29, wps=28462.9, ups=0.87, wpb=32768, bsz=64, num_updates=41700, lr=0.000154857, gnorm=0.64, train_wall=113, gb_free=7.9, wall=49760
2022-05-07 06:20:28 | INFO | train_inner | epoch 015:    430 / 2955 loss=4.773, ppl=27.33, wps=28228.2, ups=0.86, wpb=32757.8, bsz=64, num_updates=41800, lr=0.000154672, gnorm=0.641, train_wall=114, gb_free=7.9, wall=49876
2022-05-07 06:22:28 | INFO | train_inner | epoch 015:    530 / 2955 loss=4.765, ppl=27.19, wps=27448.1, ups=0.84, wpb=32768, bsz=64, num_updates=41900, lr=0.000154487, gnorm=0.638, train_wall=116, gb_free=7.9, wall=49995
2022-05-07 06:24:22 | INFO | train_inner | epoch 015:    630 / 2955 loss=4.789, ppl=27.64, wps=28620.5, ups=0.87, wpb=32768, bsz=64, num_updates=42000, lr=0.000154303, gnorm=0.64, train_wall=112, gb_free=7.9, wall=50109
2022-05-07 06:26:17 | INFO | train_inner | epoch 015:    730 / 2955 loss=4.785, ppl=27.58, wps=28634.3, ups=0.87, wpb=32768, bsz=64, num_updates=42100, lr=0.00015412, gnorm=0.643, train_wall=113, gb_free=7.9, wall=50224
2022-05-07 06:28:11 | INFO | train_inner | epoch 015:    830 / 2955 loss=4.789, ppl=27.64, wps=28650.6, ups=0.87, wpb=32768, bsz=64, num_updates=42200, lr=0.000153937, gnorm=0.638, train_wall=113, gb_free=7.9, wall=50338
2022-05-07 06:30:07 | INFO | train_inner | epoch 015:    930 / 2955 loss=4.802, ppl=27.9, wps=28225.1, ups=0.86, wpb=32768, bsz=64, num_updates=42300, lr=0.000153755, gnorm=0.643, train_wall=114, gb_free=7.9, wall=50454
2022-05-07 06:32:01 | INFO | train_inner | epoch 015:   1030 / 2955 loss=4.79, ppl=27.66, wps=28635.8, ups=0.87, wpb=32768, bsz=64, num_updates=42400, lr=0.000153574, gnorm=0.646, train_wall=113, gb_free=7.9, wall=50569
2022-05-07 06:34:01 | INFO | train_inner | epoch 015:   1130 / 2955 loss=4.796, ppl=27.78, wps=27332, ups=0.83, wpb=32768, bsz=64, num_updates=42500, lr=0.000153393, gnorm=0.645, train_wall=116, gb_free=7.9, wall=50689
2022-05-07 06:35:56 | INFO | train_inner | epoch 015:   1230 / 2955 loss=4.804, ppl=27.93, wps=28605.9, ups=0.87, wpb=32768, bsz=64, num_updates=42600, lr=0.000153213, gnorm=0.637, train_wall=112, gb_free=7.9, wall=50803
2022-05-07 06:37:50 | INFO | train_inner | epoch 015:   1330 / 2955 loss=4.82, ppl=28.24, wps=28649.4, ups=0.87, wpb=32763, bsz=64, num_updates=42700, lr=0.000153033, gnorm=0.643, train_wall=113, gb_free=7.9, wall=50918
2022-05-07 06:39:44 | INFO | train_inner | epoch 015:   1430 / 2955 loss=4.812, ppl=28.09, wps=28685, ups=0.88, wpb=32768, bsz=64, num_updates=42800, lr=0.000152854, gnorm=0.641, train_wall=113, gb_free=7.9, wall=51032
2022-05-07 06:41:39 | INFO | train_inner | epoch 015:   1530 / 2955 loss=4.801, ppl=27.87, wps=28628, ups=0.87, wpb=32768, bsz=64, num_updates=42900, lr=0.000152676, gnorm=0.641, train_wall=113, gb_free=7.9, wall=51146
2022-05-07 06:43:34 | INFO | train_inner | epoch 015:   1630 / 2955 loss=4.801, ppl=27.88, wps=28533.9, ups=0.87, wpb=32768, bsz=64, num_updates=43000, lr=0.000152499, gnorm=0.644, train_wall=113, gb_free=7.9, wall=51261
2022-05-07 06:45:29 | INFO | train_inner | epoch 015:   1730 / 2955 loss=4.809, ppl=28.02, wps=28325.5, ups=0.86, wpb=32768, bsz=64, num_updates=43100, lr=0.000152322, gnorm=0.639, train_wall=113, gb_free=7.9, wall=51377
2022-05-07 06:47:24 | INFO | train_inner | epoch 015:   1830 / 2955 loss=4.806, ppl=27.97, wps=28653.3, ups=0.87, wpb=32768, bsz=64, num_updates=43200, lr=0.000152145, gnorm=0.641, train_wall=112, gb_free=7.9, wall=51491
2022-05-07 06:49:23 | INFO | train_inner | epoch 015:   1930 / 2955 loss=4.81, ppl=28.06, wps=27537.3, ups=0.84, wpb=32768, bsz=64, num_updates=43300, lr=0.000151969, gnorm=0.639, train_wall=116, gb_free=7.9, wall=51610
2022-05-07 06:51:18 | INFO | train_inner | epoch 015:   2030 / 2955 loss=4.825, ppl=28.35, wps=28413.1, ups=0.87, wpb=32768, bsz=64, num_updates=43400, lr=0.000151794, gnorm=0.643, train_wall=113, gb_free=7.9, wall=51725
2022-05-07 06:53:13 | INFO | train_inner | epoch 015:   2130 / 2955 loss=4.815, ppl=28.15, wps=28490.8, ups=0.87, wpb=32768, bsz=64, num_updates=43500, lr=0.00015162, gnorm=0.641, train_wall=113, gb_free=7.9, wall=51840
2022-05-07 06:55:08 | INFO | train_inner | epoch 015:   2230 / 2955 loss=4.825, ppl=28.34, wps=28648.8, ups=0.87, wpb=32768, bsz=64, num_updates=43600, lr=0.000151446, gnorm=0.643, train_wall=113, gb_free=7.9, wall=51955
2022-05-07 06:57:02 | INFO | train_inner | epoch 015:   2330 / 2955 loss=4.817, ppl=28.19, wps=28690.8, ups=0.88, wpb=32768, bsz=64, num_updates=43700, lr=0.000151272, gnorm=0.642, train_wall=113, gb_free=7.9, wall=52069
2022-05-07 06:58:56 | INFO | train_inner | epoch 015:   2430 / 2955 loss=4.823, ppl=28.3, wps=28573.1, ups=0.87, wpb=32768, bsz=64, num_updates=43800, lr=0.000151099, gnorm=0.645, train_wall=113, gb_free=7.9, wall=52184
2022-05-07 07:00:53 | INFO | train_inner | epoch 015:   2530 / 2955 loss=4.824, ppl=28.33, wps=28161.1, ups=0.86, wpb=32768, bsz=64, num_updates=43900, lr=0.000150927, gnorm=0.641, train_wall=114, gb_free=7.9, wall=52300
2022-05-07 07:02:47 | INFO | train_inner | epoch 015:   2630 / 2955 loss=4.809, ppl=28.03, wps=28614.2, ups=0.87, wpb=32768, bsz=64, num_updates=44000, lr=0.000150756, gnorm=0.643, train_wall=113, gb_free=7.9, wall=52415
2022-05-07 07:04:44 | INFO | train_inner | epoch 015:   2730 / 2955 loss=4.832, ppl=28.47, wps=28171.6, ups=0.86, wpb=32768, bsz=64, num_updates=44100, lr=0.000150585, gnorm=0.644, train_wall=114, gb_free=7.9, wall=52531
2022-05-07 07:06:38 | INFO | train_inner | epoch 015:   2830 / 2955 loss=4.828, ppl=28.41, wps=28607.7, ups=0.87, wpb=32768, bsz=64, num_updates=44200, lr=0.000150414, gnorm=0.648, train_wall=113, gb_free=7.9, wall=52646
2022-05-07 07:08:36 | INFO | train_inner | epoch 015:   2930 / 2955 loss=4.836, ppl=28.56, wps=27732.7, ups=0.85, wpb=32768, bsz=64, num_updates=44300, lr=0.000150244, gnorm=0.647, train_wall=116, gb_free=7.9, wall=52764
2022-05-07 07:09:05 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-05-07 07:10:18 | INFO | valid | epoch 015 | valid on 'valid' subset | loss 4.836 | ppl 28.57 | wps 73280 | wpb 2047.4 | bsz 4 | num_updates 44325 | best_loss 4.836
2022-05-07 07:10:18 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 15 @ 44325 updates
2022-05-07 07:10:18 | INFO | fairseq.trainer | Saving checkpoint to /cluster/work/cotterell/liam/master-thesis/checkpoints/language_model/standard/checkpoint_best_standard_16_5.0000E-04_full.pt
2022-05-07 07:10:20 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/work/cotterell/liam/master-thesis/checkpoints/language_model/standard/checkpoint_best_standard_16_5.0000E-04_full.pt
2022-05-07 07:10:20 | INFO | fairseq.checkpoint_utils | Saved checkpoint checkpoints/language_model/standard/checkpoint_best_standard_16_5.0000E-04_full.pt (epoch 15 @ 44325 updates, score 4.836) (writing took 1.6301132859662175 seconds)
2022-05-07 07:10:20 | INFO | fairseq_cli.train | end of epoch 15 (average epoch stats below)
2022-05-07 07:10:20 | INFO | train | epoch 015 | loss 4.802 | ppl 27.9 | wps 27774.6 | ups 0.85 | wpb 32764.7 | bsz 64 | num_updates 44325 | lr 0.000150202 | gnorm 0.642 | train_wall 3348 | gb_free 7.9 | wall 52867
2022-05-07 07:10:20 | INFO | fairseq.data.iterators | grouped total_num_itrs = 2955
2022-05-07 07:10:20 | INFO | fairseq.trainer | begin training epoch 16
2022-05-07 07:10:20 | INFO | fairseq_cli.train | Start iterating over samples
2022-05-07 07:11:46 | INFO | train_inner | epoch 016:     75 / 2955 loss=4.767, ppl=27.23, wps=17207.1, ups=0.53, wpb=32686.1, bsz=63.8, num_updates=44400, lr=0.000150075, gnorm=0.647, train_wall=112, gb_free=7.9, wall=52954
2022-05-07 07:13:41 | INFO | train_inner | epoch 016:    175 / 2955 loss=4.741, ppl=26.73, wps=28644.5, ups=0.87, wpb=32768, bsz=64, num_updates=44500, lr=0.000149906, gnorm=0.649, train_wall=113, gb_free=7.9, wall=53068
2022-05-07 07:15:35 | INFO | train_inner | epoch 016:    275 / 2955 loss=4.763, ppl=27.15, wps=28622, ups=0.87, wpb=32768, bsz=64, num_updates=44600, lr=0.000149738, gnorm=0.642, train_wall=113, gb_free=7.9, wall=53183
2022-05-07 07:17:30 | INFO | train_inner | epoch 016:    375 / 2955 loss=4.757, ppl=27.05, wps=28625.2, ups=0.87, wpb=32768, bsz=64, num_updates=44700, lr=0.000149571, gnorm=0.645, train_wall=113, gb_free=7.9, wall=53297
2022-05-07 07:19:25 | INFO | train_inner | epoch 016:    475 / 2955 loss=4.781, ppl=27.49, wps=28529.8, ups=0.87, wpb=32763, bsz=64, num_updates=44800, lr=0.000149404, gnorm=0.644, train_wall=113, gb_free=7.9, wall=53412
2022-05-07 07:21:23 | INFO | train_inner | epoch 016:    575 / 2955 loss=4.758, ppl=27.05, wps=27566.7, ups=0.84, wpb=32768, bsz=64, num_updates=44900, lr=0.000149237, gnorm=0.649, train_wall=115, gb_free=7.9, wall=53531
2022-05-07 07:23:18 | INFO | train_inner | epoch 016:    675 / 2955 loss=4.785, ppl=27.57, wps=28483.4, ups=0.87, wpb=32768, bsz=64, num_updates=45000, lr=0.000149071, gnorm=0.654, train_wall=113, gb_free=7.9, wall=53646
2022-05-07 07:25:13 | INFO | train_inner | epoch 016:    775 / 2955 loss=4.792, ppl=27.7, wps=28548.7, ups=0.87, wpb=32768, bsz=64, num_updates=45100, lr=0.000148906, gnorm=0.649, train_wall=113, gb_free=7.9, wall=53761
2022-05-07 07:27:09 | INFO | train_inner | epoch 016:    875 / 2955 loss=4.789, ppl=27.64, wps=28355.3, ups=0.87, wpb=32768, bsz=64, num_updates=45200, lr=0.000148741, gnorm=0.653, train_wall=113, gb_free=7.9, wall=53876
2022-05-07 07:29:03 | INFO | train_inner | epoch 016:    975 / 2955 loss=4.786, ppl=27.58, wps=28577.9, ups=0.87, wpb=32768, bsz=64, num_updates=45300, lr=0.000148577, gnorm=0.647, train_wall=113, gb_free=7.9, wall=53991
2022-05-07 07:31:02 | INFO | train_inner | epoch 016:   1075 / 2955 loss=4.794, ppl=27.73, wps=27603.6, ups=0.84, wpb=32768, bsz=64, num_updates=45400, lr=0.000148413, gnorm=0.649, train_wall=116, gb_free=7.9, wall=54109
2022-05-07 07:32:57 | INFO | train_inner | epoch 016:   1175 / 2955 loss=4.785, ppl=27.57, wps=28604.7, ups=0.87, wpb=32768, bsz=64, num_updates=45500, lr=0.00014825, gnorm=0.65, train_wall=113, gb_free=7.9, wall=54224
2022-05-07 07:34:51 | INFO | train_inner | epoch 016:   1275 / 2955 loss=4.79, ppl=27.67, wps=28617.1, ups=0.87, wpb=32768, bsz=64, num_updates=45600, lr=0.000148087, gnorm=0.651, train_wall=113, gb_free=7.9, wall=54339
2022-05-07 07:36:46 | INFO | train_inner | epoch 016:   1375 / 2955 loss=4.801, ppl=27.88, wps=28621.7, ups=0.87, wpb=32757.8, bsz=64, num_updates=45700, lr=0.000147925, gnorm=0.652, train_wall=113, gb_free=7.9, wall=54453
2022-05-07 07:38:40 | INFO | train_inner | epoch 016:   1475 / 2955 loss=4.782, ppl=27.51, wps=28622.3, ups=0.87, wpb=32768, bsz=64, num_updates=45800, lr=0.000147764, gnorm=0.645, train_wall=113, gb_free=7.9, wall=54567
2022-05-07 07:40:36 | INFO | train_inner | epoch 016:   1575 / 2955 loss=4.787, ppl=27.6, wps=28400.7, ups=0.87, wpb=32768, bsz=64, num_updates=45900, lr=0.000147602, gnorm=0.652, train_wall=113, gb_free=7.9, wall=54683
2022-05-07 07:42:41 | INFO | train_inner | epoch 016:   1675 / 2955 loss=4.79, ppl=27.67, wps=26120.1, ups=0.8, wpb=32768, bsz=64, num_updates=46000, lr=0.000147442, gnorm=0.645, train_wall=114, gb_free=7.9, wall=54808
2022-05-07 07:44:43 | INFO | train_inner | epoch 016:   1775 / 2955 loss=4.782, ppl=27.52, wps=26848.4, ups=0.82, wpb=32768, bsz=64, num_updates=46100, lr=0.000147282, gnorm=0.645, train_wall=112, gb_free=7.9, wall=54930
2022-05-07 07:46:43 | INFO | train_inner | epoch 016:   1875 / 2955 loss=4.787, ppl=27.61, wps=27356.5, ups=0.83, wpb=32768, bsz=64, num_updates=46200, lr=0.000147122, gnorm=0.654, train_wall=113, gb_free=7.9, wall=55050
2022-05-07 07:48:41 | INFO | train_inner | epoch 016:   1975 / 2955 loss=4.794, ppl=27.75, wps=27659.9, ups=0.84, wpb=32768, bsz=64, num_updates=46300, lr=0.000146964, gnorm=0.649, train_wall=113, gb_free=7.9, wall=55169
2022-05-07 07:50:39 | INFO | train_inner | epoch 016:   2075 / 2955 loss=4.801, ppl=27.88, wps=27903.6, ups=0.85, wpb=32768, bsz=64, num_updates=46400, lr=0.000146805, gnorm=0.651, train_wall=113, gb_free=7.9, wall=55286
2022-05-07 07:52:41 | INFO | train_inner | epoch 016:   2175 / 2955 loss=4.81, ppl=28.05, wps=26890.6, ups=0.82, wpb=32768, bsz=64, num_updates=46500, lr=0.000146647, gnorm=0.648, train_wall=117, gb_free=7.9, wall=55408
2022-05-07 07:54:36 | INFO | train_inner | epoch 016:   2275 / 2955 loss=4.805, ppl=27.96, wps=28318.2, ups=0.86, wpb=32768, bsz=64, num_updates=46600, lr=0.00014649, gnorm=0.652, train_wall=113, gb_free=7.9, wall=55524
2022-05-07 07:56:32 | INFO | train_inner | epoch 016:   2375 / 2955 loss=4.791, ppl=27.68, wps=28377.4, ups=0.87, wpb=32768, bsz=64, num_updates=46700, lr=0.000146333, gnorm=0.647, train_wall=113, gb_free=7.9, wall=55639
2022-05-07 07:58:28 | INFO | train_inner | epoch 016:   2475 / 2955 loss=4.815, ppl=28.14, wps=28262.3, ups=0.86, wpb=32768, bsz=64, num_updates=46800, lr=0.000146176, gnorm=0.657, train_wall=113, gb_free=7.9, wall=55755
2022-05-07 08:00:24 | INFO | train_inner | epoch 016:   2575 / 2955 loss=4.805, ppl=27.95, wps=28174.6, ups=0.86, wpb=32768, bsz=64, num_updates=46900, lr=0.00014602, gnorm=0.659, train_wall=114, gb_free=7.9, wall=55871
2022-05-07 08:02:23 | INFO | train_inner | epoch 016:   2675 / 2955 loss=4.813, ppl=28.1, wps=27442, ups=0.84, wpb=32768, bsz=64, num_updates=47000, lr=0.000145865, gnorm=0.656, train_wall=117, gb_free=7.9, wall=55991
2022-05-07 08:04:18 | INFO | train_inner | epoch 016:   2775 / 2955 loss=4.781, ppl=27.49, wps=28555.6, ups=0.87, wpb=32768, bsz=64, num_updates=47100, lr=0.00014571, gnorm=0.653, train_wall=113, gb_free=7.9, wall=56106
2022-05-07 08:06:13 | INFO | train_inner | epoch 016:   2875 / 2955 loss=4.813, ppl=28.12, wps=28575.4, ups=0.87, wpb=32768, bsz=64, num_updates=47200, lr=0.000145556, gnorm=0.649, train_wall=113, gb_free=7.9, wall=56220
2022-05-07 08:07:44 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-05-07 08:08:58 | INFO | valid | epoch 016 | valid on 'valid' subset | loss 4.828 | ppl 28.41 | wps 73601.4 | wpb 2047.4 | bsz 4 | num_updates 47280 | best_loss 4.828
2022-05-07 08:08:58 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 16 @ 47280 updates
2022-05-07 08:08:58 | INFO | fairseq.trainer | Saving checkpoint to /cluster/work/cotterell/liam/master-thesis/checkpoints/language_model/standard/checkpoint_best_standard_16_5.0000E-04_full.pt
2022-05-07 08:09:00 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/work/cotterell/liam/master-thesis/checkpoints/language_model/standard/checkpoint_best_standard_16_5.0000E-04_full.pt
2022-05-07 08:09:00 | INFO | fairseq.checkpoint_utils | Saved checkpoint checkpoints/language_model/standard/checkpoint_best_standard_16_5.0000E-04_full.pt (epoch 16 @ 47280 updates, score 4.828) (writing took 1.7878645178861916 seconds)
2022-05-07 08:09:00 | INFO | fairseq_cli.train | end of epoch 16 (average epoch stats below)
2022-05-07 08:09:00 | INFO | train | epoch 016 | loss 4.788 | ppl 27.62 | wps 27509.3 | ups 0.84 | wpb 32764.7 | bsz 64 | num_updates 47280 | lr 0.000145432 | gnorm 0.65 | train_wall 3345 | gb_free 7.9 | wall 56387
2022-05-07 08:09:00 | INFO | fairseq.data.iterators | grouped total_num_itrs = 2955
2022-05-07 08:09:00 | INFO | fairseq.trainer | begin training epoch 17
2022-05-07 08:09:00 | INFO | fairseq_cli.train | Start iterating over samples
2022-05-07 08:09:23 | INFO | train_inner | epoch 017:     20 / 2955 loss=4.785, ppl=27.57, wps=17223.7, ups=0.53, wpb=32686.1, bsz=63.8, num_updates=47300, lr=0.000145402, gnorm=0.647, train_wall=112, gb_free=7.9, wall=56410
2022-05-07 08:11:19 | INFO | train_inner | epoch 017:    120 / 2955 loss=4.742, ppl=26.75, wps=28262.8, ups=0.86, wpb=32768, bsz=64, num_updates=47400, lr=0.000145248, gnorm=0.649, train_wall=114, gb_free=7.9, wall=56526
2022-05-07 08:13:13 | INFO | train_inner | epoch 017:    220 / 2955 loss=4.75, ppl=26.91, wps=28718.9, ups=0.88, wpb=32768, bsz=64, num_updates=47500, lr=0.000145095, gnorm=0.653, train_wall=112, gb_free=7.9, wall=56640
2022-05-07 08:15:07 | INFO | train_inner | epoch 017:    320 / 2955 loss=4.752, ppl=26.95, wps=28730.3, ups=0.88, wpb=32768, bsz=64, num_updates=47600, lr=0.000144943, gnorm=0.653, train_wall=112, gb_free=7.9, wall=56754
2022-05-07 08:17:02 | INFO | train_inner | epoch 017:    420 / 2955 loss=4.744, ppl=26.8, wps=28555.9, ups=0.87, wpb=32768, bsz=64, num_updates=47700, lr=0.000144791, gnorm=0.66, train_wall=112, gb_free=7.9, wall=56869
2022-05-07 08:19:01 | INFO | train_inner | epoch 017:    520 / 2955 loss=4.754, ppl=26.99, wps=27317.9, ups=0.83, wpb=32768, bsz=64, num_updates=47800, lr=0.000144639, gnorm=0.657, train_wall=112, gb_free=7.9, wall=56989
2022-05-07 08:21:05 | INFO | train_inner | epoch 017:    620 / 2955 loss=4.749, ppl=26.9, wps=26469.3, ups=0.81, wpb=32768, bsz=64, num_updates=47900, lr=0.000144488, gnorm=0.653, train_wall=114, gb_free=7.9, wall=57113
2022-05-07 08:23:05 | INFO | train_inner | epoch 017:    720 / 2955 loss=4.761, ppl=27.11, wps=27376.6, ups=0.84, wpb=32768, bsz=64, num_updates=48000, lr=0.000144338, gnorm=0.654, train_wall=112, gb_free=7.9, wall=57232
2022-05-07 08:25:03 | INFO | train_inner | epoch 017:    820 / 2955 loss=4.767, ppl=27.23, wps=27668.6, ups=0.84, wpb=32768, bsz=64, num_updates=48100, lr=0.000144187, gnorm=0.657, train_wall=112, gb_free=7.9, wall=57351
2022-05-07 08:27:01 | INFO | train_inner | epoch 017:    920 / 2955 loss=4.77, ppl=27.28, wps=27931.8, ups=0.85, wpb=32768, bsz=64, num_updates=48200, lr=0.000144038, gnorm=0.662, train_wall=112, gb_free=7.9, wall=57468
2022-05-07 08:28:57 | INFO | train_inner | epoch 017:   1020 / 2955 loss=4.761, ppl=27.11, wps=28090.2, ups=0.86, wpb=32768, bsz=64, num_updates=48300, lr=0.000143889, gnorm=0.655, train_wall=113, gb_free=7.9, wall=57585
2022-05-07 08:30:57 | INFO | train_inner | epoch 017:   1120 / 2955 loss=4.768, ppl=27.24, wps=27380.6, ups=0.84, wpb=32768, bsz=64, num_updates=48400, lr=0.00014374, gnorm=0.655, train_wall=115, gb_free=7.9, wall=57704
2022-05-07 08:32:55 | INFO | train_inner | epoch 017:   1220 / 2955 loss=4.745, ppl=26.81, wps=27818.9, ups=0.85, wpb=32768, bsz=64, num_updates=48500, lr=0.000143592, gnorm=0.653, train_wall=114, gb_free=7.9, wall=57822
2022-05-07 08:34:50 | INFO | train_inner | epoch 017:   1320 / 2955 loss=4.78, ppl=27.47, wps=28470.3, ups=0.87, wpb=32768, bsz=64, num_updates=48600, lr=0.000143444, gnorm=0.656, train_wall=112, gb_free=7.9, wall=57937
2022-05-07 08:36:45 | INFO | train_inner | epoch 017:   1420 / 2955 loss=4.787, ppl=27.61, wps=28574, ups=0.87, wpb=32768, bsz=64, num_updates=48700, lr=0.000143296, gnorm=0.662, train_wall=112, gb_free=7.9, wall=58052
2022-05-07 08:38:41 | INFO | train_inner | epoch 017:   1520 / 2955 loss=4.779, ppl=27.45, wps=28189.4, ups=0.86, wpb=32768, bsz=64, num_updates=48800, lr=0.00014315, gnorm=0.653, train_wall=114, gb_free=7.9, wall=58168
2022-05-07 08:40:40 | INFO | train_inner | epoch 017:   1620 / 2955 loss=4.775, ppl=27.37, wps=27537.8, ups=0.84, wpb=32768, bsz=64, num_updates=48900, lr=0.000143003, gnorm=0.664, train_wall=116, gb_free=7.9, wall=58287
2022-05-07 08:42:36 | INFO | train_inner | epoch 017:   1720 / 2955 loss=4.773, ppl=27.33, wps=28214.5, ups=0.86, wpb=32768, bsz=64, num_updates=49000, lr=0.000142857, gnorm=0.655, train_wall=114, gb_free=7.9, wall=58403
2022-05-07 08:44:30 | INFO | train_inner | epoch 017:   1820 / 2955 loss=4.793, ppl=27.73, wps=28632.9, ups=0.87, wpb=32768, bsz=64, num_updates=49100, lr=0.000142712, gnorm=0.661, train_wall=112, gb_free=7.9, wall=58518
2022-05-07 08:46:25 | INFO | train_inner | epoch 017:   1920 / 2955 loss=4.781, ppl=27.49, wps=28688.9, ups=0.88, wpb=32768, bsz=64, num_updates=49200, lr=0.000142566, gnorm=0.656, train_wall=112, gb_free=7.9, wall=58632
2022-05-07 08:48:19 | INFO | train_inner | epoch 017:   2020 / 2955 loss=4.782, ppl=27.5, wps=28675.9, ups=0.88, wpb=32768, bsz=64, num_updates=49300, lr=0.000142422, gnorm=0.658, train_wall=113, gb_free=7.9, wall=58746
2022-05-07 08:50:14 | INFO | train_inner | epoch 017:   2120 / 2955 loss=4.786, ppl=27.58, wps=28535.7, ups=0.87, wpb=32768, bsz=64, num_updates=49400, lr=0.000142278, gnorm=0.653, train_wall=113, gb_free=7.9, wall=58861
2022-05-07 08:52:11 | INFO | train_inner | epoch 017:   2220 / 2955 loss=4.792, ppl=27.71, wps=28014.5, ups=0.85, wpb=32768, bsz=64, num_updates=49500, lr=0.000142134, gnorm=0.659, train_wall=114, gb_free=7.9, wall=58978
2022-05-07 08:54:05 | INFO | train_inner | epoch 017:   2320 / 2955 loss=4.789, ppl=27.64, wps=28695.4, ups=0.88, wpb=32757.8, bsz=64, num_updates=49600, lr=0.00014199, gnorm=0.658, train_wall=112, gb_free=7.9, wall=59092
2022-05-07 08:55:59 | INFO | train_inner | epoch 017:   2420 / 2955 loss=4.804, ppl=27.94, wps=28744.5, ups=0.88, wpb=32768, bsz=64, num_updates=49700, lr=0.000141848, gnorm=0.662, train_wall=112, gb_free=7.9, wall=59206
2022-05-07 08:57:54 | INFO | train_inner | epoch 017:   2520 / 2955 loss=4.779, ppl=27.46, wps=28464.5, ups=0.87, wpb=32768, bsz=64, num_updates=49800, lr=0.000141705, gnorm=0.656, train_wall=113, gb_free=7.9, wall=59321
2022-05-07 08:59:52 | INFO | train_inner | epoch 017:   2620 / 2955 loss=4.8, ppl=27.86, wps=27801.1, ups=0.85, wpb=32768, bsz=64, num_updates=49900, lr=0.000141563, gnorm=0.653, train_wall=115, gb_free=7.9, wall=59439
2022-05-07 09:01:48 | INFO | train_inner | epoch 017:   2720 / 2955 loss=4.794, ppl=27.74, wps=28235.5, ups=0.86, wpb=32768, bsz=64, num_updates=50000, lr=0.000141421, gnorm=0.66, train_wall=114, gb_free=7.9, wall=59555
2022-05-07 09:03:42 | INFO | train_inner | epoch 017:   2820 / 2955 loss=4.793, ppl=27.72, wps=28666.6, ups=0.87, wpb=32763, bsz=64, num_updates=50100, lr=0.00014128, gnorm=0.66, train_wall=113, gb_free=7.9, wall=59670
2022-05-07 09:05:36 | INFO | train_inner | epoch 017:   2920 / 2955 loss=4.803, ppl=27.92, wps=28686.7, ups=0.88, wpb=32768, bsz=64, num_updates=50200, lr=0.000141139, gnorm=0.658, train_wall=113, gb_free=7.9, wall=59784
2022-05-07 09:06:16 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-05-07 09:07:29 | INFO | valid | epoch 017 | valid on 'valid' subset | loss 4.82 | ppl 28.25 | wps 74444.5 | wpb 2047.4 | bsz 4 | num_updates 50235 | best_loss 4.82
2022-05-07 09:07:29 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 17 @ 50235 updates
2022-05-07 09:07:29 | INFO | fairseq.trainer | Saving checkpoint to /cluster/work/cotterell/liam/master-thesis/checkpoints/language_model/standard/checkpoint_best_standard_16_5.0000E-04_full.pt
2022-05-07 09:07:30 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/work/cotterell/liam/master-thesis/checkpoints/language_model/standard/checkpoint_best_standard_16_5.0000E-04_full.pt
2022-05-07 09:07:30 | INFO | fairseq.checkpoint_utils | Saved checkpoint checkpoints/language_model/standard/checkpoint_best_standard_16_5.0000E-04_full.pt (epoch 17 @ 50235 updates, score 4.82) (writing took 1.6292839930392802 seconds)
2022-05-07 09:07:30 | INFO | fairseq_cli.train | end of epoch 17 (average epoch stats below)
2022-05-07 09:07:30 | INFO | train | epoch 017 | loss 4.774 | ppl 27.37 | wps 27579.2 | ups 0.84 | wpb 32764.7 | bsz 64 | num_updates 50235 | lr 0.00014109 | gnorm 0.657 | train_wall 3346 | gb_free 7.9 | wall 59898
2022-05-07 09:07:30 | INFO | fairseq.data.iterators | grouped total_num_itrs = 2955
2022-05-07 09:07:30 | INFO | fairseq.trainer | begin training epoch 18
2022-05-07 09:07:30 | INFO | fairseq_cli.train | Start iterating over samples
2022-05-07 09:08:46 | INFO | train_inner | epoch 018:     65 / 2955 loss=4.736, ppl=26.66, wps=17245.5, ups=0.53, wpb=32686.1, bsz=63.8, num_updates=50300, lr=0.000140999, gnorm=0.656, train_wall=112, gb_free=7.9, wall=59973
2022-05-07 09:10:40 | INFO | train_inner | epoch 018:    165 / 2955 loss=4.717, ppl=26.31, wps=28655.7, ups=0.87, wpb=32768, bsz=64, num_updates=50400, lr=0.000140859, gnorm=0.659, train_wall=113, gb_free=7.9, wall=60088
2022-05-07 09:12:35 | INFO | train_inner | epoch 018:    265 / 2955 loss=4.725, ppl=26.45, wps=28615.8, ups=0.87, wpb=32768, bsz=64, num_updates=50500, lr=0.00014072, gnorm=0.658, train_wall=113, gb_free=7.9, wall=60202
2022-05-07 09:14:29 | INFO | train_inner | epoch 018:    365 / 2955 loss=4.739, ppl=26.71, wps=28704.4, ups=0.88, wpb=32768, bsz=64, num_updates=50600, lr=0.00014058, gnorm=0.661, train_wall=113, gb_free=7.9, wall=60316
2022-05-07 09:16:23 | INFO | train_inner | epoch 018:    465 / 2955 loss=4.75, ppl=26.91, wps=28707.3, ups=0.88, wpb=32768, bsz=64, num_updates=50700, lr=0.000140442, gnorm=0.667, train_wall=113, gb_free=7.9, wall=60430
2022-05-07 09:18:30 | INFO | train_inner | epoch 018:    565 / 2955 loss=4.749, ppl=26.89, wps=25862.4, ups=0.79, wpb=32768, bsz=64, num_updates=50800, lr=0.000140303, gnorm=0.662, train_wall=116, gb_free=7.9, wall=60557
2022-05-07 09:20:30 | INFO | train_inner | epoch 018:    665 / 2955 loss=4.738, ppl=26.69, wps=27250.1, ups=0.83, wpb=32768, bsz=64, num_updates=50900, lr=0.000140165, gnorm=0.663, train_wall=112, gb_free=7.9, wall=60677
2022-05-07 09:22:29 | INFO | train_inner | epoch 018:    765 / 2955 loss=4.759, ppl=27.08, wps=27565.9, ups=0.84, wpb=32768, bsz=64, num_updates=51000, lr=0.000140028, gnorm=0.66, train_wall=112, gb_free=7.9, wall=60796
2022-05-07 09:24:26 | INFO | train_inner | epoch 018:    865 / 2955 loss=4.751, ppl=26.93, wps=27888.4, ups=0.85, wpb=32768, bsz=64, num_updates=51100, lr=0.000139891, gnorm=0.665, train_wall=112, gb_free=7.9, wall=60914
2022-05-07 09:26:24 | INFO | train_inner | epoch 018:    965 / 2955 loss=4.754, ppl=26.98, wps=27980, ups=0.85, wpb=32768, bsz=64, num_updates=51200, lr=0.000139754, gnorm=0.661, train_wall=112, gb_free=7.9, wall=61031
2022-05-07 09:28:24 | INFO | train_inner | epoch 018:   1065 / 2955 loss=4.761, ppl=27.11, wps=27167.2, ups=0.83, wpb=32768, bsz=64, num_updates=51300, lr=0.000139618, gnorm=0.663, train_wall=116, gb_free=7.9, wall=61152
2022-05-07 09:30:20 | INFO | train_inner | epoch 018:   1165 / 2955 loss=4.753, ppl=26.96, wps=28341.6, ups=0.86, wpb=32768, bsz=64, num_updates=51400, lr=0.000139482, gnorm=0.661, train_wall=112, gb_free=7.9, wall=61267
2022-05-07 09:32:15 | INFO | train_inner | epoch 018:   1265 / 2955 loss=4.764, ppl=27.18, wps=28352.4, ups=0.87, wpb=32768, bsz=64, num_updates=51500, lr=0.000139347, gnorm=0.662, train_wall=113, gb_free=7.9, wall=61383
2022-05-07 09:34:11 | INFO | train_inner | epoch 018:   1365 / 2955 loss=4.776, ppl=27.4, wps=28342.7, ups=0.86, wpb=32768, bsz=64, num_updates=51600, lr=0.000139212, gnorm=0.661, train_wall=113, gb_free=7.9, wall=61498
2022-05-07 09:36:06 | INFO | train_inner | epoch 018:   1465 / 2955 loss=4.767, ppl=27.23, wps=28475.9, ups=0.87, wpb=32768, bsz=64, num_updates=51700, lr=0.000139077, gnorm=0.671, train_wall=112, gb_free=7.9, wall=61613
2022-05-07 09:38:05 | INFO | train_inner | epoch 018:   1565 / 2955 loss=4.766, ppl=27.21, wps=27660.1, ups=0.84, wpb=32768, bsz=64, num_updates=51800, lr=0.000138943, gnorm=0.66, train_wall=116, gb_free=7.9, wall=61732
2022-05-07 09:39:59 | INFO | train_inner | epoch 018:   1665 / 2955 loss=4.772, ppl=27.31, wps=28527.7, ups=0.87, wpb=32757.8, bsz=64, num_updates=51900, lr=0.000138809, gnorm=0.666, train_wall=112, gb_free=7.9, wall=61847
2022-05-07 09:41:56 | INFO | train_inner | epoch 018:   1765 / 2955 loss=4.768, ppl=27.24, wps=28212.5, ups=0.86, wpb=32768, bsz=64, num_updates=52000, lr=0.000138675, gnorm=0.663, train_wall=114, gb_free=7.9, wall=61963
2022-05-07 09:43:50 | INFO | train_inner | epoch 018:   1865 / 2955 loss=4.776, ppl=27.4, wps=28643.3, ups=0.87, wpb=32768, bsz=64, num_updates=52100, lr=0.000138542, gnorm=0.666, train_wall=113, gb_free=7.9, wall=62077
2022-05-07 09:45:44 | INFO | train_inner | epoch 018:   1965 / 2955 loss=4.773, ppl=27.33, wps=28718.7, ups=0.88, wpb=32768, bsz=64, num_updates=52200, lr=0.000138409, gnorm=0.665, train_wall=112, gb_free=7.9, wall=62191
2022-05-07 09:47:38 | INFO | train_inner | epoch 018:   2065 / 2955 loss=4.769, ppl=27.27, wps=28680, ups=0.88, wpb=32768, bsz=64, num_updates=52300, lr=0.000138277, gnorm=0.664, train_wall=113, gb_free=7.9, wall=62306
2022-05-07 09:49:36 | INFO | train_inner | epoch 018:   2165 / 2955 loss=4.768, ppl=27.25, wps=27823.4, ups=0.85, wpb=32768, bsz=64, num_updates=52400, lr=0.000138145, gnorm=0.666, train_wall=115, gb_free=7.9, wall=62423
2022-05-07 09:51:30 | INFO | train_inner | epoch 018:   2265 / 2955 loss=4.774, ppl=27.37, wps=28721.2, ups=0.88, wpb=32768, bsz=64, num_updates=52500, lr=0.000138013, gnorm=0.667, train_wall=112, gb_free=7.9, wall=62537
2022-05-07 09:53:29 | INFO | train_inner | epoch 018:   2365 / 2955 loss=4.782, ppl=27.52, wps=27673.6, ups=0.84, wpb=32763, bsz=64, num_updates=52600, lr=0.000137882, gnorm=0.662, train_wall=116, gb_free=7.9, wall=62656
2022-05-07 09:55:24 | INFO | train_inner | epoch 018:   2465 / 2955 loss=4.778, ppl=27.44, wps=28377.8, ups=0.87, wpb=32768, bsz=64, num_updates=52700, lr=0.000137751, gnorm=0.663, train_wall=112, gb_free=7.9, wall=62771
2022-05-07 09:57:19 | INFO | train_inner | epoch 018:   2565 / 2955 loss=4.79, ppl=27.66, wps=28585.7, ups=0.87, wpb=32768, bsz=64, num_updates=52800, lr=0.00013762, gnorm=0.669, train_wall=112, gb_free=7.9, wall=62886
2022-05-07 09:59:13 | INFO | train_inner | epoch 018:   2665 / 2955 loss=4.777, ppl=27.41, wps=28537.4, ups=0.87, wpb=32768, bsz=64, num_updates=52900, lr=0.00013749, gnorm=0.662, train_wall=112, gb_free=7.9, wall=63001
2022-05-07 10:01:08 | INFO | train_inner | epoch 018:   2765 / 2955 loss=4.783, ppl=27.54, wps=28697.3, ups=0.88, wpb=32768, bsz=64, num_updates=53000, lr=0.000137361, gnorm=0.666, train_wall=112, gb_free=7.9, wall=63115
2022-05-07 10:03:05 | INFO | train_inner | epoch 018:   2865 / 2955 loss=4.779, ppl=27.45, wps=27854.9, ups=0.85, wpb=32768, bsz=64, num_updates=53100, lr=0.000137231, gnorm=0.665, train_wall=115, gb_free=7.9, wall=63233
2022-05-07 10:04:48 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-05-07 10:06:00 | INFO | valid | epoch 018 | valid on 'valid' subset | loss 4.812 | ppl 28.08 | wps 74073.4 | wpb 2047.4 | bsz 4 | num_updates 53190 | best_loss 4.812
2022-05-07 10:06:00 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 18 @ 53190 updates
2022-05-07 10:06:00 | INFO | fairseq.trainer | Saving checkpoint to /cluster/work/cotterell/liam/master-thesis/checkpoints/language_model/standard/checkpoint_best_standard_16_5.0000E-04_full.pt
2022-05-07 10:06:02 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/work/cotterell/liam/master-thesis/checkpoints/language_model/standard/checkpoint_best_standard_16_5.0000E-04_full.pt
2022-05-07 10:06:02 | INFO | fairseq.checkpoint_utils | Saved checkpoint checkpoints/language_model/standard/checkpoint_best_standard_16_5.0000E-04_full.pt (epoch 18 @ 53190 updates, score 4.812) (writing took 1.52610529307276 seconds)
2022-05-07 10:06:02 | INFO | fairseq_cli.train | end of epoch 18 (average epoch stats below)
2022-05-07 10:06:02 | INFO | train | epoch 018 | loss 4.762 | ppl 27.14 | wps 27571.8 | ups 0.84 | wpb 32764.7 | bsz 64 | num_updates 53190 | lr 0.000137115 | gnorm 0.663 | train_wall 3346 | gb_free 7.9 | wall 63409
2022-05-07 10:06:02 | INFO | fairseq.data.iterators | grouped total_num_itrs = 2955
2022-05-07 10:06:02 | INFO | fairseq.trainer | begin training epoch 19
2022-05-07 10:06:02 | INFO | fairseq_cli.train | Start iterating over samples
2022-05-07 10:06:13 | INFO | train_inner | epoch 019:     10 / 2955 loss=4.773, ppl=27.34, wps=17375, ups=0.53, wpb=32686.1, bsz=63.8, num_updates=53200, lr=0.000137102, gnorm=0.666, train_wall=112, gb_free=7.9, wall=63421
2022-05-07 10:08:08 | INFO | train_inner | epoch 019:    110 / 2955 loss=4.702, ppl=26.03, wps=28704.3, ups=0.88, wpb=32768, bsz=64, num_updates=53300, lr=0.000136973, gnorm=0.666, train_wall=112, gb_free=7.9, wall=63535
2022-05-07 10:10:02 | INFO | train_inner | epoch 019:    210 / 2955 loss=4.73, ppl=26.54, wps=28702.9, ups=0.88, wpb=32768, bsz=64, num_updates=53400, lr=0.000136845, gnorm=0.669, train_wall=112, gb_free=7.9, wall=63649
2022-05-07 10:11:59 | INFO | train_inner | epoch 019:    310 / 2955 loss=4.722, ppl=26.38, wps=28048.8, ups=0.86, wpb=32768, bsz=64, num_updates=53500, lr=0.000136717, gnorm=0.666, train_wall=114, gb_free=7.9, wall=63766
2022-05-07 10:13:53 | INFO | train_inner | epoch 019:    410 / 2955 loss=4.74, ppl=26.72, wps=28693, ups=0.88, wpb=32768, bsz=64, num_updates=53600, lr=0.00013659, gnorm=0.667, train_wall=112, gb_free=7.9, wall=63880
2022-05-07 10:15:47 | INFO | train_inner | epoch 019:    510 / 2955 loss=4.721, ppl=26.38, wps=28647.4, ups=0.87, wpb=32768, bsz=64, num_updates=53700, lr=0.000136462, gnorm=0.673, train_wall=112, gb_free=7.9, wall=63994
2022-05-07 10:17:46 | INFO | train_inner | epoch 019:    610 / 2955 loss=4.737, ppl=26.66, wps=27593.2, ups=0.84, wpb=32768, bsz=64, num_updates=53800, lr=0.000136335, gnorm=0.668, train_wall=115, gb_free=7.9, wall=64113
2022-05-07 10:19:40 | INFO | train_inner | epoch 019:    710 / 2955 loss=4.757, ppl=27.04, wps=28702.4, ups=0.88, wpb=32768, bsz=64, num_updates=53900, lr=0.000136209, gnorm=0.679, train_wall=112, gb_free=7.9, wall=64227
2022-05-07 10:21:34 | INFO | train_inner | epoch 019:    810 / 2955 loss=4.74, ppl=26.72, wps=28701.2, ups=0.88, wpb=32768, bsz=64, num_updates=54000, lr=0.000136083, gnorm=0.67, train_wall=112, gb_free=7.9, wall=64342
2022-05-07 10:23:30 | INFO | train_inner | epoch 019:    910 / 2955 loss=4.738, ppl=26.69, wps=28304.8, ups=0.86, wpb=32768, bsz=64, num_updates=54100, lr=0.000135957, gnorm=0.674, train_wall=114, gb_free=7.9, wall=64457
2022-05-07 10:25:24 | INFO | train_inner | epoch 019:   1010 / 2955 loss=4.752, ppl=26.94, wps=28705.4, ups=0.88, wpb=32768, bsz=64, num_updates=54200, lr=0.000135831, gnorm=0.668, train_wall=112, gb_free=7.9, wall=64571
2022-05-07 10:27:20 | INFO | train_inner | epoch 019:   1110 / 2955 loss=4.737, ppl=26.68, wps=28228, ups=0.86, wpb=32757.8, bsz=64, num_updates=54300, lr=0.000135706, gnorm=0.67, train_wall=112, gb_free=7.9, wall=64688
2022-05-07 10:29:14 | INFO | train_inner | epoch 019:   1210 / 2955 loss=4.758, ppl=27.05, wps=28745.2, ups=0.88, wpb=32768, bsz=64, num_updates=54400, lr=0.000135582, gnorm=0.675, train_wall=112, gb_free=7.9, wall=64802
2022-05-07 10:31:09 | INFO | train_inner | epoch 019:   1310 / 2955 loss=4.759, ppl=27.08, wps=28436.8, ups=0.87, wpb=32768, bsz=64, num_updates=54500, lr=0.000135457, gnorm=0.674, train_wall=113, gb_free=7.9, wall=64917
2022-05-07 10:33:05 | INFO | train_inner | epoch 019:   1410 / 2955 loss=4.759, ppl=27.08, wps=28284.9, ups=0.86, wpb=32768, bsz=64, num_updates=54600, lr=0.000135333, gnorm=0.668, train_wall=114, gb_free=7.9, wall=65033
2022-05-07 10:34:59 | INFO | train_inner | epoch 019:   1510 / 2955 loss=4.748, ppl=26.87, wps=28713.2, ups=0.88, wpb=32768, bsz=64, num_updates=54700, lr=0.000135209, gnorm=0.671, train_wall=112, gb_free=7.9, wall=65147
2022-05-07 10:36:54 | INFO | train_inner | epoch 019:   1610 / 2955 loss=4.773, ppl=27.34, wps=28560.5, ups=0.87, wpb=32768, bsz=64, num_updates=54800, lr=0.000135086, gnorm=0.675, train_wall=112, gb_free=7.9, wall=65261
2022-05-07 10:38:48 | INFO | train_inner | epoch 019:   1710 / 2955 loss=4.756, ppl=27.02, wps=28721.1, ups=0.88, wpb=32768, bsz=64, num_updates=54900, lr=0.000134963, gnorm=0.668, train_wall=112, gb_free=7.9, wall=65376
2022-05-07 10:40:42 | INFO | train_inner | epoch 019:   1810 / 2955 loss=4.762, ppl=27.13, wps=28720.2, ups=0.88, wpb=32768, bsz=64, num_updates=55000, lr=0.00013484, gnorm=0.668, train_wall=112, gb_free=7.9, wall=65490
2022-05-07 10:42:40 | INFO | train_inner | epoch 019:   1910 / 2955 loss=4.748, ppl=26.87, wps=27921.1, ups=0.85, wpb=32768, bsz=64, num_updates=55100, lr=0.000134718, gnorm=0.666, train_wall=116, gb_free=7.9, wall=65607
2022-05-07 10:44:35 | INFO | train_inner | epoch 019:   2010 / 2955 loss=4.752, ppl=26.95, wps=28461.8, ups=0.87, wpb=32768, bsz=64, num_updates=55200, lr=0.000134595, gnorm=0.671, train_wall=112, gb_free=7.9, wall=65722
2022-05-07 10:46:30 | INFO | train_inner | epoch 019:   2110 / 2955 loss=4.758, ppl=27.06, wps=28527.4, ups=0.87, wpb=32768, bsz=64, num_updates=55300, lr=0.000134474, gnorm=0.675, train_wall=113, gb_free=7.9, wall=65837
2022-05-07 10:48:24 | INFO | train_inner | epoch 019:   2210 / 2955 loss=4.76, ppl=27.09, wps=28731.1, ups=0.88, wpb=32768, bsz=64, num_updates=55400, lr=0.000134352, gnorm=0.67, train_wall=112, gb_free=7.9, wall=65951
2022-05-07 10:50:19 | INFO | train_inner | epoch 019:   2310 / 2955 loss=4.763, ppl=27.16, wps=28447.7, ups=0.87, wpb=32768, bsz=64, num_updates=55500, lr=0.000134231, gnorm=0.671, train_wall=113, gb_free=7.9, wall=66066
2022-05-07 10:52:15 | INFO | train_inner | epoch 019:   2410 / 2955 loss=4.77, ppl=27.28, wps=28238.6, ups=0.86, wpb=32768, bsz=64, num_updates=55600, lr=0.00013411, gnorm=0.673, train_wall=114, gb_free=7.9, wall=66182
2022-05-07 10:54:09 | INFO | train_inner | epoch 019:   2510 / 2955 loss=4.768, ppl=27.25, wps=28648.7, ups=0.87, wpb=32763, bsz=64, num_updates=55700, lr=0.00013399, gnorm=0.672, train_wall=113, gb_free=7.9, wall=66297
2022-05-07 10:56:03 | INFO | train_inner | epoch 019:   2610 / 2955 loss=4.763, ppl=27.15, wps=28753.9, ups=0.88, wpb=32768, bsz=64, num_updates=55800, lr=0.00013387, gnorm=0.671, train_wall=112, gb_free=7.9, wall=66411
2022-05-07 10:57:58 | INFO | train_inner | epoch 019:   2710 / 2955 loss=4.769, ppl=27.27, wps=28597.1, ups=0.87, wpb=32768, bsz=64, num_updates=55900, lr=0.00013375, gnorm=0.672, train_wall=112, gb_free=7.9, wall=66525
2022-05-07 11:00:00 | INFO | train_inner | epoch 019:   2810 / 2955 loss=4.752, ppl=26.95, wps=26764, ups=0.82, wpb=32768, bsz=64, num_updates=56000, lr=0.000133631, gnorm=0.668, train_wall=116, gb_free=7.9, wall=66648
2022-05-07 11:02:02 | INFO | train_inner | epoch 019:   2910 / 2955 loss=4.782, ppl=27.51, wps=26815.4, ups=0.82, wpb=32768, bsz=64, num_updates=56100, lr=0.000133511, gnorm=0.673, train_wall=112, gb_free=7.9, wall=66770
2022-05-07 11:02:56 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-05-07 11:04:10 | INFO | valid | epoch 019 | valid on 'valid' subset | loss 4.806 | ppl 27.96 | wps 73357.8 | wpb 2047.4 | bsz 4 | num_updates 56145 | best_loss 4.806
2022-05-07 11:04:10 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 19 @ 56145 updates
2022-05-07 11:04:10 | INFO | fairseq.trainer | Saving checkpoint to /cluster/work/cotterell/liam/master-thesis/checkpoints/language_model/standard/checkpoint_best_standard_16_5.0000E-04_full.pt
2022-05-07 11:04:12 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/work/cotterell/liam/master-thesis/checkpoints/language_model/standard/checkpoint_best_standard_16_5.0000E-04_full.pt
2022-05-07 11:04:12 | INFO | fairseq.checkpoint_utils | Saved checkpoint checkpoints/language_model/standard/checkpoint_best_standard_16_5.0000E-04_full.pt (epoch 19 @ 56145 updates, score 4.806) (writing took 1.7711238670162857 seconds)
2022-05-07 11:04:12 | INFO | fairseq_cli.train | end of epoch 19 (average epoch stats below)
2022-05-07 11:04:12 | INFO | train | epoch 019 | loss 4.751 | ppl 26.93 | wps 27740.3 | ups 0.85 | wpb 32764.7 | bsz 64 | num_updates 56145 | lr 0.000133458 | gnorm 0.671 | train_wall 3341 | gb_free 7.9 | wall 66899
2022-05-07 11:04:12 | INFO | fairseq.data.iterators | grouped total_num_itrs = 2955
2022-05-07 11:04:12 | INFO | fairseq.trainer | begin training epoch 20
2022-05-07 11:04:12 | INFO | fairseq_cli.train | Start iterating over samples
2022-05-07 11:05:19 | INFO | train_inner | epoch 020:     55 / 2955 loss=4.744, ppl=26.79, wps=16634.5, ups=0.51, wpb=32686.1, bsz=63.8, num_updates=56200, lr=0.000133393, gnorm=0.673, train_wall=112, gb_free=7.9, wall=66966
2022-05-07 11:07:18 | INFO | train_inner | epoch 020:    155 / 2955 loss=4.702, ppl=26.04, wps=27490.2, ups=0.84, wpb=32768, bsz=64, num_updates=56300, lr=0.000133274, gnorm=0.67, train_wall=112, gb_free=7.9, wall=67086
2022-05-07 11:09:16 | INFO | train_inner | epoch 020:    255 / 2955 loss=4.709, ppl=26.15, wps=27922.2, ups=0.85, wpb=32768, bsz=64, num_updates=56400, lr=0.000133156, gnorm=0.677, train_wall=113, gb_free=7.9, wall=67203
2022-05-07 11:11:15 | INFO | train_inner | epoch 020:    355 / 2955 loss=4.708, ppl=26.14, wps=27518.5, ups=0.84, wpb=32757.8, bsz=64, num_updates=56500, lr=0.000133038, gnorm=0.673, train_wall=114, gb_free=7.9, wall=67322
2022-05-07 11:13:11 | INFO | train_inner | epoch 020:    455 / 2955 loss=4.729, ppl=26.51, wps=28034.9, ups=0.86, wpb=32768, bsz=64, num_updates=56600, lr=0.00013292, gnorm=0.675, train_wall=113, gb_free=7.9, wall=67439
2022-05-07 11:15:40 | INFO | train_inner | epoch 020:    555 / 2955 loss=4.71, ppl=26.16, wps=22083, ups=0.67, wpb=32768, bsz=64, num_updates=56700, lr=0.000132803, gnorm=0.672, train_wall=135, gb_free=7.9, wall=67587
2022-05-07 11:17:48 | INFO | train_inner | epoch 020:    655 / 2955 loss=4.73, ppl=26.54, wps=25645.3, ups=0.78, wpb=32768, bsz=64, num_updates=56800, lr=0.000132686, gnorm=0.675, train_wall=120, gb_free=7.9, wall=67715
2022-05-07 11:19:44 | INFO | train_inner | epoch 020:    755 / 2955 loss=4.731, ppl=26.55, wps=28269.3, ups=0.86, wpb=32768, bsz=64, num_updates=56900, lr=0.00013257, gnorm=0.68, train_wall=113, gb_free=7.9, wall=67831
2022-05-07 11:21:39 | INFO | train_inner | epoch 020:    855 / 2955 loss=4.723, ppl=26.4, wps=28319.9, ups=0.86, wpb=32763, bsz=64, num_updates=57000, lr=0.000132453, gnorm=0.679, train_wall=113, gb_free=7.9, wall=67947
2022-05-07 11:23:34 | INFO | train_inner | epoch 020:    955 / 2955 loss=4.74, ppl=26.73, wps=28496.4, ups=0.87, wpb=32768, bsz=64, num_updates=57100, lr=0.000132337, gnorm=0.675, train_wall=113, gb_free=7.9, wall=68062
2022-05-07 11:25:29 | INFO | train_inner | epoch 020:   1055 / 2955 loss=4.741, ppl=26.74, wps=28526.2, ups=0.87, wpb=32768, bsz=64, num_updates=57200, lr=0.000132221, gnorm=0.677, train_wall=112, gb_free=7.9, wall=68176
2022-05-07 11:27:29 | INFO | train_inner | epoch 020:   1155 / 2955 loss=4.74, ppl=26.72, wps=27383.7, ups=0.84, wpb=32768, bsz=64, num_updates=57300, lr=0.000132106, gnorm=0.678, train_wall=115, gb_free=7.9, wall=68296
2022-05-07 11:29:24 | INFO | train_inner | epoch 020:   1255 / 2955 loss=4.723, ppl=26.41, wps=28466.7, ups=0.87, wpb=32768, bsz=64, num_updates=57400, lr=0.000131991, gnorm=0.675, train_wall=112, gb_free=7.9, wall=68411
2022-05-07 11:31:22 | INFO | train_inner | epoch 020:   1355 / 2955 loss=4.742, ppl=26.76, wps=27835.1, ups=0.85, wpb=32768, bsz=64, num_updates=57500, lr=0.000131876, gnorm=0.677, train_wall=115, gb_free=7.9, wall=68529
2022-05-07 11:33:16 | INFO | train_inner | epoch 020:   1455 / 2955 loss=4.75, ppl=26.91, wps=28601.5, ups=0.87, wpb=32768, bsz=64, num_updates=57600, lr=0.000131762, gnorm=0.676, train_wall=112, gb_free=7.9, wall=68643
2022-05-07 11:35:10 | INFO | train_inner | epoch 020:   1555 / 2955 loss=4.759, ppl=27.08, wps=28669.4, ups=0.87, wpb=32768, bsz=64, num_updates=57700, lr=0.000131647, gnorm=0.677, train_wall=113, gb_free=7.9, wall=68758
2022-05-07 11:37:05 | INFO | train_inner | epoch 020:   1655 / 2955 loss=4.747, ppl=26.85, wps=28609.7, ups=0.87, wpb=32768, bsz=64, num_updates=57800, lr=0.000131533, gnorm=0.678, train_wall=113, gb_free=7.9, wall=68872
2022-05-07 11:39:02 | INFO | train_inner | epoch 020:   1755 / 2955 loss=4.732, ppl=26.57, wps=28030.9, ups=0.86, wpb=32768, bsz=64, num_updates=57900, lr=0.00013142, gnorm=0.677, train_wall=115, gb_free=7.9, wall=68989
2022-05-07 11:40:56 | INFO | train_inner | epoch 020:   1855 / 2955 loss=4.753, ppl=26.96, wps=28682.5, ups=0.88, wpb=32768, bsz=64, num_updates=58000, lr=0.000131306, gnorm=0.681, train_wall=113, gb_free=7.9, wall=69103
2022-05-07 11:42:50 | INFO | train_inner | epoch 020:   1955 / 2955 loss=4.762, ppl=27.13, wps=28692.5, ups=0.88, wpb=32768, bsz=64, num_updates=58100, lr=0.000131193, gnorm=0.675, train_wall=113, gb_free=7.9, wall=69218
2022-05-07 11:44:50 | INFO | train_inner | epoch 020:   2055 / 2955 loss=4.752, ppl=26.95, wps=27406.1, ups=0.84, wpb=32768, bsz=64, num_updates=58200, lr=0.000131081, gnorm=0.676, train_wall=117, gb_free=7.9, wall=69337
2022-05-07 11:46:47 | INFO | train_inner | epoch 020:   2155 / 2955 loss=4.747, ppl=26.86, wps=28014.3, ups=0.85, wpb=32768, bsz=64, num_updates=58300, lr=0.000130968, gnorm=0.676, train_wall=114, gb_free=7.9, wall=69454
2022-05-07 11:48:43 | INFO | train_inner | epoch 020:   2255 / 2955 loss=4.777, ppl=27.41, wps=28129, ups=0.86, wpb=32768, bsz=64, num_updates=58400, lr=0.000130856, gnorm=0.679, train_wall=113, gb_free=7.9, wall=69571
2022-05-07 11:50:42 | INFO | train_inner | epoch 020:   2355 / 2955 loss=4.753, ppl=26.96, wps=27622.2, ups=0.84, wpb=32768, bsz=64, num_updates=58500, lr=0.000130744, gnorm=0.68, train_wall=115, gb_free=7.9, wall=69689
2022-05-07 11:52:38 | INFO | train_inner | epoch 020:   2455 / 2955 loss=4.754, ppl=26.98, wps=28167.7, ups=0.86, wpb=32768, bsz=64, num_updates=58600, lr=0.000130632, gnorm=0.677, train_wall=114, gb_free=7.9, wall=69806
2022-05-07 11:54:33 | INFO | train_inner | epoch 020:   2555 / 2955 loss=4.768, ppl=27.24, wps=28620.6, ups=0.87, wpb=32768, bsz=64, num_updates=58700, lr=0.000130521, gnorm=0.68, train_wall=113, gb_free=7.9, wall=69920
2022-05-07 11:56:27 | INFO | train_inner | epoch 020:   2655 / 2955 loss=4.749, ppl=26.89, wps=28670, ups=0.87, wpb=32768, bsz=64, num_updates=58800, lr=0.00013041, gnorm=0.675, train_wall=112, gb_free=7.9, wall=70034
2022-05-07 11:58:22 | INFO | train_inner | epoch 020:   2755 / 2955 loss=4.757, ppl=27.04, wps=28642.1, ups=0.87, wpb=32768, bsz=64, num_updates=58900, lr=0.000130299, gnorm=0.679, train_wall=113, gb_free=7.9, wall=70149
2022-05-07 12:00:18 | INFO | train_inner | epoch 020:   2855 / 2955 loss=4.762, ppl=27.13, wps=28114.8, ups=0.86, wpb=32768, bsz=64, num_updates=59000, lr=0.000130189, gnorm=0.675, train_wall=115, gb_free=7.9, wall=70265
2022-05-07 12:02:15 | INFO | train_inner | epoch 020:   2955 / 2955 loss=4.757, ppl=27.03, wps=27968.6, ups=0.86, wpb=32686.1, bsz=63.8, num_updates=59100, lr=0.000130079, gnorm=0.681, train_wall=114, gb_free=7.9, wall=70382
2022-05-07 12:02:15 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-05-07 12:03:28 | INFO | valid | epoch 020 | valid on 'valid' subset | loss 4.798 | ppl 27.82 | wps 74039.5 | wpb 2047.4 | bsz 4 | num_updates 59100 | best_loss 4.798
2022-05-07 12:03:28 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 20 @ 59100 updates
2022-05-07 12:03:28 | INFO | fairseq.trainer | Saving checkpoint to /cluster/work/cotterell/liam/master-thesis/checkpoints/language_model/standard/checkpoint_best_standard_16_5.0000E-04_full.pt
2022-05-07 12:03:29 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/work/cotterell/liam/master-thesis/checkpoints/language_model/standard/checkpoint_best_standard_16_5.0000E-04_full.pt
2022-05-07 12:03:29 | INFO | fairseq.checkpoint_utils | Saved checkpoint checkpoints/language_model/standard/checkpoint_best_standard_16_5.0000E-04_full.pt (epoch 20 @ 59100 updates, score 4.798) (writing took 1.434319194406271 seconds)
2022-05-07 12:03:29 | INFO | fairseq_cli.train | end of epoch 20 (average epoch stats below)
2022-05-07 12:03:29 | INFO | train | epoch 020 | loss 4.741 | ppl 26.74 | wps 27215.7 | ups 0.83 | wpb 32764.7 | bsz 64 | num_updates 59100 | lr 0.000130079 | gnorm 0.677 | train_wall 3381 | gb_free 7.9 | wall 70457
2022-05-07 12:03:30 | INFO | fairseq.data.iterators | grouped total_num_itrs = 2955
2022-05-07 12:03:30 | INFO | fairseq.trainer | begin training epoch 21
2022-05-07 12:03:30 | INFO | fairseq_cli.train | Start iterating over samples
2022-05-07 12:05:24 | INFO | train_inner | epoch 021:    100 / 2955 loss=4.675, ppl=25.55, wps=17323.6, ups=0.53, wpb=32768, bsz=64, num_updates=59200, lr=0.000129969, gnorm=0.677, train_wall=113, gb_free=7.9, wall=70571
2022-05-07 12:07:20 | INFO | train_inner | epoch 021:    200 / 2955 loss=4.699, ppl=25.97, wps=28329.6, ups=0.86, wpb=32768, bsz=64, num_updates=59300, lr=0.000129859, gnorm=0.679, train_wall=113, gb_free=7.9, wall=70687
2022-05-07 12:09:14 | INFO | train_inner | epoch 021:    300 / 2955 loss=4.711, ppl=26.19, wps=28568.4, ups=0.87, wpb=32768, bsz=64, num_updates=59400, lr=0.00012975, gnorm=0.68, train_wall=113, gb_free=7.9, wall=70802
2022-05-07 12:11:09 | INFO | train_inner | epoch 021:    400 / 2955 loss=4.721, ppl=26.37, wps=28631.6, ups=0.87, wpb=32768, bsz=64, num_updates=59500, lr=0.000129641, gnorm=0.685, train_wall=113, gb_free=7.9, wall=70916
2022-05-07 12:13:05 | INFO | train_inner | epoch 021:    500 / 2955 loss=4.712, ppl=26.21, wps=28125.1, ups=0.86, wpb=32768, bsz=64, num_updates=59600, lr=0.000129532, gnorm=0.681, train_wall=115, gb_free=7.9, wall=71033
2022-05-07 12:15:06 | INFO | train_inner | epoch 021:    600 / 2955 loss=4.713, ppl=26.22, wps=27276.1, ups=0.83, wpb=32768, bsz=64, num_updates=59700, lr=0.000129423, gnorm=0.687, train_wall=116, gb_free=7.9, wall=71153
2022-05-07 12:17:00 | INFO | train_inner | epoch 021:    700 / 2955 loss=4.72, ppl=26.36, wps=28585, ups=0.87, wpb=32768, bsz=64, num_updates=59800, lr=0.000129315, gnorm=0.685, train_wall=113, gb_free=7.9, wall=71268
2022-05-07 12:18:55 | INFO | train_inner | epoch 021:    800 / 2955 loss=4.729, ppl=26.53, wps=28577.9, ups=0.87, wpb=32768, bsz=64, num_updates=59900, lr=0.000129207, gnorm=0.685, train_wall=113, gb_free=7.9, wall=71382
2022-05-07 12:20:49 | INFO | train_inner | epoch 021:    900 / 2955 loss=4.718, ppl=26.33, wps=28603.4, ups=0.87, wpb=32768, bsz=64, num_updates=60000, lr=0.000129099, gnorm=0.685, train_wall=113, gb_free=7.9, wall=71497
2022-05-07 12:22:45 | INFO | train_inner | epoch 021:   1000 / 2955 loss=4.724, ppl=26.42, wps=28385.3, ups=0.87, wpb=32768, bsz=64, num_updates=60100, lr=0.000128992, gnorm=0.681, train_wall=114, gb_free=7.9, wall=71612
2022-05-07 12:24:45 | INFO | train_inner | epoch 021:   1100 / 2955 loss=4.723, ppl=26.41, wps=27275, ups=0.83, wpb=32768, bsz=64, num_updates=60200, lr=0.000128885, gnorm=0.684, train_wall=117, gb_free=7.9, wall=71732
2022-05-07 12:26:41 | INFO | train_inner | epoch 021:   1200 / 2955 loss=4.723, ppl=26.41, wps=28331.4, ups=0.86, wpb=32757.8, bsz=64, num_updates=60300, lr=0.000128778, gnorm=0.683, train_wall=113, gb_free=7.9, wall=71848
User defined signal 2
Sender: LSF System <lsfadmin@eu-lo-s4-039>
Subject: Job 218694701: <train_lang_standard_16_5.0000E-04_full> in cluster <euler> Exited

Job <train_lang_standard_16_5.0000E-04_full> was submitted from host <eu-login-07> by user <euler_username> in cluster <euler> at Mon May 16 17:38:31 2022
Job was executed on host(s) <4*eu-lo-s4-039>, in queue <gpu.24h>, as user <euler_username> in cluster <euler> at Mon May 16 17:39:04 2022
</cluster/home/euler_username> was used as the home directory.
</cluster/work/cotterell/liam/master-thesis> was used as the working directory.
Started at Mon May 16 17:39:04 2022
Terminated at Tue May 17 13:39:15 2022
Results reported at Tue May 17 13:39:15 2022

Your job looked like:

------------------------------------------------------------
# LSBATCH: User input
fairseq-train data/xsum-lang-full --save-dir checkpoints/language_model/standard --update-freq 16 --lr 0.0005 --checkpoint-suffix _standard_16_5.0000E-04_full --restore-file checkpoints/language_model/standard/checkpoint_best.pt --task language_modeling --arch transformer_lm --share-decoder-input-output-embed --dropout 0.1 --optimizer adam --adam-betas "(0.9, 0.98)" --weight-decay 0.01 --clip-norm 0.0 --lr-scheduler inverse_sqrt --warmup-updates 4000 --warmup-init-lr 1e-07 --tokens-per-sample 512 --sample-break-mode none --max-tokens 2048 --no-epoch-checkpoints --no-last-checkpoints --patience 5
------------------------------------------------------------

TERM_RUNLIMIT: job killed after reaching LSF run time limit.
Exited with exit code 140.

Resource usage summary:

    CPU time :                                   72516.00 sec.
    Max Memory :                                 4034 MB
    Average Memory :                             2108.48 MB
    Total Requested Memory :                     8192.00 MB
    Delta Memory :                               4158.00 MB
    Max Swap :                                   -
    Max Processes :                              4
    Max Threads :                                14
    Run time :                                   72013 sec.
    Turnaround time :                            72044 sec.

The output (if any) follows:

2022-05-16 17:41:30 | INFO | fairseq.tasks.text_to_speech | Please install tensorboardX: pip install tensorboardX
2022-05-16 17:41:49 | INFO | fairseq_cli.train | {'_name': None, 'common': {'_name': None, 'no_progress_bar': False, 'log_interval': 100, 'log_format': None, 'log_file': None, 'tensorboard_logdir': None, 'wandb_project': None, 'azureml_logging': False, 'seed': 1, 'cpu': False, 'tpu': False, 'bf16': False, 'memory_efficient_bf16': False, 'fp16': False, 'memory_efficient_fp16': False, 'fp16_no_flatten_grads': False, 'fp16_init_scale': 128, 'fp16_scale_window': None, 'fp16_scale_tolerance': 0.0, 'on_cpu_convert_precision': False, 'min_loss_scale': 0.0001, 'threshold_loss_scale': None, 'amp': False, 'amp_batch_retries': 2, 'amp_init_scale': 128, 'amp_scale_window': None, 'user_dir': None, 'empty_cache_freq': 0, 'all_gather_list_size': 16384, 'model_parallel_size': 1, 'quantization_config_path': None, 'profile': False, 'reset_logging': False, 'suppress_crashes': False, 'use_plasma_view': False, 'plasma_path': '/tmp/plasma'}, 'common_eval': {'_name': None, 'path': None, 'post_process': None, 'quiet': False, 'model_overrides': '{}', 'results_path': None, 'print_tokens': False}, 'distributed_training': {'_name': None, 'distributed_world_size': 1, 'distributed_num_procs': 1, 'distributed_rank': 0, 'distributed_backend': 'nccl', 'distributed_init_method': None, 'distributed_port': -1, 'device_id': 0, 'distributed_no_spawn': False, 'ddp_backend': 'pytorch_ddp', 'ddp_comm_hook': 'none', 'bucket_cap_mb': 25, 'fix_batches_to_gpus': False, 'find_unused_parameters': False, 'gradient_as_bucket_view': False, 'fast_stat_sync': False, 'heartbeat_timeout': -1, 'broadcast_buffers': False, 'slowmo_momentum': None, 'slowmo_base_algorithm': 'localsgd', 'localsgd_frequency': 3, 'nprocs_per_node': 1, 'pipeline_model_parallel': False, 'pipeline_balance': None, 'pipeline_devices': None, 'pipeline_chunks': 0, 'pipeline_encoder_balance': None, 'pipeline_encoder_devices': None, 'pipeline_decoder_balance': None, 'pipeline_decoder_devices': None, 'pipeline_checkpoint': 'never', 'zero_sharding': 'none', 'fp16': False, 'memory_efficient_fp16': False, 'tpu': False, 'no_reshard_after_forward': False, 'fp32_reduce_scatter': False, 'cpu_offload': False, 'use_sharded_state': False, 'not_fsdp_flatten_parameters': False}, 'dataset': {'_name': None, 'num_workers': 1, 'skip_invalid_size_inputs_valid_test': False, 'max_tokens': 2048, 'batch_size': None, 'required_batch_size_multiple': 8, 'required_seq_len_multiple': 1, 'dataset_impl': None, 'data_buffer_size': 10, 'train_subset': 'train', 'valid_subset': 'valid', 'combine_valid_subsets': None, 'ignore_unused_valid_subsets': False, 'validate_interval': 1, 'validate_interval_updates': 0, 'validate_after_updates': 0, 'fixed_validation_seed': None, 'disable_validation': False, 'max_tokens_valid': 2048, 'batch_size_valid': None, 'max_valid_steps': None, 'curriculum': 0, 'gen_subset': 'test', 'num_shards': 1, 'shard_id': 0, 'grouped_shuffling': False, 'update_epoch_batch_itr': False, 'update_ordered_indices_seed': False}, 'optimization': {'_name': None, 'max_epoch': 0, 'max_update': 0, 'stop_time_hours': 0.0, 'clip_norm': 0.0, 'sentence_avg': False, 'update_freq': [16], 'lr': [0.0005], 'stop_min_lr': -1.0, 'use_bmuf': False, 'skip_remainder_batch': False}, 'checkpoint': {'_name': None, 'save_dir': 'checkpoints/language_model/standard', 'restore_file': 'checkpoints/language_model/standard/checkpoint_best.pt', 'continue_once': None, 'finetune_from_model': None, 'reset_dataloader': False, 'reset_lr_scheduler': False, 'reset_meters': False, 'reset_optimizer': False, 'optimizer_overrides': '{}', 'save_interval': 1, 'save_interval_updates': 0, 'keep_interval_updates': -1, 'keep_interval_updates_pattern': -1, 'keep_last_epochs': -1, 'keep_best_checkpoints': -1, 'no_save': False, 'no_epoch_checkpoints': True, 'no_last_checkpoints': True, 'no_save_optimizer_state': False, 'best_checkpoint_metric': 'loss', 'maximize_best_checkpoint_metric': False, 'patience': 5, 'checkpoint_suffix': '_standard_16_5.0000E-04_full', 'checkpoint_shard_count': 1, 'load_checkpoint_on_all_dp_ranks': False, 'write_checkpoints_asynchronously': False, 'model_parallel_size': 1}, 'bmuf': {'_name': None, 'block_lr': 1.0, 'block_momentum': 0.875, 'global_sync_iter': 50, 'warmup_iterations': 500, 'use_nbm': False, 'average_sync': False, 'distributed_world_size': 1}, 'generation': {'_name': None, 'beam': 5, 'nbest': 1, 'max_len_a': 0.0, 'max_len_b': 200, 'min_len': 1, 'match_source_len': False, 'unnormalized': False, 'no_early_stop': False, 'no_beamable_mm': False, 'lenpen': 1.0, 'unkpen': 0.0, 'replace_unk': None, 'sacrebleu': False, 'score_reference': False, 'prefix_size': 0, 'no_repeat_ngram_size': 0, 'sampling': False, 'sampling_topk': -1, 'sampling_topp': -1.0, 'constraints': None, 'temperature': 1.0, 'diverse_beam_groups': -1, 'diverse_beam_strength': 0.5, 'diversity_rate': -1.0, 'print_alignment': None, 'print_step': False, 'lm_path': None, 'ent_threshold': 0.0, 'lm_weight': 0.0, 'iter_decode_eos_penalty': 0.0, 'iter_decode_max_iter': 10, 'iter_decode_force_max_iter': False, 'iter_decode_with_beam': 1, 'iter_decode_with_external_reranker': False, 'retain_iter_history': False, 'retain_dropout': False, 'retain_dropout_modules': None, 'decoding_format': None, 'no_seed_provided': False}, 'eval_lm': {'_name': None, 'output_word_probs': False, 'output_word_stats': False, 'context_window': 0, 'softmax_batch': 9223372036854775807}, 'interactive': {'_name': None, 'buffer_size': 0, 'input': '-'}, 'model': {'_name': 'transformer_lm', 'activation_fn': relu, 'dropout': 0.1, 'attention_dropout': 0.0, 'activation_dropout': 0.0, 'relu_dropout': 0.0, 'decoder_embed_dim': 512, 'decoder_output_dim': 512, 'decoder_input_dim': 512, 'decoder_ffn_embed_dim': 2048, 'decoder_layers': 6, 'decoder_attention_heads': 8, 'decoder_normalize_before': False, 'no_decoder_final_norm': False, 'adaptive_softmax_cutoff': None, 'adaptive_softmax_dropout': 0.0, 'adaptive_softmax_factor': 4.0, 'no_token_positional_embeddings': False, 'share_decoder_input_output_embed': True, 'character_embeddings': False, 'character_filters': '[(1, 64), (2, 128), (3, 192), (4, 256), (5, 256), (6, 256), (7, 256)]', 'character_embedding_dim': 4, 'char_embedder_highway_layers': 2, 'adaptive_input': False, 'adaptive_input_factor': 4.0, 'adaptive_input_cutoff': None, 'tie_adaptive_weights': False, 'tie_adaptive_proj': False, 'decoder_learned_pos': False, 'layernorm_embedding': False, 'no_scale_embedding': False, 'checkpoint_activations': False, 'offload_activations': False, 'decoder_layerdrop': 0.0, 'decoder_layers_to_keep': None, 'quant_noise_pq': 0.0, 'quant_noise_pq_block_size': 8, 'quant_noise_scalar': 0.0, 'min_params_to_wrap': 100000000, 'base_layers': 0, 'base_sublayers': 1, 'base_shuffle': 1, 'scale_fc': False, 'scale_attn': False, 'scale_heads': False, 'scale_resids': False, 'add_bos_token': False, 'tokens_per_sample': 512, 'max_target_positions': None, 'tpu': False}, 'task': {'_name': 'language_modeling', 'data': 'data/xsum-lang-full', 'sample_break_mode': none, 'tokens_per_sample': 512, 'output_dictionary_size': -1, 'self_target': False, 'future_target': False, 'past_target': False, 'add_bos_token': False, 'max_target_positions': None, 'shorten_method': none, 'shorten_data_split_list': '', 'pad_to_fixed_length': False, 'pad_to_fixed_bsz': False, 'seed': 1, 'batch_size': None, 'batch_size_valid': None, 'dataset_impl': None, 'data_buffer_size': 10, 'tpu': False, 'use_plasma_view': False, 'plasma_path': '/tmp/plasma'}, 'criterion': {'_name': 'cross_entropy', 'sentence_avg': False}, 'optimizer': {'_name': 'adam', 'adam_betas': '(0.9, 0.98)', 'adam_eps': 1e-08, 'weight_decay': 0.01, 'use_old_adam': False, 'fp16_adam_stats': False, 'tpu': False, 'lr': [0.0005]}, 'lr_scheduler': {'_name': 'inverse_sqrt', 'warmup_updates': 4000, 'warmup_init_lr': 1e-07, 'lr': [0.0005]}, 'scoring': {'_name': 'bleu', 'pad': 1, 'eos': 2, 'unk': 3}, 'bpe': None, 'tokenizer': None, 'ema': {'_name': None, 'store_ema': False, 'ema_decay': 0.9999, 'ema_start_update': 0, 'ema_seed_model': None, 'ema_update_freq': 1, 'ema_fp32': False}, 'simul_type': None}
2022-05-16 17:41:49 | INFO | fairseq.tasks.language_modeling | dictionary: 49992 types
2022-05-16 17:41:54 | INFO | fairseq_cli.train | TransformerLanguageModel(
  (decoder): TransformerDecoder(
    (dropout_module): FairseqDropout()
    (embed_tokens): Embedding(49992, 512, padding_idx=1)
    (embed_positions): SinusoidalPositionalEmbedding()
    (layers): ModuleList(
      (0): TransformerDecoderLayerBase(
        (dropout_module): FairseqDropout()
        (self_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=512, out_features=512, bias=True)
          (v_proj): Linear(in_features=512, out_features=512, bias=True)
          (q_proj): Linear(in_features=512, out_features=512, bias=True)
          (out_proj): Linear(in_features=512, out_features=512, bias=True)
        )
        (activation_dropout_module): FairseqDropout()
        (self_attn_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
        (fc1): Linear(in_features=512, out_features=2048, bias=True)
        (fc2): Linear(in_features=2048, out_features=512, bias=True)
        (final_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
      )
      (1): TransformerDecoderLayerBase(
        (dropout_module): FairseqDropout()
        (self_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=512, out_features=512, bias=True)
          (v_proj): Linear(in_features=512, out_features=512, bias=True)
          (q_proj): Linear(in_features=512, out_features=512, bias=True)
          (out_proj): Linear(in_features=512, out_features=512, bias=True)
        )
        (activation_dropout_module): FairseqDropout()
        (self_attn_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
        (fc1): Linear(in_features=512, out_features=2048, bias=True)
        (fc2): Linear(in_features=2048, out_features=512, bias=True)
        (final_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
      )
      (2): TransformerDecoderLayerBase(
        (dropout_module): FairseqDropout()
        (self_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=512, out_features=512, bias=True)
          (v_proj): Linear(in_features=512, out_features=512, bias=True)
          (q_proj): Linear(in_features=512, out_features=512, bias=True)
          (out_proj): Linear(in_features=512, out_features=512, bias=True)
        )
        (activation_dropout_module): FairseqDropout()
        (self_attn_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
        (fc1): Linear(in_features=512, out_features=2048, bias=True)
        (fc2): Linear(in_features=2048, out_features=512, bias=True)
        (final_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
      )
      (3): TransformerDecoderLayerBase(
        (dropout_module): FairseqDropout()
        (self_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=512, out_features=512, bias=True)
          (v_proj): Linear(in_features=512, out_features=512, bias=True)
          (q_proj): Linear(in_features=512, out_features=512, bias=True)
          (out_proj): Linear(in_features=512, out_features=512, bias=True)
        )
        (activation_dropout_module): FairseqDropout()
        (self_attn_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
        (fc1): Linear(in_features=512, out_features=2048, bias=True)
        (fc2): Linear(in_features=2048, out_features=512, bias=True)
        (final_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
      )
      (4): TransformerDecoderLayerBase(
        (dropout_module): FairseqDropout()
        (self_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=512, out_features=512, bias=True)
          (v_proj): Linear(in_features=512, out_features=512, bias=True)
          (q_proj): Linear(in_features=512, out_features=512, bias=True)
          (out_proj): Linear(in_features=512, out_features=512, bias=True)
        )
        (activation_dropout_module): FairseqDropout()
        (self_attn_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
        (fc1): Linear(in_features=512, out_features=2048, bias=True)
        (fc2): Linear(in_features=2048, out_features=512, bias=True)
        (final_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
      )
      (5): TransformerDecoderLayerBase(
        (dropout_module): FairseqDropout()
        (self_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=512, out_features=512, bias=True)
          (v_proj): Linear(in_features=512, out_features=512, bias=True)
          (q_proj): Linear(in_features=512, out_features=512, bias=True)
          (out_proj): Linear(in_features=512, out_features=512, bias=True)
        )
        (activation_dropout_module): FairseqDropout()
        (self_attn_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
        (fc1): Linear(in_features=512, out_features=2048, bias=True)
        (fc2): Linear(in_features=2048, out_features=512, bias=True)
        (final_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
      )
    )
    (output_projection): Linear(in_features=512, out_features=49992, bias=False)
  )
)
2022-05-16 17:41:54 | INFO | fairseq_cli.train | task: LanguageModelingTask
2022-05-16 17:41:54 | INFO | fairseq_cli.train | model: TransformerLanguageModel
2022-05-16 17:41:54 | INFO | fairseq_cli.train | criterion: CrossEntropyCriterion
2022-05-16 17:41:54 | INFO | fairseq_cli.train | num. shared model params: 44,510,208 (num. trained: 44,510,208)
2022-05-16 17:41:54 | INFO | fairseq_cli.train | num. expert model params: 0 (num. trained: 0)
2022-05-16 17:41:54 | INFO | fairseq.data.data_utils | loaded 22,664 examples from: data/xsum-lang-full/valid
2022-05-16 17:42:33 | INFO | fairseq.trainer | detected shared parameter: decoder.embed_tokens.weight <- decoder.output_projection.weight
2022-05-16 17:42:33 | INFO | fairseq.utils | ***********************CUDA enviroments for all 1 workers***********************
2022-05-16 17:42:33 | INFO | fairseq.utils | rank   0: capabilities =  6.1  ; total memory = 7.929 GB ; name = NVIDIA GeForce GTX 1080                 
2022-05-16 17:42:33 | INFO | fairseq.utils | ***********************CUDA enviroments for all 1 workers***********************
2022-05-16 17:42:33 | INFO | fairseq_cli.train | training on 1 devices (GPUs/TPUs)
2022-05-16 17:42:33 | INFO | fairseq_cli.train | max tokens per device = 2048 and max sentences per device = None
2022-05-16 17:42:33 | INFO | fairseq.trainer | Preparing to load checkpoint checkpoints/language_model/standard/checkpoint_best_standard_16_5.0000E-04_full.pt
2022-05-16 17:42:39 | INFO | fairseq.trainer | Loaded checkpoint checkpoints/language_model/standard/checkpoint_best_standard_16_5.0000E-04_full.pt (epoch 27 @ 76830 updates)
2022-05-16 17:42:39 | INFO | fairseq.trainer | loading train data for epoch 27
2022-05-16 17:42:40 | INFO | fairseq.data.data_utils | loaded 408,090 examples from: data/xsum-lang-full/train
2022-05-16 17:42:41 | INFO | fairseq.data.iterators | grouped total_num_itrs = 2955
2022-05-16 17:42:41 | INFO | fairseq.trainer | begin training epoch 27
2022-05-16 17:42:41 | INFO | fairseq_cli.train | Start iterating over samples
2022-05-16 17:45:24 | INFO | train_inner | epoch 027:     70 / 2955 loss=4.648, ppl=25.08, wps=14556.9, ups=0.44, wpb=32768, bsz=64, num_updates=76900, lr=0.000114035, gnorm=0.714, train_wall=160, gb_free=5.1, wall=171
2022-05-16 17:49:10 | INFO | train_inner | epoch 027:    170 / 2955 loss=4.658, ppl=25.25, wps=14480.2, ups=0.44, wpb=32768, bsz=64, num_updates=77000, lr=0.000113961, gnorm=0.719, train_wall=224, gb_free=5.1, wall=397
2022-05-16 17:53:02 | INFO | train_inner | epoch 027:    270 / 2955 loss=4.675, ppl=25.54, wps=14126.4, ups=0.43, wpb=32757.8, bsz=64, num_updates=77100, lr=0.000113887, gnorm=0.717, train_wall=226, gb_free=5.1, wall=629
2022-05-16 17:57:00 | INFO | train_inner | epoch 027:    370 / 2955 loss=4.677, ppl=25.58, wps=13801.7, ups=0.42, wpb=32768, bsz=64, num_updates=77200, lr=0.000113813, gnorm=0.718, train_wall=228, gb_free=5.1, wall=866
2022-05-16 18:00:54 | INFO | train_inner | epoch 027:    470 / 2955 loss=4.662, ppl=25.31, wps=13999.2, ups=0.43, wpb=32768, bsz=64, num_updates=77300, lr=0.000113739, gnorm=0.713, train_wall=226, gb_free=5.1, wall=1100
2022-05-16 18:04:48 | INFO | train_inner | epoch 027:    570 / 2955 loss=4.662, ppl=25.32, wps=13967.6, ups=0.43, wpb=32768, bsz=64, num_updates=77400, lr=0.000113666, gnorm=0.721, train_wall=224, gb_free=5.1, wall=1335
2022-05-16 18:08:51 | INFO | train_inner | epoch 027:    670 / 2955 loss=4.664, ppl=25.35, wps=13495.3, ups=0.41, wpb=32768, bsz=64, num_updates=77500, lr=0.000113592, gnorm=0.718, train_wall=231, gb_free=5.1, wall=1578
2022-05-16 18:12:52 | INFO | train_inner | epoch 027:    770 / 2955 loss=4.676, ppl=25.56, wps=13581.3, ups=0.41, wpb=32768, bsz=64, num_updates=77600, lr=0.000113519, gnorm=0.715, train_wall=231, gb_free=5.1, wall=1819
2022-05-16 18:16:46 | INFO | train_inner | epoch 027:    870 / 2955 loss=4.678, ppl=25.59, wps=14052.7, ups=0.43, wpb=32768, bsz=64, num_updates=77700, lr=0.000113446, gnorm=0.716, train_wall=225, gb_free=5.1, wall=2052
2022-05-16 18:20:37 | INFO | train_inner | epoch 027:    970 / 2955 loss=4.681, ppl=25.65, wps=14136, ups=0.43, wpb=32768, bsz=64, num_updates=77800, lr=0.000113373, gnorm=0.719, train_wall=225, gb_free=5.1, wall=2284
2022-05-16 18:24:37 | INFO | train_inner | epoch 027:   1070 / 2955 loss=4.679, ppl=25.61, wps=13698.9, ups=0.42, wpb=32768, bsz=64, num_updates=77900, lr=0.0001133, gnorm=0.72, train_wall=233, gb_free=5.1, wall=2523
2022-05-16 18:28:34 | INFO | train_inner | epoch 027:   1170 / 2955 loss=4.689, ppl=25.79, wps=13811.6, ups=0.42, wpb=32768, bsz=64, num_updates=78000, lr=0.000113228, gnorm=0.72, train_wall=227, gb_free=5.1, wall=2761
2022-05-16 18:32:28 | INFO | train_inner | epoch 027:   1270 / 2955 loss=4.681, ppl=25.65, wps=14024, ups=0.43, wpb=32768, bsz=64, num_updates=78100, lr=0.000113155, gnorm=0.719, train_wall=225, gb_free=5.1, wall=2994
2022-05-16 18:36:21 | INFO | train_inner | epoch 027:   1370 / 2955 loss=4.688, ppl=25.77, wps=14058.8, ups=0.43, wpb=32768, bsz=64, num_updates=78200, lr=0.000113083, gnorm=0.718, train_wall=225, gb_free=5.1, wall=3227
2022-05-16 18:40:17 | INFO | train_inner | epoch 027:   1470 / 2955 loss=4.681, ppl=25.66, wps=13889.2, ups=0.42, wpb=32768, bsz=64, num_updates=78300, lr=0.000113011, gnorm=0.719, train_wall=229, gb_free=5.1, wall=3463
2022-05-16 18:44:13 | INFO | train_inner | epoch 027:   1570 / 2955 loss=4.676, ppl=25.57, wps=13878.7, ups=0.42, wpb=32768, bsz=64, num_updates=78400, lr=0.000112938, gnorm=0.722, train_wall=229, gb_free=5.1, wall=3699
2022-05-16 18:48:09 | INFO | train_inner | epoch 027:   1670 / 2955 loss=4.683, ppl=25.69, wps=13863.2, ups=0.42, wpb=32768, bsz=64, num_updates=78500, lr=0.000112867, gnorm=0.723, train_wall=226, gb_free=5.1, wall=3936
2022-05-16 18:52:02 | INFO | train_inner | epoch 027:   1770 / 2955 loss=4.687, ppl=25.76, wps=14036.7, ups=0.43, wpb=32768, bsz=64, num_updates=78600, lr=0.000112795, gnorm=0.719, train_wall=225, gb_free=5.1, wall=4169
2022-05-16 18:56:01 | INFO | train_inner | epoch 027:   1870 / 2955 loss=4.682, ppl=25.67, wps=13725.3, ups=0.42, wpb=32768, bsz=64, num_updates=78700, lr=0.000112723, gnorm=0.717, train_wall=230, gb_free=5.1, wall=4408
2022-05-16 18:59:55 | INFO | train_inner | epoch 027:   1970 / 2955 loss=4.687, ppl=25.77, wps=14010.4, ups=0.43, wpb=32768, bsz=64, num_updates=78800, lr=0.000112651, gnorm=0.717, train_wall=227, gb_free=5.1, wall=4642
2022-05-16 19:03:46 | INFO | train_inner | epoch 027:   2070 / 2955 loss=4.699, ppl=25.97, wps=14205.1, ups=0.43, wpb=32768, bsz=64, num_updates=78900, lr=0.00011258, gnorm=0.724, train_wall=225, gb_free=5.1, wall=4872
2022-05-16 19:07:40 | INFO | train_inner | epoch 027:   2170 / 2955 loss=4.706, ppl=26.09, wps=14007.2, ups=0.43, wpb=32768, bsz=64, num_updates=79000, lr=0.000112509, gnorm=0.722, train_wall=228, gb_free=5.1, wall=5106
2022-05-16 19:11:32 | INFO | train_inner | epoch 027:   2270 / 2955 loss=4.695, ppl=25.9, wps=14102.2, ups=0.43, wpb=32768, bsz=64, num_updates=79100, lr=0.000112438, gnorm=0.716, train_wall=228, gb_free=5.1, wall=5339
2022-05-16 19:15:24 | INFO | train_inner | epoch 027:   2370 / 2955 loss=4.724, ppl=26.43, wps=14119.7, ups=0.43, wpb=32768, bsz=64, num_updates=79200, lr=0.000112367, gnorm=0.723, train_wall=228, gb_free=5.1, wall=5571
2022-05-16 19:19:13 | INFO | train_inner | epoch 027:   2470 / 2955 loss=4.726, ppl=26.46, wps=14289.1, ups=0.44, wpb=32763, bsz=64, num_updates=79300, lr=0.000112296, gnorm=0.722, train_wall=226, gb_free=5.1, wall=5800
2022-05-16 19:23:04 | INFO | train_inner | epoch 027:   2570 / 2955 loss=4.689, ppl=25.8, wps=14230.9, ups=0.43, wpb=32768, bsz=64, num_updates=79400, lr=0.000112225, gnorm=0.722, train_wall=227, gb_free=5.1, wall=6030
2022-05-16 19:26:53 | INFO | train_inner | epoch 027:   2670 / 2955 loss=4.713, ppl=26.22, wps=14280.6, ups=0.44, wpb=32768, bsz=64, num_updates=79500, lr=0.000112154, gnorm=0.723, train_wall=226, gb_free=5.1, wall=6260
2022-05-16 19:30:46 | INFO | train_inner | epoch 027:   2770 / 2955 loss=4.702, ppl=26.02, wps=14088.7, ups=0.43, wpb=32768, bsz=64, num_updates=79600, lr=0.000112084, gnorm=0.721, train_wall=229, gb_free=5.1, wall=6492
2022-05-16 19:34:38 | INFO | train_inner | epoch 027:   2870 / 2955 loss=4.708, ppl=26.14, wps=14092.2, ups=0.43, wpb=32768, bsz=64, num_updates=79700, lr=0.000112014, gnorm=0.721, train_wall=226, gb_free=5.1, wall=6725
2022-05-16 19:37:53 | INFO | fairseq_cli.train | begin validation on "valid" subset
/cluster/work/cotterell/liam/fairseq/fairseq/utils.py:374: UserWarning: amp_C fused kernels unavailable, disabling multi_tensor_l2norm; you may get better performance by installing NVIDIA's apex library
  warnings.warn(
2022-05-16 19:40:13 | INFO | valid | epoch 027 | valid on 'valid' subset | loss 4.768 | ppl 27.24 | wps 38510.4 | wpb 2047.4 | bsz 4 | num_updates 79785 | best_loss 4.768
2022-05-16 19:40:13 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 27 @ 79785 updates
2022-05-16 19:40:13 | INFO | fairseq.trainer | Saving checkpoint to /cluster/work/cotterell/liam/master-thesis/checkpoints/language_model/standard/checkpoint_best_standard_16_5.0000E-04_full.pt
2022-05-16 19:40:16 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/work/cotterell/liam/master-thesis/checkpoints/language_model/standard/checkpoint_best_standard_16_5.0000E-04_full.pt
2022-05-16 19:40:16 | INFO | fairseq.checkpoint_utils | Saved checkpoint checkpoints/language_model/standard/checkpoint_best_standard_16_5.0000E-04_full.pt (epoch 27 @ 79785 updates, score 4.768) (writing took 2.9642065507359803 seconds)
2022-05-16 19:40:16 | INFO | fairseq_cli.train | end of epoch 27 (average epoch stats below)
2022-05-16 19:40:16 | INFO | train | epoch 027 | loss 4.686 | ppl 25.75 | wps 13733.6 | ups 0.42 | wpb 32764.7 | bsz 64 | num_updates 79785 | lr 0.000111954 | gnorm 0.719 | train_wall 6711 | gb_free 5.1 | wall 7063
2022-05-16 19:40:16 | INFO | fairseq.data.iterators | grouped total_num_itrs = 2955
2022-05-16 19:40:16 | INFO | fairseq.trainer | begin training epoch 28
2022-05-16 19:40:16 | INFO | fairseq_cli.train | Start iterating over samples
2022-05-16 19:40:51 | INFO | train_inner | epoch 028:     15 / 2955 loss=4.706, ppl=26.1, wps=8770.3, ups=0.27, wpb=32686.1, bsz=63.8, num_updates=79800, lr=0.000111943, gnorm=0.723, train_wall=226, gb_free=5.1, wall=7098
2022-05-16 19:44:40 | INFO | train_inner | epoch 028:    115 / 2955 loss=4.643, ppl=24.98, wps=14324.4, ups=0.44, wpb=32757.8, bsz=64, num_updates=79900, lr=0.000111873, gnorm=0.718, train_wall=226, gb_free=5.1, wall=7326
2022-05-16 19:48:30 | INFO | train_inner | epoch 028:    215 / 2955 loss=4.639, ppl=24.91, wps=14250, ups=0.43, wpb=32768, bsz=64, num_updates=80000, lr=0.000111803, gnorm=0.716, train_wall=226, gb_free=5.1, wall=7556
2022-05-16 19:52:26 | INFO | train_inner | epoch 028:    315 / 2955 loss=4.659, ppl=25.26, wps=13887.5, ups=0.42, wpb=32768, bsz=64, num_updates=80100, lr=0.000111734, gnorm=0.723, train_wall=233, gb_free=5.1, wall=7792
2022-05-16 19:56:14 | INFO | train_inner | epoch 028:    415 / 2955 loss=4.665, ppl=25.37, wps=14328.8, ups=0.44, wpb=32768, bsz=64, num_updates=80200, lr=0.000111664, gnorm=0.721, train_wall=226, gb_free=5.1, wall=8021
2022-05-16 20:00:05 | INFO | train_inner | epoch 028:    515 / 2955 loss=4.659, ppl=25.26, wps=14178.2, ups=0.43, wpb=32768, bsz=64, num_updates=80300, lr=0.000111594, gnorm=0.725, train_wall=228, gb_free=5.1, wall=8252
2022-05-16 20:04:01 | INFO | train_inner | epoch 028:    615 / 2955 loss=4.681, ppl=25.65, wps=13879.7, ups=0.42, wpb=32768, bsz=64, num_updates=80400, lr=0.000111525, gnorm=0.729, train_wall=232, gb_free=5.1, wall=8488
2022-05-16 20:07:51 | INFO | train_inner | epoch 028:    715 / 2955 loss=4.688, ppl=25.78, wps=14301, ups=0.44, wpb=32768, bsz=64, num_updates=80500, lr=0.000111456, gnorm=0.73, train_wall=226, gb_free=5.1, wall=8717
2022-05-16 20:11:43 | INFO | train_inner | epoch 028:    815 / 2955 loss=4.671, ppl=25.48, wps=14072.9, ups=0.43, wpb=32768, bsz=64, num_updates=80600, lr=0.000111386, gnorm=0.722, train_wall=230, gb_free=5.1, wall=8950
2022-05-16 20:15:33 | INFO | train_inner | epoch 028:    915 / 2955 loss=4.678, ppl=25.6, wps=14273.3, ups=0.44, wpb=32768, bsz=64, num_updates=80700, lr=0.000111317, gnorm=0.725, train_wall=227, gb_free=5.1, wall=9180
2022-05-16 20:19:24 | INFO | train_inner | epoch 028:   1015 / 2955 loss=4.676, ppl=25.56, wps=14183, ups=0.43, wpb=32768, bsz=64, num_updates=80800, lr=0.000111249, gnorm=0.731, train_wall=226, gb_free=5.1, wall=9411
2022-05-16 20:23:15 | INFO | train_inner | epoch 028:   1115 / 2955 loss=4.662, ppl=25.33, wps=14164.7, ups=0.43, wpb=32768, bsz=64, num_updates=80900, lr=0.00011118, gnorm=0.726, train_wall=227, gb_free=5.1, wall=9642
2022-05-16 20:27:08 | INFO | train_inner | epoch 028:   1215 / 2955 loss=4.667, ppl=25.4, wps=14069.7, ups=0.43, wpb=32768, bsz=64, num_updates=81000, lr=0.000111111, gnorm=0.727, train_wall=229, gb_free=5.1, wall=9875
2022-05-16 20:30:59 | INFO | train_inner | epoch 028:   1315 / 2955 loss=4.671, ppl=25.47, wps=14200.7, ups=0.43, wpb=32768, bsz=64, num_updates=81100, lr=0.000111043, gnorm=0.723, train_wall=227, gb_free=5.1, wall=10106
2022-05-16 20:34:49 | INFO | train_inner | epoch 028:   1415 / 2955 loss=4.673, ppl=25.51, wps=14247.7, ups=0.43, wpb=32768, bsz=64, num_updates=81200, lr=0.000110974, gnorm=0.72, train_wall=226, gb_free=5.1, wall=10336
2022-05-16 20:38:43 | INFO | train_inner | epoch 028:   1515 / 2955 loss=4.688, ppl=25.77, wps=14015.5, ups=0.43, wpb=32768, bsz=64, num_updates=81300, lr=0.000110906, gnorm=0.726, train_wall=229, gb_free=5.1, wall=10570
2022-05-16 20:42:34 | INFO | train_inner | epoch 028:   1615 / 2955 loss=4.699, ppl=25.97, wps=14197.7, ups=0.43, wpb=32768, bsz=64, num_updates=81400, lr=0.000110838, gnorm=0.724, train_wall=228, gb_free=5.1, wall=10800
2022-05-16 20:46:23 | INFO | train_inner | epoch 028:   1715 / 2955 loss=4.688, ppl=25.78, wps=14299.3, ups=0.44, wpb=32768, bsz=64, num_updates=81500, lr=0.00011077, gnorm=0.727, train_wall=227, gb_free=5.1, wall=11030
2022-05-16 20:50:17 | INFO | train_inner | epoch 028:   1815 / 2955 loss=4.686, ppl=25.74, wps=13983.3, ups=0.43, wpb=32768, bsz=64, num_updates=81600, lr=0.000110702, gnorm=0.727, train_wall=231, gb_free=5.1, wall=11264
2022-05-16 20:54:09 | INFO | train_inner | epoch 028:   1915 / 2955 loss=4.683, ppl=25.69, wps=14126.2, ups=0.43, wpb=32768, bsz=64, num_updates=81700, lr=0.000110634, gnorm=0.724, train_wall=229, gb_free=5.1, wall=11496
2022-05-16 20:58:00 | INFO | train_inner | epoch 028:   2015 / 2955 loss=4.689, ppl=25.79, wps=14186.5, ups=0.43, wpb=32763, bsz=64, num_updates=81800, lr=0.000110566, gnorm=0.727, train_wall=226, gb_free=5.1, wall=11727
2022-05-16 21:01:51 | INFO | train_inner | epoch 028:   2115 / 2955 loss=4.708, ppl=26.14, wps=14217.1, ups=0.43, wpb=32768, bsz=64, num_updates=81900, lr=0.000110499, gnorm=0.723, train_wall=227, gb_free=5.1, wall=11957
2022-05-16 21:05:40 | INFO | train_inner | epoch 028:   2215 / 2955 loss=4.711, ppl=26.19, wps=14288.9, ups=0.44, wpb=32768, bsz=64, num_updates=82000, lr=0.000110432, gnorm=0.727, train_wall=226, gb_free=5.1, wall=12187
2022-05-16 21:09:33 | INFO | train_inner | epoch 028:   2315 / 2955 loss=4.696, ppl=25.92, wps=14041, ups=0.43, wpb=32768, bsz=64, num_updates=82100, lr=0.000110364, gnorm=0.725, train_wall=230, gb_free=5.1, wall=12420
2022-05-16 21:13:23 | INFO | train_inner | epoch 028:   2415 / 2955 loss=4.676, ppl=25.56, wps=14235.4, ups=0.43, wpb=32768, bsz=64, num_updates=82200, lr=0.000110297, gnorm=0.724, train_wall=228, gb_free=5.1, wall=12650
2022-05-16 21:17:16 | INFO | train_inner | epoch 028:   2515 / 2955 loss=4.697, ppl=25.94, wps=14118.8, ups=0.43, wpb=32768, bsz=64, num_updates=82300, lr=0.00011023, gnorm=0.729, train_wall=229, gb_free=5.1, wall=12882
2022-05-16 21:21:08 | INFO | train_inner | epoch 028:   2615 / 2955 loss=4.695, ppl=25.9, wps=14090.9, ups=0.43, wpb=32768, bsz=64, num_updates=82400, lr=0.000110163, gnorm=0.726, train_wall=228, gb_free=5.1, wall=13115
2022-05-16 21:24:57 | INFO | train_inner | epoch 028:   2715 / 2955 loss=4.699, ppl=25.97, wps=14313, ups=0.44, wpb=32768, bsz=64, num_updates=82500, lr=0.000110096, gnorm=0.73, train_wall=226, gb_free=5.1, wall=13344
2022-05-16 21:28:48 | INFO | train_inner | epoch 028:   2815 / 2955 loss=4.699, ppl=25.97, wps=14162.7, ups=0.43, wpb=32768, bsz=64, num_updates=82600, lr=0.00011003, gnorm=0.724, train_wall=228, gb_free=5.1, wall=13575
2022-05-16 21:32:40 | INFO | train_inner | epoch 028:   2915 / 2955 loss=4.694, ppl=25.88, wps=14161.3, ups=0.43, wpb=32768, bsz=64, num_updates=82700, lr=0.000109963, gnorm=0.726, train_wall=229, gb_free=5.1, wall=13807
2022-05-16 21:34:10 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-05-16 21:36:25 | INFO | valid | epoch 028 | valid on 'valid' subset | loss 4.765 | ppl 27.19 | wps 39939.2 | wpb 2047.4 | bsz 4 | num_updates 82740 | best_loss 4.765
2022-05-16 21:36:25 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 28 @ 82740 updates
2022-05-16 21:36:25 | INFO | fairseq.trainer | Saving checkpoint to /cluster/work/cotterell/liam/master-thesis/checkpoints/language_model/standard/checkpoint_best_standard_16_5.0000E-04_full.pt
2022-05-16 21:36:27 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/work/cotterell/liam/master-thesis/checkpoints/language_model/standard/checkpoint_best_standard_16_5.0000E-04_full.pt
2022-05-16 21:36:27 | INFO | fairseq.checkpoint_utils | Saved checkpoint checkpoints/language_model/standard/checkpoint_best_standard_16_5.0000E-04_full.pt (epoch 28 @ 82740 updates, score 4.765) (writing took 2.286776766180992 seconds)
2022-05-16 21:36:27 | INFO | fairseq_cli.train | end of epoch 28 (average epoch stats below)
2022-05-16 21:36:27 | INFO | train | epoch 028 | loss 4.681 | ppl 25.65 | wps 13888.6 | ups 0.42 | wpb 32764.7 | bsz 64 | num_updates 82740 | lr 0.000109937 | gnorm 0.725 | train_wall 6734 | gb_free 5.1 | wall 14034
2022-05-16 21:36:28 | INFO | fairseq.data.iterators | grouped total_num_itrs = 2955
2022-05-16 21:36:28 | INFO | fairseq.trainer | begin training epoch 29
2022-05-16 21:36:28 | INFO | fairseq_cli.train | Start iterating over samples
2022-05-16 21:38:44 | INFO | train_inner | epoch 029:     60 / 2955 loss=4.664, ppl=25.35, wps=8964.6, ups=0.27, wpb=32686.1, bsz=63.8, num_updates=82800, lr=0.000109897, gnorm=0.734, train_wall=225, gb_free=5.1, wall=14171
2022-05-16 21:42:33 | INFO | train_inner | epoch 029:    160 / 2955 loss=4.655, ppl=25.19, wps=14327.6, ups=0.44, wpb=32768, bsz=64, num_updates=82900, lr=0.00010983, gnorm=0.727, train_wall=226, gb_free=5.1, wall=14400
2022-05-16 21:46:23 | INFO | train_inner | epoch 029:    260 / 2955 loss=4.631, ppl=24.77, wps=14278.9, ups=0.44, wpb=32768, bsz=64, num_updates=83000, lr=0.000109764, gnorm=0.725, train_wall=225, gb_free=5.1, wall=14629
2022-05-16 21:50:14 | INFO | train_inner | epoch 029:    360 / 2955 loss=4.647, ppl=25.06, wps=14165.9, ups=0.43, wpb=32768, bsz=64, num_updates=83100, lr=0.000109698, gnorm=0.726, train_wall=228, gb_free=5.1, wall=14861
2022-05-16 21:54:03 | INFO | train_inner | epoch 029:    460 / 2955 loss=4.653, ppl=25.17, wps=14302.8, ups=0.44, wpb=32768, bsz=64, num_updates=83200, lr=0.000109632, gnorm=0.731, train_wall=227, gb_free=5.1, wall=15090
2022-05-16 21:57:55 | INFO | train_inner | epoch 029:    560 / 2955 loss=4.677, ppl=25.58, wps=14156.6, ups=0.43, wpb=32768, bsz=64, num_updates=83300, lr=0.000109566, gnorm=0.729, train_wall=228, gb_free=5.1, wall=15321
2022-05-16 22:01:44 | INFO | train_inner | epoch 029:    660 / 2955 loss=4.674, ppl=25.53, wps=14297.3, ups=0.44, wpb=32768, bsz=64, num_updates=83400, lr=0.000109501, gnorm=0.728, train_wall=227, gb_free=5.1, wall=15550
2022-05-16 22:05:32 | INFO | train_inner | epoch 029:    760 / 2955 loss=4.652, ppl=25.14, wps=14332.9, ups=0.44, wpb=32768, bsz=64, num_updates=83500, lr=0.000109435, gnorm=0.73, train_wall=226, gb_free=5.1, wall=15779
2022-05-16 22:09:25 | INFO | train_inner | epoch 029:    860 / 2955 loss=4.655, ppl=25.19, wps=14103.2, ups=0.43, wpb=32768, bsz=64, num_updates=83600, lr=0.00010937, gnorm=0.727, train_wall=229, gb_free=5.1, wall=16011
2022-05-16 22:13:14 | INFO | train_inner | epoch 029:    960 / 2955 loss=4.667, ppl=25.41, wps=14304.7, ups=0.44, wpb=32768, bsz=64, num_updates=83700, lr=0.000109304, gnorm=0.728, train_wall=226, gb_free=5.1, wall=16240
2022-05-16 22:17:03 | INFO | train_inner | epoch 029:   1060 / 2955 loss=4.673, ppl=25.51, wps=14325.9, ups=0.44, wpb=32768, bsz=64, num_updates=83800, lr=0.000109239, gnorm=0.734, train_wall=225, gb_free=5.1, wall=16469
2022-05-16 22:20:58 | INFO | train_inner | epoch 029:   1160 / 2955 loss=4.667, ppl=25.41, wps=13912.8, ups=0.42, wpb=32768, bsz=64, num_updates=83900, lr=0.000109174, gnorm=0.724, train_wall=233, gb_free=5.1, wall=16705
2022-05-16 22:24:47 | INFO | train_inner | epoch 029:   1260 / 2955 loss=4.673, ppl=25.51, wps=14322, ups=0.44, wpb=32768, bsz=64, num_updates=84000, lr=0.000109109, gnorm=0.732, train_wall=227, gb_free=5.1, wall=16934
2022-05-16 22:28:45 | INFO | train_inner | epoch 029:   1360 / 2955 loss=4.667, ppl=25.4, wps=13750.3, ups=0.42, wpb=32768, bsz=64, num_updates=84100, lr=0.000109044, gnorm=0.729, train_wall=231, gb_free=5.1, wall=17172
2022-05-16 22:32:35 | INFO | train_inner | epoch 029:   1460 / 2955 loss=4.671, ppl=25.47, wps=14257.8, ups=0.44, wpb=32768, bsz=64, num_updates=84200, lr=0.000108979, gnorm=0.723, train_wall=227, gb_free=5.1, wall=17402
2022-05-16 22:36:23 | INFO | train_inner | epoch 029:   1560 / 2955 loss=4.692, ppl=25.86, wps=14373.2, ups=0.44, wpb=32768, bsz=64, num_updates=84300, lr=0.000108915, gnorm=0.729, train_wall=226, gb_free=5.1, wall=17630
2022-05-16 22:40:18 | INFO | train_inner | epoch 029:   1660 / 2955 loss=4.682, ppl=25.67, wps=13941.7, ups=0.43, wpb=32768, bsz=64, num_updates=84400, lr=0.00010885, gnorm=0.73, train_wall=231, gb_free=5.1, wall=17865
2022-05-16 22:44:09 | INFO | train_inner | epoch 029:   1760 / 2955 loss=4.695, ppl=25.9, wps=14186.7, ups=0.43, wpb=32768, bsz=64, num_updates=84500, lr=0.000108786, gnorm=0.736, train_wall=229, gb_free=5.1, wall=18096
2022-05-16 22:48:12 | INFO | train_inner | epoch 029:   1860 / 2955 loss=4.695, ppl=25.9, wps=13484.2, ups=0.41, wpb=32768, bsz=64, num_updates=84600, lr=0.000108721, gnorm=0.73, train_wall=232, gb_free=5.1, wall=18339
2022-05-16 22:52:10 | INFO | train_inner | epoch 029:   1960 / 2955 loss=4.675, ppl=25.54, wps=13738.1, ups=0.42, wpb=32757.8, bsz=64, num_updates=84700, lr=0.000108657, gnorm=0.732, train_wall=229, gb_free=5.1, wall=18577
2022-05-16 22:56:04 | INFO | train_inner | epoch 029:   2060 / 2955 loss=4.699, ppl=25.97, wps=14040.5, ups=0.43, wpb=32763, bsz=64, num_updates=84800, lr=0.000108593, gnorm=0.733, train_wall=225, gb_free=5.1, wall=18810
2022-05-16 22:59:57 | INFO | train_inner | epoch 029:   2160 / 2955 loss=4.682, ppl=25.67, wps=14030.7, ups=0.43, wpb=32768, bsz=64, num_updates=84900, lr=0.000108529, gnorm=0.728, train_wall=225, gb_free=5.1, wall=19044
2022-05-16 23:03:53 | INFO | train_inner | epoch 029:   2260 / 2955 loss=4.675, ppl=25.54, wps=13877.5, ups=0.42, wpb=32768, bsz=64, num_updates=85000, lr=0.000108465, gnorm=0.733, train_wall=229, gb_free=5.1, wall=19280
2022-05-16 23:07:57 | INFO | train_inner | epoch 029:   2360 / 2955 loss=4.681, ppl=25.65, wps=13453.2, ups=0.41, wpb=32768, bsz=64, num_updates=85100, lr=0.000108401, gnorm=0.739, train_wall=234, gb_free=5.1, wall=19524
2022-05-16 23:11:50 | INFO | train_inner | epoch 029:   2460 / 2955 loss=4.692, ppl=25.85, wps=14041.5, ups=0.43, wpb=32768, bsz=64, num_updates=85200, lr=0.000108338, gnorm=0.729, train_wall=229, gb_free=5.1, wall=19757
2022-05-16 23:15:46 | INFO | train_inner | epoch 029:   2560 / 2955 loss=4.682, ppl=25.66, wps=13909.9, ups=0.42, wpb=32768, bsz=64, num_updates=85300, lr=0.000108274, gnorm=0.732, train_wall=231, gb_free=5.1, wall=19993
2022-05-16 23:19:35 | INFO | train_inner | epoch 029:   2660 / 2955 loss=4.684, ppl=25.7, wps=14302.2, ups=0.44, wpb=32768, bsz=64, num_updates=85400, lr=0.000108211, gnorm=0.732, train_wall=225, gb_free=5.1, wall=20222
2022-05-16 23:23:25 | INFO | train_inner | epoch 029:   2760 / 2955 loss=4.688, ppl=25.78, wps=14260.3, ups=0.44, wpb=32768, bsz=64, num_updates=85500, lr=0.000108148, gnorm=0.73, train_wall=227, gb_free=5.1, wall=20452
2022-05-16 23:27:26 | INFO | train_inner | epoch 029:   2860 / 2955 loss=4.703, ppl=26.05, wps=13578.6, ups=0.41, wpb=32768, bsz=64, num_updates=85600, lr=0.000108084, gnorm=0.732, train_wall=232, gb_free=5.1, wall=20693
2022-05-16 23:31:08 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-05-16 23:33:29 | INFO | valid | epoch 029 | valid on 'valid' subset | loss 4.762 | ppl 27.13 | wps 38148.2 | wpb 2047.4 | bsz 4 | num_updates 85695 | best_loss 4.762
2022-05-16 23:33:29 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 29 @ 85695 updates
2022-05-16 23:33:29 | INFO | fairseq.trainer | Saving checkpoint to /cluster/work/cotterell/liam/master-thesis/checkpoints/language_model/standard/checkpoint_best_standard_16_5.0000E-04_full.pt
2022-05-16 23:33:32 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/work/cotterell/liam/master-thesis/checkpoints/language_model/standard/checkpoint_best_standard_16_5.0000E-04_full.pt
2022-05-16 23:33:32 | INFO | fairseq.checkpoint_utils | Saved checkpoint checkpoints/language_model/standard/checkpoint_best_standard_16_5.0000E-04_full.pt (epoch 29 @ 85695 updates, score 4.762) (writing took 2.908495804760605 seconds)
2022-05-16 23:33:32 | INFO | fairseq_cli.train | end of epoch 29 (average epoch stats below)
2022-05-16 23:33:32 | INFO | train | epoch 029 | loss 4.675 | ppl 25.54 | wps 13783.4 | ups 0.42 | wpb 32764.7 | bsz 64 | num_updates 85695 | lr 0.000108024 | gnorm 0.73 | train_wall 6737 | gb_free 5.1 | wall 21058
2022-05-16 23:33:32 | INFO | fairseq.data.iterators | grouped total_num_itrs = 2955
2022-05-16 23:33:32 | INFO | fairseq.trainer | begin training epoch 30
2022-05-16 23:33:32 | INFO | fairseq_cli.train | Start iterating over samples
2022-05-16 23:33:44 | INFO | train_inner | epoch 030:      5 / 2955 loss=4.691, ppl=25.83, wps=8653.7, ups=0.26, wpb=32686.1, bsz=63.8, num_updates=85700, lr=0.000108021, gnorm=0.735, train_wall=225, gb_free=5.1, wall=21071
2022-05-16 23:37:39 | INFO | train_inner | epoch 030:    105 / 2955 loss=4.645, ppl=25.03, wps=13933.4, ups=0.43, wpb=32768, bsz=64, num_updates=85800, lr=0.000107958, gnorm=0.73, train_wall=226, gb_free=5.1, wall=21306
2022-05-16 23:41:31 | INFO | train_inner | epoch 030:    205 / 2955 loss=4.629, ppl=24.74, wps=14157.2, ups=0.43, wpb=32768, bsz=64, num_updates=85900, lr=0.000107896, gnorm=0.73, train_wall=226, gb_free=5.1, wall=21537
2022-05-16 23:45:33 | INFO | train_inner | epoch 030:    305 / 2955 loss=4.642, ppl=24.96, wps=13504.9, ups=0.41, wpb=32768, bsz=64, num_updates=86000, lr=0.000107833, gnorm=0.734, train_wall=237, gb_free=5.1, wall=21780
2022-05-16 23:49:24 | INFO | train_inner | epoch 030:    405 / 2955 loss=4.652, ppl=25.15, wps=14221.7, ups=0.43, wpb=32768, bsz=64, num_updates=86100, lr=0.00010777, gnorm=0.732, train_wall=226, gb_free=5.1, wall=22010
2022-05-16 23:53:14 | INFO | train_inner | epoch 030:    505 / 2955 loss=4.653, ppl=25.16, wps=14228.3, ups=0.43, wpb=32768, bsz=64, num_updates=86200, lr=0.000107708, gnorm=0.732, train_wall=227, gb_free=5.1, wall=22241
2022-05-16 23:57:13 | INFO | train_inner | epoch 030:    605 / 2955 loss=4.659, ppl=25.27, wps=13732.9, ups=0.42, wpb=32768, bsz=64, num_updates=86300, lr=0.000107645, gnorm=0.733, train_wall=234, gb_free=5.1, wall=22479
2022-05-17 00:01:05 | INFO | train_inner | epoch 030:    705 / 2955 loss=4.658, ppl=25.24, wps=14091.9, ups=0.43, wpb=32768, bsz=64, num_updates=86400, lr=0.000107583, gnorm=0.731, train_wall=226, gb_free=5.1, wall=22712
2022-05-17 00:05:04 | INFO | train_inner | epoch 030:    805 / 2955 loss=4.644, ppl=25, wps=13718.8, ups=0.42, wpb=32768, bsz=64, num_updates=86500, lr=0.000107521, gnorm=0.732, train_wall=233, gb_free=5.1, wall=22951
2022-05-17 00:08:53 | INFO | train_inner | epoch 030:    905 / 2955 loss=4.661, ppl=25.3, wps=14289.8, ups=0.44, wpb=32768, bsz=64, num_updates=86600, lr=0.000107459, gnorm=0.736, train_wall=226, gb_free=5.1, wall=23180
2022-05-17 00:12:44 | INFO | train_inner | epoch 030:   1005 / 2955 loss=4.662, ppl=25.32, wps=14180.7, ups=0.43, wpb=32752.8, bsz=64, num_updates=86700, lr=0.000107397, gnorm=0.737, train_wall=227, gb_free=5.1, wall=23411
2022-05-17 00:16:35 | INFO | train_inner | epoch 030:   1105 / 2955 loss=4.67, ppl=25.46, wps=14213.4, ups=0.43, wpb=32768, bsz=64, num_updates=86800, lr=0.000107335, gnorm=0.735, train_wall=227, gb_free=5.1, wall=23641
2022-05-17 00:20:26 | INFO | train_inner | epoch 030:   1205 / 2955 loss=4.679, ppl=25.61, wps=14168.1, ups=0.43, wpb=32768, bsz=64, num_updates=86900, lr=0.000107273, gnorm=0.735, train_wall=226, gb_free=5.1, wall=23873
2022-05-17 00:24:28 | INFO | train_inner | epoch 030:   1305 / 2955 loss=4.671, ppl=25.48, wps=13554.4, ups=0.41, wpb=32768, bsz=64, num_updates=87000, lr=0.000107211, gnorm=0.735, train_wall=235, gb_free=5.1, wall=24115
2022-05-17 00:28:17 | INFO | train_inner | epoch 030:   1405 / 2955 loss=4.677, ppl=25.57, wps=14294.9, ups=0.44, wpb=32768, bsz=64, num_updates=87100, lr=0.00010715, gnorm=0.739, train_wall=227, gb_free=5.1, wall=24344
2022-05-17 00:32:07 | INFO | train_inner | epoch 030:   1505 / 2955 loss=4.659, ppl=25.26, wps=14277.1, ups=0.44, wpb=32768, bsz=64, num_updates=87200, lr=0.000107088, gnorm=0.739, train_wall=227, gb_free=5.1, wall=24573
2022-05-17 00:36:06 | INFO | train_inner | epoch 030:   1605 / 2955 loss=4.685, ppl=25.72, wps=13703.2, ups=0.42, wpb=32768, bsz=64, num_updates=87300, lr=0.000107027, gnorm=0.737, train_wall=236, gb_free=5.1, wall=24812
2022-05-17 00:39:55 | INFO | train_inner | epoch 030:   1705 / 2955 loss=4.684, ppl=25.7, wps=14302.6, ups=0.44, wpb=32768, bsz=64, num_updates=87400, lr=0.000106966, gnorm=0.734, train_wall=227, gb_free=5.1, wall=25042
2022-05-17 00:43:54 | INFO | train_inner | epoch 030:   1805 / 2955 loss=4.672, ppl=25.49, wps=13725.3, ups=0.42, wpb=32768, bsz=64, num_updates=87500, lr=0.000106904, gnorm=0.738, train_wall=234, gb_free=5.1, wall=25280
2022-05-17 00:47:45 | INFO | train_inner | epoch 030:   1905 / 2955 loss=4.682, ppl=25.66, wps=14169.5, ups=0.43, wpb=32768, bsz=64, num_updates=87600, lr=0.000106843, gnorm=0.736, train_wall=228, gb_free=5.1, wall=25512
2022-05-17 00:51:36 | INFO | train_inner | epoch 030:   2005 / 2955 loss=4.674, ppl=25.53, wps=14184.9, ups=0.43, wpb=32768, bsz=64, num_updates=87700, lr=0.000106783, gnorm=0.737, train_wall=227, gb_free=5.1, wall=25743
2022-05-17 00:55:29 | INFO | train_inner | epoch 030:   2105 / 2955 loss=4.683, ppl=25.69, wps=14033.3, ups=0.43, wpb=32768, bsz=64, num_updates=87800, lr=0.000106722, gnorm=0.739, train_wall=230, gb_free=5.1, wall=25976
2022-05-17 00:59:18 | INFO | train_inner | epoch 030:   2205 / 2955 loss=4.678, ppl=25.6, wps=14300.4, ups=0.44, wpb=32768, bsz=64, num_updates=87900, lr=0.000106661, gnorm=0.736, train_wall=227, gb_free=5.1, wall=26205
2022-05-17 01:03:11 | INFO | train_inner | epoch 030:   2305 / 2955 loss=4.67, ppl=25.46, wps=14077.1, ups=0.43, wpb=32768, bsz=64, num_updates=88000, lr=0.0001066, gnorm=0.741, train_wall=230, gb_free=5.1, wall=26438
2022-05-17 01:07:02 | INFO | train_inner | epoch 030:   2405 / 2955 loss=4.688, ppl=25.78, wps=14185.8, ups=0.43, wpb=32768, bsz=64, num_updates=88100, lr=0.00010654, gnorm=0.745, train_wall=227, gb_free=5.1, wall=26669
2022-05-17 01:10:52 | INFO | train_inner | epoch 030:   2505 / 2955 loss=4.697, ppl=25.93, wps=14286.2, ups=0.44, wpb=32768, bsz=64, num_updates=88200, lr=0.000106479, gnorm=0.738, train_wall=227, gb_free=5.1, wall=26898
2022-05-17 01:14:51 | INFO | train_inner | epoch 030:   2605 / 2955 loss=4.684, ppl=25.71, wps=13680.9, ups=0.42, wpb=32768, bsz=64, num_updates=88300, lr=0.000106419, gnorm=0.738, train_wall=235, gb_free=5.1, wall=27138
2022-05-17 01:18:40 | INFO | train_inner | epoch 030:   2705 / 2955 loss=4.675, ppl=25.54, wps=14304.2, ups=0.44, wpb=32768, bsz=64, num_updates=88400, lr=0.000106359, gnorm=0.735, train_wall=227, gb_free=5.1, wall=27367
2022-05-17 01:22:32 | INFO | train_inner | epoch 030:   2805 / 2955 loss=4.692, ppl=25.85, wps=14145.9, ups=0.43, wpb=32768, bsz=64, num_updates=88500, lr=0.000106299, gnorm=0.736, train_wall=228, gb_free=5.1, wall=27599
2022-05-17 01:26:31 | INFO | train_inner | epoch 030:   2905 / 2955 loss=4.68, ppl=25.64, wps=13681.1, ups=0.42, wpb=32768, bsz=64, num_updates=88600, lr=0.000106239, gnorm=0.731, train_wall=235, gb_free=5.1, wall=27838
2022-05-17 01:28:27 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-05-17 01:30:46 | INFO | valid | epoch 030 | valid on 'valid' subset | loss 4.757 | ppl 27.05 | wps 38554.3 | wpb 2047.4 | bsz 4 | num_updates 88650 | best_loss 4.757
2022-05-17 01:30:46 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 30 @ 88650 updates
2022-05-17 01:30:46 | INFO | fairseq.trainer | Saving checkpoint to /cluster/work/cotterell/liam/master-thesis/checkpoints/language_model/standard/checkpoint_best_standard_16_5.0000E-04_full.pt
2022-05-17 01:30:49 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/work/cotterell/liam/master-thesis/checkpoints/language_model/standard/checkpoint_best_standard_16_5.0000E-04_full.pt
2022-05-17 01:30:49 | INFO | fairseq.checkpoint_utils | Saved checkpoint checkpoints/language_model/standard/checkpoint_best_standard_16_5.0000E-04_full.pt (epoch 30 @ 88650 updates, score 4.757) (writing took 2.964914547279477 seconds)
2022-05-17 01:30:49 | INFO | fairseq_cli.train | end of epoch 30 (average epoch stats below)
2022-05-17 01:30:49 | INFO | train | epoch 030 | loss 4.669 | ppl 25.45 | wps 13757.8 | ups 0.42 | wpb 32764.7 | bsz 64 | num_updates 88650 | lr 0.000106209 | gnorm 0.736 | train_wall 6769 | gb_free 5.1 | wall 28096
2022-05-17 01:30:49 | INFO | fairseq.data.iterators | grouped total_num_itrs = 2955
2022-05-17 01:30:49 | INFO | fairseq.trainer | begin training epoch 31
2022-05-17 01:30:49 | INFO | fairseq_cli.train | Start iterating over samples
2022-05-17 01:32:44 | INFO | train_inner | epoch 031:     50 / 2955 loss=4.656, ppl=25.22, wps=8761.6, ups=0.27, wpb=32686.1, bsz=63.8, num_updates=88700, lr=0.000106179, gnorm=0.742, train_wall=226, gb_free=5.1, wall=28211
2022-05-17 01:36:36 | INFO | train_inner | epoch 031:    150 / 2955 loss=4.621, ppl=24.61, wps=14175.5, ups=0.43, wpb=32768, bsz=64, num_updates=88800, lr=0.000106119, gnorm=0.733, train_wall=228, gb_free=5.1, wall=28442
2022-05-17 01:40:26 | INFO | train_inner | epoch 031:    250 / 2955 loss=4.641, ppl=24.95, wps=14235.5, ups=0.43, wpb=32768, bsz=64, num_updates=88900, lr=0.000106059, gnorm=0.737, train_wall=227, gb_free=5.1, wall=28673
2022-05-17 01:44:27 | INFO | train_inner | epoch 031:    350 / 2955 loss=4.646, ppl=25.04, wps=13558.2, ups=0.41, wpb=32768, bsz=64, num_updates=89000, lr=0.000106, gnorm=0.741, train_wall=237, gb_free=5.1, wall=28914
2022-05-17 01:48:17 | INFO | train_inner | epoch 031:    450 / 2955 loss=4.644, ppl=25, wps=14295.6, ups=0.44, wpb=32768, bsz=64, num_updates=89100, lr=0.00010594, gnorm=0.742, train_wall=226, gb_free=5.1, wall=29143
2022-05-17 01:52:13 | INFO | train_inner | epoch 031:    550 / 2955 loss=4.645, ppl=25.02, wps=13877.8, ups=0.42, wpb=32768, bsz=64, num_updates=89200, lr=0.000105881, gnorm=0.738, train_wall=232, gb_free=5.1, wall=29380
2022-05-17 01:56:05 | INFO | train_inner | epoch 031:    650 / 2955 loss=4.646, ppl=25.03, wps=14083.9, ups=0.43, wpb=32768, bsz=64, num_updates=89300, lr=0.000105822, gnorm=0.738, train_wall=229, gb_free=5.1, wall=29612
2022-05-17 01:59:56 | INFO | train_inner | epoch 031:    750 / 2955 loss=4.648, ppl=25.07, wps=14222.7, ups=0.43, wpb=32768, bsz=64, num_updates=89400, lr=0.000105762, gnorm=0.741, train_wall=228, gb_free=5.1, wall=29843
2022-05-17 02:03:47 | INFO | train_inner | epoch 031:    850 / 2955 loss=4.654, ppl=25.17, wps=14193.1, ups=0.43, wpb=32768, bsz=64, num_updates=89500, lr=0.000105703, gnorm=0.743, train_wall=228, gb_free=5.1, wall=30073
2022-05-17 02:07:37 | INFO | train_inner | epoch 031:    950 / 2955 loss=4.647, ppl=25.06, wps=14262.8, ups=0.44, wpb=32768, bsz=64, num_updates=89600, lr=0.000105644, gnorm=0.742, train_wall=227, gb_free=5.1, wall=30303
2022-05-17 02:11:28 | INFO | train_inner | epoch 031:   1050 / 2955 loss=4.656, ppl=25.22, wps=14134.9, ups=0.43, wpb=32768, bsz=64, num_updates=89700, lr=0.000105585, gnorm=0.742, train_wall=228, gb_free=5.1, wall=30535
2022-05-17 02:15:20 | INFO | train_inner | epoch 031:   1150 / 2955 loss=4.654, ppl=25.18, wps=14142.8, ups=0.43, wpb=32768, bsz=64, num_updates=89800, lr=0.000105527, gnorm=0.739, train_wall=229, gb_free=5.1, wall=30767
2022-05-17 02:19:12 | INFO | train_inner | epoch 031:   1250 / 2955 loss=4.666, ppl=25.38, wps=14115, ups=0.43, wpb=32768, bsz=64, num_updates=89900, lr=0.000105468, gnorm=0.735, train_wall=228, gb_free=5.1, wall=30999
2022-05-17 02:23:19 | INFO | train_inner | epoch 031:   1350 / 2955 loss=4.662, ppl=25.31, wps=13266.8, ups=0.4, wpb=32768, bsz=64, num_updates=90000, lr=0.000105409, gnorm=0.742, train_wall=231, gb_free=5.1, wall=31246
2022-05-17 02:27:16 | INFO | train_inner | epoch 031:   1450 / 2955 loss=4.669, ppl=25.45, wps=13825.6, ups=0.42, wpb=32768, bsz=64, num_updates=90100, lr=0.000105351, gnorm=0.744, train_wall=228, gb_free=5.1, wall=31483
2022-05-17 02:31:22 | INFO | train_inner | epoch 031:   1550 / 2955 loss=4.665, ppl=25.37, wps=13313.2, ups=0.41, wpb=32768, bsz=64, num_updates=90200, lr=0.000105292, gnorm=0.74, train_wall=235, gb_free=5.1, wall=31729
2022-05-17 02:35:22 | INFO | train_inner | epoch 031:   1650 / 2955 loss=4.673, ppl=25.51, wps=13687.7, ups=0.42, wpb=32757.8, bsz=64, num_updates=90300, lr=0.000105234, gnorm=0.741, train_wall=227, gb_free=5.1, wall=31968
2022-05-17 02:39:17 | INFO | train_inner | epoch 031:   1750 / 2955 loss=4.664, ppl=25.36, wps=13943.7, ups=0.43, wpb=32768, bsz=64, num_updates=90400, lr=0.000105176, gnorm=0.744, train_wall=226, gb_free=5.1, wall=32203
2022-05-17 02:43:23 | INFO | train_inner | epoch 031:   1850 / 2955 loss=4.662, ppl=25.32, wps=13320.8, ups=0.41, wpb=32768, bsz=64, num_updates=90500, lr=0.000105118, gnorm=0.743, train_wall=233, gb_free=5.1, wall=32449
2022-05-17 02:47:20 | INFO | train_inner | epoch 031:   1950 / 2955 loss=4.683, ppl=25.69, wps=13819.7, ups=0.42, wpb=32768, bsz=64, num_updates=90600, lr=0.00010506, gnorm=0.739, train_wall=227, gb_free=5.1, wall=32686
2022-05-17 02:51:21 | INFO | train_inner | epoch 031:   2050 / 2955 loss=4.676, ppl=25.56, wps=13561.3, ups=0.41, wpb=32768, bsz=64, num_updates=90700, lr=0.000105002, gnorm=0.739, train_wall=232, gb_free=5.1, wall=32928
2022-05-17 02:55:19 | INFO | train_inner | epoch 031:   2150 / 2955 loss=4.678, ppl=25.6, wps=13792, ups=0.42, wpb=32768, bsz=64, num_updates=90800, lr=0.000104944, gnorm=0.738, train_wall=229, gb_free=5.1, wall=33166
2022-05-17 02:59:11 | INFO | train_inner | epoch 031:   2250 / 2955 loss=4.676, ppl=25.56, wps=14119.6, ups=0.43, wpb=32768, bsz=64, num_updates=90900, lr=0.000104886, gnorm=0.738, train_wall=226, gb_free=5.1, wall=33398
2022-05-17 03:03:11 | INFO | train_inner | epoch 031:   2350 / 2955 loss=4.679, ppl=25.62, wps=13639.8, ups=0.42, wpb=32768, bsz=64, num_updates=91000, lr=0.000104828, gnorm=0.74, train_wall=230, gb_free=5.1, wall=33638
2022-05-17 03:07:10 | INFO | train_inner | epoch 031:   2450 / 2955 loss=4.67, ppl=25.46, wps=13751.1, ups=0.42, wpb=32768, bsz=64, num_updates=91100, lr=0.000104771, gnorm=0.742, train_wall=228, gb_free=5.1, wall=33876
2022-05-17 03:11:08 | INFO | train_inner | epoch 031:   2550 / 2955 loss=4.688, ppl=25.77, wps=13728.7, ups=0.42, wpb=32768, bsz=64, num_updates=91200, lr=0.000104713, gnorm=0.743, train_wall=228, gb_free=5.1, wall=34115
2022-05-17 03:15:19 | INFO | train_inner | epoch 031:   2650 / 2955 loss=4.687, ppl=25.76, wps=13080.5, ups=0.4, wpb=32768, bsz=64, num_updates=91300, lr=0.000104656, gnorm=0.743, train_wall=234, gb_free=5.1, wall=34366
2022-05-17 03:19:16 | INFO | train_inner | epoch 031:   2750 / 2955 loss=4.692, ppl=25.85, wps=13836.4, ups=0.42, wpb=32768, bsz=64, num_updates=91400, lr=0.000104599, gnorm=0.742, train_wall=226, gb_free=5.1, wall=34602
2022-05-17 03:23:18 | INFO | train_inner | epoch 031:   2850 / 2955 loss=4.688, ppl=25.78, wps=13512.2, ups=0.41, wpb=32768, bsz=64, num_updates=91500, lr=0.000104542, gnorm=0.741, train_wall=234, gb_free=5.1, wall=34845
2022-05-17 03:27:16 | INFO | train_inner | epoch 031:   2950 / 2955 loss=4.696, ppl=25.91, wps=13767.4, ups=0.42, wpb=32763, bsz=64, num_updates=91600, lr=0.000104485, gnorm=0.742, train_wall=227, gb_free=5.1, wall=35083
2022-05-17 03:27:27 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-05-17 03:29:43 | INFO | valid | epoch 031 | valid on 'valid' subset | loss 4.754 | ppl 26.99 | wps 39637.8 | wpb 2047.4 | bsz 4 | num_updates 91605 | best_loss 4.754
2022-05-17 03:29:43 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 31 @ 91605 updates
2022-05-17 03:29:43 | INFO | fairseq.trainer | Saving checkpoint to /cluster/work/cotterell/liam/master-thesis/checkpoints/language_model/standard/checkpoint_best_standard_16_5.0000E-04_full.pt
2022-05-17 03:29:46 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/work/cotterell/liam/master-thesis/checkpoints/language_model/standard/checkpoint_best_standard_16_5.0000E-04_full.pt
2022-05-17 03:29:46 | INFO | fairseq.checkpoint_utils | Saved checkpoint checkpoints/language_model/standard/checkpoint_best_standard_16_5.0000E-04_full.pt (epoch 31 @ 91605 updates, score 4.754) (writing took 3.5241794660687447 seconds)
2022-05-17 03:29:46 | INFO | fairseq_cli.train | end of epoch 31 (average epoch stats below)
2022-05-17 03:29:46 | INFO | train | epoch 031 | loss 4.664 | ppl 25.35 | wps 13565.8 | ups 0.41 | wpb 32764.7 | bsz 64 | num_updates 91605 | lr 0.000104482 | gnorm 0.74 | train_wall 6773 | gb_free 5.1 | wall 35233
2022-05-17 03:29:46 | INFO | fairseq.data.iterators | grouped total_num_itrs = 2955
2022-05-17 03:29:46 | INFO | fairseq.trainer | begin training epoch 32
2022-05-17 03:29:46 | INFO | fairseq_cli.train | Start iterating over samples
2022-05-17 03:33:34 | INFO | train_inner | epoch 032:     95 / 2955 loss=4.638, ppl=24.9, wps=8642.1, ups=0.26, wpb=32686.1, bsz=63.8, num_updates=91700, lr=0.000104428, gnorm=0.745, train_wall=227, gb_free=5.1, wall=35461
2022-05-17 03:37:37 | INFO | train_inner | epoch 032:    195 / 2955 loss=4.631, ppl=24.78, wps=13495.3, ups=0.41, wpb=32768, bsz=64, num_updates=91800, lr=0.000104371, gnorm=0.74, train_wall=230, gb_free=5.1, wall=35704
2022-05-17 03:41:36 | INFO | train_inner | epoch 032:    295 / 2955 loss=4.636, ppl=24.86, wps=13692.7, ups=0.42, wpb=32768, bsz=64, num_updates=91900, lr=0.000104314, gnorm=0.743, train_wall=229, gb_free=5.1, wall=35943
2022-05-17 03:45:33 | INFO | train_inner | epoch 032:    395 / 2955 loss=4.614, ppl=24.48, wps=13877, ups=0.42, wpb=32768, bsz=64, num_updates=92000, lr=0.000104257, gnorm=0.743, train_wall=228, gb_free=5.1, wall=36179
2022-05-17 03:49:31 | INFO | train_inner | epoch 032:    495 / 2955 loss=4.635, ppl=24.84, wps=13771.6, ups=0.42, wpb=32768, bsz=64, num_updates=92100, lr=0.000104201, gnorm=0.744, train_wall=229, gb_free=5.1, wall=36417
2022-05-17 03:53:25 | INFO | train_inner | epoch 032:    595 / 2955 loss=4.652, ppl=25.14, wps=13985.1, ups=0.43, wpb=32768, bsz=64, num_updates=92200, lr=0.000104144, gnorm=0.744, train_wall=227, gb_free=5.1, wall=36652
2022-05-17 03:57:21 | INFO | train_inner | epoch 032:    695 / 2955 loss=4.627, ppl=24.71, wps=13896.4, ups=0.42, wpb=32768, bsz=64, num_updates=92300, lr=0.000104088, gnorm=0.747, train_wall=231, gb_free=5.1, wall=36887
2022-05-17 04:01:15 | INFO | train_inner | epoch 032:    795 / 2955 loss=4.657, ppl=25.23, wps=13990.3, ups=0.43, wpb=32768, bsz=64, num_updates=92400, lr=0.000104031, gnorm=0.745, train_wall=228, gb_free=5.1, wall=37122
2022-05-17 04:05:14 | INFO | train_inner | epoch 032:    895 / 2955 loss=4.654, ppl=25.17, wps=13715.6, ups=0.42, wpb=32768, bsz=64, num_updates=92500, lr=0.000103975, gnorm=0.746, train_wall=226, gb_free=5.1, wall=37361
2022-05-17 04:09:16 | INFO | train_inner | epoch 032:    995 / 2955 loss=4.654, ppl=25.17, wps=13513.2, ups=0.41, wpb=32768, bsz=64, num_updates=92600, lr=0.000103919, gnorm=0.746, train_wall=231, gb_free=5.1, wall=37603
2022-05-17 04:13:15 | INFO | train_inner | epoch 032:   1095 / 2955 loss=4.657, ppl=25.23, wps=13746.7, ups=0.42, wpb=32768, bsz=64, num_updates=92700, lr=0.000103863, gnorm=0.745, train_wall=229, gb_free=5.1, wall=37841
2022-05-17 04:17:14 | INFO | train_inner | epoch 032:   1195 / 2955 loss=4.653, ppl=25.16, wps=13716.3, ups=0.42, wpb=32768, bsz=64, num_updates=92800, lr=0.000103807, gnorm=0.744, train_wall=229, gb_free=5.1, wall=38080
2022-05-17 04:21:11 | INFO | train_inner | epoch 032:   1295 / 2955 loss=4.668, ppl=25.42, wps=13813.2, ups=0.42, wpb=32768, bsz=64, num_updates=92900, lr=0.000103751, gnorm=0.745, train_wall=228, gb_free=5.1, wall=38318
2022-05-17 04:25:04 | INFO | train_inner | epoch 032:   1395 / 2955 loss=4.67, ppl=25.45, wps=14065.2, ups=0.43, wpb=32768, bsz=64, num_updates=93000, lr=0.000103695, gnorm=0.744, train_wall=226, gb_free=5.1, wall=38551
2022-05-17 04:29:07 | INFO | train_inner | epoch 032:   1495 / 2955 loss=4.673, ppl=25.51, wps=13477.6, ups=0.41, wpb=32768, bsz=64, num_updates=93100, lr=0.000103639, gnorm=0.745, train_wall=234, gb_free=5.1, wall=38794
2022-05-17 04:33:06 | INFO | train_inner | epoch 032:   1595 / 2955 loss=4.661, ppl=25.3, wps=13729.1, ups=0.42, wpb=32768, bsz=64, num_updates=93200, lr=0.000103584, gnorm=0.743, train_wall=227, gb_free=5.1, wall=39032
2022-05-17 04:37:13 | INFO | train_inner | epoch 032:   1695 / 2955 loss=4.67, ppl=25.46, wps=13260.7, ups=0.4, wpb=32768, bsz=64, num_updates=93300, lr=0.000103528, gnorm=0.745, train_wall=232, gb_free=5.1, wall=39279
2022-05-17 04:41:11 | INFO | train_inner | epoch 032:   1795 / 2955 loss=4.664, ppl=25.35, wps=13765.9, ups=0.42, wpb=32768, bsz=64, num_updates=93400, lr=0.000103473, gnorm=0.75, train_wall=227, gb_free=5.1, wall=39517
2022-05-17 04:45:05 | INFO | train_inner | epoch 032:   1895 / 2955 loss=4.677, ppl=25.58, wps=13993.1, ups=0.43, wpb=32768, bsz=64, num_updates=93500, lr=0.000103418, gnorm=0.748, train_wall=226, gb_free=5.1, wall=39752
2022-05-17 04:49:12 | INFO | train_inner | epoch 032:   1995 / 2955 loss=4.671, ppl=25.47, wps=13254.1, ups=0.4, wpb=32768, bsz=64, num_updates=93600, lr=0.000103362, gnorm=0.743, train_wall=234, gb_free=5.1, wall=39999
2022-05-17 04:53:11 | INFO | train_inner | epoch 032:   2095 / 2955 loss=4.659, ppl=25.26, wps=13715.8, ups=0.42, wpb=32768, bsz=64, num_updates=93700, lr=0.000103307, gnorm=0.742, train_wall=227, gb_free=5.1, wall=40238
2022-05-17 04:57:10 | INFO | train_inner | epoch 032:   2195 / 2955 loss=4.667, ppl=25.4, wps=13732.5, ups=0.42, wpb=32757.8, bsz=64, num_updates=93800, lr=0.000103252, gnorm=0.752, train_wall=228, gb_free=5.1, wall=40476
2022-05-17 05:01:17 | INFO | train_inner | epoch 032:   2295 / 2955 loss=4.659, ppl=25.27, wps=13249.3, ups=0.4, wpb=32768, bsz=64, num_updates=93900, lr=0.000103197, gnorm=0.741, train_wall=234, gb_free=5.1, wall=40724
2022-05-17 05:05:13 | INFO | train_inner | epoch 032:   2395 / 2955 loss=4.666, ppl=25.38, wps=13902.6, ups=0.42, wpb=32768, bsz=64, num_updates=94000, lr=0.000103142, gnorm=0.747, train_wall=226, gb_free=5.1, wall=40959
2022-05-17 05:09:11 | INFO | train_inner | epoch 032:   2495 / 2955 loss=4.694, ppl=25.89, wps=13735.3, ups=0.42, wpb=32768, bsz=64, num_updates=94100, lr=0.000103087, gnorm=0.75, train_wall=230, gb_free=5.1, wall=41198
2022-05-17 05:13:06 | INFO | train_inner | epoch 032:   2595 / 2955 loss=4.661, ppl=25.29, wps=13937.1, ups=0.43, wpb=32768, bsz=64, num_updates=94200, lr=0.000103033, gnorm=0.747, train_wall=227, gb_free=5.1, wall=41433
2022-05-17 05:17:06 | INFO | train_inner | epoch 032:   2695 / 2955 loss=4.672, ppl=25.49, wps=13659.7, ups=0.42, wpb=32768, bsz=64, num_updates=94300, lr=0.000102978, gnorm=0.745, train_wall=230, gb_free=5.1, wall=41673
2022-05-17 05:21:12 | INFO | train_inner | epoch 032:   2795 / 2955 loss=4.682, ppl=25.66, wps=13358.2, ups=0.41, wpb=32768, bsz=64, num_updates=94400, lr=0.000102923, gnorm=0.748, train_wall=233, gb_free=5.1, wall=41918
2022-05-17 05:25:07 | INFO | train_inner | epoch 032:   2895 / 2955 loss=4.68, ppl=25.63, wps=13908.5, ups=0.42, wpb=32763, bsz=64, num_updates=94500, lr=0.000102869, gnorm=0.744, train_wall=226, gb_free=5.1, wall=42154
2022-05-17 05:27:28 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-05-17 05:29:43 | INFO | valid | epoch 032 | valid on 'valid' subset | loss 4.752 | ppl 26.95 | wps 39831.2 | wpb 2047.4 | bsz 4 | num_updates 94560 | best_loss 4.752
2022-05-17 05:29:43 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 32 @ 94560 updates
2022-05-17 05:29:43 | INFO | fairseq.trainer | Saving checkpoint to /cluster/work/cotterell/liam/master-thesis/checkpoints/language_model/standard/checkpoint_best_standard_16_5.0000E-04_full.pt
2022-05-17 05:29:46 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/work/cotterell/liam/master-thesis/checkpoints/language_model/standard/checkpoint_best_standard_16_5.0000E-04_full.pt
2022-05-17 05:29:46 | INFO | fairseq.checkpoint_utils | Saved checkpoint checkpoints/language_model/standard/checkpoint_best_standard_16_5.0000E-04_full.pt (epoch 32 @ 94560 updates, score 4.752) (writing took 2.4967624191194773 seconds)
2022-05-17 05:29:46 | INFO | fairseq_cli.train | end of epoch 32 (average epoch stats below)
2022-05-17 05:29:46 | INFO | train | epoch 032 | loss 4.659 | ppl 25.27 | wps 13448.2 | ups 0.41 | wpb 32764.7 | bsz 64 | num_updates 94560 | lr 0.000102836 | gnorm 0.745 | train_wall 6767 | gb_free 5.1 | wall 42432
2022-05-17 05:29:46 | INFO | fairseq.data.iterators | grouped total_num_itrs = 2955
2022-05-17 05:29:46 | INFO | fairseq.trainer | begin training epoch 33
2022-05-17 05:29:46 | INFO | fairseq_cli.train | Start iterating over samples
2022-05-17 05:31:24 | INFO | train_inner | epoch 033:     40 / 2955 loss=4.655, ppl=25.19, wps=8675.3, ups=0.27, wpb=32686.1, bsz=63.8, num_updates=94600, lr=0.000102815, gnorm=0.743, train_wall=229, gb_free=5.1, wall=42531
2022-05-17 05:35:24 | INFO | train_inner | epoch 033:    140 / 2955 loss=4.613, ppl=24.47, wps=13621.5, ups=0.42, wpb=32768, bsz=64, num_updates=94700, lr=0.00010276, gnorm=0.742, train_wall=230, gb_free=5.1, wall=42771
2022-05-17 05:39:20 | INFO | train_inner | epoch 033:    240 / 2955 loss=4.628, ppl=24.72, wps=13934, ups=0.43, wpb=32768, bsz=64, num_updates=94800, lr=0.000102706, gnorm=0.746, train_wall=229, gb_free=5.1, wall=43006
2022-05-17 05:43:22 | INFO | train_inner | epoch 033:    340 / 2955 loss=4.633, ppl=24.82, wps=13508.7, ups=0.41, wpb=32768, bsz=64, num_updates=94900, lr=0.000102652, gnorm=0.748, train_wall=235, gb_free=5.1, wall=43249
2022-05-17 05:47:16 | INFO | train_inner | epoch 033:    440 / 2955 loss=4.629, ppl=24.75, wps=13993.8, ups=0.43, wpb=32768, bsz=64, num_updates=95000, lr=0.000102598, gnorm=0.749, train_wall=230, gb_free=5.1, wall=43483
2022-05-17 05:51:08 | INFO | train_inner | epoch 033:    540 / 2955 loss=4.637, ppl=24.89, wps=14169.1, ups=0.43, wpb=32768, bsz=64, num_updates=95100, lr=0.000102544, gnorm=0.751, train_wall=226, gb_free=5.1, wall=43714
2022-05-17 05:55:05 | INFO | train_inner | epoch 033:    640 / 2955 loss=4.637, ppl=24.88, wps=13810.8, ups=0.42, wpb=32768, bsz=64, num_updates=95200, lr=0.00010249, gnorm=0.755, train_wall=231, gb_free=5.1, wall=43952
2022-05-17 05:58:58 | INFO | train_inner | epoch 033:    740 / 2955 loss=4.631, ppl=24.78, wps=14050.9, ups=0.43, wpb=32768, bsz=64, num_updates=95300, lr=0.000102436, gnorm=0.75, train_wall=229, gb_free=5.1, wall=44185
2022-05-17 06:02:57 | INFO | train_inner | epoch 033:    840 / 2955 loss=4.641, ppl=24.95, wps=13711.1, ups=0.42, wpb=32768, bsz=64, num_updates=95400, lr=0.000102383, gnorm=0.745, train_wall=234, gb_free=5.1, wall=44424
2022-05-17 06:06:55 | INFO | train_inner | epoch 033:    940 / 2955 loss=4.633, ppl=24.82, wps=13792.3, ups=0.42, wpb=32768, bsz=64, num_updates=95500, lr=0.000102329, gnorm=0.748, train_wall=230, gb_free=5.1, wall=44661
2022-05-17 06:10:56 | INFO | train_inner | epoch 033:   1040 / 2955 loss=4.664, ppl=25.35, wps=13597, ups=0.41, wpb=32768, bsz=64, num_updates=95600, lr=0.000102275, gnorm=0.748, train_wall=228, gb_free=5.1, wall=44902
2022-05-17 06:14:59 | INFO | train_inner | epoch 033:   1140 / 2955 loss=4.66, ppl=25.29, wps=13460.6, ups=0.41, wpb=32768, bsz=64, num_updates=95700, lr=0.000102222, gnorm=0.75, train_wall=233, gb_free=5.1, wall=45146
2022-05-17 06:19:00 | INFO | train_inner | epoch 033:   1240 / 2955 loss=4.668, ppl=25.42, wps=13614.9, ups=0.42, wpb=32768, bsz=64, num_updates=95800, lr=0.000102169, gnorm=0.749, train_wall=230, gb_free=5.1, wall=45386
2022-05-17 06:23:08 | INFO | train_inner | epoch 033:   1340 / 2955 loss=4.665, ppl=25.38, wps=13204.5, ups=0.4, wpb=32768, bsz=64, num_updates=95900, lr=0.000102115, gnorm=0.754, train_wall=234, gb_free=5.1, wall=45635
2022-05-17 06:27:08 | INFO | train_inner | epoch 033:   1440 / 2955 loss=4.664, ppl=25.36, wps=13662.3, ups=0.42, wpb=32768, bsz=64, num_updates=96000, lr=0.000102062, gnorm=0.752, train_wall=230, gb_free=5.1, wall=45875
2022-05-17 06:31:06 | INFO | train_inner | epoch 033:   1540 / 2955 loss=4.652, ppl=25.14, wps=13728.9, ups=0.42, wpb=32768, bsz=64, num_updates=96100, lr=0.000102009, gnorm=0.75, train_wall=228, gb_free=5.1, wall=46113
2022-05-17 06:35:10 | INFO | train_inner | epoch 033:   1640 / 2955 loss=4.653, ppl=25.16, wps=13466.3, ups=0.41, wpb=32768, bsz=64, num_updates=96200, lr=0.000101956, gnorm=0.749, train_wall=232, gb_free=5.1, wall=46357
2022-05-17 06:39:07 | INFO | train_inner | epoch 033:   1740 / 2955 loss=4.672, ppl=25.5, wps=13795.3, ups=0.42, wpb=32768, bsz=64, num_updates=96300, lr=0.000101903, gnorm=0.752, train_wall=228, gb_free=5.1, wall=46594
2022-05-17 06:43:15 | INFO | train_inner | epoch 033:   1840 / 2955 loss=4.647, ppl=25.06, wps=13223.7, ups=0.4, wpb=32768, bsz=64, num_updates=96400, lr=0.00010185, gnorm=0.748, train_wall=234, gb_free=5.1, wall=46842
2022-05-17 06:47:16 | INFO | train_inner | epoch 033:   1940 / 2955 loss=4.661, ppl=25.3, wps=13603.2, ups=0.42, wpb=32768, bsz=64, num_updates=96500, lr=0.000101797, gnorm=0.753, train_wall=230, gb_free=5.1, wall=47083
2022-05-17 06:51:13 | INFO | train_inner | epoch 033:   2040 / 2955 loss=4.672, ppl=25.48, wps=13841.5, ups=0.42, wpb=32752.8, bsz=64, num_updates=96600, lr=0.000101745, gnorm=0.748, train_wall=227, gb_free=5.1, wall=47319
2022-05-17 06:55:15 | INFO | train_inner | epoch 033:   2140 / 2955 loss=4.671, ppl=25.47, wps=13529.8, ups=0.41, wpb=32768, bsz=64, num_updates=96700, lr=0.000101692, gnorm=0.753, train_wall=232, gb_free=5.1, wall=47562
2022-05-17 06:59:14 | INFO | train_inner | epoch 033:   2240 / 2955 loss=4.679, ppl=25.61, wps=13730.1, ups=0.42, wpb=32768, bsz=64, num_updates=96800, lr=0.000101639, gnorm=0.749, train_wall=230, gb_free=5.1, wall=47800
2022-05-17 07:03:12 | INFO | train_inner | epoch 033:   2340 / 2955 loss=4.659, ppl=25.26, wps=13737.7, ups=0.42, wpb=32768, bsz=64, num_updates=96900, lr=0.000101587, gnorm=0.744, train_wall=228, gb_free=5.1, wall=48039
2022-05-17 07:07:15 | INFO | train_inner | epoch 033:   2440 / 2955 loss=4.662, ppl=25.32, wps=13502.2, ups=0.41, wpb=32768, bsz=64, num_updates=97000, lr=0.000101535, gnorm=0.754, train_wall=232, gb_free=5.1, wall=48281
2022-05-17 07:11:10 | INFO | train_inner | epoch 033:   2540 / 2955 loss=4.675, ppl=25.55, wps=13926.8, ups=0.43, wpb=32768, bsz=64, num_updates=97100, lr=0.000101482, gnorm=0.749, train_wall=227, gb_free=5.1, wall=48517
2022-05-17 07:15:10 | INFO | train_inner | epoch 033:   2640 / 2955 loss=4.659, ppl=25.26, wps=13675.5, ups=0.42, wpb=32768, bsz=64, num_updates=97200, lr=0.00010143, gnorm=0.749, train_wall=228, gb_free=5.1, wall=48756
2022-05-17 07:19:13 | INFO | train_inner | epoch 033:   2740 / 2955 loss=4.677, ppl=25.58, wps=13486, ups=0.41, wpb=32768, bsz=64, num_updates=97300, lr=0.000101378, gnorm=0.754, train_wall=233, gb_free=5.1, wall=48999
2022-05-17 07:23:09 | INFO | train_inner | epoch 033:   2840 / 2955 loss=4.666, ppl=25.38, wps=13856, ups=0.42, wpb=32768, bsz=64, num_updates=97400, lr=0.000101326, gnorm=0.751, train_wall=228, gb_free=5.1, wall=49236
2022-05-17 07:27:08 | INFO | train_inner | epoch 033:   2940 / 2955 loss=4.677, ppl=25.59, wps=13707.6, ups=0.42, wpb=32768, bsz=64, num_updates=97500, lr=0.000101274, gnorm=0.747, train_wall=232, gb_free=5.1, wall=49475
2022-05-17 07:27:46 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-05-17 07:30:02 | INFO | valid | epoch 033 | valid on 'valid' subset | loss 4.749 | ppl 26.9 | wps 39665.6 | wpb 2047.4 | bsz 4 | num_updates 97515 | best_loss 4.749
2022-05-17 07:30:02 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 33 @ 97515 updates
2022-05-17 07:30:02 | INFO | fairseq.trainer | Saving checkpoint to /cluster/work/cotterell/liam/master-thesis/checkpoints/language_model/standard/checkpoint_best_standard_16_5.0000E-04_full.pt
2022-05-17 07:30:05 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/work/cotterell/liam/master-thesis/checkpoints/language_model/standard/checkpoint_best_standard_16_5.0000E-04_full.pt
2022-05-17 07:30:05 | INFO | fairseq.checkpoint_utils | Saved checkpoint checkpoints/language_model/standard/checkpoint_best_standard_16_5.0000E-04_full.pt (epoch 33 @ 97515 updates, score 4.749) (writing took 2.7261338578537107 seconds)
2022-05-17 07:30:05 | INFO | fairseq_cli.train | end of epoch 33 (average epoch stats below)
2022-05-17 07:30:05 | INFO | train | epoch 033 | loss 4.654 | ppl 25.18 | wps 13411.2 | ups 0.41 | wpb 32764.7 | bsz 64 | num_updates 97515 | lr 0.000101266 | gnorm 0.749 | train_wall 6809 | gb_free 5.1 | wall 49652
2022-05-17 07:30:05 | INFO | fairseq.data.iterators | grouped total_num_itrs = 2955
2022-05-17 07:30:05 | INFO | fairseq.trainer | begin training epoch 34
2022-05-17 07:30:05 | INFO | fairseq_cli.train | Start iterating over samples
2022-05-17 07:33:30 | INFO | train_inner | epoch 034:     85 / 2955 loss=4.619, ppl=24.58, wps=8552.3, ups=0.26, wpb=32686.1, bsz=63.8, num_updates=97600, lr=0.000101222, gnorm=0.75, train_wall=229, gb_free=5.1, wall=49857
2022-05-17 07:37:27 | INFO | train_inner | epoch 034:    185 / 2955 loss=4.626, ppl=24.69, wps=13833.4, ups=0.42, wpb=32768, bsz=64, num_updates=97700, lr=0.00010117, gnorm=0.757, train_wall=226, gb_free=5.1, wall=50094
2022-05-17 07:41:34 | INFO | train_inner | epoch 034:    285 / 2955 loss=4.616, ppl=24.52, wps=13285, ups=0.41, wpb=32768, bsz=64, num_updates=97800, lr=0.000101118, gnorm=0.75, train_wall=235, gb_free=5.1, wall=50341
2022-05-17 07:45:32 | INFO | train_inner | epoch 034:    385 / 2955 loss=4.617, ppl=24.54, wps=13789.5, ups=0.42, wpb=32768, bsz=64, num_updates=97900, lr=0.000101067, gnorm=0.75, train_wall=227, gb_free=5.1, wall=50578
2022-05-17 07:49:35 | INFO | train_inner | epoch 034:    485 / 2955 loss=4.635, ppl=24.84, wps=13483.7, ups=0.41, wpb=32768, bsz=64, num_updates=98000, lr=0.000101015, gnorm=0.752, train_wall=232, gb_free=5.1, wall=50821
2022-05-17 07:53:31 | INFO | train_inner | epoch 034:    585 / 2955 loss=4.628, ppl=24.73, wps=13884.5, ups=0.42, wpb=32768, bsz=64, num_updates=98100, lr=0.000100964, gnorm=0.759, train_wall=227, gb_free=5.1, wall=51057
2022-05-17 07:57:23 | INFO | train_inner | epoch 034:    685 / 2955 loss=4.626, ppl=24.69, wps=14085.1, ups=0.43, wpb=32768, bsz=64, num_updates=98200, lr=0.000100912, gnorm=0.752, train_wall=226, gb_free=5.1, wall=51290
2022-05-17 08:01:23 | INFO | train_inner | epoch 034:    785 / 2955 loss=4.641, ppl=24.95, wps=13634.5, ups=0.42, wpb=32757.8, bsz=64, num_updates=98300, lr=0.000100861, gnorm=0.754, train_wall=233, gb_free=5.1, wall=51530
2022-05-17 08:05:16 | INFO | train_inner | epoch 034:    885 / 2955 loss=4.65, ppl=25.11, wps=14109.3, ups=0.43, wpb=32768, bsz=64, num_updates=98400, lr=0.00010081, gnorm=0.753, train_wall=227, gb_free=5.1, wall=51762
2022-05-17 08:09:16 | INFO | train_inner | epoch 034:    985 / 2955 loss=4.648, ppl=25.07, wps=13657.5, ups=0.42, wpb=32768, bsz=64, num_updates=98500, lr=0.000100759, gnorm=0.752, train_wall=235, gb_free=5.1, wall=52002
2022-05-17 08:13:09 | INFO | train_inner | epoch 034:   1085 / 2955 loss=4.62, ppl=24.59, wps=14066.5, ups=0.43, wpb=32768, bsz=64, num_updates=98600, lr=0.000100707, gnorm=0.75, train_wall=226, gb_free=5.1, wall=52235
2022-05-17 08:17:04 | INFO | train_inner | epoch 034:   1185 / 2955 loss=4.639, ppl=24.92, wps=13916.4, ups=0.42, wpb=32768, bsz=64, num_updates=98700, lr=0.000100656, gnorm=0.755, train_wall=226, gb_free=5.1, wall=52471
2022-05-17 08:21:03 | INFO | train_inner | epoch 034:   1285 / 2955 loss=4.649, ppl=25.1, wps=13729.4, ups=0.42, wpb=32768, bsz=64, num_updates=98800, lr=0.000100605, gnorm=0.748, train_wall=230, gb_free=5.1, wall=52709
2022-05-17 08:25:00 | INFO | train_inner | epoch 034:   1385 / 2955 loss=4.663, ppl=25.33, wps=13787.7, ups=0.42, wpb=32768, bsz=64, num_updates=98900, lr=0.000100555, gnorm=0.756, train_wall=227, gb_free=5.1, wall=52947
2022-05-17 08:28:59 | INFO | train_inner | epoch 034:   1485 / 2955 loss=4.651, ppl=25.12, wps=13729.7, ups=0.42, wpb=32763, bsz=64, num_updates=99000, lr=0.000100504, gnorm=0.75, train_wall=227, gb_free=5.1, wall=53186
2022-05-17 08:32:58 | INFO | train_inner | epoch 034:   1585 / 2955 loss=4.632, ppl=24.8, wps=13731.4, ups=0.42, wpb=32768, bsz=64, num_updates=99100, lr=0.000100453, gnorm=0.752, train_wall=228, gb_free=5.1, wall=53424
2022-05-17 08:36:56 | INFO | train_inner | epoch 034:   1685 / 2955 loss=4.64, ppl=24.93, wps=13772.9, ups=0.42, wpb=32768, bsz=64, num_updates=99200, lr=0.000100402, gnorm=0.754, train_wall=226, gb_free=5.1, wall=53662
2022-05-17 08:40:53 | INFO | train_inner | epoch 034:   1785 / 2955 loss=4.657, ppl=25.22, wps=13815.6, ups=0.42, wpb=32768, bsz=64, num_updates=99300, lr=0.000100352, gnorm=0.749, train_wall=227, gb_free=5.1, wall=53900
2022-05-17 08:44:52 | INFO | train_inner | epoch 034:   1885 / 2955 loss=4.668, ppl=25.43, wps=13670.7, ups=0.42, wpb=32768, bsz=64, num_updates=99400, lr=0.000100301, gnorm=0.76, train_wall=228, gb_free=5.1, wall=54139
2022-05-17 08:48:51 | INFO | train_inner | epoch 034:   1985 / 2955 loss=4.647, ppl=25.06, wps=13733.5, ups=0.42, wpb=32768, bsz=64, num_updates=99500, lr=0.000100251, gnorm=0.752, train_wall=227, gb_free=5.1, wall=54378
2022-05-17 08:52:46 | INFO | train_inner | epoch 034:   2085 / 2955 loss=4.673, ppl=25.51, wps=13956.2, ups=0.43, wpb=32768, bsz=64, num_updates=99600, lr=0.000100201, gnorm=0.754, train_wall=227, gb_free=5.1, wall=54613
2022-05-17 08:56:41 | INFO | train_inner | epoch 034:   2185 / 2955 loss=4.668, ppl=25.43, wps=13933, ups=0.43, wpb=32768, bsz=64, num_updates=99700, lr=0.00010015, gnorm=0.757, train_wall=228, gb_free=5.1, wall=54848
2022-05-17 09:00:34 | INFO | train_inner | epoch 034:   2285 / 2955 loss=4.676, ppl=25.56, wps=14042.4, ups=0.43, wpb=32768, bsz=64, num_updates=99800, lr=0.0001001, gnorm=0.753, train_wall=227, gb_free=5.1, wall=55081
2022-05-17 09:04:33 | INFO | train_inner | epoch 034:   2385 / 2955 loss=4.678, ppl=25.6, wps=13760.8, ups=0.42, wpb=32768, bsz=64, num_updates=99900, lr=0.00010005, gnorm=0.757, train_wall=231, gb_free=5.1, wall=55319
2022-05-17 09:08:33 | INFO | train_inner | epoch 034:   2485 / 2955 loss=4.683, ppl=25.69, wps=13644.8, ups=0.42, wpb=32768, bsz=64, num_updates=100000, lr=0.0001, gnorm=0.753, train_wall=227, gb_free=5.1, wall=55559
2022-05-17 09:12:29 | INFO | train_inner | epoch 034:   2585 / 2955 loss=4.682, ppl=25.67, wps=13857.5, ups=0.42, wpb=32768, bsz=64, num_updates=100100, lr=9.995e-05, gnorm=0.756, train_wall=226, gb_free=5.1, wall=55796
2022-05-17 09:16:29 | INFO | train_inner | epoch 034:   2685 / 2955 loss=4.682, ppl=25.67, wps=13668.1, ups=0.42, wpb=32768, bsz=64, num_updates=100200, lr=9.99001e-05, gnorm=0.756, train_wall=231, gb_free=5.1, wall=56036
2022-05-17 09:20:22 | INFO | train_inner | epoch 034:   2785 / 2955 loss=4.661, ppl=25.3, wps=14060, ups=0.43, wpb=32768, bsz=64, num_updates=100300, lr=9.98503e-05, gnorm=0.752, train_wall=227, gb_free=5.1, wall=56269
2022-05-17 09:24:14 | INFO | train_inner | epoch 034:   2885 / 2955 loss=4.672, ppl=25.49, wps=14104.2, ups=0.43, wpb=32768, bsz=64, num_updates=100400, lr=9.98006e-05, gnorm=0.756, train_wall=227, gb_free=5.1, wall=56501
2022-05-17 09:26:57 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-05-17 09:29:12 | INFO | valid | epoch 034 | valid on 'valid' subset | loss 4.746 | ppl 26.84 | wps 39613.7 | wpb 2047.4 | bsz 4 | num_updates 100470 | best_loss 4.746
2022-05-17 09:29:12 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 34 @ 100470 updates
2022-05-17 09:29:12 | INFO | fairseq.trainer | Saving checkpoint to /cluster/work/cotterell/liam/master-thesis/checkpoints/language_model/standard/checkpoint_best_standard_16_5.0000E-04_full.pt
2022-05-17 09:29:15 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/work/cotterell/liam/master-thesis/checkpoints/language_model/standard/checkpoint_best_standard_16_5.0000E-04_full.pt
2022-05-17 09:29:15 | INFO | fairseq.checkpoint_utils | Saved checkpoint checkpoints/language_model/standard/checkpoint_best_standard_16_5.0000E-04_full.pt (epoch 34 @ 100470 updates, score 4.746) (writing took 3.060201868414879 seconds)
2022-05-17 09:29:15 | INFO | fairseq_cli.train | end of epoch 34 (average epoch stats below)
2022-05-17 09:29:15 | INFO | train | epoch 034 | loss 4.65 | ppl 25.1 | wps 13540.3 | ups 0.41 | wpb 32764.7 | bsz 64 | num_updates 100470 | lr 9.97658e-05 | gnorm 0.753 | train_wall 6744 | gb_free 5.1 | wall 56802
2022-05-17 09:29:16 | INFO | fairseq.data.iterators | grouped total_num_itrs = 2955
2022-05-17 09:29:16 | INFO | fairseq.trainer | begin training epoch 35
2022-05-17 09:29:16 | INFO | fairseq_cli.train | Start iterating over samples
2022-05-17 09:30:27 | INFO | train_inner | epoch 035:     30 / 2955 loss=4.644, ppl=25, wps=8767, ups=0.27, wpb=32686.1, bsz=63.8, num_updates=100500, lr=9.97509e-05, gnorm=0.753, train_wall=227, gb_free=5.1, wall=56874
2022-05-17 09:34:18 | INFO | train_inner | epoch 035:    130 / 2955 loss=4.625, ppl=24.68, wps=14210.9, ups=0.43, wpb=32768, bsz=64, num_updates=100600, lr=9.97013e-05, gnorm=0.753, train_wall=227, gb_free=5.1, wall=57104
2022-05-17 09:38:10 | INFO | train_inner | epoch 035:    230 / 2955 loss=4.615, ppl=24.5, wps=14091.5, ups=0.43, wpb=32768, bsz=64, num_updates=100700, lr=9.96518e-05, gnorm=0.753, train_wall=228, gb_free=5.1, wall=57337
2022-05-17 09:42:02 | INFO | train_inner | epoch 035:    330 / 2955 loss=4.61, ppl=24.41, wps=14151.5, ups=0.43, wpb=32768, bsz=64, num_updates=100800, lr=9.96024e-05, gnorm=0.753, train_wall=227, gb_free=5.1, wall=57569
2022-05-17 09:45:52 | INFO | train_inner | epoch 035:    430 / 2955 loss=4.621, ppl=24.6, wps=14208.6, ups=0.43, wpb=32768, bsz=64, num_updates=100900, lr=9.9553e-05, gnorm=0.757, train_wall=227, gb_free=5.1, wall=57799
2022-05-17 09:49:48 | INFO | train_inner | epoch 035:    530 / 2955 loss=4.629, ppl=24.75, wps=13895.3, ups=0.42, wpb=32763, bsz=64, num_updates=101000, lr=9.95037e-05, gnorm=0.76, train_wall=230, gb_free=5.1, wall=58035
2022-05-17 09:53:40 | INFO | train_inner | epoch 035:    630 / 2955 loss=4.638, ppl=24.9, wps=14115.2, ups=0.43, wpb=32768, bsz=64, num_updates=101100, lr=9.94545e-05, gnorm=0.756, train_wall=228, gb_free=5.1, wall=58267
2022-05-17 09:57:30 | INFO | train_inner | epoch 035:    730 / 2955 loss=4.63, ppl=24.76, wps=14291.8, ups=0.44, wpb=32768, bsz=64, num_updates=101200, lr=9.94053e-05, gnorm=0.756, train_wall=226, gb_free=5.1, wall=58496
2022-05-17 10:01:29 | INFO | train_inner | epoch 035:    830 / 2955 loss=4.623, ppl=24.64, wps=13699.3, ups=0.42, wpb=32768, bsz=64, num_updates=101300, lr=9.93563e-05, gnorm=0.757, train_wall=233, gb_free=5.1, wall=58736
2022-05-17 10:05:18 | INFO | train_inner | epoch 035:    930 / 2955 loss=4.645, ppl=25.03, wps=14314.5, ups=0.44, wpb=32768, bsz=64, num_updates=101400, lr=9.93073e-05, gnorm=0.755, train_wall=226, gb_free=5.1, wall=58965
2022-05-17 10:09:10 | INFO | train_inner | epoch 035:   1030 / 2955 loss=4.636, ppl=24.86, wps=14098.7, ups=0.43, wpb=32768, bsz=64, num_updates=101500, lr=9.92583e-05, gnorm=0.757, train_wall=229, gb_free=5.1, wall=59197
2022-05-17 10:13:03 | INFO | train_inner | epoch 035:   1130 / 2955 loss=4.626, ppl=24.7, wps=14062.2, ups=0.43, wpb=32768, bsz=64, num_updates=101600, lr=9.92095e-05, gnorm=0.758, train_wall=229, gb_free=5.1, wall=59430
2022-05-17 10:16:54 | INFO | train_inner | epoch 035:   1230 / 2955 loss=4.643, ppl=24.99, wps=14208, ups=0.43, wpb=32768, bsz=64, num_updates=101700, lr=9.91607e-05, gnorm=0.758, train_wall=227, gb_free=5.1, wall=59661
2022-05-17 10:20:56 | INFO | train_inner | epoch 035:   1330 / 2955 loss=4.636, ppl=24.86, wps=13554.9, ups=0.41, wpb=32768, bsz=64, num_updates=101800, lr=9.9112e-05, gnorm=0.758, train_wall=232, gb_free=5.1, wall=59902
2022-05-17 10:24:51 | INFO | train_inner | epoch 035:   1430 / 2955 loss=4.648, ppl=25.07, wps=13912.5, ups=0.42, wpb=32757.8, bsz=64, num_updates=101900, lr=9.90633e-05, gnorm=0.757, train_wall=226, gb_free=5.1, wall=60138
2022-05-17 10:28:46 | INFO | train_inner | epoch 035:   1530 / 2955 loss=4.666, ppl=25.39, wps=13974.2, ups=0.43, wpb=32768, bsz=64, num_updates=102000, lr=9.90148e-05, gnorm=0.757, train_wall=228, gb_free=5.1, wall=60372
2022-05-17 10:32:40 | INFO | train_inner | epoch 035:   1630 / 2955 loss=4.642, ppl=24.96, wps=13962.5, ups=0.43, wpb=32768, bsz=64, num_updates=102100, lr=9.89663e-05, gnorm=0.758, train_wall=227, gb_free=5.1, wall=60607
2022-05-17 10:36:33 | INFO | train_inner | epoch 035:   1730 / 2955 loss=4.639, ppl=24.92, wps=14053.3, ups=0.43, wpb=32768, bsz=64, num_updates=102200, lr=9.89178e-05, gnorm=0.761, train_wall=227, gb_free=5.1, wall=60840
2022-05-17 10:40:33 | INFO | train_inner | epoch 035:   1830 / 2955 loss=4.663, ppl=25.34, wps=13671.5, ups=0.42, wpb=32768, bsz=64, num_updates=102300, lr=9.88695e-05, gnorm=0.764, train_wall=230, gb_free=5.1, wall=61080
2022-05-17 10:44:31 | INFO | train_inner | epoch 035:   1930 / 2955 loss=4.659, ppl=25.26, wps=13797.3, ups=0.42, wpb=32768, bsz=64, num_updates=102400, lr=9.88212e-05, gnorm=0.764, train_wall=227, gb_free=5.1, wall=61317
2022-05-17 10:48:27 | INFO | train_inner | epoch 035:   2030 / 2955 loss=4.653, ppl=25.16, wps=13860.4, ups=0.42, wpb=32768, bsz=64, num_updates=102500, lr=9.8773e-05, gnorm=0.756, train_wall=228, gb_free=5.1, wall=61554
2022-05-17 10:52:21 | INFO | train_inner | epoch 035:   2130 / 2955 loss=4.648, ppl=25.07, wps=13984.4, ups=0.43, wpb=32768, bsz=64, num_updates=102600, lr=9.87248e-05, gnorm=0.759, train_wall=227, gb_free=5.1, wall=61788
2022-05-17 10:56:15 | INFO | train_inner | epoch 035:   2230 / 2955 loss=4.669, ppl=25.44, wps=14054.1, ups=0.43, wpb=32768, bsz=64, num_updates=102700, lr=9.86767e-05, gnorm=0.76, train_wall=227, gb_free=5.1, wall=62021
2022-05-17 11:00:12 | INFO | train_inner | epoch 035:   2330 / 2955 loss=4.658, ppl=25.25, wps=13816.6, ups=0.42, wpb=32768, bsz=64, num_updates=102800, lr=9.86287e-05, gnorm=0.76, train_wall=232, gb_free=5.1, wall=62258
2022-05-17 11:04:03 | INFO | train_inner | epoch 035:   2430 / 2955 loss=4.664, ppl=25.36, wps=14155.4, ups=0.43, wpb=32768, bsz=64, num_updates=102900, lr=9.85808e-05, gnorm=0.754, train_wall=226, gb_free=5.1, wall=62490
2022-05-17 11:07:54 | INFO | train_inner | epoch 035:   2530 / 2955 loss=4.667, ppl=25.41, wps=14207.1, ups=0.43, wpb=32768, bsz=64, num_updates=103000, lr=9.85329e-05, gnorm=0.759, train_wall=227, gb_free=5.1, wall=62721
2022-05-17 11:11:50 | INFO | train_inner | epoch 035:   2630 / 2955 loss=4.67, ppl=25.45, wps=13899.6, ups=0.42, wpb=32768, bsz=64, num_updates=103100, lr=9.84851e-05, gnorm=0.762, train_wall=230, gb_free=5.1, wall=62956
2022-05-17 11:15:48 | INFO | train_inner | epoch 035:   2730 / 2955 loss=4.676, ppl=25.56, wps=13728, ups=0.42, wpb=32768, bsz=64, num_updates=103200, lr=9.84374e-05, gnorm=0.757, train_wall=228, gb_free=5.1, wall=63195
2022-05-17 11:19:48 | INFO | train_inner | epoch 035:   2830 / 2955 loss=4.656, ppl=25.22, wps=13662.4, ups=0.42, wpb=32768, bsz=64, num_updates=103300, lr=9.83897e-05, gnorm=0.76, train_wall=231, gb_free=5.1, wall=63435
2022-05-17 11:23:46 | INFO | train_inner | epoch 035:   2930 / 2955 loss=4.669, ppl=25.43, wps=13791.6, ups=0.42, wpb=32768, bsz=64, num_updates=103400, lr=9.83422e-05, gnorm=0.761, train_wall=228, gb_free=5.1, wall=63672
2022-05-17 11:24:44 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-05-17 11:27:00 | INFO | valid | epoch 035 | valid on 'valid' subset | loss 4.745 | ppl 26.81 | wps 39824.9 | wpb 2047.4 | bsz 4 | num_updates 103425 | best_loss 4.745
2022-05-17 11:27:00 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 35 @ 103425 updates
2022-05-17 11:27:00 | INFO | fairseq.trainer | Saving checkpoint to /cluster/work/cotterell/liam/master-thesis/checkpoints/language_model/standard/checkpoint_best_standard_16_5.0000E-04_full.pt
2022-05-17 11:27:02 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/work/cotterell/liam/master-thesis/checkpoints/language_model/standard/checkpoint_best_standard_16_5.0000E-04_full.pt
2022-05-17 11:27:02 | INFO | fairseq.checkpoint_utils | Saved checkpoint checkpoints/language_model/standard/checkpoint_best_standard_16_5.0000E-04_full.pt (epoch 35 @ 103425 updates, score 4.745) (writing took 2.41389994090423 seconds)
2022-05-17 11:27:02 | INFO | fairseq_cli.train | end of epoch 35 (average epoch stats below)
2022-05-17 11:27:02 | INFO | train | epoch 035 | loss 4.645 | ppl 25.03 | wps 13701.2 | ups 0.42 | wpb 32764.7 | bsz 64 | num_updates 103425 | lr 9.83303e-05 | gnorm 0.758 | train_wall 6743 | gb_free 5.1 | wall 63869
2022-05-17 11:27:02 | INFO | fairseq.data.iterators | grouped total_num_itrs = 2955
2022-05-17 11:27:02 | INFO | fairseq.trainer | begin training epoch 36
2022-05-17 11:27:02 | INFO | fairseq_cli.train | Start iterating over samples
2022-05-17 11:29:58 | INFO | train_inner | epoch 036:     75 / 2955 loss=4.609, ppl=24.41, wps=8783, ups=0.27, wpb=32675.8, bsz=63.8, num_updates=103500, lr=9.82946e-05, gnorm=0.765, train_wall=227, gb_free=5.1, wall=64044
2022-05-17 11:33:57 | INFO | train_inner | epoch 036:    175 / 2955 loss=4.607, ppl=24.37, wps=13683.2, ups=0.42, wpb=32768, bsz=64, num_updates=103600, lr=9.82472e-05, gnorm=0.756, train_wall=232, gb_free=5.1, wall=64284
2022-05-17 11:37:56 | INFO | train_inner | epoch 036:    275 / 2955 loss=4.628, ppl=24.72, wps=13725.5, ups=0.42, wpb=32768, bsz=64, num_updates=103700, lr=9.81998e-05, gnorm=0.758, train_wall=227, gb_free=5.1, wall=64523
2022-05-17 11:42:00 | INFO | train_inner | epoch 036:    375 / 2955 loss=4.617, ppl=24.53, wps=13438.3, ups=0.41, wpb=32768, bsz=64, num_updates=103800, lr=9.81525e-05, gnorm=0.756, train_wall=233, gb_free=5.1, wall=64767
2022-05-17 11:45:59 | INFO | train_inner | epoch 036:    475 / 2955 loss=4.626, ppl=24.69, wps=13712.4, ups=0.42, wpb=32768, bsz=64, num_updates=103900, lr=9.81052e-05, gnorm=0.763, train_wall=227, gb_free=5.1, wall=65006
2022-05-17 11:49:58 | INFO | train_inner | epoch 036:    575 / 2955 loss=4.616, ppl=24.52, wps=13698.5, ups=0.42, wpb=32768, bsz=64, num_updates=104000, lr=9.80581e-05, gnorm=0.763, train_wall=226, gb_free=5.1, wall=65245
2022-05-17 11:54:03 | INFO | train_inner | epoch 036:    675 / 2955 loss=4.628, ppl=24.72, wps=13387, ups=0.41, wpb=32768, bsz=64, num_updates=104100, lr=9.8011e-05, gnorm=0.761, train_wall=233, gb_free=5.1, wall=65490
2022-05-17 11:58:04 | INFO | train_inner | epoch 036:    775 / 2955 loss=4.627, ppl=24.71, wps=13578.5, ups=0.41, wpb=32768, bsz=64, num_updates=104200, lr=9.79639e-05, gnorm=0.758, train_wall=228, gb_free=5.1, wall=65731
2022-05-17 12:02:06 | INFO | train_inner | epoch 036:    875 / 2955 loss=4.636, ppl=24.87, wps=13544, ups=0.41, wpb=32763, bsz=64, num_updates=104300, lr=9.79169e-05, gnorm=0.76, train_wall=230, gb_free=5.1, wall=65973
2022-05-17 12:06:02 | INFO | train_inner | epoch 036:    975 / 2955 loss=4.628, ppl=24.72, wps=13896, ups=0.42, wpb=32768, bsz=64, num_updates=104400, lr=9.787e-05, gnorm=0.761, train_wall=227, gb_free=5.1, wall=66209
2022-05-17 12:09:57 | INFO | train_inner | epoch 036:   1075 / 2955 loss=4.646, ppl=25.05, wps=13911.5, ups=0.42, wpb=32768, bsz=64, num_updates=104500, lr=9.78232e-05, gnorm=0.758, train_wall=227, gb_free=5.1, wall=66444
2022-05-17 12:13:59 | INFO | train_inner | epoch 036:   1175 / 2955 loss=4.638, ppl=24.89, wps=13590.7, ups=0.41, wpb=32768, bsz=64, num_updates=104600, lr=9.77764e-05, gnorm=0.759, train_wall=234, gb_free=5.1, wall=66685
2022-05-17 12:17:59 | INFO | train_inner | epoch 036:   1275 / 2955 loss=4.643, ppl=24.98, wps=13609.3, ups=0.42, wpb=32768, bsz=64, num_updates=104700, lr=9.77297e-05, gnorm=0.768, train_wall=228, gb_free=5.1, wall=66926
2022-05-17 12:21:59 | INFO | train_inner | epoch 036:   1375 / 2955 loss=4.649, ppl=25.1, wps=13670.2, ups=0.42, wpb=32768, bsz=64, num_updates=104800, lr=9.76831e-05, gnorm=0.763, train_wall=228, gb_free=5.1, wall=67166
2022-05-17 12:26:00 | INFO | train_inner | epoch 036:   1475 / 2955 loss=4.647, ppl=25.05, wps=13595.3, ups=0.41, wpb=32768, bsz=64, num_updates=104900, lr=9.76365e-05, gnorm=0.765, train_wall=229, gb_free=5.1, wall=67407
2022-05-17 12:29:59 | INFO | train_inner | epoch 036:   1575 / 2955 loss=4.648, ppl=25.08, wps=13709.1, ups=0.42, wpb=32768, bsz=64, num_updates=105000, lr=9.759e-05, gnorm=0.763, train_wall=227, gb_free=5.1, wall=67646
2022-05-17 12:34:03 | INFO | train_inner | epoch 036:   1675 / 2955 loss=4.645, ppl=25.01, wps=13442.7, ups=0.41, wpb=32768, bsz=64, num_updates=105100, lr=9.75436e-05, gnorm=0.762, train_wall=234, gb_free=5.1, wall=67890
2022-05-17 12:37:59 | INFO | train_inner | epoch 036:   1775 / 2955 loss=4.65, ppl=25.1, wps=13894.6, ups=0.42, wpb=32768, bsz=64, num_updates=105200, lr=9.74972e-05, gnorm=0.759, train_wall=228, gb_free=5.1, wall=68125
2022-05-17 12:41:55 | INFO | train_inner | epoch 036:   1875 / 2955 loss=4.642, ppl=24.97, wps=13868.7, ups=0.42, wpb=32768, bsz=64, num_updates=105300, lr=9.74509e-05, gnorm=0.767, train_wall=228, gb_free=5.1, wall=68362
2022-05-17 12:45:54 | INFO | train_inner | epoch 036:   1975 / 2955 loss=4.639, ppl=24.92, wps=13678.1, ups=0.42, wpb=32768, bsz=64, num_updates=105400, lr=9.74047e-05, gnorm=0.763, train_wall=229, gb_free=5.1, wall=68601
2022-05-17 12:49:52 | INFO | train_inner | epoch 036:   2075 / 2955 loss=4.643, ppl=24.99, wps=13822.8, ups=0.42, wpb=32768, bsz=64, num_updates=105500, lr=9.73585e-05, gnorm=0.76, train_wall=227, gb_free=5.1, wall=68838
2022-05-17 12:53:57 | INFO | train_inner | epoch 036:   2175 / 2955 loss=4.635, ppl=24.85, wps=13338.1, ups=0.41, wpb=32768, bsz=64, num_updates=105600, lr=9.73124e-05, gnorm=0.761, train_wall=234, gb_free=5.1, wall=69084
2022-05-17 12:57:57 | INFO | train_inner | epoch 036:   2275 / 2955 loss=4.669, ppl=25.43, wps=13641.6, ups=0.42, wpb=32768, bsz=64, num_updates=105700, lr=9.72663e-05, gnorm=0.767, train_wall=228, gb_free=5.1, wall=69324
2022-05-17 13:01:57 | INFO | train_inner | epoch 036:   2375 / 2955 loss=4.664, ppl=25.36, wps=13684.4, ups=0.42, wpb=32768, bsz=64, num_updates=105800, lr=9.72203e-05, gnorm=0.761, train_wall=228, gb_free=5.1, wall=69564
2022-05-17 13:06:03 | INFO | train_inner | epoch 036:   2475 / 2955 loss=4.661, ppl=25.3, wps=13325.6, ups=0.41, wpb=32768, bsz=64, num_updates=105900, lr=9.71744e-05, gnorm=0.765, train_wall=233, gb_free=5.1, wall=69810
2022-05-17 13:10:00 | INFO | train_inner | epoch 036:   2575 / 2955 loss=4.65, ppl=25.11, wps=13811.1, ups=0.42, wpb=32768, bsz=64, num_updates=106000, lr=9.71286e-05, gnorm=0.763, train_wall=227, gb_free=5.1, wall=70047
2022-05-17 13:14:02 | INFO | train_inner | epoch 036:   2675 / 2955 loss=4.669, ppl=25.45, wps=13563.1, ups=0.41, wpb=32768, bsz=64, num_updates=106100, lr=9.70828e-05, gnorm=0.767, train_wall=231, gb_free=5.1, wall=70288
2022-05-17 13:18:02 | INFO | train_inner | epoch 036:   2775 / 2955 loss=4.661, ppl=25.29, wps=13648.8, ups=0.42, wpb=32768, bsz=64, num_updates=106200, lr=9.70371e-05, gnorm=0.761, train_wall=229, gb_free=5.1, wall=70528
2022-05-17 13:22:00 | INFO | train_inner | epoch 036:   2875 / 2955 loss=4.664, ppl=25.35, wps=13770.7, ups=0.42, wpb=32768, bsz=64, num_updates=106300, lr=9.69914e-05, gnorm=0.765, train_wall=228, gb_free=5.1, wall=70766
2022-05-17 13:25:11 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-05-17 13:27:27 | INFO | valid | epoch 036 | valid on 'valid' subset | loss 4.742 | ppl 26.75 | wps 39356 | wpb 2047.4 | bsz 4 | num_updates 106380 | best_loss 4.742
2022-05-17 13:27:27 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 36 @ 106380 updates
2022-05-17 13:27:27 | INFO | fairseq.trainer | Saving checkpoint to /cluster/work/cotterell/liam/master-thesis/checkpoints/language_model/standard/checkpoint_best_standard_16_5.0000E-04_full.pt
2022-05-17 13:27:30 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/work/cotterell/liam/master-thesis/checkpoints/language_model/standard/checkpoint_best_standard_16_5.0000E-04_full.pt
2022-05-17 13:27:30 | INFO | fairseq.checkpoint_utils | Saved checkpoint checkpoints/language_model/standard/checkpoint_best_standard_16_5.0000E-04_full.pt (epoch 36 @ 106380 updates, score 4.742) (writing took 2.7278086887672544 seconds)
2022-05-17 13:27:30 | INFO | fairseq_cli.train | end of epoch 36 (average epoch stats below)
2022-05-17 13:27:30 | INFO | train | epoch 036 | loss 4.641 | ppl 24.95 | wps 13394.8 | ups 0.41 | wpb 32764.7 | bsz 64 | num_updates 106380 | lr 9.6955e-05 | gnorm 0.762 | train_wall 6772 | gb_free 5.1 | wall 71097
2022-05-17 13:27:30 | INFO | fairseq.data.iterators | grouped total_num_itrs = 2955
2022-05-17 13:27:30 | INFO | fairseq.trainer | begin training epoch 37
2022-05-17 13:27:30 | INFO | fairseq_cli.train | Start iterating over samples
2022-05-17 13:28:20 | INFO | train_inner | epoch 037:     20 / 2955 loss=4.652, ppl=25.14, wps=8602.5, ups=0.26, wpb=32686.1, bsz=63.8, num_updates=106400, lr=9.69458e-05, gnorm=0.765, train_wall=228, gb_free=5.1, wall=71146
2022-05-17 13:32:21 | INFO | train_inner | epoch 037:    120 / 2955 loss=4.612, ppl=24.46, wps=13558.5, ups=0.41, wpb=32768, bsz=64, num_updates=106500, lr=9.69003e-05, gnorm=0.759, train_wall=228, gb_free=5.1, wall=71388
2022-05-17 13:36:21 | INFO | train_inner | epoch 037:    220 / 2955 loss=4.609, ppl=24.4, wps=13675.7, ups=0.42, wpb=32768, bsz=64, num_updates=106600, lr=9.68549e-05, gnorm=0.764, train_wall=229, gb_free=5.1, wall=71628
User defined signal 2
