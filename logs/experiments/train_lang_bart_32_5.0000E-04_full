Sender: LSF System <lsfadmin@eu-g3-030>
Subject: Job 218889028: <train_lang_bart_32_5.0000E-04_full> in cluster <euler> Exited

Job <train_lang_bart_32_5.0000E-04_full> was submitted from host <eu-login-33> by user <euler_username> in cluster <euler> at Tue May 17 21:43:58 2022
Job was executed on host(s) <4*eu-g3-030>, in queue <gpu.120h>, as user <euler_username> in cluster <euler> at Tue May 17 21:44:21 2022
</cluster/home/euler_username> was used as the home directory.
</cluster/work/cotterell/liam/master-thesis> was used as the working directory.
Started at Tue May 17 21:44:21 2022
Terminated at Wed May 18 06:45:13 2022
Results reported at Wed May 18 06:45:13 2022

Your job looked like:

------------------------------------------------------------
# LSBATCH: User input
fairseq-train data/xsum-lang-bart-full --save-dir checkpoints/language_model/bart --update-freq 32 --lr 0.0005 --checkpoint-suffix _bart_32_5.0000E-04_full --task language_modeling --arch transformer_lm --share-decoder-input-output-embed --dropout 0.1 --optimizer adam --adam-betas "(0.9, 0.98)" --weight-decay 0.01 --clip-norm 0.0 --lr-scheduler inverse_sqrt --warmup-updates 4000 --warmup-init-lr 1e-07 --tokens-per-sample 512 --sample-break-mode none --max-tokens 2048 --no-epoch-checkpoints --no-last-checkpoints --patience 5
------------------------------------------------------------

Exited with exit code 1.

Resource usage summary:

    CPU time :                                   27332.98 sec.
    Max Memory :                                 4251 MB
    Average Memory :                             2604.36 MB
    Total Requested Memory :                     8192.00 MB
    Delta Memory :                               3941.00 MB
    Max Swap :                                   726 MB
    Max Processes :                              4
    Max Threads :                                14
    Run time :                                   32452 sec.
    Turnaround time :                            32475 sec.

The output (if any) follows:

2022-05-17 21:46:39 | INFO | fairseq.tasks.text_to_speech | Please install tensorboardX: pip install tensorboardX
2022-05-17 21:46:53 | INFO | fairseq_cli.train | {'_name': None, 'common': {'_name': None, 'no_progress_bar': False, 'log_interval': 100, 'log_format': None, 'log_file': None, 'tensorboard_logdir': None, 'wandb_project': None, 'azureml_logging': False, 'seed': 1, 'cpu': False, 'tpu': False, 'bf16': False, 'memory_efficient_bf16': False, 'fp16': False, 'memory_efficient_fp16': False, 'fp16_no_flatten_grads': False, 'fp16_init_scale': 128, 'fp16_scale_window': None, 'fp16_scale_tolerance': 0.0, 'on_cpu_convert_precision': False, 'min_loss_scale': 0.0001, 'threshold_loss_scale': None, 'amp': False, 'amp_batch_retries': 2, 'amp_init_scale': 128, 'amp_scale_window': None, 'user_dir': None, 'empty_cache_freq': 0, 'all_gather_list_size': 16384, 'model_parallel_size': 1, 'quantization_config_path': None, 'profile': False, 'reset_logging': False, 'suppress_crashes': False, 'use_plasma_view': False, 'plasma_path': '/tmp/plasma'}, 'common_eval': {'_name': None, 'path': None, 'post_process': None, 'quiet': False, 'model_overrides': '{}', 'results_path': None, 'print_tokens': False}, 'distributed_training': {'_name': None, 'distributed_world_size': 1, 'distributed_num_procs': 1, 'distributed_rank': 0, 'distributed_backend': 'nccl', 'distributed_init_method': None, 'distributed_port': -1, 'device_id': 0, 'distributed_no_spawn': False, 'ddp_backend': 'pytorch_ddp', 'ddp_comm_hook': 'none', 'bucket_cap_mb': 25, 'fix_batches_to_gpus': False, 'find_unused_parameters': False, 'gradient_as_bucket_view': False, 'fast_stat_sync': False, 'heartbeat_timeout': -1, 'broadcast_buffers': False, 'slowmo_momentum': None, 'slowmo_base_algorithm': 'localsgd', 'localsgd_frequency': 3, 'nprocs_per_node': 1, 'pipeline_model_parallel': False, 'pipeline_balance': None, 'pipeline_devices': None, 'pipeline_chunks': 0, 'pipeline_encoder_balance': None, 'pipeline_encoder_devices': None, 'pipeline_decoder_balance': None, 'pipeline_decoder_devices': None, 'pipeline_checkpoint': 'never', 'zero_sharding': 'none', 'fp16': False, 'memory_efficient_fp16': False, 'tpu': False, 'no_reshard_after_forward': False, 'fp32_reduce_scatter': False, 'cpu_offload': False, 'use_sharded_state': False, 'not_fsdp_flatten_parameters': False}, 'dataset': {'_name': None, 'num_workers': 1, 'skip_invalid_size_inputs_valid_test': False, 'max_tokens': 2048, 'batch_size': None, 'required_batch_size_multiple': 8, 'required_seq_len_multiple': 1, 'dataset_impl': None, 'data_buffer_size': 10, 'train_subset': 'train', 'valid_subset': 'valid', 'combine_valid_subsets': None, 'ignore_unused_valid_subsets': False, 'validate_interval': 1, 'validate_interval_updates': 0, 'validate_after_updates': 0, 'fixed_validation_seed': None, 'disable_validation': False, 'max_tokens_valid': 2048, 'batch_size_valid': None, 'max_valid_steps': None, 'curriculum': 0, 'gen_subset': 'test', 'num_shards': 1, 'shard_id': 0, 'grouped_shuffling': False, 'update_epoch_batch_itr': False, 'update_ordered_indices_seed': False}, 'optimization': {'_name': None, 'max_epoch': 0, 'max_update': 0, 'stop_time_hours': 0.0, 'clip_norm': 0.0, 'sentence_avg': False, 'update_freq': [32], 'lr': [0.0005], 'stop_min_lr': -1.0, 'use_bmuf': False, 'skip_remainder_batch': False}, 'checkpoint': {'_name': None, 'save_dir': 'checkpoints/language_model/bart', 'restore_file': 'checkpoint_last.pt', 'continue_once': None, 'finetune_from_model': None, 'reset_dataloader': False, 'reset_lr_scheduler': False, 'reset_meters': False, 'reset_optimizer': False, 'optimizer_overrides': '{}', 'save_interval': 1, 'save_interval_updates': 0, 'keep_interval_updates': -1, 'keep_interval_updates_pattern': -1, 'keep_last_epochs': -1, 'keep_best_checkpoints': -1, 'no_save': False, 'no_epoch_checkpoints': True, 'no_last_checkpoints': True, 'no_save_optimizer_state': False, 'best_checkpoint_metric': 'loss', 'maximize_best_checkpoint_metric': False, 'patience': 5, 'checkpoint_suffix': '_bart_32_5.0000E-04_full', 'checkpoint_shard_count': 1, 'load_checkpoint_on_all_dp_ranks': False, 'write_checkpoints_asynchronously': False, 'model_parallel_size': 1}, 'bmuf': {'_name': None, 'block_lr': 1.0, 'block_momentum': 0.875, 'global_sync_iter': 50, 'warmup_iterations': 500, 'use_nbm': False, 'average_sync': False, 'distributed_world_size': 1}, 'generation': {'_name': None, 'beam': 5, 'nbest': 1, 'max_len_a': 0.0, 'max_len_b': 200, 'min_len': 1, 'match_source_len': False, 'unnormalized': False, 'no_early_stop': False, 'no_beamable_mm': False, 'lenpen': 1.0, 'unkpen': 0.0, 'replace_unk': None, 'sacrebleu': False, 'score_reference': False, 'prefix_size': 0, 'no_repeat_ngram_size': 0, 'sampling': False, 'sampling_topk': -1, 'sampling_topp': -1.0, 'constraints': None, 'temperature': 1.0, 'diverse_beam_groups': -1, 'diverse_beam_strength': 0.5, 'diversity_rate': -1.0, 'print_alignment': None, 'print_step': False, 'lm_path': None, 'ent_threshold': 0.0, 'lm_weight': 0.0, 'iter_decode_eos_penalty': 0.0, 'iter_decode_max_iter': 10, 'iter_decode_force_max_iter': False, 'iter_decode_with_beam': 1, 'iter_decode_with_external_reranker': False, 'retain_iter_history': False, 'retain_dropout': False, 'retain_dropout_modules': None, 'decoding_format': None, 'no_seed_provided': False}, 'eval_lm': {'_name': None, 'output_word_probs': False, 'output_word_stats': False, 'context_window': 0, 'softmax_batch': 9223372036854775807}, 'interactive': {'_name': None, 'buffer_size': 0, 'input': '-'}, 'model': {'_name': 'transformer_lm', 'activation_fn': relu, 'dropout': 0.1, 'attention_dropout': 0.0, 'activation_dropout': 0.0, 'relu_dropout': 0.0, 'decoder_embed_dim': 512, 'decoder_output_dim': 512, 'decoder_input_dim': 512, 'decoder_ffn_embed_dim': 2048, 'decoder_layers': 6, 'decoder_attention_heads': 8, 'decoder_normalize_before': False, 'no_decoder_final_norm': False, 'adaptive_softmax_cutoff': None, 'adaptive_softmax_dropout': 0.0, 'adaptive_softmax_factor': 4.0, 'no_token_positional_embeddings': False, 'share_decoder_input_output_embed': True, 'character_embeddings': False, 'character_filters': '[(1, 64), (2, 128), (3, 192), (4, 256), (5, 256), (6, 256), (7, 256)]', 'character_embedding_dim': 4, 'char_embedder_highway_layers': 2, 'adaptive_input': False, 'adaptive_input_factor': 4.0, 'adaptive_input_cutoff': None, 'tie_adaptive_weights': False, 'tie_adaptive_proj': False, 'decoder_learned_pos': False, 'layernorm_embedding': False, 'no_scale_embedding': False, 'checkpoint_activations': False, 'offload_activations': False, 'decoder_layerdrop': 0.0, 'decoder_layers_to_keep': None, 'quant_noise_pq': 0.0, 'quant_noise_pq_block_size': 8, 'quant_noise_scalar': 0.0, 'min_params_to_wrap': 100000000, 'base_layers': 0, 'base_sublayers': 1, 'base_shuffle': 1, 'scale_fc': False, 'scale_attn': False, 'scale_heads': False, 'scale_resids': False, 'add_bos_token': False, 'tokens_per_sample': 512, 'max_target_positions': None, 'tpu': False}, 'task': {'_name': 'language_modeling', 'data': 'data/xsum-lang-bart-full', 'sample_break_mode': none, 'tokens_per_sample': 512, 'output_dictionary_size': -1, 'self_target': False, 'future_target': False, 'past_target': False, 'add_bos_token': False, 'max_target_positions': None, 'shorten_method': none, 'shorten_data_split_list': '', 'pad_to_fixed_length': False, 'pad_to_fixed_bsz': False, 'seed': 1, 'batch_size': None, 'batch_size_valid': None, 'dataset_impl': None, 'data_buffer_size': 10, 'tpu': False, 'use_plasma_view': False, 'plasma_path': '/tmp/plasma'}, 'criterion': {'_name': 'cross_entropy', 'sentence_avg': False}, 'optimizer': {'_name': 'adam', 'adam_betas': '(0.9, 0.98)', 'adam_eps': 1e-08, 'weight_decay': 0.01, 'use_old_adam': False, 'fp16_adam_stats': False, 'tpu': False, 'lr': [0.0005]}, 'lr_scheduler': {'_name': 'inverse_sqrt', 'warmup_updates': 4000, 'warmup_init_lr': 1e-07, 'lr': [0.0005]}, 'scoring': {'_name': 'bleu', 'pad': 1, 'eos': 2, 'unk': 3}, 'bpe': None, 'tokenizer': None, 'ema': {'_name': None, 'store_ema': False, 'ema_decay': 0.9999, 'ema_start_update': 0, 'ema_seed_model': None, 'ema_update_freq': 1, 'ema_fp32': False}, 'simul_type': None}
2022-05-17 21:46:53 | INFO | fairseq.tasks.language_modeling | dictionary: 50264 types
2022-05-17 21:46:56 | INFO | fairseq_cli.train | TransformerLanguageModel(
  (decoder): TransformerDecoder(
    (dropout_module): FairseqDropout()
    (embed_tokens): Embedding(50264, 512, padding_idx=1)
    (embed_positions): SinusoidalPositionalEmbedding()
    (layers): ModuleList(
      (0): TransformerDecoderLayerBase(
        (dropout_module): FairseqDropout()
        (self_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=512, out_features=512, bias=True)
          (v_proj): Linear(in_features=512, out_features=512, bias=True)
          (q_proj): Linear(in_features=512, out_features=512, bias=True)
          (out_proj): Linear(in_features=512, out_features=512, bias=True)
        )
        (activation_dropout_module): FairseqDropout()
        (self_attn_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
        (fc1): Linear(in_features=512, out_features=2048, bias=True)
        (fc2): Linear(in_features=2048, out_features=512, bias=True)
        (final_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
      )
      (1): TransformerDecoderLayerBase(
        (dropout_module): FairseqDropout()
        (self_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=512, out_features=512, bias=True)
          (v_proj): Linear(in_features=512, out_features=512, bias=True)
          (q_proj): Linear(in_features=512, out_features=512, bias=True)
          (out_proj): Linear(in_features=512, out_features=512, bias=True)
        )
        (activation_dropout_module): FairseqDropout()
        (self_attn_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
        (fc1): Linear(in_features=512, out_features=2048, bias=True)
        (fc2): Linear(in_features=2048, out_features=512, bias=True)
        (final_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
      )
      (2): TransformerDecoderLayerBase(
        (dropout_module): FairseqDropout()
        (self_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=512, out_features=512, bias=True)
          (v_proj): Linear(in_features=512, out_features=512, bias=True)
          (q_proj): Linear(in_features=512, out_features=512, bias=True)
          (out_proj): Linear(in_features=512, out_features=512, bias=True)
        )
        (activation_dropout_module): FairseqDropout()
        (self_attn_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
        (fc1): Linear(in_features=512, out_features=2048, bias=True)
        (fc2): Linear(in_features=2048, out_features=512, bias=True)
        (final_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
      )
      (3): TransformerDecoderLayerBase(
        (dropout_module): FairseqDropout()
        (self_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=512, out_features=512, bias=True)
          (v_proj): Linear(in_features=512, out_features=512, bias=True)
          (q_proj): Linear(in_features=512, out_features=512, bias=True)
          (out_proj): Linear(in_features=512, out_features=512, bias=True)
        )
        (activation_dropout_module): FairseqDropout()
        (self_attn_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
        (fc1): Linear(in_features=512, out_features=2048, bias=True)
        (fc2): Linear(in_features=2048, out_features=512, bias=True)
        (final_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
      )
      (4): TransformerDecoderLayerBase(
        (dropout_module): FairseqDropout()
        (self_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=512, out_features=512, bias=True)
          (v_proj): Linear(in_features=512, out_features=512, bias=True)
          (q_proj): Linear(in_features=512, out_features=512, bias=True)
          (out_proj): Linear(in_features=512, out_features=512, bias=True)
        )
        (activation_dropout_module): FairseqDropout()
        (self_attn_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
        (fc1): Linear(in_features=512, out_features=2048, bias=True)
        (fc2): Linear(in_features=2048, out_features=512, bias=True)
        (final_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
      )
      (5): TransformerDecoderLayerBase(
        (dropout_module): FairseqDropout()
        (self_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=512, out_features=512, bias=True)
          (v_proj): Linear(in_features=512, out_features=512, bias=True)
          (q_proj): Linear(in_features=512, out_features=512, bias=True)
          (out_proj): Linear(in_features=512, out_features=512, bias=True)
        )
        (activation_dropout_module): FairseqDropout()
        (self_attn_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
        (fc1): Linear(in_features=512, out_features=2048, bias=True)
        (fc2): Linear(in_features=2048, out_features=512, bias=True)
        (final_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
      )
    )
    (output_projection): Linear(in_features=512, out_features=50264, bias=False)
  )
)
2022-05-17 21:46:56 | INFO | fairseq_cli.train | task: LanguageModelingTask
2022-05-17 21:46:56 | INFO | fairseq_cli.train | model: TransformerLanguageModel
2022-05-17 21:46:56 | INFO | fairseq_cli.train | criterion: CrossEntropyCriterion
2022-05-17 21:46:56 | INFO | fairseq_cli.train | num. shared model params: 44,649,472 (num. trained: 44,649,472)
2022-05-17 21:46:56 | INFO | fairseq_cli.train | num. expert model params: 0 (num. trained: 0)
2022-05-17 21:46:56 | INFO | fairseq.data.data_utils | loaded 22,664 examples from: data/xsum-lang-bart-full/valid
2022-05-17 21:47:39 | INFO | fairseq.trainer | detected shared parameter: decoder.embed_tokens.weight <- decoder.output_projection.weight
2022-05-17 21:47:39 | INFO | fairseq.utils | ***********************CUDA enviroments for all 1 workers***********************
2022-05-17 21:47:39 | INFO | fairseq.utils | rank   0: capabilities =  7.5  ; total memory = 10.761 GB ; name = NVIDIA GeForce RTX 2080 Ti              
2022-05-17 21:47:39 | INFO | fairseq.utils | ***********************CUDA enviroments for all 1 workers***********************
2022-05-17 21:47:39 | INFO | fairseq_cli.train | training on 1 devices (GPUs/TPUs)
2022-05-17 21:47:39 | INFO | fairseq_cli.train | max tokens per device = 2048 and max sentences per device = None
2022-05-17 21:47:39 | INFO | fairseq.trainer | Preparing to load checkpoint checkpoints/language_model/bart/checkpoint_last_bart_32_5.0000E-04_full.pt
2022-05-17 21:47:39 | INFO | fairseq.trainer | No existing checkpoint found checkpoints/language_model/bart/checkpoint_last_bart_32_5.0000E-04_full.pt
2022-05-17 21:47:39 | INFO | fairseq.trainer | loading train data for epoch 1
2022-05-17 21:47:39 | INFO | fairseq.data.data_utils | loaded 408,090 examples from: data/xsum-lang-bart-full/train
2022-05-17 21:47:40 | INFO | fairseq.trainer | NOTE: your device may support faster training with --fp16 or --amp
2022-05-17 21:47:40 | INFO | fairseq.data.iterators | grouped total_num_itrs = 1550
2022-05-17 21:47:40 | INFO | fairseq.trainer | begin training epoch 1
2022-05-17 21:47:40 | INFO | fairseq_cli.train | Start iterating over samples
2022-05-17 21:51:32 | INFO | train_inner | epoch 001:    100 / 1550 loss=14.911, ppl=30812.9, wps=29323.4, ups=0.45, wpb=65534.5, bsz=128, num_updates=100, lr=1.25975e-05, gnorm=3.011, train_wall=223, gb_free=7.9, wall=233
2022-05-17 21:55:16 | INFO | train_inner | epoch 001:    200 / 1550 loss=12.886, ppl=7567.42, wps=29135.6, ups=0.44, wpb=65536, bsz=128, num_updates=200, lr=2.5095e-05, gnorm=1.067, train_wall=218, gb_free=7.9, wall=458
2022-05-17 21:59:35 | INFO | train_inner | epoch 001:    300 / 1550 loss=11.428, ppl=2754.49, wps=25333.1, ups=0.39, wpb=65536, bsz=128, num_updates=300, lr=3.75925e-05, gnorm=0.681, train_wall=220, gb_free=7.9, wall=716
2022-05-17 22:04:00 | INFO | train_inner | epoch 001:    400 / 1550 loss=10.381, ppl=1333.11, wps=24788.8, ups=0.38, wpb=65536, bsz=128, num_updates=400, lr=5.009e-05, gnorm=0.5, train_wall=217, gb_free=7.9, wall=981
2022-05-17 22:08:18 | INFO | train_inner | epoch 001:    500 / 1550 loss=9.814, ppl=900.04, wps=25382.1, ups=0.39, wpb=65536, bsz=128, num_updates=500, lr=6.25875e-05, gnorm=0.485, train_wall=222, gb_free=7.9, wall=1239
2022-05-17 22:12:18 | INFO | train_inner | epoch 001:    600 / 1550 loss=9.366, ppl=659.78, wps=27328.9, ups=0.42, wpb=65536, bsz=128, num_updates=600, lr=7.5085e-05, gnorm=0.596, train_wall=217, gb_free=7.9, wall=1479
2022-05-17 22:16:10 | INFO | train_inner | epoch 001:    700 / 1550 loss=9.018, ppl=518.36, wps=28249.1, ups=0.43, wpb=65536, bsz=128, num_updates=700, lr=8.75825e-05, gnorm=0.593, train_wall=217, gb_free=7.9, wall=1711
2022-05-17 22:20:37 | INFO | train_inner | epoch 001:    800 / 1550 loss=8.744, ppl=428.71, wps=24472.4, ups=0.37, wpb=65536, bsz=128, num_updates=800, lr=0.00010008, gnorm=0.697, train_wall=221, gb_free=7.9, wall=1979
2022-05-17 22:24:53 | INFO | train_inner | epoch 001:    900 / 1550 loss=8.489, ppl=359.37, wps=25618.3, ups=0.39, wpb=65536, bsz=128, num_updates=900, lr=0.000112578, gnorm=0.75, train_wall=217, gb_free=7.9, wall=2234
2022-05-17 22:29:01 | INFO | train_inner | epoch 001:   1000 / 1550 loss=8.284, ppl=311.61, wps=26486.2, ups=0.4, wpb=65536, bsz=128, num_updates=1000, lr=0.000125075, gnorm=0.78, train_wall=221, gb_free=7.9, wall=2482
2022-05-17 22:32:56 | INFO | train_inner | epoch 001:   1100 / 1550 loss=8.115, ppl=277.22, wps=27837.5, ups=0.42, wpb=65536, bsz=128, num_updates=1100, lr=0.000137573, gnorm=0.803, train_wall=218, gb_free=7.9, wall=2717
2022-05-17 22:36:45 | INFO | train_inner | epoch 001:   1200 / 1550 loss=7.954, ppl=247.9, wps=28566.6, ups=0.44, wpb=65536, bsz=128, num_updates=1200, lr=0.00015007, gnorm=0.823, train_wall=218, gb_free=7.9, wall=2947
2022-05-17 22:40:37 | INFO | train_inner | epoch 001:   1300 / 1550 loss=7.785, ppl=220.5, wps=28270.5, ups=0.43, wpb=65536, bsz=128, num_updates=1300, lr=0.000162568, gnorm=0.867, train_wall=220, gb_free=7.9, wall=3179
2022-05-17 22:44:25 | INFO | train_inner | epoch 001:   1400 / 1550 loss=7.632, ppl=198.39, wps=28750.7, ups=0.44, wpb=65536, bsz=128, num_updates=1400, lr=0.000175065, gnorm=0.871, train_wall=218, gb_free=7.9, wall=3407
2022-05-17 22:48:14 | INFO | train_inner | epoch 001:   1500 / 1550 loss=7.506, ppl=181.74, wps=28637.4, ups=0.44, wpb=65536, bsz=128, num_updates=1500, lr=0.000187563, gnorm=0.893, train_wall=219, gb_free=7.9, wall=3635
2022-05-17 22:50:08 | INFO | fairseq_cli.train | begin validation on "valid" subset
/cluster/work/cotterell/liam/fairseq/fairseq/utils.py:374: UserWarning: amp_C fused kernels unavailable, disabling multi_tensor_l2norm; you may get better performance by installing NVIDIA's apex library
  warnings.warn(
2022-05-17 22:51:25 | INFO | valid | epoch 001 | valid on 'valid' subset | loss 7.259 | ppl 153.2 | wps 74142.5 | wpb 2047.4 | bsz 4 | num_updates 1550
2022-05-17 22:51:25 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 1 @ 1550 updates
2022-05-17 22:51:25 | INFO | fairseq.trainer | Saving checkpoint to /cluster/work/cotterell/liam/master-thesis/checkpoints/language_model/bart/checkpoint_best_bart_32_5.0000E-04_full.pt
2022-05-17 22:51:26 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/work/cotterell/liam/master-thesis/checkpoints/language_model/bart/checkpoint_best_bart_32_5.0000E-04_full.pt
2022-05-17 22:51:26 | INFO | fairseq.checkpoint_utils | Saved checkpoint checkpoints/language_model/bart/checkpoint_best_bart_32_5.0000E-04_full.pt (epoch 1 @ 1550 updates, score 7.259) (writing took 1.5679174470715225 seconds)
2022-05-17 22:51:26 | INFO | fairseq_cli.train | end of epoch 1 (average epoch stats below)
2022-05-17 22:51:26 | INFO | train | epoch 001 | loss 9.421 | ppl 685.3 | wps 26597.4 | ups 0.41 | wpb 65522.7 | bsz 128 | num_updates 1550 | lr 0.000193811 | gnorm 0.894 | train_wall 3394 | gb_free 7.9 | wall 3828
2022-05-17 22:51:26 | INFO | fairseq.data.iterators | grouped total_num_itrs = 1550
2022-05-17 22:51:26 | INFO | fairseq.trainer | begin training epoch 2
2022-05-17 22:51:26 | INFO | fairseq_cli.train | Start iterating over samples
2022-05-17 22:53:20 | INFO | train_inner | epoch 002:     50 / 1550 loss=7.372, ppl=165.7, wps=21353.8, ups=0.33, wpb=65331.2, bsz=127.6, num_updates=1600, lr=0.00020006, gnorm=0.891, train_wall=217, gb_free=7.9, wall=3941
2022-05-17 22:57:08 | INFO | train_inner | epoch 002:    150 / 1550 loss=7.248, ppl=151.98, wps=28706.5, ups=0.44, wpb=65536, bsz=128, num_updates=1700, lr=0.000212558, gnorm=0.902, train_wall=218, gb_free=7.9, wall=4170
2022-05-17 23:00:58 | INFO | train_inner | epoch 002:    250 / 1550 loss=7.158, ppl=142.83, wps=28526.2, ups=0.44, wpb=65536, bsz=128, num_updates=1800, lr=0.000225055, gnorm=0.891, train_wall=221, gb_free=7.9, wall=4399
2022-05-17 23:05:54 | INFO | train_inner | epoch 002:    350 / 1550 loss=7.066, ppl=133.97, wps=22134.5, ups=0.34, wpb=65536, bsz=128, num_updates=1900, lr=0.000237553, gnorm=0.885, train_wall=227, gb_free=7.9, wall=4695
2022-05-17 23:10:14 | INFO | train_inner | epoch 002:    450 / 1550 loss=6.954, ppl=123.99, wps=25209.8, ups=0.38, wpb=65536, bsz=128, num_updates=2000, lr=0.00025005, gnorm=0.872, train_wall=220, gb_free=7.9, wall=4955
2022-05-17 23:14:34 | INFO | train_inner | epoch 002:    550 / 1550 loss=6.874, ppl=117.28, wps=25216.1, ups=0.38, wpb=65534.5, bsz=128, num_updates=2100, lr=0.000262548, gnorm=0.873, train_wall=222, gb_free=7.9, wall=5215
2022-05-17 23:18:51 | INFO | train_inner | epoch 002:    650 / 1550 loss=6.788, ppl=110.5, wps=25476.5, ups=0.39, wpb=65536, bsz=128, num_updates=2200, lr=0.000275045, gnorm=0.842, train_wall=218, gb_free=7.9, wall=5473
2022-05-17 23:22:58 | INFO | train_inner | epoch 002:    750 / 1550 loss=6.706, ppl=104.38, wps=26563.9, ups=0.41, wpb=65536, bsz=128, num_updates=2300, lr=0.000287543, gnorm=0.82, train_wall=220, gb_free=7.9, wall=5719
2022-05-17 23:27:18 | INFO | train_inner | epoch 002:    850 / 1550 loss=6.635, ppl=99.37, wps=25188.5, ups=0.38, wpb=65536, bsz=128, num_updates=2400, lr=0.00030004, gnorm=0.817, train_wall=224, gb_free=7.9, wall=5979
2022-05-17 23:31:41 | INFO | train_inner | epoch 002:    950 / 1550 loss=6.547, ppl=93.49, wps=24963, ups=0.38, wpb=65536, bsz=128, num_updates=2500, lr=0.000312538, gnorm=0.79, train_wall=218, gb_free=7.9, wall=6242
2022-05-17 23:37:15 | INFO | train_inner | epoch 002:   1050 / 1550 loss=6.484, ppl=89.5, wps=19587.8, ups=0.3, wpb=65536, bsz=128, num_updates=2600, lr=0.000325035, gnorm=0.779, train_wall=237, gb_free=7.9, wall=6577
2022-05-17 23:41:32 | INFO | train_inner | epoch 002:   1150 / 1550 loss=6.444, ppl=87.03, wps=25534.2, ups=0.39, wpb=65536, bsz=128, num_updates=2700, lr=0.000337533, gnorm=0.766, train_wall=218, gb_free=7.9, wall=6833
2022-05-17 23:45:41 | INFO | train_inner | epoch 002:   1250 / 1550 loss=6.37, ppl=82.72, wps=26262.8, ups=0.4, wpb=65536, bsz=128, num_updates=2800, lr=0.00035003, gnorm=0.755, train_wall=221, gb_free=7.9, wall=7083
2022-05-17 23:49:37 | INFO | train_inner | epoch 002:   1350 / 1550 loss=6.312, ppl=79.44, wps=27848.3, ups=0.42, wpb=65536, bsz=128, num_updates=2900, lr=0.000362528, gnorm=0.729, train_wall=219, gb_free=7.9, wall=7318
2022-05-17 23:53:26 | INFO | train_inner | epoch 002:   1450 / 1550 loss=6.265, ppl=76.9, wps=28586.8, ups=0.44, wpb=65536, bsz=128, num_updates=3000, lr=0.000375025, gnorm=0.734, train_wall=218, gb_free=7.9, wall=7547
2022-05-17 23:57:19 | INFO | train_inner | epoch 002:   1550 / 1550 loss=6.214, ppl=74.21, wps=27996.2, ups=0.43, wpb=65331.2, bsz=127.6, num_updates=3100, lr=0.000387523, gnorm=0.713, train_wall=222, gb_free=7.9, wall=7781
2022-05-17 23:57:19 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-05-17 23:58:36 | INFO | valid | epoch 002 | valid on 'valid' subset | loss 6.05 | ppl 66.27 | wps 73966.7 | wpb 2047.4 | bsz 4 | num_updates 3100 | best_loss 6.05
2022-05-17 23:58:36 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 2 @ 3100 updates
2022-05-17 23:58:36 | INFO | fairseq.trainer | Saving checkpoint to /cluster/work/cotterell/liam/master-thesis/checkpoints/language_model/bart/checkpoint_best_bart_32_5.0000E-04_full.pt
2022-05-17 23:58:38 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/work/cotterell/liam/master-thesis/checkpoints/language_model/bart/checkpoint_best_bart_32_5.0000E-04_full.pt
2022-05-17 23:58:38 | INFO | fairseq.checkpoint_utils | Saved checkpoint checkpoints/language_model/bart/checkpoint_best_bart_32_5.0000E-04_full.pt (epoch 2 @ 3100 updates, score 6.05) (writing took 1.6504514389671385 seconds)
2022-05-17 23:58:38 | INFO | fairseq_cli.train | end of epoch 2 (average epoch stats below)
2022-05-17 23:58:38 | INFO | train | epoch 002 | loss 6.693 | ppl 103.43 | wps 25192.8 | ups 0.38 | wpb 65522.7 | bsz 128 | num_updates 3100 | lr 0.000387523 | gnorm 0.814 | train_wall 3432 | gb_free 7.9 | wall 7859
2022-05-17 23:58:38 | INFO | fairseq.data.iterators | grouped total_num_itrs = 1550
2022-05-17 23:58:38 | INFO | fairseq.trainer | begin training epoch 3
2022-05-17 23:58:38 | INFO | fairseq_cli.train | Start iterating over samples
2022-05-18 00:02:23 | INFO | train_inner | epoch 003:    100 / 1550 loss=6.128, ppl=69.96, wps=21586.4, ups=0.33, wpb=65534.5, bsz=128, num_updates=3200, lr=0.00040002, gnorm=0.715, train_wall=218, gb_free=7.9, wall=8084
2022-05-18 00:06:09 | INFO | train_inner | epoch 003:    200 / 1550 loss=6.1, ppl=68.6, wps=29013.2, ups=0.44, wpb=65536, bsz=128, num_updates=3300, lr=0.000412518, gnorm=0.692, train_wall=219, gb_free=7.9, wall=8310
2022-05-18 00:09:58 | INFO | train_inner | epoch 003:    300 / 1550 loss=6.047, ppl=66.14, wps=28639.9, ups=0.44, wpb=65536, bsz=128, num_updates=3400, lr=0.000425015, gnorm=0.684, train_wall=219, gb_free=7.9, wall=8539
2022-05-18 00:13:43 | INFO | train_inner | epoch 003:    400 / 1550 loss=6.011, ppl=64.51, wps=29094.8, ups=0.44, wpb=65536, bsz=128, num_updates=3500, lr=0.000437513, gnorm=0.679, train_wall=218, gb_free=7.9, wall=8764
2022-05-18 00:17:29 | INFO | train_inner | epoch 003:    500 / 1550 loss=5.96, ppl=62.23, wps=29023.6, ups=0.44, wpb=65536, bsz=128, num_updates=3600, lr=0.00045001, gnorm=0.655, train_wall=218, gb_free=7.9, wall=8990
2022-05-18 00:21:16 | INFO | train_inner | epoch 003:    600 / 1550 loss=5.938, ppl=61.32, wps=28774, ups=0.44, wpb=65536, bsz=128, num_updates=3700, lr=0.000462508, gnorm=0.654, train_wall=219, gb_free=7.9, wall=9218
2022-05-18 00:25:05 | INFO | train_inner | epoch 003:    700 / 1550 loss=5.9, ppl=59.69, wps=28732.6, ups=0.44, wpb=65536, bsz=128, num_updates=3800, lr=0.000475005, gnorm=0.635, train_wall=219, gb_free=7.9, wall=9446
2022-05-18 00:28:53 | INFO | train_inner | epoch 003:    800 / 1550 loss=5.855, ppl=57.88, wps=28655.8, ups=0.44, wpb=65536, bsz=128, num_updates=3900, lr=0.000487503, gnorm=0.632, train_wall=219, gb_free=7.9, wall=9675
2022-05-18 00:32:44 | INFO | train_inner | epoch 003:    900 / 1550 loss=5.837, ppl=57.17, wps=28412.4, ups=0.43, wpb=65536, bsz=128, num_updates=4000, lr=0.0005, gnorm=0.626, train_wall=219, gb_free=7.9, wall=9905
2022-05-18 00:36:30 | INFO | train_inner | epoch 003:   1000 / 1550 loss=5.813, ppl=56.22, wps=28934.6, ups=0.44, wpb=65536, bsz=128, num_updates=4100, lr=0.000493865, gnorm=0.607, train_wall=218, gb_free=7.9, wall=10132
2022-05-18 00:40:17 | INFO | train_inner | epoch 003:   1100 / 1550 loss=5.779, ppl=54.9, wps=28899.6, ups=0.44, wpb=65536, bsz=128, num_updates=4200, lr=0.00048795, gnorm=0.591, train_wall=219, gb_free=7.9, wall=10359
2022-05-18 00:44:04 | INFO | train_inner | epoch 003:   1200 / 1550 loss=5.745, ppl=53.63, wps=28916.2, ups=0.44, wpb=65536, bsz=128, num_updates=4300, lr=0.000482243, gnorm=0.573, train_wall=218, gb_free=7.9, wall=10585
2022-05-18 00:47:49 | INFO | train_inner | epoch 003:   1300 / 1550 loss=5.712, ppl=52.41, wps=29134.1, ups=0.44, wpb=65536, bsz=128, num_updates=4400, lr=0.000476731, gnorm=0.58, train_wall=218, gb_free=7.9, wall=10810
2022-05-18 00:51:37 | INFO | train_inner | epoch 003:   1400 / 1550 loss=5.689, ppl=51.59, wps=28703, ups=0.44, wpb=65536, bsz=128, num_updates=4500, lr=0.000471405, gnorm=0.566, train_wall=219, gb_free=7.9, wall=11038
2022-05-18 00:55:24 | INFO | train_inner | epoch 003:   1500 / 1550 loss=5.657, ppl=50.47, wps=28908.2, ups=0.44, wpb=65536, bsz=128, num_updates=4600, lr=0.000466252, gnorm=0.552, train_wall=219, gb_free=7.9, wall=11265
2022-05-18 00:57:16 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-05-18 00:58:31 | INFO | valid | epoch 003 | valid on 'valid' subset | loss 5.505 | ppl 45.4 | wps 74945.8 | wpb 2047.4 | bsz 4 | num_updates 4650 | best_loss 5.505
2022-05-18 00:58:31 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 3 @ 4650 updates
2022-05-18 00:58:31 | INFO | fairseq.trainer | Saving checkpoint to /cluster/work/cotterell/liam/master-thesis/checkpoints/language_model/bart/checkpoint_best_bart_32_5.0000E-04_full.pt
2022-05-18 00:58:33 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/work/cotterell/liam/master-thesis/checkpoints/language_model/bart/checkpoint_best_bart_32_5.0000E-04_full.pt
2022-05-18 00:58:33 | INFO | fairseq.checkpoint_utils | Saved checkpoint checkpoints/language_model/bart/checkpoint_best_bart_32_5.0000E-04_full.pt (epoch 3 @ 4650 updates, score 5.505) (writing took 1.7286898400634527 seconds)
2022-05-18 00:58:33 | INFO | fairseq_cli.train | end of epoch 3 (average epoch stats below)
2022-05-18 00:58:33 | INFO | train | epoch 003 | loss 5.871 | ppl 58.51 | wps 28246 | ups 0.43 | wpb 65522.7 | bsz 128 | num_updates 4650 | lr 0.000463739 | gnorm 0.627 | train_wall 3386 | gb_free 7.9 | wall 11454
2022-05-18 00:58:33 | INFO | fairseq.data.iterators | grouped total_num_itrs = 1550
2022-05-18 00:58:33 | INFO | fairseq.trainer | begin training epoch 4
2022-05-18 00:58:33 | INFO | fairseq_cli.train | Start iterating over samples
2022-05-18 01:00:51 | INFO | train_inner | epoch 004:     50 / 1550 loss=5.6, ppl=48.51, wps=19974.1, ups=0.31, wpb=65331.2, bsz=127.6, num_updates=4700, lr=0.000461266, gnorm=0.552, train_wall=220, gb_free=7.9, wall=11592
2022-05-18 01:05:24 | INFO | train_inner | epoch 004:    150 / 1550 loss=5.557, ppl=47.07, wps=24001.2, ups=0.37, wpb=65536, bsz=128, num_updates=4800, lr=0.000456435, gnorm=0.544, train_wall=219, gb_free=7.9, wall=11865
2022-05-18 01:09:38 | INFO | train_inner | epoch 004:    250 / 1550 loss=5.552, ppl=46.93, wps=25838.6, ups=0.39, wpb=65536, bsz=128, num_updates=4900, lr=0.000451754, gnorm=0.548, train_wall=218, gb_free=7.9, wall=12119
2022-05-18 01:13:43 | INFO | train_inner | epoch 004:    350 / 1550 loss=5.526, ppl=46.07, wps=26708.5, ups=0.41, wpb=65536, bsz=128, num_updates=5000, lr=0.000447214, gnorm=0.537, train_wall=219, gb_free=7.9, wall=12364
2022-05-18 01:17:40 | INFO | train_inner | epoch 004:    450 / 1550 loss=5.506, ppl=45.45, wps=27600, ups=0.42, wpb=65536, bsz=128, num_updates=5100, lr=0.000442807, gnorm=0.533, train_wall=217, gb_free=7.9, wall=12602
2022-05-18 01:21:34 | INFO | train_inner | epoch 004:    550 / 1550 loss=5.493, ppl=45.02, wps=28002.1, ups=0.43, wpb=65536, bsz=128, num_updates=5200, lr=0.000438529, gnorm=0.536, train_wall=219, gb_free=7.9, wall=12836
2022-05-18 01:25:27 | INFO | train_inner | epoch 004:    650 / 1550 loss=5.469, ppl=44.31, wps=28226.2, ups=0.43, wpb=65536, bsz=128, num_updates=5300, lr=0.000434372, gnorm=0.528, train_wall=218, gb_free=7.9, wall=13068
2022-05-18 01:29:16 | INFO | train_inner | epoch 004:    750 / 1550 loss=5.461, ppl=44.06, wps=28592.3, ups=0.44, wpb=65536, bsz=128, num_updates=5400, lr=0.000430331, gnorm=0.54, train_wall=219, gb_free=7.9, wall=13297
2022-05-18 01:33:08 | INFO | train_inner | epoch 004:    850 / 1550 loss=5.445, ppl=43.58, wps=28203.1, ups=0.43, wpb=65536, bsz=128, num_updates=5500, lr=0.000426401, gnorm=0.533, train_wall=219, gb_free=7.9, wall=13530
2022-05-18 01:37:15 | INFO | train_inner | epoch 004:    950 / 1550 loss=5.431, ppl=43.15, wps=26513.7, ups=0.4, wpb=65536, bsz=128, num_updates=5600, lr=0.000422577, gnorm=0.518, train_wall=228, gb_free=7.9, wall=13777
2022-05-18 01:41:20 | INFO | train_inner | epoch 004:   1050 / 1550 loss=5.422, ppl=42.86, wps=26769.2, ups=0.41, wpb=65536, bsz=128, num_updates=5700, lr=0.000418854, gnorm=0.526, train_wall=218, gb_free=7.9, wall=14022
2022-05-18 01:46:28 | INFO | train_inner | epoch 004:   1150 / 1550 loss=5.415, ppl=42.65, wps=21318.8, ups=0.33, wpb=65534.5, bsz=128, num_updates=5800, lr=0.000415227, gnorm=0.521, train_wall=242, gb_free=7.9, wall=14329
2022-05-18 01:50:48 | INFO | train_inner | epoch 004:   1250 / 1550 loss=5.396, ppl=42.1, wps=25186.1, ups=0.38, wpb=65536, bsz=128, num_updates=5900, lr=0.000411693, gnorm=0.525, train_wall=218, gb_free=7.9, wall=14589
2022-05-18 01:54:55 | INFO | train_inner | epoch 004:   1350 / 1550 loss=5.378, ppl=41.58, wps=26484.5, ups=0.4, wpb=65536, bsz=128, num_updates=6000, lr=0.000408248, gnorm=0.515, train_wall=219, gb_free=7.9, wall=14837
2022-05-18 01:58:55 | INFO | train_inner | epoch 004:   1450 / 1550 loss=5.368, ppl=41.3, wps=27328.6, ups=0.42, wpb=65536, bsz=128, num_updates=6100, lr=0.000404888, gnorm=0.524, train_wall=219, gb_free=7.9, wall=15076
2022-05-18 02:02:50 | INFO | train_inner | epoch 004:   1550 / 1550 loss=5.358, ppl=41.02, wps=27796.2, ups=0.43, wpb=65331.2, bsz=127.6, num_updates=6200, lr=0.00040161, gnorm=0.52, train_wall=220, gb_free=7.9, wall=15311
2022-05-18 02:02:50 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-05-18 02:04:06 | INFO | valid | epoch 004 | valid on 'valid' subset | loss 5.245 | ppl 37.93 | wps 74428.1 | wpb 2047.4 | bsz 4 | num_updates 6200 | best_loss 5.245
2022-05-18 02:04:06 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 4 @ 6200 updates
2022-05-18 02:04:06 | INFO | fairseq.trainer | Saving checkpoint to /cluster/work/cotterell/liam/master-thesis/checkpoints/language_model/bart/checkpoint_best_bart_32_5.0000E-04_full.pt
2022-05-18 02:04:09 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/work/cotterell/liam/master-thesis/checkpoints/language_model/bart/checkpoint_best_bart_32_5.0000E-04_full.pt
2022-05-18 02:04:09 | INFO | fairseq.checkpoint_utils | Saved checkpoint checkpoints/language_model/bart/checkpoint_best_bart_32_5.0000E-04_full.pt (epoch 4 @ 6200 updates, score 5.245) (writing took 2.677577355876565 seconds)
2022-05-18 02:04:09 | INFO | fairseq_cli.train | end of epoch 4 (average epoch stats below)
2022-05-18 02:04:09 | INFO | train | epoch 004 | loss 5.455 | ppl 43.87 | wps 25804.2 | ups 0.39 | wpb 65522.7 | bsz 128 | num_updates 6200 | lr 0.00040161 | gnorm 0.53 | train_wall 3424 | gb_free 7.9 | wall 15390
2022-05-18 02:04:09 | INFO | fairseq.data.iterators | grouped total_num_itrs = 1550
2022-05-18 02:04:09 | INFO | fairseq.trainer | begin training epoch 5
2022-05-18 02:04:09 | INFO | fairseq_cli.train | Start iterating over samples
2022-05-18 02:08:39 | INFO | train_inner | epoch 005:    100 / 1550 loss=5.278, ppl=38.8, wps=18793, ups=0.29, wpb=65536, bsz=128, num_updates=6300, lr=0.00039841, gnorm=0.528, train_wall=223, gb_free=7.9, wall=15660
2022-05-18 02:13:31 | INFO | train_inner | epoch 005:    200 / 1550 loss=5.274, ppl=38.68, wps=22428.9, ups=0.34, wpb=65536, bsz=128, num_updates=6400, lr=0.000395285, gnorm=0.519, train_wall=229, gb_free=7.9, wall=15952
2022-05-18 02:17:57 | INFO | train_inner | epoch 005:    300 / 1550 loss=5.276, ppl=38.74, wps=24616, ups=0.38, wpb=65536, bsz=128, num_updates=6500, lr=0.000392232, gnorm=0.514, train_wall=222, gb_free=7.9, wall=16219
2022-05-18 02:22:36 | INFO | train_inner | epoch 005:    400 / 1550 loss=5.263, ppl=38.39, wps=23486.3, ups=0.36, wpb=65536, bsz=128, num_updates=6600, lr=0.000389249, gnorm=0.517, train_wall=221, gb_free=7.9, wall=16498
2022-05-18 02:28:04 | INFO | train_inner | epoch 005:    500 / 1550 loss=5.264, ppl=38.41, wps=19979.9, ups=0.3, wpb=65536, bsz=128, num_updates=6700, lr=0.000386334, gnorm=0.522, train_wall=249, gb_free=7.9, wall=16826
2022-05-18 02:32:30 | INFO | train_inner | epoch 005:    600 / 1550 loss=5.249, ppl=38.02, wps=24688.6, ups=0.38, wpb=65536, bsz=128, num_updates=6800, lr=0.000383482, gnorm=0.511, train_wall=219, gb_free=7.9, wall=17091
2022-05-18 02:36:53 | INFO | train_inner | epoch 005:    700 / 1550 loss=5.261, ppl=38.35, wps=24870.5, ups=0.38, wpb=65536, bsz=128, num_updates=6900, lr=0.000380693, gnorm=0.514, train_wall=223, gb_free=7.9, wall=17355
2022-05-18 02:42:11 | INFO | train_inner | epoch 005:    800 / 1550 loss=5.245, ppl=37.92, wps=20645.7, ups=0.32, wpb=65536, bsz=128, num_updates=7000, lr=0.000377964, gnorm=0.511, train_wall=240, gb_free=7.9, wall=17672
2022-05-18 02:47:48 | INFO | train_inner | epoch 005:    900 / 1550 loss=5.231, ppl=37.55, wps=19442.4, ups=0.3, wpb=65536, bsz=128, num_updates=7100, lr=0.000375293, gnorm=0.508, train_wall=248, gb_free=7.9, wall=18009
2022-05-18 02:53:04 | INFO | train_inner | epoch 005:   1000 / 1550 loss=5.225, ppl=37.39, wps=20739.9, ups=0.32, wpb=65536, bsz=128, num_updates=7200, lr=0.000372678, gnorm=0.521, train_wall=235, gb_free=7.9, wall=18325
2022-05-18 02:58:39 | INFO | train_inner | epoch 005:   1100 / 1550 loss=5.224, ppl=37.37, wps=19568.9, ups=0.3, wpb=65536, bsz=128, num_updates=7300, lr=0.000370117, gnorm=0.507, train_wall=259, gb_free=7.9, wall=18660
2022-05-18 03:03:17 | INFO | train_inner | epoch 005:   1200 / 1550 loss=5.221, ppl=37.31, wps=23588.7, ups=0.36, wpb=65536, bsz=128, num_updates=7400, lr=0.000367607, gnorm=0.517, train_wall=219, gb_free=7.9, wall=18938
2022-05-18 03:07:45 | INFO | train_inner | epoch 005:   1300 / 1550 loss=5.214, ppl=37.13, wps=24451.2, ups=0.37, wpb=65536, bsz=128, num_updates=7500, lr=0.000365148, gnorm=0.521, train_wall=225, gb_free=7.9, wall=19206
2022-05-18 03:13:17 | INFO | train_inner | epoch 005:   1400 / 1550 loss=5.215, ppl=37.13, wps=19706.4, ups=0.3, wpb=65534.5, bsz=128, num_updates=7600, lr=0.000362738, gnorm=0.529, train_wall=244, gb_free=7.9, wall=19538
2022-05-18 03:19:04 | INFO | train_inner | epoch 005:   1500 / 1550 loss=5.188, ppl=36.44, wps=18880, ups=0.29, wpb=65536, bsz=128, num_updates=7700, lr=0.000360375, gnorm=0.522, train_wall=246, gb_free=7.9, wall=19886
2022-05-18 03:22:26 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-05-18 03:23:41 | INFO | valid | epoch 005 | valid on 'valid' subset | loss 5.103 | ppl 34.37 | wps 75061.7 | wpb 2047.4 | bsz 4 | num_updates 7750 | best_loss 5.103
2022-05-18 03:23:41 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 5 @ 7750 updates
2022-05-18 03:23:41 | INFO | fairseq.trainer | Saving checkpoint to /cluster/work/cotterell/liam/master-thesis/checkpoints/language_model/bart/checkpoint_best_bart_32_5.0000E-04_full.pt
2022-05-18 03:23:44 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/work/cotterell/liam/master-thesis/checkpoints/language_model/bart/checkpoint_best_bart_32_5.0000E-04_full.pt
2022-05-18 03:23:44 | INFO | fairseq.checkpoint_utils | Saved checkpoint checkpoints/language_model/bart/checkpoint_best_bart_32_5.0000E-04_full.pt (epoch 5 @ 7750 updates, score 5.103) (writing took 2.641795417293906 seconds)
2022-05-18 03:23:44 | INFO | fairseq_cli.train | end of epoch 5 (average epoch stats below)
2022-05-18 03:23:44 | INFO | train | epoch 005 | loss 5.24 | ppl 37.79 | wps 21268.5 | ups 0.32 | wpb 65522.7 | bsz 128 | num_updates 7750 | lr 0.000359211 | gnorm 0.518 | train_wall 3660 | gb_free 7.9 | wall 20165
2022-05-18 03:23:44 | INFO | fairseq.data.iterators | grouped total_num_itrs = 1550
2022-05-18 03:23:44 | INFO | fairseq.trainer | begin training epoch 6
2022-05-18 03:23:44 | INFO | fairseq_cli.train | Start iterating over samples
2022-05-18 03:27:53 | INFO | train_inner | epoch 006:     50 / 1550 loss=5.144, ppl=35.35, wps=12348.4, ups=0.19, wpb=65331.2, bsz=127.6, num_updates=7800, lr=0.000358057, gnorm=0.52, train_wall=323, gb_free=7.9, wall=20415
2022-05-18 03:32:47 | INFO | train_inner | epoch 006:    150 / 1550 loss=5.116, ppl=34.68, wps=22301.3, ups=0.34, wpb=65536, bsz=128, num_updates=7900, lr=0.000355784, gnorm=0.506, train_wall=228, gb_free=7.9, wall=20708
2022-05-18 03:37:46 | INFO | train_inner | epoch 006:    250 / 1550 loss=5.117, ppl=34.7, wps=21962.7, ups=0.34, wpb=65536, bsz=128, num_updates=8000, lr=0.000353553, gnorm=0.509, train_wall=233, gb_free=7.9, wall=21007
2022-05-18 03:41:59 | INFO | train_inner | epoch 006:    350 / 1550 loss=5.125, ppl=34.91, wps=25845.8, ups=0.39, wpb=65536, bsz=128, num_updates=8100, lr=0.000351364, gnorm=0.517, train_wall=218, gb_free=7.9, wall=21260
2022-05-18 03:46:23 | INFO | train_inner | epoch 006:    450 / 1550 loss=5.129, ppl=35, wps=24831, ups=0.38, wpb=65536, bsz=128, num_updates=8200, lr=0.000349215, gnorm=0.519, train_wall=225, gb_free=7.9, wall=21524
2022-05-18 03:53:08 | INFO | train_inner | epoch 006:    550 / 1550 loss=5.116, ppl=34.67, wps=16202.6, ups=0.25, wpb=65536, bsz=128, num_updates=8300, lr=0.000347105, gnorm=0.511, train_wall=286, gb_free=7.9, wall=21929
2022-05-18 03:58:55 | INFO | train_inner | epoch 006:    650 / 1550 loss=5.112, ppl=34.59, wps=18870.4, ups=0.29, wpb=65536, bsz=128, num_updates=8400, lr=0.000345033, gnorm=0.512, train_wall=269, gb_free=7.9, wall=22276
2022-05-18 04:04:31 | INFO | train_inner | epoch 006:    750 / 1550 loss=5.118, ppl=34.73, wps=19467.9, ups=0.3, wpb=65536, bsz=128, num_updates=8500, lr=0.000342997, gnorm=0.522, train_wall=241, gb_free=7.9, wall=22613
2022-05-18 04:10:11 | INFO | train_inner | epoch 006:    850 / 1550 loss=5.127, ppl=34.94, wps=19329.2, ups=0.29, wpb=65536, bsz=128, num_updates=8600, lr=0.000340997, gnorm=0.512, train_wall=259, gb_free=7.9, wall=22952
2022-05-18 04:14:51 | INFO | train_inner | epoch 006:    950 / 1550 loss=5.101, ppl=34.33, wps=23364.2, ups=0.36, wpb=65536, bsz=128, num_updates=8700, lr=0.000339032, gnorm=0.517, train_wall=217, gb_free=7.9, wall=23232
2022-05-18 04:19:10 | INFO | train_inner | epoch 006:   1050 / 1550 loss=5.107, ppl=34.45, wps=25277.2, ups=0.39, wpb=65536, bsz=128, num_updates=8800, lr=0.0003371, gnorm=0.525, train_wall=217, gb_free=7.9, wall=23492
2022-05-18 04:24:05 | INFO | train_inner | epoch 006:   1150 / 1550 loss=5.101, ppl=34.32, wps=22239.2, ups=0.34, wpb=65536, bsz=128, num_updates=8900, lr=0.000335201, gnorm=0.51, train_wall=226, gb_free=7.9, wall=23786
2022-05-18 04:29:15 | INFO | train_inner | epoch 006:   1250 / 1550 loss=5.091, ppl=34.08, wps=21138.9, ups=0.32, wpb=65534.5, bsz=128, num_updates=9000, lr=0.000333333, gnorm=0.524, train_wall=234, gb_free=7.9, wall=24096
2022-05-18 04:36:01 | INFO | train_inner | epoch 006:   1350 / 1550 loss=5.092, ppl=34.11, wps=16161.1, ups=0.25, wpb=65536, bsz=128, num_updates=9100, lr=0.000331497, gnorm=0.514, train_wall=302, gb_free=7.9, wall=24502
2022-05-18 04:41:33 | INFO | train_inner | epoch 006:   1450 / 1550 loss=5.084, ppl=33.92, wps=19713.8, ups=0.3, wpb=65536, bsz=128, num_updates=9200, lr=0.00032969, gnorm=0.518, train_wall=256, gb_free=7.9, wall=24834
2022-05-18 04:47:10 | INFO | train_inner | epoch 006:   1550 / 1550 loss=5.08, ppl=33.82, wps=19409.2, ups=0.3, wpb=65331.2, bsz=127.6, num_updates=9300, lr=0.000327913, gnorm=0.514, train_wall=248, gb_free=7.9, wall=25171
2022-05-18 04:47:10 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-05-18 04:48:25 | INFO | valid | epoch 006 | valid on 'valid' subset | loss 5.015 | ppl 32.34 | wps 74875.9 | wpb 2047.4 | bsz 4 | num_updates 9300 | best_loss 5.015
2022-05-18 04:48:25 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 6 @ 9300 updates
2022-05-18 04:48:25 | INFO | fairseq.trainer | Saving checkpoint to /cluster/work/cotterell/liam/master-thesis/checkpoints/language_model/bart/checkpoint_best_bart_32_5.0000E-04_full.pt
2022-05-18 04:48:27 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/work/cotterell/liam/master-thesis/checkpoints/language_model/bart/checkpoint_best_bart_32_5.0000E-04_full.pt
2022-05-18 04:48:27 | INFO | fairseq.checkpoint_utils | Saved checkpoint checkpoints/language_model/bart/checkpoint_best_bart_32_5.0000E-04_full.pt (epoch 6 @ 9300 updates, score 5.015) (writing took 1.558098359964788 seconds)
2022-05-18 04:48:27 | INFO | fairseq_cli.train | end of epoch 6 (average epoch stats below)
2022-05-18 04:48:27 | INFO | train | epoch 006 | loss 5.108 | ppl 34.48 | wps 19981.4 | ups 0.3 | wpb 65522.7 | bsz 128 | num_updates 9300 | lr 0.000327913 | gnorm 0.516 | train_wall 3826 | gb_free 7.9 | wall 25248
2022-05-18 04:48:27 | INFO | fairseq.data.iterators | grouped total_num_itrs = 1550
2022-05-18 04:48:27 | INFO | fairseq.trainer | begin training epoch 7
2022-05-18 04:48:27 | INFO | fairseq_cli.train | Start iterating over samples
2022-05-18 04:53:10 | INFO | train_inner | epoch 007:    100 / 1550 loss=5.008, ppl=32.19, wps=18198.3, ups=0.28, wpb=65536, bsz=128, num_updates=9400, lr=0.000326164, gnorm=0.511, train_wall=218, gb_free=7.9, wall=25531
2022-05-18 04:57:31 | INFO | train_inner | epoch 007:    200 / 1550 loss=5.019, ppl=32.42, wps=25033.3, ups=0.38, wpb=65536, bsz=128, num_updates=9500, lr=0.000324443, gnorm=0.521, train_wall=220, gb_free=7.9, wall=25793
2022-05-18 05:01:37 | INFO | train_inner | epoch 007:    300 / 1550 loss=5.023, ppl=32.51, wps=26651.6, ups=0.41, wpb=65536, bsz=128, num_updates=9600, lr=0.000322749, gnorm=0.514, train_wall=218, gb_free=7.9, wall=26039
2022-05-18 05:08:58 | INFO | train_inner | epoch 007:    400 / 1550 loss=5.023, ppl=32.51, wps=14876.6, ups=0.23, wpb=65536, bsz=128, num_updates=9700, lr=0.000321081, gnorm=0.517, train_wall=304, gb_free=7.9, wall=26479
2022-05-18 05:14:37 | INFO | train_inner | epoch 007:    500 / 1550 loss=5.016, ppl=32.37, wps=19299.4, ups=0.29, wpb=65536, bsz=128, num_updates=9800, lr=0.000319438, gnorm=0.509, train_wall=254, gb_free=7.9, wall=26819
2022-05-18 05:19:41 | INFO | train_inner | epoch 007:    600 / 1550 loss=5.029, ppl=32.66, wps=21572.7, ups=0.33, wpb=65536, bsz=128, num_updates=9900, lr=0.000317821, gnorm=0.518, train_wall=229, gb_free=7.9, wall=27123
2022-05-18 05:24:47 | INFO | train_inner | epoch 007:    700 / 1550 loss=5.01, ppl=32.23, wps=21429.5, ups=0.33, wpb=65536, bsz=128, num_updates=10000, lr=0.000316228, gnorm=0.513, train_wall=234, gb_free=7.9, wall=27428
2022-05-18 05:29:48 | INFO | train_inner | epoch 007:    800 / 1550 loss=5.017, ppl=32.37, wps=21784.4, ups=0.33, wpb=65536, bsz=128, num_updates=10100, lr=0.000314658, gnorm=0.512, train_wall=234, gb_free=7.9, wall=27729
2022-05-18 05:35:56 | INFO | train_inner | epoch 007:    900 / 1550 loss=5.018, ppl=32.4, wps=17791.4, ups=0.27, wpb=65536, bsz=128, num_updates=10200, lr=0.000313112, gnorm=0.52, train_wall=266, gb_free=7.9, wall=28098
2022-05-18 05:41:44 | INFO | train_inner | epoch 007:   1000 / 1550 loss=5.017, ppl=32.39, wps=18857.6, ups=0.29, wpb=65534.5, bsz=128, num_updates=10300, lr=0.000311588, gnorm=0.521, train_wall=264, gb_free=7.9, wall=28445
2022-05-18 05:46:40 | INFO | train_inner | epoch 007:   1100 / 1550 loss=5.012, ppl=32.27, wps=22141.5, ups=0.34, wpb=65536, bsz=128, num_updates=10400, lr=0.000310087, gnorm=0.515, train_wall=227, gb_free=7.9, wall=28741
2022-05-18 05:52:04 | INFO | train_inner | epoch 007:   1200 / 1550 loss=5.024, ppl=32.54, wps=20186.2, ups=0.31, wpb=65536, bsz=128, num_updates=10500, lr=0.000308607, gnorm=0.508, train_wall=247, gb_free=7.9, wall=29066
2022-05-18 05:57:34 | INFO | train_inner | epoch 007:   1300 / 1550 loss=5.012, ppl=32.28, wps=19898.9, ups=0.3, wpb=65536, bsz=128, num_updates=10600, lr=0.000307148, gnorm=0.514, train_wall=246, gb_free=7.9, wall=29395
2022-05-18 06:02:02 | INFO | train_inner | epoch 007:   1400 / 1550 loss=5.013, ppl=32.29, wps=24424.3, ups=0.37, wpb=65536, bsz=128, num_updates=10700, lr=0.000305709, gnorm=0.51, train_wall=222, gb_free=7.9, wall=29663
2022-05-18 06:06:43 | INFO | train_inner | epoch 007:   1500 / 1550 loss=5.001, ppl=32.02, wps=23362.7, ups=0.36, wpb=65536, bsz=128, num_updates=10800, lr=0.00030429, gnorm=0.519, train_wall=224, gb_free=7.9, wall=29944
2022-05-18 06:08:51 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-05-18 06:10:25 | INFO | valid | epoch 007 | valid on 'valid' subset | loss 4.95 | ppl 30.91 | wps 60354.2 | wpb 2047.4 | bsz 4 | num_updates 10850 | best_loss 4.95
2022-05-18 06:10:25 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 7 @ 10850 updates
2022-05-18 06:10:25 | INFO | fairseq.trainer | Saving checkpoint to /cluster/work/cotterell/liam/master-thesis/checkpoints/language_model/bart/checkpoint_best_bart_32_5.0000E-04_full.pt
2022-05-18 06:10:27 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/work/cotterell/liam/master-thesis/checkpoints/language_model/bart/checkpoint_best_bart_32_5.0000E-04_full.pt
2022-05-18 06:10:27 | INFO | fairseq.checkpoint_utils | Saved checkpoint checkpoints/language_model/bart/checkpoint_best_bart_32_5.0000E-04_full.pt (epoch 7 @ 10850 updates, score 4.95) (writing took 2.673938866239041 seconds)
2022-05-18 06:10:27 | INFO | fairseq_cli.train | end of epoch 7 (average epoch stats below)
2022-05-18 06:10:27 | INFO | train | epoch 007 | loss 5.016 | ppl 32.36 | wps 20639.5 | ups 0.31 | wpb 65522.7 | bsz 128 | num_updates 10850 | lr 0.000303588 | gnorm 0.515 | train_wall 3716 | gb_free 7.9 | wall 30169
2022-05-18 06:10:28 | INFO | fairseq.data.iterators | grouped total_num_itrs = 1550
2022-05-18 06:10:28 | INFO | fairseq.trainer | begin training epoch 8
2022-05-18 06:10:28 | INFO | fairseq_cli.train | Start iterating over samples
2022-05-18 06:13:08 | INFO | train_inner | epoch 008:     50 / 1550 loss=4.977, ppl=31.49, wps=16952.5, ups=0.26, wpb=65331.2, bsz=127.6, num_updates=10900, lr=0.000302891, gnorm=0.515, train_wall=218, gb_free=7.9, wall=30329
2022-05-18 06:20:24 | INFO | train_inner | epoch 008:    150 / 1550 loss=4.935, ppl=30.58, wps=15035.6, ups=0.23, wpb=65536, bsz=128, num_updates=11000, lr=0.000301511, gnorm=0.515, train_wall=315, gb_free=7.9, wall=30765
2022-05-18 06:26:28 | INFO | train_inner | epoch 008:    250 / 1550 loss=4.937, ppl=30.63, wps=18003.8, ups=0.27, wpb=65536, bsz=128, num_updates=11100, lr=0.00030015, gnorm=0.521, train_wall=274, gb_free=7.9, wall=31129
2022-05-18 06:32:49 | INFO | train_inner | epoch 008:    350 / 1550 loss=4.96, ppl=31.13, wps=17189, ups=0.26, wpb=65536, bsz=128, num_updates=11200, lr=0.000298807, gnorm=0.513, train_wall=286, gb_free=7.9, wall=31510
2022-05-18 06:37:12 | INFO | train_inner | epoch 008:    450 / 1550 loss=4.955, ppl=31.01, wps=24896.3, ups=0.38, wpb=65536, bsz=128, num_updates=11300, lr=0.000297482, gnorm=0.521, train_wall=218, gb_free=7.9, wall=31774
ERROR: Unexpected bus error encountered in worker. This might be caused by insufficient shared memory (shm).
 Traceback (most recent call last):
  File "/cluster/work/cotterell/liam/fair_env/lib64/python3.8/site-packages/torch/utils/data/dataloader.py", line 990, in _try_get_data
    data = self._data_queue.get(timeout=timeout)
  File "/cluster/apps/nss/gcc-6.3.0/python/3.8.5/x86_64/lib64/python3.8/queue.py", line 178, in get
    raise Empty
_queue.Empty

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/cluster/work/cotterell/liam/fair_env/bin/fairseq-train", line 33, in <module>
    sys.exit(load_entry_point('fairseq', 'console_scripts', 'fairseq-train')())
  File "/cluster/work/cotterell/liam/fairseq/fairseq_cli/train.py", line 535, in cli_main
    distributed_utils.call_main(cfg, main)
  File "/cluster/work/cotterell/liam/fairseq/fairseq/distributed/utils.py", line 369, in call_main
    main(cfg, **kwargs)
  File "/cluster/work/cotterell/liam/fairseq/fairseq_cli/train.py", line 190, in main
    valid_losses, should_stop = train(cfg, trainer, task, epoch_itr)
  File "/cluster/apps/nss/gcc-6.3.0/python/3.8.5/x86_64/lib64/python3.8/contextlib.py", line 75, in inner
    return func(*args, **kwds)
  File "/cluster/work/cotterell/liam/fairseq/fairseq_cli/train.py", line 301, in train
    for i, samples in enumerate(progress):
  File "/cluster/work/cotterell/liam/fairseq/fairseq/logging/progress_bar.py", line 261, in __iter__
    for i, obj in enumerate(self.iterable, start=self.n):
  File "/cluster/work/cotterell/liam/fairseq/fairseq/data/iterators.py", line 56, in __next__
    x = next(self._itr)
  File "/cluster/work/cotterell/liam/fairseq/fairseq/data/iterators.py", line 557, in _chunk_iterator
    for x in itr:
  File "/cluster/work/cotterell/liam/fairseq/fairseq/data/iterators.py", line 56, in __next__
    x = next(self._itr)
  File "/cluster/work/cotterell/liam/fairseq/fairseq/data/iterators.py", line 690, in __next__
    raise item
  File "/cluster/work/cotterell/liam/fairseq/fairseq/data/iterators.py", line 620, in run
    for item in self._source:
  File "/cluster/work/cotterell/liam/fair_env/lib64/python3.8/site-packages/torch/utils/data/dataloader.py", line 521, in __next__
    data = self._next_data()
  File "/cluster/work/cotterell/liam/fair_env/lib64/python3.8/site-packages/torch/utils/data/dataloader.py", line 1186, in _next_data
    idx, data = self._get_data()
  File "/cluster/work/cotterell/liam/fair_env/lib64/python3.8/site-packages/torch/utils/data/dataloader.py", line 1142, in _get_data
    success, data = self._try_get_data()
  File "/cluster/work/cotterell/liam/fair_env/lib64/python3.8/site-packages/torch/utils/data/dataloader.py", line 1003, in _try_get_data
    raise RuntimeError('DataLoader worker (pid(s) {}) exited unexpectedly'.format(pids_str)) from e
RuntimeError: DataLoader worker (pid(s) 27987) exited unexpectedly
Sender: LSF System <lsfadmin@eu-g3-048>
Subject: Job 218938764: <train_lang_bart_32_5.0000E-04_full> in cluster <euler> Exited

Job <train_lang_bart_32_5.0000E-04_full> was submitted from host <eu-login-45> by user <euler_username> in cluster <euler> at Wed May 18 09:40:03 2022
Job was executed on host(s) <4*eu-g3-048>, in queue <gpu.120h>, as user <euler_username> in cluster <euler> at Wed May 18 09:40:36 2022
</cluster/home/euler_username> was used as the home directory.
</cluster/work/cotterell/liam/master-thesis> was used as the working directory.
Started at Wed May 18 09:40:36 2022
Terminated at Fri May 20 19:49:27 2022
Results reported at Fri May 20 19:49:27 2022

Your job looked like:

------------------------------------------------------------
# LSBATCH: User input
fairseq-train data/xsum-lang-bart-full --save-dir checkpoints/language_model/bart --update-freq 32 --lr 0.0005 --checkpoint-suffix _bart_32_5.0000E-04_full --restore-file checkpoints/language_model/bart/checkpoint_best.pt --task language_modeling --arch transformer_lm --share-decoder-input-output-embed --dropout 0.1 --optimizer adam --adam-betas "(0.9, 0.98)" --weight-decay 0.01 --clip-norm 0.0 --lr-scheduler inverse_sqrt --warmup-updates 4000 --warmup-init-lr 1e-07 --tokens-per-sample 512 --sample-break-mode none --max-tokens 2048 --no-epoch-checkpoints --no-last-checkpoints --patience 5
------------------------------------------------------------

Exited with exit code 135.

Resource usage summary:

    CPU time :                                   148177.00 sec.
    Max Memory :                                 4704 MB
    Average Memory :                             2579.78 MB
    Total Requested Memory :                     8192.00 MB
    Delta Memory :                               3488.00 MB
    Max Swap :                                   971 MB
    Max Processes :                              4
    Max Threads :                                14
    Run time :                                   209330 sec.
    Turnaround time :                            209364 sec.

The output (if any) follows:

2022-05-18 09:42:18 | INFO | fairseq.tasks.text_to_speech | Please install tensorboardX: pip install tensorboardX
2022-05-18 09:42:26 | INFO | fairseq_cli.train | {'_name': None, 'common': {'_name': None, 'no_progress_bar': False, 'log_interval': 100, 'log_format': None, 'log_file': None, 'tensorboard_logdir': None, 'wandb_project': None, 'azureml_logging': False, 'seed': 1, 'cpu': False, 'tpu': False, 'bf16': False, 'memory_efficient_bf16': False, 'fp16': False, 'memory_efficient_fp16': False, 'fp16_no_flatten_grads': False, 'fp16_init_scale': 128, 'fp16_scale_window': None, 'fp16_scale_tolerance': 0.0, 'on_cpu_convert_precision': False, 'min_loss_scale': 0.0001, 'threshold_loss_scale': None, 'amp': False, 'amp_batch_retries': 2, 'amp_init_scale': 128, 'amp_scale_window': None, 'user_dir': None, 'empty_cache_freq': 0, 'all_gather_list_size': 16384, 'model_parallel_size': 1, 'quantization_config_path': None, 'profile': False, 'reset_logging': False, 'suppress_crashes': False, 'use_plasma_view': False, 'plasma_path': '/tmp/plasma'}, 'common_eval': {'_name': None, 'path': None, 'post_process': None, 'quiet': False, 'model_overrides': '{}', 'results_path': None, 'print_tokens': False}, 'distributed_training': {'_name': None, 'distributed_world_size': 1, 'distributed_num_procs': 1, 'distributed_rank': 0, 'distributed_backend': 'nccl', 'distributed_init_method': None, 'distributed_port': -1, 'device_id': 0, 'distributed_no_spawn': False, 'ddp_backend': 'pytorch_ddp', 'ddp_comm_hook': 'none', 'bucket_cap_mb': 25, 'fix_batches_to_gpus': False, 'find_unused_parameters': False, 'gradient_as_bucket_view': False, 'fast_stat_sync': False, 'heartbeat_timeout': -1, 'broadcast_buffers': False, 'slowmo_momentum': None, 'slowmo_base_algorithm': 'localsgd', 'localsgd_frequency': 3, 'nprocs_per_node': 1, 'pipeline_model_parallel': False, 'pipeline_balance': None, 'pipeline_devices': None, 'pipeline_chunks': 0, 'pipeline_encoder_balance': None, 'pipeline_encoder_devices': None, 'pipeline_decoder_balance': None, 'pipeline_decoder_devices': None, 'pipeline_checkpoint': 'never', 'zero_sharding': 'none', 'fp16': False, 'memory_efficient_fp16': False, 'tpu': False, 'no_reshard_after_forward': False, 'fp32_reduce_scatter': False, 'cpu_offload': False, 'use_sharded_state': False, 'not_fsdp_flatten_parameters': False}, 'dataset': {'_name': None, 'num_workers': 1, 'skip_invalid_size_inputs_valid_test': False, 'max_tokens': 2048, 'batch_size': None, 'required_batch_size_multiple': 8, 'required_seq_len_multiple': 1, 'dataset_impl': None, 'data_buffer_size': 10, 'train_subset': 'train', 'valid_subset': 'valid', 'combine_valid_subsets': None, 'ignore_unused_valid_subsets': False, 'validate_interval': 1, 'validate_interval_updates': 0, 'validate_after_updates': 0, 'fixed_validation_seed': None, 'disable_validation': False, 'max_tokens_valid': 2048, 'batch_size_valid': None, 'max_valid_steps': None, 'curriculum': 0, 'gen_subset': 'test', 'num_shards': 1, 'shard_id': 0, 'grouped_shuffling': False, 'update_epoch_batch_itr': False, 'update_ordered_indices_seed': False}, 'optimization': {'_name': None, 'max_epoch': 0, 'max_update': 0, 'stop_time_hours': 0.0, 'clip_norm': 0.0, 'sentence_avg': False, 'update_freq': [32], 'lr': [0.0005], 'stop_min_lr': -1.0, 'use_bmuf': False, 'skip_remainder_batch': False}, 'checkpoint': {'_name': None, 'save_dir': 'checkpoints/language_model/bart', 'restore_file': 'checkpoints/language_model/bart/checkpoint_best.pt', 'continue_once': None, 'finetune_from_model': None, 'reset_dataloader': False, 'reset_lr_scheduler': False, 'reset_meters': False, 'reset_optimizer': False, 'optimizer_overrides': '{}', 'save_interval': 1, 'save_interval_updates': 0, 'keep_interval_updates': -1, 'keep_interval_updates_pattern': -1, 'keep_last_epochs': -1, 'keep_best_checkpoints': -1, 'no_save': False, 'no_epoch_checkpoints': True, 'no_last_checkpoints': True, 'no_save_optimizer_state': False, 'best_checkpoint_metric': 'loss', 'maximize_best_checkpoint_metric': False, 'patience': 5, 'checkpoint_suffix': '_bart_32_5.0000E-04_full', 'checkpoint_shard_count': 1, 'load_checkpoint_on_all_dp_ranks': False, 'write_checkpoints_asynchronously': False, 'model_parallel_size': 1}, 'bmuf': {'_name': None, 'block_lr': 1.0, 'block_momentum': 0.875, 'global_sync_iter': 50, 'warmup_iterations': 500, 'use_nbm': False, 'average_sync': False, 'distributed_world_size': 1}, 'generation': {'_name': None, 'beam': 5, 'nbest': 1, 'max_len_a': 0.0, 'max_len_b': 200, 'min_len': 1, 'match_source_len': False, 'unnormalized': False, 'no_early_stop': False, 'no_beamable_mm': False, 'lenpen': 1.0, 'unkpen': 0.0, 'replace_unk': None, 'sacrebleu': False, 'score_reference': False, 'prefix_size': 0, 'no_repeat_ngram_size': 0, 'sampling': False, 'sampling_topk': -1, 'sampling_topp': -1.0, 'constraints': None, 'temperature': 1.0, 'diverse_beam_groups': -1, 'diverse_beam_strength': 0.5, 'diversity_rate': -1.0, 'print_alignment': None, 'print_step': False, 'lm_path': None, 'ent_threshold': 0.0, 'lm_weight': 0.0, 'iter_decode_eos_penalty': 0.0, 'iter_decode_max_iter': 10, 'iter_decode_force_max_iter': False, 'iter_decode_with_beam': 1, 'iter_decode_with_external_reranker': False, 'retain_iter_history': False, 'retain_dropout': False, 'retain_dropout_modules': None, 'decoding_format': None, 'no_seed_provided': False}, 'eval_lm': {'_name': None, 'output_word_probs': False, 'output_word_stats': False, 'context_window': 0, 'softmax_batch': 9223372036854775807}, 'interactive': {'_name': None, 'buffer_size': 0, 'input': '-'}, 'model': {'_name': 'transformer_lm', 'activation_fn': relu, 'dropout': 0.1, 'attention_dropout': 0.0, 'activation_dropout': 0.0, 'relu_dropout': 0.0, 'decoder_embed_dim': 512, 'decoder_output_dim': 512, 'decoder_input_dim': 512, 'decoder_ffn_embed_dim': 2048, 'decoder_layers': 6, 'decoder_attention_heads': 8, 'decoder_normalize_before': False, 'no_decoder_final_norm': False, 'adaptive_softmax_cutoff': None, 'adaptive_softmax_dropout': 0.0, 'adaptive_softmax_factor': 4.0, 'no_token_positional_embeddings': False, 'share_decoder_input_output_embed': True, 'character_embeddings': False, 'character_filters': '[(1, 64), (2, 128), (3, 192), (4, 256), (5, 256), (6, 256), (7, 256)]', 'character_embedding_dim': 4, 'char_embedder_highway_layers': 2, 'adaptive_input': False, 'adaptive_input_factor': 4.0, 'adaptive_input_cutoff': None, 'tie_adaptive_weights': False, 'tie_adaptive_proj': False, 'decoder_learned_pos': False, 'layernorm_embedding': False, 'no_scale_embedding': False, 'checkpoint_activations': False, 'offload_activations': False, 'decoder_layerdrop': 0.0, 'decoder_layers_to_keep': None, 'quant_noise_pq': 0.0, 'quant_noise_pq_block_size': 8, 'quant_noise_scalar': 0.0, 'min_params_to_wrap': 100000000, 'base_layers': 0, 'base_sublayers': 1, 'base_shuffle': 1, 'scale_fc': False, 'scale_attn': False, 'scale_heads': False, 'scale_resids': False, 'add_bos_token': False, 'tokens_per_sample': 512, 'max_target_positions': None, 'tpu': False}, 'task': {'_name': 'language_modeling', 'data': 'data/xsum-lang-bart-full', 'sample_break_mode': none, 'tokens_per_sample': 512, 'output_dictionary_size': -1, 'self_target': False, 'future_target': False, 'past_target': False, 'add_bos_token': False, 'max_target_positions': None, 'shorten_method': none, 'shorten_data_split_list': '', 'pad_to_fixed_length': False, 'pad_to_fixed_bsz': False, 'seed': 1, 'batch_size': None, 'batch_size_valid': None, 'dataset_impl': None, 'data_buffer_size': 10, 'tpu': False, 'use_plasma_view': False, 'plasma_path': '/tmp/plasma'}, 'criterion': {'_name': 'cross_entropy', 'sentence_avg': False}, 'optimizer': {'_name': 'adam', 'adam_betas': '(0.9, 0.98)', 'adam_eps': 1e-08, 'weight_decay': 0.01, 'use_old_adam': False, 'fp16_adam_stats': False, 'tpu': False, 'lr': [0.0005]}, 'lr_scheduler': {'_name': 'inverse_sqrt', 'warmup_updates': 4000, 'warmup_init_lr': 1e-07, 'lr': [0.0005]}, 'scoring': {'_name': 'bleu', 'pad': 1, 'eos': 2, 'unk': 3}, 'bpe': None, 'tokenizer': None, 'ema': {'_name': None, 'store_ema': False, 'ema_decay': 0.9999, 'ema_start_update': 0, 'ema_seed_model': None, 'ema_update_freq': 1, 'ema_fp32': False}, 'simul_type': None}
2022-05-18 09:42:26 | INFO | fairseq.tasks.language_modeling | dictionary: 50264 types
2022-05-18 09:42:28 | INFO | fairseq_cli.train | TransformerLanguageModel(
  (decoder): TransformerDecoder(
    (dropout_module): FairseqDropout()
    (embed_tokens): Embedding(50264, 512, padding_idx=1)
    (embed_positions): SinusoidalPositionalEmbedding()
    (layers): ModuleList(
      (0): TransformerDecoderLayerBase(
        (dropout_module): FairseqDropout()
        (self_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=512, out_features=512, bias=True)
          (v_proj): Linear(in_features=512, out_features=512, bias=True)
          (q_proj): Linear(in_features=512, out_features=512, bias=True)
          (out_proj): Linear(in_features=512, out_features=512, bias=True)
        )
        (activation_dropout_module): FairseqDropout()
        (self_attn_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
        (fc1): Linear(in_features=512, out_features=2048, bias=True)
        (fc2): Linear(in_features=2048, out_features=512, bias=True)
        (final_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
      )
      (1): TransformerDecoderLayerBase(
        (dropout_module): FairseqDropout()
        (self_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=512, out_features=512, bias=True)
          (v_proj): Linear(in_features=512, out_features=512, bias=True)
          (q_proj): Linear(in_features=512, out_features=512, bias=True)
          (out_proj): Linear(in_features=512, out_features=512, bias=True)
        )
        (activation_dropout_module): FairseqDropout()
        (self_attn_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
        (fc1): Linear(in_features=512, out_features=2048, bias=True)
        (fc2): Linear(in_features=2048, out_features=512, bias=True)
        (final_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
      )
      (2): TransformerDecoderLayerBase(
        (dropout_module): FairseqDropout()
        (self_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=512, out_features=512, bias=True)
          (v_proj): Linear(in_features=512, out_features=512, bias=True)
          (q_proj): Linear(in_features=512, out_features=512, bias=True)
          (out_proj): Linear(in_features=512, out_features=512, bias=True)
        )
        (activation_dropout_module): FairseqDropout()
        (self_attn_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
        (fc1): Linear(in_features=512, out_features=2048, bias=True)
        (fc2): Linear(in_features=2048, out_features=512, bias=True)
        (final_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
      )
      (3): TransformerDecoderLayerBase(
        (dropout_module): FairseqDropout()
        (self_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=512, out_features=512, bias=True)
          (v_proj): Linear(in_features=512, out_features=512, bias=True)
          (q_proj): Linear(in_features=512, out_features=512, bias=True)
          (out_proj): Linear(in_features=512, out_features=512, bias=True)
        )
        (activation_dropout_module): FairseqDropout()
        (self_attn_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
        (fc1): Linear(in_features=512, out_features=2048, bias=True)
        (fc2): Linear(in_features=2048, out_features=512, bias=True)
        (final_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
      )
      (4): TransformerDecoderLayerBase(
        (dropout_module): FairseqDropout()
        (self_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=512, out_features=512, bias=True)
          (v_proj): Linear(in_features=512, out_features=512, bias=True)
          (q_proj): Linear(in_features=512, out_features=512, bias=True)
          (out_proj): Linear(in_features=512, out_features=512, bias=True)
        )
        (activation_dropout_module): FairseqDropout()
        (self_attn_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
        (fc1): Linear(in_features=512, out_features=2048, bias=True)
        (fc2): Linear(in_features=2048, out_features=512, bias=True)
        (final_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
      )
      (5): TransformerDecoderLayerBase(
        (dropout_module): FairseqDropout()
        (self_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=512, out_features=512, bias=True)
          (v_proj): Linear(in_features=512, out_features=512, bias=True)
          (q_proj): Linear(in_features=512, out_features=512, bias=True)
          (out_proj): Linear(in_features=512, out_features=512, bias=True)
        )
        (activation_dropout_module): FairseqDropout()
        (self_attn_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
        (fc1): Linear(in_features=512, out_features=2048, bias=True)
        (fc2): Linear(in_features=2048, out_features=512, bias=True)
        (final_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
      )
    )
    (output_projection): Linear(in_features=512, out_features=50264, bias=False)
  )
)
2022-05-18 09:42:28 | INFO | fairseq_cli.train | task: LanguageModelingTask
2022-05-18 09:42:28 | INFO | fairseq_cli.train | model: TransformerLanguageModel
2022-05-18 09:42:28 | INFO | fairseq_cli.train | criterion: CrossEntropyCriterion
2022-05-18 09:42:28 | INFO | fairseq_cli.train | num. shared model params: 44,649,472 (num. trained: 44,649,472)
2022-05-18 09:42:28 | INFO | fairseq_cli.train | num. expert model params: 0 (num. trained: 0)
2022-05-18 09:42:28 | INFO | fairseq.data.data_utils | loaded 22,664 examples from: data/xsum-lang-bart-full/valid
2022-05-18 09:43:11 | INFO | fairseq.trainer | detected shared parameter: decoder.embed_tokens.weight <- decoder.output_projection.weight
2022-05-18 09:43:11 | INFO | fairseq.utils | ***********************CUDA enviroments for all 1 workers***********************
2022-05-18 09:43:11 | INFO | fairseq.utils | rank   0: capabilities =  7.5  ; total memory = 10.761 GB ; name = NVIDIA GeForce RTX 2080 Ti              
2022-05-18 09:43:11 | INFO | fairseq.utils | ***********************CUDA enviroments for all 1 workers***********************
2022-05-18 09:43:11 | INFO | fairseq_cli.train | training on 1 devices (GPUs/TPUs)
2022-05-18 09:43:11 | INFO | fairseq_cli.train | max tokens per device = 2048 and max sentences per device = None
2022-05-18 09:43:11 | INFO | fairseq.trainer | Preparing to load checkpoint checkpoints/language_model/bart/checkpoint_best_bart_32_5.0000E-04_full.pt
2022-05-18 09:43:14 | INFO | fairseq.trainer | NOTE: your device may support faster training with --fp16 or --amp
2022-05-18 09:43:14 | INFO | fairseq.trainer | Loaded checkpoint checkpoints/language_model/bart/checkpoint_best_bart_32_5.0000E-04_full.pt (epoch 8 @ 10850 updates)
2022-05-18 09:43:14 | INFO | fairseq.trainer | loading train data for epoch 8
2022-05-18 09:43:14 | INFO | fairseq.data.data_utils | loaded 408,090 examples from: data/xsum-lang-bart-full/train
2022-05-18 09:43:15 | INFO | fairseq.data.iterators | grouped total_num_itrs = 1550
2022-05-18 09:43:15 | INFO | fairseq.trainer | begin training epoch 8
2022-05-18 09:43:15 | INFO | fairseq_cli.train | Start iterating over samples
2022-05-18 09:45:20 | INFO | train_inner | epoch 008:     50 / 1550 loss=4.938, ppl=30.65, wps=27346.2, ups=0.42, wpb=65536, bsz=128, num_updates=10900, lr=0.000302891, gnorm=0.51, train_wall=119, gb_free=7.9, wall=129
2022-05-18 09:50:03 | INFO | train_inner | epoch 008:    150 / 1550 loss=4.935, ppl=30.58, wps=23151.4, ups=0.35, wpb=65536, bsz=128, num_updates=11000, lr=0.000301511, gnorm=0.515, train_wall=252, gb_free=7.9, wall=412
2022-05-18 09:55:45 | INFO | train_inner | epoch 008:    250 / 1550 loss=4.937, ppl=30.63, wps=19186.7, ups=0.29, wpb=65536, bsz=128, num_updates=11100, lr=0.00030015, gnorm=0.521, train_wall=244, gb_free=7.9, wall=754
2022-05-18 10:00:58 | INFO | train_inner | epoch 008:    350 / 1550 loss=4.96, ppl=31.13, wps=20949.6, ups=0.32, wpb=65536, bsz=128, num_updates=11200, lr=0.000298807, gnorm=0.513, train_wall=236, gb_free=7.9, wall=1067
2022-05-18 10:05:50 | INFO | train_inner | epoch 008:    450 / 1550 loss=4.955, ppl=31.01, wps=22425.3, ups=0.34, wpb=65536, bsz=128, num_updates=11300, lr=0.000297482, gnorm=0.521, train_wall=231, gb_free=7.9, wall=1359
2022-05-18 10:11:16 | INFO | train_inner | epoch 008:    550 / 1550 loss=4.947, ppl=30.85, wps=20134.5, ups=0.31, wpb=65536, bsz=128, num_updates=11400, lr=0.000296174, gnorm=0.519, train_wall=253, gb_free=7.9, wall=1684
2022-05-18 10:16:30 | INFO | train_inner | epoch 008:    650 / 1550 loss=4.957, ppl=31.06, wps=20868.4, ups=0.32, wpb=65536, bsz=128, num_updates=11500, lr=0.000294884, gnorm=0.52, train_wall=233, gb_free=7.9, wall=1998
2022-05-18 10:21:37 | INFO | train_inner | epoch 008:    750 / 1550 loss=4.96, ppl=31.12, wps=21350.8, ups=0.33, wpb=65536, bsz=128, num_updates=11600, lr=0.00029361, gnorm=0.513, train_wall=232, gb_free=7.9, wall=2305
2022-05-18 10:26:57 | INFO | train_inner | epoch 008:    850 / 1550 loss=4.962, ppl=31.16, wps=20417.6, ups=0.31, wpb=65534.5, bsz=128, num_updates=11700, lr=0.000292353, gnorm=0.516, train_wall=243, gb_free=7.9, wall=2626
2022-05-18 10:32:32 | INFO | train_inner | epoch 008:    950 / 1550 loss=4.952, ppl=30.96, wps=19605.7, ups=0.3, wpb=65536, bsz=128, num_updates=11800, lr=0.000291111, gnorm=0.521, train_wall=254, gb_free=7.9, wall=2961
2022-05-18 10:38:20 | INFO | train_inner | epoch 008:   1050 / 1550 loss=4.952, ppl=30.94, wps=18825.4, ups=0.29, wpb=65536, bsz=128, num_updates=11900, lr=0.000289886, gnorm=0.515, train_wall=262, gb_free=7.9, wall=3309
2022-05-18 10:43:31 | INFO | train_inner | epoch 008:   1150 / 1550 loss=4.946, ppl=30.82, wps=21038.6, ups=0.32, wpb=65536, bsz=128, num_updates=12000, lr=0.000288675, gnorm=0.529, train_wall=238, gb_free=7.9, wall=3620
2022-05-18 10:49:07 | INFO | train_inner | epoch 008:   1250 / 1550 loss=4.951, ppl=30.92, wps=19503.9, ups=0.3, wpb=65536, bsz=128, num_updates=12100, lr=0.00028748, gnorm=0.524, train_wall=241, gb_free=7.9, wall=3956
2022-05-18 10:54:17 | INFO | train_inner | epoch 008:   1350 / 1550 loss=4.92, ppl=30.28, wps=21183.3, ups=0.32, wpb=65536, bsz=128, num_updates=12200, lr=0.000286299, gnorm=0.511, train_wall=243, gb_free=7.9, wall=4266
2022-05-18 10:59:55 | INFO | train_inner | epoch 008:   1450 / 1550 loss=4.949, ppl=30.89, wps=19383.5, ups=0.3, wpb=65536, bsz=128, num_updates=12300, lr=0.000285133, gnorm=0.519, train_wall=248, gb_free=7.9, wall=4604
2022-05-18 11:05:38 | INFO | train_inner | epoch 008:   1550 / 1550 loss=4.945, ppl=30.8, wps=19051.2, ups=0.29, wpb=65331.2, bsz=127.6, num_updates=12400, lr=0.000283981, gnorm=0.525, train_wall=257, gb_free=7.9, wall=4947
2022-05-18 11:05:38 | INFO | fairseq_cli.train | begin validation on "valid" subset
/cluster/work/cotterell/liam/fairseq/fairseq/utils.py:374: UserWarning: amp_C fused kernels unavailable, disabling multi_tensor_l2norm; you may get better performance by installing NVIDIA's apex library
  warnings.warn(
2022-05-18 11:06:55 | INFO | valid | epoch 008 | valid on 'valid' subset | loss 4.901 | ppl 29.87 | wps 73255.8 | wpb 2047.4 | bsz 4 | num_updates 12400 | best_loss 4.901
2022-05-18 11:06:55 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 8 @ 12400 updates
2022-05-18 11:06:55 | INFO | fairseq.trainer | Saving checkpoint to /cluster/work/cotterell/liam/master-thesis/checkpoints/language_model/bart/checkpoint_best_bart_32_5.0000E-04_full.pt
2022-05-18 11:06:57 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/work/cotterell/liam/master-thesis/checkpoints/language_model/bart/checkpoint_best_bart_32_5.0000E-04_full.pt
2022-05-18 11:06:57 | INFO | fairseq.checkpoint_utils | Saved checkpoint checkpoints/language_model/bart/checkpoint_best_bart_32_5.0000E-04_full.pt (epoch 8 @ 12400 updates, score 4.901) (writing took 1.678394516929984 seconds)
2022-05-18 11:06:57 | INFO | fairseq_cli.train | end of epoch 8 (average epoch stats below)
2022-05-18 11:06:57 | INFO | train | epoch 008 | loss 4.948 | ppl 30.87 | wps 20243.3 | ups 0.31 | wpb 65522.7 | bsz 128 | num_updates 12400 | lr 0.000283981 | gnorm 0.519 | train_wall 3788 | gb_free 7.9 | wall 5025
2022-05-18 11:06:57 | INFO | fairseq.data.iterators | grouped total_num_itrs = 1550
2022-05-18 11:06:57 | INFO | fairseq.trainer | begin training epoch 9
2022-05-18 11:06:57 | INFO | fairseq_cli.train | Start iterating over samples
2022-05-18 11:12:46 | INFO | train_inner | epoch 009:    100 / 1550 loss=4.862, ppl=29.08, wps=15287.4, ups=0.23, wpb=65536, bsz=128, num_updates=12500, lr=0.000282843, gnorm=0.52, train_wall=251, gb_free=7.9, wall=5375
2022-05-18 11:19:07 | INFO | train_inner | epoch 009:    200 / 1550 loss=4.885, ppl=29.55, wps=17209.1, ups=0.26, wpb=65536, bsz=128, num_updates=12600, lr=0.000281718, gnorm=0.521, train_wall=270, gb_free=7.9, wall=5756
2022-05-18 11:25:24 | INFO | train_inner | epoch 009:    300 / 1550 loss=4.893, ppl=29.72, wps=17406.5, ups=0.27, wpb=65534.5, bsz=128, num_updates=12700, lr=0.000280607, gnorm=0.518, train_wall=260, gb_free=7.9, wall=6133
2022-05-18 11:31:41 | INFO | train_inner | epoch 009:    400 / 1550 loss=4.901, ppl=29.88, wps=17375.4, ups=0.27, wpb=65536, bsz=128, num_updates=12800, lr=0.000279508, gnorm=0.53, train_wall=267, gb_free=7.9, wall=6510
2022-05-18 11:38:02 | INFO | train_inner | epoch 009:    500 / 1550 loss=4.886, ppl=29.56, wps=17195.9, ups=0.26, wpb=65536, bsz=128, num_updates=12900, lr=0.000278423, gnorm=0.525, train_wall=271, gb_free=7.9, wall=6891
2022-05-18 11:44:16 | INFO | train_inner | epoch 009:    600 / 1550 loss=4.893, ppl=29.71, wps=17543.9, ups=0.27, wpb=65536, bsz=128, num_updates=13000, lr=0.00027735, gnorm=0.527, train_wall=271, gb_free=7.9, wall=7265
2022-05-18 11:49:52 | INFO | train_inner | epoch 009:    700 / 1550 loss=4.902, ppl=29.89, wps=19457.2, ups=0.3, wpb=65536, bsz=128, num_updates=13100, lr=0.000276289, gnorm=0.519, train_wall=248, gb_free=7.9, wall=7601
2022-05-18 11:55:27 | INFO | train_inner | epoch 009:    800 / 1550 loss=4.912, ppl=30.11, wps=19587.2, ups=0.3, wpb=65536, bsz=128, num_updates=13200, lr=0.000275241, gnorm=0.523, train_wall=248, gb_free=7.9, wall=7936
2022-05-18 12:01:44 | INFO | train_inner | epoch 009:    900 / 1550 loss=4.907, ppl=30, wps=17397.7, ups=0.27, wpb=65536, bsz=128, num_updates=13300, lr=0.000274204, gnorm=0.522, train_wall=271, gb_free=7.9, wall=8313
2022-05-18 12:07:09 | INFO | train_inner | epoch 009:   1000 / 1550 loss=4.893, ppl=29.71, wps=20180.9, ups=0.31, wpb=65536, bsz=128, num_updates=13400, lr=0.000273179, gnorm=0.522, train_wall=250, gb_free=7.9, wall=8637
2022-05-18 12:13:07 | INFO | train_inner | epoch 009:   1100 / 1550 loss=4.903, ppl=29.92, wps=18258.4, ups=0.28, wpb=65536, bsz=128, num_updates=13500, lr=0.000272166, gnorm=0.527, train_wall=262, gb_free=7.9, wall=8996
2022-05-18 12:18:54 | INFO | train_inner | epoch 009:   1200 / 1550 loss=4.904, ppl=29.93, wps=18911.5, ups=0.29, wpb=65536, bsz=128, num_updates=13600, lr=0.000271163, gnorm=0.521, train_wall=261, gb_free=7.9, wall=9343
2022-05-18 12:25:44 | INFO | train_inner | epoch 009:   1300 / 1550 loss=4.886, ppl=29.56, wps=15989, ups=0.24, wpb=65536, bsz=128, num_updates=13700, lr=0.000270172, gnorm=0.523, train_wall=289, gb_free=7.9, wall=9753
2022-05-18 12:31:51 | INFO | train_inner | epoch 009:   1400 / 1550 loss=4.891, ppl=29.66, wps=17827.7, ups=0.27, wpb=65536, bsz=128, num_updates=13800, lr=0.000269191, gnorm=0.521, train_wall=268, gb_free=7.9, wall=10120
2022-05-18 12:37:40 | INFO | train_inner | epoch 009:   1500 / 1550 loss=4.901, ppl=29.88, wps=18797.6, ups=0.29, wpb=65536, bsz=128, num_updates=13900, lr=0.000268221, gnorm=0.524, train_wall=261, gb_free=7.9, wall=10469
2022-05-18 12:40:28 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-05-18 12:41:52 | INFO | valid | epoch 009 | valid on 'valid' subset | loss 4.863 | ppl 29.09 | wps 66981.8 | wpb 2047.4 | bsz 4 | num_updates 13950 | best_loss 4.863
2022-05-18 12:41:52 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 9 @ 13950 updates
2022-05-18 12:41:52 | INFO | fairseq.trainer | Saving checkpoint to /cluster/work/cotterell/liam/master-thesis/checkpoints/language_model/bart/checkpoint_best_bart_32_5.0000E-04_full.pt
2022-05-18 12:41:54 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/work/cotterell/liam/master-thesis/checkpoints/language_model/bart/checkpoint_best_bart_32_5.0000E-04_full.pt
2022-05-18 12:41:54 | INFO | fairseq.checkpoint_utils | Saved checkpoint checkpoints/language_model/bart/checkpoint_best_bart_32_5.0000E-04_full.pt (epoch 9 @ 13950 updates, score 4.863) (writing took 1.658188350033015 seconds)
2022-05-18 12:41:54 | INFO | fairseq_cli.train | end of epoch 9 (average epoch stats below)
2022-05-18 12:41:54 | INFO | train | epoch 009 | loss 4.895 | ppl 29.75 | wps 17825.4 | ups 0.27 | wpb 65522.7 | bsz 128 | num_updates 13950 | lr 0.00026774 | gnorm 0.523 | train_wall 4071 | gb_free 7.9 | wall 10723
2022-05-18 12:41:54 | INFO | fairseq.data.iterators | grouped total_num_itrs = 1550
2022-05-18 12:41:54 | INFO | fairseq.trainer | begin training epoch 10
2022-05-18 12:41:54 | INFO | fairseq_cli.train | Start iterating over samples
2022-05-18 12:45:14 | INFO | train_inner | epoch 010:     50 / 1550 loss=4.87, ppl=29.24, wps=14409.4, ups=0.22, wpb=65331.2, bsz=127.6, num_updates=14000, lr=0.000267261, gnorm=0.522, train_wall=264, gb_free=7.9, wall=10922
2022-05-18 12:51:10 | INFO | train_inner | epoch 010:    150 / 1550 loss=4.836, ppl=28.56, wps=18400.3, ups=0.28, wpb=65536, bsz=128, num_updates=14100, lr=0.000266312, gnorm=0.531, train_wall=267, gb_free=7.9, wall=11279
2022-05-18 12:56:52 | INFO | train_inner | epoch 010:    250 / 1550 loss=4.838, ppl=28.61, wps=19135.9, ups=0.29, wpb=65536, bsz=128, num_updates=14200, lr=0.000265372, gnorm=0.525, train_wall=254, gb_free=7.9, wall=11621
2022-05-18 13:02:24 | INFO | train_inner | epoch 010:    350 / 1550 loss=4.845, ppl=28.73, wps=19758.1, ups=0.3, wpb=65536, bsz=128, num_updates=14300, lr=0.000264443, gnorm=0.524, train_wall=256, gb_free=7.9, wall=11953
2022-05-18 13:07:49 | INFO | train_inner | epoch 010:    450 / 1550 loss=4.849, ppl=28.81, wps=20128.1, ups=0.31, wpb=65536, bsz=128, num_updates=14400, lr=0.000263523, gnorm=0.516, train_wall=249, gb_free=7.9, wall=12278
2022-05-18 13:13:44 | INFO | train_inner | epoch 010:    550 / 1550 loss=4.855, ppl=28.93, wps=18492.1, ups=0.28, wpb=65536, bsz=128, num_updates=14500, lr=0.000262613, gnorm=0.525, train_wall=271, gb_free=7.9, wall=12633
2022-05-18 13:19:24 | INFO | train_inner | epoch 010:    650 / 1550 loss=4.848, ppl=28.8, wps=19260.3, ups=0.29, wpb=65536, bsz=128, num_updates=14600, lr=0.000261712, gnorm=0.527, train_wall=258, gb_free=7.9, wall=12973
2022-05-18 13:25:17 | INFO | train_inner | epoch 010:    750 / 1550 loss=4.853, ppl=28.9, wps=18556.5, ups=0.28, wpb=65536, bsz=128, num_updates=14700, lr=0.00026082, gnorm=0.526, train_wall=273, gb_free=7.9, wall=13326
2022-05-18 13:31:12 | INFO | train_inner | epoch 010:    850 / 1550 loss=4.851, ppl=28.85, wps=18466.4, ups=0.28, wpb=65536, bsz=128, num_updates=14800, lr=0.000259938, gnorm=0.522, train_wall=269, gb_free=7.9, wall=13681
2022-05-18 13:37:38 | INFO | train_inner | epoch 010:    950 / 1550 loss=4.864, ppl=29.12, wps=16972.8, ups=0.26, wpb=65536, bsz=128, num_updates=14900, lr=0.000259064, gnorm=0.533, train_wall=287, gb_free=7.9, wall=14067
2022-05-18 13:43:30 | INFO | train_inner | epoch 010:   1050 / 1550 loss=4.854, ppl=28.93, wps=18611.3, ups=0.28, wpb=65536, bsz=128, num_updates=15000, lr=0.000258199, gnorm=0.528, train_wall=262, gb_free=7.9, wall=14419
2022-05-18 13:49:24 | INFO | train_inner | epoch 010:   1150 / 1550 loss=4.852, ppl=28.89, wps=18559.4, ups=0.28, wpb=65534.5, bsz=128, num_updates=15100, lr=0.000257343, gnorm=0.521, train_wall=255, gb_free=7.9, wall=14772
2022-05-18 13:55:22 | INFO | train_inner | epoch 010:   1250 / 1550 loss=4.855, ppl=28.93, wps=18264.6, ups=0.28, wpb=65536, bsz=128, num_updates=15200, lr=0.000256495, gnorm=0.526, train_wall=274, gb_free=7.9, wall=15131
2022-05-18 14:01:44 | INFO | train_inner | epoch 010:   1350 / 1550 loss=4.856, ppl=28.96, wps=17155.2, ups=0.26, wpb=65536, bsz=128, num_updates=15300, lr=0.000255655, gnorm=0.528, train_wall=280, gb_free=7.9, wall=15513
2022-05-18 14:07:06 | INFO | train_inner | epoch 010:   1450 / 1550 loss=4.86, ppl=29.04, wps=20351.8, ups=0.31, wpb=65536, bsz=128, num_updates=15400, lr=0.000254824, gnorm=0.526, train_wall=244, gb_free=7.9, wall=15835
2022-05-18 14:13:10 | INFO | train_inner | epoch 010:   1550 / 1550 loss=4.863, ppl=29.1, wps=17943.2, ups=0.27, wpb=65331.2, bsz=127.6, num_updates=15500, lr=0.000254, gnorm=0.534, train_wall=272, gb_free=7.9, wall=16199
2022-05-18 14:13:10 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-05-18 14:14:26 | INFO | valid | epoch 010 | valid on 'valid' subset | loss 4.834 | ppl 28.53 | wps 75252.2 | wpb 2047.4 | bsz 4 | num_updates 15500 | best_loss 4.834
2022-05-18 14:14:26 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 10 @ 15500 updates
2022-05-18 14:14:26 | INFO | fairseq.trainer | Saving checkpoint to /cluster/work/cotterell/liam/master-thesis/checkpoints/language_model/bart/checkpoint_best_bart_32_5.0000E-04_full.pt
2022-05-18 14:14:28 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/work/cotterell/liam/master-thesis/checkpoints/language_model/bart/checkpoint_best_bart_32_5.0000E-04_full.pt
2022-05-18 14:14:28 | INFO | fairseq.checkpoint_utils | Saved checkpoint checkpoints/language_model/bart/checkpoint_best_bart_32_5.0000E-04_full.pt (epoch 10 @ 15500 updates, score 4.834) (writing took 1.6645154189318419 seconds)
2022-05-18 14:14:28 | INFO | fairseq_cli.train | end of epoch 10 (average epoch stats below)
2022-05-18 14:14:28 | INFO | train | epoch 010 | loss 4.852 | ppl 28.87 | wps 18287.4 | ups 0.28 | wpb 65522.7 | bsz 128 | num_updates 15500 | lr 0.000254 | gnorm 0.526 | train_wall 4113 | gb_free 7.9 | wall 16277
2022-05-18 14:14:28 | INFO | fairseq.data.iterators | grouped total_num_itrs = 1550
2022-05-18 14:14:28 | INFO | fairseq.trainer | begin training epoch 11
2022-05-18 14:14:28 | INFO | fairseq_cli.train | Start iterating over samples
2022-05-18 14:20:48 | INFO | train_inner | epoch 011:    100 / 1550 loss=4.8, ppl=27.86, wps=14325.5, ups=0.22, wpb=65534.5, bsz=128, num_updates=15600, lr=0.000253185, gnorm=0.541, train_wall=276, gb_free=7.9, wall=16657
2022-05-18 14:26:35 | INFO | train_inner | epoch 011:    200 / 1550 loss=4.789, ppl=27.65, wps=18910, ups=0.29, wpb=65536, bsz=128, num_updates=15700, lr=0.000252377, gnorm=0.531, train_wall=253, gb_free=7.9, wall=17003
2022-05-18 14:32:12 | INFO | train_inner | epoch 011:    300 / 1550 loss=4.818, ppl=28.22, wps=19426.8, ups=0.3, wpb=65536, bsz=128, num_updates=15800, lr=0.000251577, gnorm=0.534, train_wall=247, gb_free=7.9, wall=17341
2022-05-18 14:37:56 | INFO | train_inner | epoch 011:    400 / 1550 loss=4.813, ppl=28.1, wps=19062.8, ups=0.29, wpb=65536, bsz=128, num_updates=15900, lr=0.000250785, gnorm=0.523, train_wall=255, gb_free=7.9, wall=17685
2022-05-18 14:44:03 | INFO | train_inner | epoch 011:    500 / 1550 loss=4.803, ppl=27.92, wps=17849.3, ups=0.27, wpb=65536, bsz=128, num_updates=16000, lr=0.00025, gnorm=0.529, train_wall=262, gb_free=7.9, wall=18052
2022-05-18 14:49:36 | INFO | train_inner | epoch 011:    600 / 1550 loss=4.816, ppl=28.17, wps=19651.1, ups=0.3, wpb=65536, bsz=128, num_updates=16100, lr=0.000249222, gnorm=0.535, train_wall=252, gb_free=7.9, wall=18385
2022-05-18 14:55:15 | INFO | train_inner | epoch 011:    700 / 1550 loss=4.816, ppl=28.17, wps=19329.4, ups=0.29, wpb=65536, bsz=128, num_updates=16200, lr=0.000248452, gnorm=0.534, train_wall=257, gb_free=7.9, wall=18724
2022-05-18 15:01:12 | INFO | train_inner | epoch 011:    800 / 1550 loss=4.822, ppl=28.29, wps=18374.2, ups=0.28, wpb=65536, bsz=128, num_updates=16300, lr=0.000247689, gnorm=0.536, train_wall=262, gb_free=7.9, wall=19081
2022-05-18 15:06:24 | INFO | train_inner | epoch 011:    900 / 1550 loss=4.821, ppl=28.27, wps=21031.7, ups=0.32, wpb=65536, bsz=128, num_updates=16400, lr=0.000246932, gnorm=0.529, train_wall=236, gb_free=7.9, wall=19393
2022-05-18 15:15:41 | INFO | train_inner | epoch 011:   1000 / 1550 loss=4.812, ppl=28.09, wps=11759.1, ups=0.18, wpb=65536, bsz=128, num_updates=16500, lr=0.000246183, gnorm=0.529, train_wall=380, gb_free=7.9, wall=19950
2022-05-18 15:26:46 | INFO | train_inner | epoch 011:   1100 / 1550 loss=4.816, ppl=28.16, wps=9847.6, ups=0.15, wpb=65536, bsz=128, num_updates=16600, lr=0.00024544, gnorm=0.53, train_wall=457, gb_free=7.9, wall=20615
2022-05-18 15:36:21 | INFO | train_inner | epoch 011:   1200 / 1550 loss=4.824, ppl=28.33, wps=11403.4, ups=0.17, wpb=65536, bsz=128, num_updates=16700, lr=0.000244704, gnorm=0.532, train_wall=397, gb_free=7.9, wall=21190
2022-05-18 15:44:08 | INFO | train_inner | epoch 011:   1300 / 1550 loss=4.824, ppl=28.32, wps=14050, ups=0.21, wpb=65536, bsz=128, num_updates=16800, lr=0.000243975, gnorm=0.532, train_wall=321, gb_free=7.9, wall=21656
2022-05-18 15:51:19 | INFO | train_inner | epoch 011:   1400 / 1550 loss=4.826, ppl=28.36, wps=15206, ups=0.23, wpb=65536, bsz=128, num_updates=16900, lr=0.000243252, gnorm=0.529, train_wall=309, gb_free=7.9, wall=22087
2022-05-18 15:58:49 | INFO | train_inner | epoch 011:   1500 / 1550 loss=4.819, ppl=28.23, wps=14541.5, ups=0.22, wpb=65536, bsz=128, num_updates=17000, lr=0.000242536, gnorm=0.528, train_wall=321, gb_free=7.9, wall=22538
2022-05-18 16:02:03 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-05-18 16:04:08 | INFO | valid | epoch 011 | valid on 'valid' subset | loss 4.809 | ppl 28.03 | wps 48296.6 | wpb 2047.4 | bsz 4 | num_updates 17050 | best_loss 4.809
2022-05-18 16:04:08 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 11 @ 17050 updates
2022-05-18 16:04:08 | INFO | fairseq.trainer | Saving checkpoint to /cluster/work/cotterell/liam/master-thesis/checkpoints/language_model/bart/checkpoint_best_bart_32_5.0000E-04_full.pt
2022-05-18 16:04:09 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/work/cotterell/liam/master-thesis/checkpoints/language_model/bart/checkpoint_best_bart_32_5.0000E-04_full.pt
2022-05-18 16:04:09 | INFO | fairseq.checkpoint_utils | Saved checkpoint checkpoints/language_model/bart/checkpoint_best_bart_32_5.0000E-04_full.pt (epoch 11 @ 17050 updates, score 4.809) (writing took 1.580096217803657 seconds)
2022-05-18 16:04:09 | INFO | fairseq_cli.train | end of epoch 11 (average epoch stats below)
2022-05-18 16:04:09 | INFO | train | epoch 011 | loss 4.815 | ppl 28.15 | wps 15430.9 | ups 0.24 | wpb 65522.7 | bsz 128 | num_updates 17050 | lr 0.00024218 | gnorm 0.531 | train_wall 4623 | gb_free 7.9 | wall 22858
2022-05-18 16:04:10 | INFO | fairseq.data.iterators | grouped total_num_itrs = 1550
2022-05-18 16:04:10 | INFO | fairseq.trainer | begin training epoch 12
2022-05-18 16:04:10 | INFO | fairseq_cli.train | Start iterating over samples
2022-05-18 16:21:03 | INFO | train_inner | epoch 012:     50 / 1550 loss=4.794, ppl=27.74, wps=4898.5, ups=0.07, wpb=65331.2, bsz=127.6, num_updates=17100, lr=0.000241825, gnorm=0.533, train_wall=1019, gb_free=7.9, wall=23872
2022-05-18 16:33:08 | INFO | train_inner | epoch 012:    150 / 1550 loss=4.762, ppl=27.13, wps=9038.1, ups=0.14, wpb=65536, bsz=128, num_updates=17200, lr=0.000241121, gnorm=0.528, train_wall=495, gb_free=7.9, wall=24597
2022-05-18 16:41:29 | INFO | train_inner | epoch 012:    250 / 1550 loss=4.764, ppl=27.18, wps=13092.9, ups=0.2, wpb=65536, bsz=128, num_updates=17300, lr=0.000240424, gnorm=0.532, train_wall=351, gb_free=7.9, wall=25098
2022-05-18 16:49:02 | INFO | train_inner | epoch 012:    350 / 1550 loss=4.78, ppl=27.48, wps=14451.5, ups=0.22, wpb=65536, bsz=128, num_updates=17400, lr=0.000239732, gnorm=0.533, train_wall=322, gb_free=7.9, wall=25551
2022-05-18 16:56:07 | INFO | train_inner | epoch 012:    450 / 1550 loss=4.786, ppl=27.59, wps=15438.4, ups=0.24, wpb=65536, bsz=128, num_updates=17500, lr=0.000239046, gnorm=0.536, train_wall=304, gb_free=7.9, wall=25976
2022-05-18 17:03:05 | INFO | train_inner | epoch 012:    550 / 1550 loss=4.776, ppl=27.4, wps=15660, ups=0.24, wpb=65536, bsz=128, num_updates=17600, lr=0.000238366, gnorm=0.536, train_wall=290, gb_free=7.9, wall=26394
2022-05-18 17:21:42 | INFO | train_inner | epoch 012:    650 / 1550 loss=4.795, ppl=27.76, wps=5867.1, ups=0.09, wpb=65536, bsz=128, num_updates=17700, lr=0.000237691, gnorm=0.527, train_wall=879, gb_free=7.9, wall=27511
2022-05-18 17:28:59 | INFO | train_inner | epoch 012:    750 / 1550 loss=4.781, ppl=27.48, wps=15001.4, ups=0.23, wpb=65536, bsz=128, num_updates=17800, lr=0.000237023, gnorm=0.534, train_wall=291, gb_free=7.9, wall=27948
2022-05-18 17:34:42 | INFO | train_inner | epoch 012:    850 / 1550 loss=4.803, ppl=27.91, wps=19106.3, ups=0.29, wpb=65536, bsz=128, num_updates=17900, lr=0.00023636, gnorm=0.53, train_wall=246, gb_free=7.9, wall=28291
2022-05-18 17:41:12 | INFO | train_inner | epoch 012:    950 / 1550 loss=4.8, ppl=27.85, wps=16809.7, ups=0.26, wpb=65536, bsz=128, num_updates=18000, lr=0.000235702, gnorm=0.533, train_wall=276, gb_free=7.9, wall=28681
2022-05-18 17:52:04 | INFO | train_inner | epoch 012:   1050 / 1550 loss=4.792, ppl=27.7, wps=10051.7, ups=0.15, wpb=65536, bsz=128, num_updates=18100, lr=0.00023505, gnorm=0.538, train_wall=434, gb_free=7.9, wall=29333
2022-05-18 17:58:30 | INFO | train_inner | epoch 012:   1150 / 1550 loss=4.804, ppl=27.93, wps=16977.2, ups=0.26, wpb=65536, bsz=128, num_updates=18200, lr=0.000234404, gnorm=0.532, train_wall=272, gb_free=7.9, wall=29719
2022-05-18 18:06:17 | INFO | train_inner | epoch 012:   1250 / 1550 loss=4.788, ppl=27.63, wps=14043, ups=0.21, wpb=65536, bsz=128, num_updates=18300, lr=0.000233762, gnorm=0.529, train_wall=357, gb_free=7.9, wall=30185
2022-05-18 18:14:19 | INFO | train_inner | epoch 012:   1350 / 1550 loss=4.789, ppl=27.64, wps=13595.1, ups=0.21, wpb=65536, bsz=128, num_updates=18400, lr=0.000233126, gnorm=0.534, train_wall=303, gb_free=7.9, wall=30668
2022-05-18 18:25:34 | INFO | train_inner | epoch 012:   1450 / 1550 loss=4.787, ppl=27.61, wps=9710.2, ups=0.15, wpb=65534.5, bsz=128, num_updates=18500, lr=0.000232495, gnorm=0.533, train_wall=450, gb_free=7.9, wall=31342
2022-05-18 18:31:29 | INFO | train_inner | epoch 012:   1550 / 1550 loss=4.777, ppl=27.42, wps=18378.4, ups=0.28, wpb=65331.2, bsz=127.6, num_updates=18600, lr=0.000231869, gnorm=0.53, train_wall=255, gb_free=7.9, wall=31698
2022-05-18 18:31:29 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-05-18 18:32:55 | INFO | valid | epoch 012 | valid on 'valid' subset | loss 4.789 | ppl 27.65 | wps 66879 | wpb 2047.4 | bsz 4 | num_updates 18600 | best_loss 4.789
2022-05-18 18:32:55 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 12 @ 18600 updates
2022-05-18 18:32:55 | INFO | fairseq.trainer | Saving checkpoint to /cluster/work/cotterell/liam/master-thesis/checkpoints/language_model/bart/checkpoint_best_bart_32_5.0000E-04_full.pt
2022-05-18 18:32:57 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/work/cotterell/liam/master-thesis/checkpoints/language_model/bart/checkpoint_best_bart_32_5.0000E-04_full.pt
2022-05-18 18:32:57 | INFO | fairseq.checkpoint_utils | Saved checkpoint checkpoints/language_model/bart/checkpoint_best_bart_32_5.0000E-04_full.pt (epoch 12 @ 18600 updates, score 4.789) (writing took 1.8830801639705896 seconds)
2022-05-18 18:32:57 | INFO | fairseq_cli.train | end of epoch 12 (average epoch stats below)
2022-05-18 18:32:57 | INFO | train | epoch 012 | loss 4.785 | ppl 27.56 | wps 11375.8 | ups 0.17 | wpb 65522.7 | bsz 128 | num_updates 18600 | lr 0.000231869 | gnorm 0.532 | train_wall 6404 | gb_free 7.9 | wall 31786
2022-05-18 18:32:57 | INFO | fairseq.data.iterators | grouped total_num_itrs = 1550
2022-05-18 18:32:57 | INFO | fairseq.trainer | begin training epoch 13
2022-05-18 18:32:57 | INFO | fairseq_cli.train | Start iterating over samples
2022-05-18 18:39:17 | INFO | train_inner | epoch 013:    100 / 1550 loss=4.735, ppl=26.62, wps=14007.1, ups=0.21, wpb=65536, bsz=128, num_updates=18700, lr=0.000231249, gnorm=0.534, train_wall=259, gb_free=7.9, wall=32166
2022-05-18 18:45:28 | INFO | train_inner | epoch 013:    200 / 1550 loss=4.746, ppl=26.84, wps=17660.9, ups=0.27, wpb=65536, bsz=128, num_updates=18800, lr=0.000230633, gnorm=0.541, train_wall=270, gb_free=7.9, wall=32537
2022-05-18 18:51:40 | INFO | train_inner | epoch 013:    300 / 1550 loss=4.754, ppl=26.98, wps=17628.9, ups=0.27, wpb=65536, bsz=128, num_updates=18900, lr=0.000230022, gnorm=0.541, train_wall=278, gb_free=7.9, wall=32909
2022-05-18 18:57:26 | INFO | train_inner | epoch 013:    400 / 1550 loss=4.761, ppl=27.12, wps=18919.9, ups=0.29, wpb=65536, bsz=128, num_updates=19000, lr=0.000229416, gnorm=0.533, train_wall=251, gb_free=7.9, wall=33255
2022-05-18 19:03:25 | INFO | train_inner | epoch 013:    500 / 1550 loss=4.756, ppl=27.03, wps=18284.3, ups=0.28, wpb=65536, bsz=128, num_updates=19100, lr=0.000228814, gnorm=0.531, train_wall=262, gb_free=7.9, wall=33613
2022-05-18 19:09:07 | INFO | train_inner | epoch 013:    600 / 1550 loss=4.743, ppl=26.79, wps=19145.6, ups=0.29, wpb=65536, bsz=128, num_updates=19200, lr=0.000228218, gnorm=0.531, train_wall=254, gb_free=7.9, wall=33956
2022-05-18 19:20:18 | INFO | train_inner | epoch 013:    700 / 1550 loss=4.756, ppl=27.03, wps=9764.4, ups=0.15, wpb=65536, bsz=128, num_updates=19300, lr=0.000227626, gnorm=0.535, train_wall=484, gb_free=7.9, wall=34627
2022-05-18 19:28:56 | INFO | train_inner | epoch 013:    800 / 1550 loss=4.766, ppl=27.21, wps=12661.7, ups=0.19, wpb=65534.5, bsz=128, num_updates=19400, lr=0.000227038, gnorm=0.533, train_wall=365, gb_free=7.9, wall=35145
2022-05-18 19:35:18 | INFO | train_inner | epoch 013:    900 / 1550 loss=4.767, ppl=27.23, wps=17152.5, ups=0.26, wpb=65536, bsz=128, num_updates=19500, lr=0.000226455, gnorm=0.538, train_wall=273, gb_free=7.9, wall=35527
2022-05-18 19:40:45 | INFO | train_inner | epoch 013:   1000 / 1550 loss=4.76, ppl=27.09, wps=20005.2, ups=0.31, wpb=65536, bsz=128, num_updates=19600, lr=0.000225877, gnorm=0.531, train_wall=240, gb_free=7.9, wall=35854
2022-05-18 19:46:37 | INFO | train_inner | epoch 013:   1100 / 1550 loss=4.764, ppl=27.17, wps=18639.1, ups=0.28, wpb=65536, bsz=128, num_updates=19700, lr=0.000225303, gnorm=0.537, train_wall=263, gb_free=7.9, wall=36206
2022-05-18 19:52:55 | INFO | train_inner | epoch 013:   1200 / 1550 loss=4.763, ppl=27.15, wps=17333, ups=0.26, wpb=65536, bsz=128, num_updates=19800, lr=0.000224733, gnorm=0.536, train_wall=274, gb_free=7.9, wall=36584
2022-05-18 19:58:47 | INFO | train_inner | epoch 013:   1300 / 1550 loss=4.757, ppl=27.05, wps=18630.6, ups=0.28, wpb=65536, bsz=128, num_updates=19900, lr=0.000224168, gnorm=0.536, train_wall=252, gb_free=7.9, wall=36936
2022-05-18 20:04:31 | INFO | train_inner | epoch 013:   1400 / 1550 loss=4.777, ppl=27.42, wps=19049.6, ups=0.29, wpb=65536, bsz=128, num_updates=20000, lr=0.000223607, gnorm=0.529, train_wall=246, gb_free=7.9, wall=37280
2022-05-18 20:09:43 | INFO | train_inner | epoch 013:   1500 / 1550 loss=4.765, ppl=27.2, wps=20994.2, ups=0.32, wpb=65536, bsz=128, num_updates=20100, lr=0.00022305, gnorm=0.537, train_wall=231, gb_free=7.9, wall=37592
2022-05-18 20:12:31 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-05-18 20:14:35 | INFO | valid | epoch 013 | valid on 'valid' subset | loss 4.769 | ppl 27.27 | wps 45417.7 | wpb 2047.4 | bsz 4 | num_updates 20150 | best_loss 4.769
2022-05-18 20:14:35 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 13 @ 20150 updates
2022-05-18 20:14:35 | INFO | fairseq.trainer | Saving checkpoint to /cluster/work/cotterell/liam/master-thesis/checkpoints/language_model/bart/checkpoint_best_bart_32_5.0000E-04_full.pt
2022-05-18 20:14:37 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/work/cotterell/liam/master-thesis/checkpoints/language_model/bart/checkpoint_best_bart_32_5.0000E-04_full.pt
2022-05-18 20:14:37 | INFO | fairseq.checkpoint_utils | Saved checkpoint checkpoints/language_model/bart/checkpoint_best_bart_32_5.0000E-04_full.pt (epoch 13 @ 20150 updates, score 4.769) (writing took 2.025002308189869 seconds)
2022-05-18 20:14:37 | INFO | fairseq_cli.train | end of epoch 13 (average epoch stats below)
2022-05-18 20:14:37 | INFO | train | epoch 013 | loss 4.758 | ppl 27.06 | wps 16648.9 | ups 0.25 | wpb 65522.7 | bsz 128 | num_updates 20150 | lr 0.000222773 | gnorm 0.535 | train_wall 4329 | gb_free 7.9 | wall 37886
2022-05-18 20:14:37 | INFO | fairseq.data.iterators | grouped total_num_itrs = 1550
2022-05-18 20:14:37 | INFO | fairseq.trainer | begin training epoch 14
2022-05-18 20:14:37 | INFO | fairseq_cli.train | Start iterating over samples
2022-05-18 20:22:04 | INFO | train_inner | epoch 014:     50 / 1550 loss=4.728, ppl=26.51, wps=8815.3, ups=0.13, wpb=65331.2, bsz=127.6, num_updates=20200, lr=0.000222497, gnorm=0.537, train_wall=423, gb_free=7.9, wall=38333
2022-05-18 20:32:46 | INFO | train_inner | epoch 014:    150 / 1550 loss=4.713, ppl=26.23, wps=10210.9, ups=0.16, wpb=65536, bsz=128, num_updates=20300, lr=0.000221948, gnorm=0.538, train_wall=463, gb_free=7.9, wall=38975
2022-05-18 20:43:46 | INFO | train_inner | epoch 014:    250 / 1550 loss=4.723, ppl=26.42, wps=9936.3, ups=0.15, wpb=65536, bsz=128, num_updates=20400, lr=0.000221404, gnorm=0.537, train_wall=453, gb_free=7.9, wall=39634
2022-05-18 20:51:23 | INFO | train_inner | epoch 014:    350 / 1550 loss=4.717, ppl=26.3, wps=14331.3, ups=0.22, wpb=65536, bsz=128, num_updates=20500, lr=0.000220863, gnorm=0.536, train_wall=332, gb_free=7.9, wall=40092
2022-05-18 21:01:58 | INFO | train_inner | epoch 014:    450 / 1550 loss=4.733, ppl=26.59, wps=10314.7, ups=0.16, wpb=65536, bsz=128, num_updates=20600, lr=0.000220326, gnorm=0.545, train_wall=448, gb_free=7.9, wall=40727
2022-05-18 21:10:38 | INFO | train_inner | epoch 014:    550 / 1550 loss=4.742, ppl=26.76, wps=12619.5, ups=0.19, wpb=65536, bsz=128, num_updates=20700, lr=0.000219793, gnorm=0.544, train_wall=364, gb_free=7.9, wall=41246
2022-05-18 21:24:37 | INFO | train_inner | epoch 014:    650 / 1550 loss=4.733, ppl=26.59, wps=7810, ups=0.12, wpb=65536, bsz=128, num_updates=20800, lr=0.000219265, gnorm=0.538, train_wall=630, gb_free=7.9, wall=42085
2022-05-18 21:35:07 | INFO | train_inner | epoch 014:    750 / 1550 loss=4.742, ppl=26.75, wps=10403.1, ups=0.16, wpb=65536, bsz=128, num_updates=20900, lr=0.000218739, gnorm=0.542, train_wall=479, gb_free=7.9, wall=42715
2022-05-18 21:40:50 | INFO | train_inner | epoch 014:    850 / 1550 loss=4.736, ppl=26.65, wps=19067.3, ups=0.29, wpb=65536, bsz=128, num_updates=21000, lr=0.000218218, gnorm=0.538, train_wall=250, gb_free=7.9, wall=43059
2022-05-18 21:46:15 | INFO | train_inner | epoch 014:    950 / 1550 loss=4.743, ppl=26.79, wps=20188.1, ups=0.31, wpb=65536, bsz=128, num_updates=21100, lr=0.0002177, gnorm=0.537, train_wall=256, gb_free=7.9, wall=43384
2022-05-18 21:51:28 | INFO | train_inner | epoch 014:   1050 / 1550 loss=4.733, ppl=26.6, wps=20923.3, ups=0.32, wpb=65536, bsz=128, num_updates=21200, lr=0.000217186, gnorm=0.555, train_wall=242, gb_free=7.9, wall=43697
2022-05-18 21:56:55 | INFO | train_inner | epoch 014:   1150 / 1550 loss=4.743, ppl=26.77, wps=20054.5, ups=0.31, wpb=65536, bsz=128, num_updates=21300, lr=0.000216676, gnorm=0.541, train_wall=251, gb_free=7.9, wall=44024
2022-05-18 22:07:27 | INFO | train_inner | epoch 014:   1250 / 1550 loss=4.744, ppl=26.8, wps=10372.8, ups=0.16, wpb=65536, bsz=128, num_updates=21400, lr=0.000216169, gnorm=0.534, train_wall=463, gb_free=7.9, wall=44656
2022-05-18 22:18:28 | INFO | train_inner | epoch 014:   1350 / 1550 loss=4.75, ppl=26.91, wps=9905.2, ups=0.15, wpb=65536, bsz=128, num_updates=21500, lr=0.000215666, gnorm=0.537, train_wall=485, gb_free=7.9, wall=45317
2022-05-18 22:39:42 | INFO | train_inner | epoch 014:   1450 / 1550 loss=4.744, ppl=26.79, wps=5146, ups=0.08, wpb=65534.5, bsz=128, num_updates=21600, lr=0.000215166, gnorm=0.543, train_wall=1041, gb_free=7.9, wall=46591
2022-05-18 22:47:47 | INFO | train_inner | epoch 014:   1550 / 1550 loss=4.749, ppl=26.88, wps=13480, ups=0.21, wpb=65331.2, bsz=127.6, num_updates=21700, lr=0.000214669, gnorm=0.54, train_wall=356, gb_free=7.9, wall=47075
2022-05-18 22:47:47 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-05-18 22:49:21 | INFO | valid | epoch 014 | valid on 'valid' subset | loss 4.755 | ppl 27 | wps 59428.6 | wpb 2047.4 | bsz 4 | num_updates 21700 | best_loss 4.755
2022-05-18 22:49:21 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 14 @ 21700 updates
2022-05-18 22:49:21 | INFO | fairseq.trainer | Saving checkpoint to /cluster/work/cotterell/liam/master-thesis/checkpoints/language_model/bart/checkpoint_best_bart_32_5.0000E-04_full.pt
2022-05-18 22:49:23 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/work/cotterell/liam/master-thesis/checkpoints/language_model/bart/checkpoint_best_bart_32_5.0000E-04_full.pt
2022-05-18 22:49:23 | INFO | fairseq.checkpoint_utils | Saved checkpoint checkpoints/language_model/bart/checkpoint_best_bart_32_5.0000E-04_full.pt (epoch 14 @ 21700 updates, score 4.755) (writing took 1.8516671848483384 seconds)
2022-05-18 22:49:23 | INFO | fairseq_cli.train | end of epoch 14 (average epoch stats below)
2022-05-18 22:49:23 | INFO | train | epoch 014 | loss 4.735 | ppl 26.63 | wps 10936.7 | ups 0.17 | wpb 65522.7 | bsz 128 | num_updates 21700 | lr 0.000214669 | gnorm 0.54 | train_wall 6812 | gb_free 7.9 | wall 47172
2022-05-18 22:49:24 | INFO | fairseq.data.iterators | grouped total_num_itrs = 1550
2022-05-18 22:49:24 | INFO | fairseq.trainer | begin training epoch 15
2022-05-18 22:49:24 | INFO | fairseq_cli.train | Start iterating over samples
2022-05-18 22:55:47 | INFO | train_inner | epoch 015:    100 / 1550 loss=4.69, ppl=25.81, wps=13640.1, ups=0.21, wpb=65536, bsz=128, num_updates=21800, lr=0.000214176, gnorm=0.538, train_wall=284, gb_free=7.9, wall=47556
2022-05-18 23:01:00 | INFO | train_inner | epoch 015:    200 / 1550 loss=4.696, ppl=25.92, wps=20932.4, ups=0.32, wpb=65536, bsz=128, num_updates=21900, lr=0.000213687, gnorm=0.544, train_wall=245, gb_free=7.9, wall=47869
2022-05-18 23:06:27 | INFO | train_inner | epoch 015:    300 / 1550 loss=4.703, ppl=26.04, wps=20032.9, ups=0.31, wpb=65536, bsz=128, num_updates=22000, lr=0.000213201, gnorm=0.533, train_wall=258, gb_free=7.9, wall=48196
2022-05-18 23:11:52 | INFO | train_inner | epoch 015:    400 / 1550 loss=4.707, ppl=26.12, wps=20153.1, ups=0.31, wpb=65536, bsz=128, num_updates=22100, lr=0.000212718, gnorm=0.547, train_wall=252, gb_free=7.9, wall=48521
2022-05-18 23:17:40 | INFO | train_inner | epoch 015:    500 / 1550 loss=4.714, ppl=26.25, wps=18837.4, ups=0.29, wpb=65536, bsz=128, num_updates=22200, lr=0.000212238, gnorm=0.541, train_wall=274, gb_free=7.9, wall=48869
2022-05-18 23:37:43 | INFO | train_inner | epoch 015:    600 / 1550 loss=4.714, ppl=26.24, wps=5448.7, ups=0.08, wpb=65536, bsz=128, num_updates=22300, lr=0.000211762, gnorm=0.546, train_wall=984, gb_free=7.9, wall=50072
2022-05-18 23:50:09 | INFO | train_inner | epoch 015:    700 / 1550 loss=4.707, ppl=26.12, wps=8782.8, ups=0.13, wpb=65536, bsz=128, num_updates=22400, lr=0.000211289, gnorm=0.542, train_wall=506, gb_free=7.9, wall=50818
2022-05-18 23:59:19 | INFO | train_inner | epoch 015:    800 / 1550 loss=4.724, ppl=26.43, wps=11919.6, ups=0.18, wpb=65534.5, bsz=128, num_updates=22500, lr=0.000210819, gnorm=0.539, train_wall=399, gb_free=7.9, wall=51368
2022-05-19 00:06:54 | INFO | train_inner | epoch 015:    900 / 1550 loss=4.728, ppl=26.51, wps=14404, ups=0.22, wpb=65536, bsz=128, num_updates=22600, lr=0.000210352, gnorm=0.548, train_wall=333, gb_free=7.9, wall=51823
2022-05-19 00:12:04 | INFO | train_inner | epoch 015:   1000 / 1550 loss=4.716, ppl=26.28, wps=21122.4, ups=0.32, wpb=65536, bsz=128, num_updates=22700, lr=0.000209888, gnorm=0.541, train_wall=240, gb_free=7.9, wall=52133
2022-05-19 00:17:31 | INFO | train_inner | epoch 015:   1100 / 1550 loss=4.723, ppl=26.41, wps=20034.7, ups=0.31, wpb=65536, bsz=128, num_updates=22800, lr=0.000209427, gnorm=0.548, train_wall=259, gb_free=7.9, wall=52460
2022-05-19 00:23:15 | INFO | train_inner | epoch 015:   1200 / 1550 loss=4.722, ppl=26.39, wps=19098.4, ups=0.29, wpb=65536, bsz=128, num_updates=22900, lr=0.000208969, gnorm=0.536, train_wall=265, gb_free=7.9, wall=52803
2022-05-19 00:39:01 | INFO | train_inner | epoch 015:   1300 / 1550 loss=4.725, ppl=26.44, wps=6923.5, ups=0.11, wpb=65536, bsz=128, num_updates=23000, lr=0.000208514, gnorm=0.543, train_wall=765, gb_free=7.9, wall=53750
2022-05-19 00:44:52 | INFO | train_inner | epoch 015:   1400 / 1550 loss=4.72, ppl=26.36, wps=18689.3, ups=0.29, wpb=65536, bsz=128, num_updates=23100, lr=0.000208063, gnorm=0.543, train_wall=258, gb_free=7.9, wall=54101
2022-05-19 00:50:11 | INFO | train_inner | epoch 015:   1500 / 1550 loss=4.726, ppl=26.47, wps=20524, ups=0.31, wpb=65536, bsz=128, num_updates=23200, lr=0.000207614, gnorm=0.543, train_wall=246, gb_free=7.9, wall=54420
2022-05-19 00:52:39 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-05-19 00:54:06 | INFO | valid | epoch 015 | valid on 'valid' subset | loss 4.744 | ppl 26.79 | wps 65280.6 | wpb 2047.4 | bsz 4 | num_updates 23250 | best_loss 4.744
2022-05-19 00:54:06 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 15 @ 23250 updates
2022-05-19 00:54:06 | INFO | fairseq.trainer | Saving checkpoint to /cluster/work/cotterell/liam/master-thesis/checkpoints/language_model/bart/checkpoint_best_bart_32_5.0000E-04_full.pt
2022-05-19 00:54:07 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/work/cotterell/liam/master-thesis/checkpoints/language_model/bart/checkpoint_best_bart_32_5.0000E-04_full.pt
2022-05-19 00:54:07 | INFO | fairseq.checkpoint_utils | Saved checkpoint checkpoints/language_model/bart/checkpoint_best_bart_32_5.0000E-04_full.pt (epoch 15 @ 23250 updates, score 4.744) (writing took 1.7439704886637628 seconds)
2022-05-19 00:54:07 | INFO | fairseq_cli.train | end of epoch 15 (average epoch stats below)
2022-05-19 00:54:07 | INFO | train | epoch 015 | loss 4.714 | ppl 26.25 | wps 13570.3 | ups 0.21 | wpb 65522.7 | bsz 128 | num_updates 23250 | lr 0.00020739 | gnorm 0.542 | train_wall 5683 | gb_free 7.9 | wall 54656
2022-05-19 00:54:08 | INFO | fairseq.data.iterators | grouped total_num_itrs = 1550
2022-05-19 00:54:08 | INFO | fairseq.trainer | begin training epoch 16
2022-05-19 00:54:08 | INFO | fairseq_cli.train | Start iterating over samples
2022-05-19 00:56:48 | INFO | train_inner | epoch 016:     50 / 1550 loss=4.683, ppl=25.68, wps=16453.5, ups=0.25, wpb=65331.2, bsz=127.6, num_updates=23300, lr=0.000207168, gnorm=0.544, train_wall=237, gb_free=7.9, wall=54817
2022-05-19 01:02:16 | INFO | train_inner | epoch 016:    150 / 1550 loss=4.673, ppl=25.52, wps=20005.1, ups=0.31, wpb=65536, bsz=128, num_updates=23400, lr=0.000206725, gnorm=0.54, train_wall=254, gb_free=7.9, wall=55145
2022-05-19 01:07:36 | INFO | train_inner | epoch 016:    250 / 1550 loss=4.685, ppl=25.72, wps=20451.3, ups=0.31, wpb=65536, bsz=128, num_updates=23500, lr=0.000206284, gnorm=0.545, train_wall=248, gb_free=7.9, wall=55465
2022-05-19 01:12:16 | INFO | train_inner | epoch 016:    350 / 1550 loss=4.685, ppl=25.72, wps=23446.3, ups=0.36, wpb=65534.5, bsz=128, num_updates=23600, lr=0.000205847, gnorm=0.545, train_wall=226, gb_free=7.9, wall=55745
2022-05-19 01:18:03 | INFO | train_inner | epoch 016:    450 / 1550 loss=4.693, ppl=25.86, wps=18902.8, ups=0.29, wpb=65536, bsz=128, num_updates=23700, lr=0.000205412, gnorm=0.545, train_wall=271, gb_free=7.9, wall=56091
2022-05-19 01:22:50 | INFO | train_inner | epoch 016:    550 / 1550 loss=4.693, ppl=25.87, wps=22767.9, ups=0.35, wpb=65536, bsz=128, num_updates=23800, lr=0.00020498, gnorm=0.547, train_wall=233, gb_free=7.9, wall=56379
2022-05-19 01:27:51 | INFO | train_inner | epoch 016:    650 / 1550 loss=4.692, ppl=25.85, wps=21816.8, ups=0.33, wpb=65536, bsz=128, num_updates=23900, lr=0.000204551, gnorm=0.549, train_wall=243, gb_free=7.9, wall=56680
2022-05-19 01:42:59 | INFO | train_inner | epoch 016:    750 / 1550 loss=4.704, ppl=26.06, wps=7213.5, ups=0.11, wpb=65536, bsz=128, num_updates=24000, lr=0.000204124, gnorm=0.543, train_wall=700, gb_free=7.9, wall=57588
2022-05-19 01:48:05 | INFO | train_inner | epoch 016:    850 / 1550 loss=4.7, ppl=26, wps=21414.3, ups=0.33, wpb=65536, bsz=128, num_updates=24100, lr=0.0002037, gnorm=0.547, train_wall=242, gb_free=7.9, wall=57894
2022-05-19 01:53:16 | INFO | train_inner | epoch 016:    950 / 1550 loss=4.707, ppl=26.12, wps=21121, ups=0.32, wpb=65536, bsz=128, num_updates=24200, lr=0.000203279, gnorm=0.547, train_wall=247, gb_free=7.9, wall=58204
2022-05-19 01:58:16 | INFO | train_inner | epoch 016:   1050 / 1550 loss=4.693, ppl=25.86, wps=21799, ups=0.33, wpb=65536, bsz=128, num_updates=24300, lr=0.00020286, gnorm=0.535, train_wall=238, gb_free=7.9, wall=58505
2022-05-19 02:03:22 | INFO | train_inner | epoch 016:   1150 / 1550 loss=4.701, ppl=26.01, wps=21450.2, ups=0.33, wpb=65536, bsz=128, num_updates=24400, lr=0.000202444, gnorm=0.543, train_wall=244, gb_free=7.9, wall=58811
2022-05-19 02:08:28 | INFO | train_inner | epoch 016:   1250 / 1550 loss=4.712, ppl=26.21, wps=21414.1, ups=0.33, wpb=65536, bsz=128, num_updates=24500, lr=0.000202031, gnorm=0.554, train_wall=245, gb_free=7.9, wall=59117
2022-05-19 02:14:03 | INFO | train_inner | epoch 016:   1350 / 1550 loss=4.71, ppl=26.17, wps=19530.4, ups=0.3, wpb=65536, bsz=128, num_updates=24600, lr=0.000201619, gnorm=0.543, train_wall=262, gb_free=7.9, wall=59452
2022-05-19 02:19:33 | INFO | train_inner | epoch 016:   1450 / 1550 loss=4.707, ppl=26.11, wps=19880.5, ups=0.3, wpb=65536, bsz=128, num_updates=24700, lr=0.000201211, gnorm=0.555, train_wall=254, gb_free=7.9, wall=59782
2022-05-19 02:25:00 | INFO | train_inner | epoch 016:   1550 / 1550 loss=4.712, ppl=26.21, wps=20003.8, ups=0.31, wpb=65331.2, bsz=127.6, num_updates=24800, lr=0.000200805, gnorm=0.56, train_wall=257, gb_free=7.9, wall=60108
2022-05-19 02:25:00 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-05-19 02:26:18 | INFO | valid | epoch 016 | valid on 'valid' subset | loss 4.733 | ppl 26.59 | wps 72785.3 | wpb 2047.4 | bsz 4 | num_updates 24800 | best_loss 4.733
2022-05-19 02:26:18 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 16 @ 24800 updates
2022-05-19 02:26:18 | INFO | fairseq.trainer | Saving checkpoint to /cluster/work/cotterell/liam/master-thesis/checkpoints/language_model/bart/checkpoint_best_bart_32_5.0000E-04_full.pt
2022-05-19 02:26:19 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/work/cotterell/liam/master-thesis/checkpoints/language_model/bart/checkpoint_best_bart_32_5.0000E-04_full.pt
2022-05-19 02:26:19 | INFO | fairseq.checkpoint_utils | Saved checkpoint checkpoints/language_model/bart/checkpoint_best_bart_32_5.0000E-04_full.pt (epoch 16 @ 24800 updates, score 4.733) (writing took 1.7385720959864557 seconds)
2022-05-19 02:26:19 | INFO | fairseq_cli.train | end of epoch 16 (average epoch stats below)
2022-05-19 02:26:19 | INFO | train | epoch 016 | loss 4.696 | ppl 25.92 | wps 18358.5 | ups 0.28 | wpb 65522.7 | bsz 128 | num_updates 24800 | lr 0.000200805 | gnorm 0.547 | train_wall 4283 | gb_free 7.9 | wall 60188
2022-05-19 02:26:20 | INFO | fairseq.data.iterators | grouped total_num_itrs = 1550
2022-05-19 02:26:20 | INFO | fairseq.trainer | begin training epoch 17
2022-05-19 02:26:20 | INFO | fairseq_cli.train | Start iterating over samples
2022-05-19 02:33:32 | INFO | train_inner | epoch 017:    100 / 1550 loss=4.653, ppl=25.16, wps=12800.6, ups=0.2, wpb=65536, bsz=128, num_updates=24900, lr=0.000200401, gnorm=0.537, train_wall=338, gb_free=7.9, wall=60620
2022-05-19 02:47:24 | INFO | train_inner | epoch 017:    200 / 1550 loss=4.662, ppl=25.32, wps=7872.8, ups=0.12, wpb=65536, bsz=128, num_updates=25000, lr=0.0002, gnorm=0.543, train_wall=654, gb_free=7.9, wall=61453
2022-05-19 02:53:51 | INFO | train_inner | epoch 017:    300 / 1550 loss=4.661, ppl=25.3, wps=16941.3, ups=0.26, wpb=65536, bsz=128, num_updates=25100, lr=0.000199601, gnorm=0.543, train_wall=299, gb_free=7.9, wall=61840
2022-05-19 02:59:36 | INFO | train_inner | epoch 017:    400 / 1550 loss=4.667, ppl=25.4, wps=18992.2, ups=0.29, wpb=65536, bsz=128, num_updates=25200, lr=0.000199205, gnorm=0.543, train_wall=263, gb_free=7.9, wall=62185
2022-05-19 03:04:54 | INFO | train_inner | epoch 017:    500 / 1550 loss=4.669, ppl=25.43, wps=20636.7, ups=0.31, wpb=65536, bsz=128, num_updates=25300, lr=0.000198811, gnorm=0.546, train_wall=255, gb_free=7.9, wall=62502
2022-05-19 03:11:07 | INFO | train_inner | epoch 017:    600 / 1550 loss=4.671, ppl=25.48, wps=17535.3, ups=0.27, wpb=65536, bsz=128, num_updates=25400, lr=0.000198419, gnorm=0.55, train_wall=294, gb_free=7.9, wall=62876
2022-05-19 03:16:48 | INFO | train_inner | epoch 017:    700 / 1550 loss=4.674, ppl=25.53, wps=19207.8, ups=0.29, wpb=65536, bsz=128, num_updates=25500, lr=0.00019803, gnorm=0.55, train_wall=266, gb_free=7.9, wall=63217
2022-05-19 03:22:27 | INFO | train_inner | epoch 017:    800 / 1550 loss=4.678, ppl=25.59, wps=19378.8, ups=0.3, wpb=65536, bsz=128, num_updates=25600, lr=0.000197642, gnorm=0.548, train_wall=267, gb_free=7.9, wall=63555
2022-05-19 03:28:28 | INFO | train_inner | epoch 017:    900 / 1550 loss=4.682, ppl=25.68, wps=18126.9, ups=0.28, wpb=65536, bsz=128, num_updates=25700, lr=0.000197257, gnorm=0.545, train_wall=274, gb_free=7.9, wall=63917
2022-05-19 03:46:30 | INFO | train_inner | epoch 017:   1000 / 1550 loss=4.694, ppl=25.89, wps=6056, ups=0.09, wpb=65536, bsz=128, num_updates=25800, lr=0.000196875, gnorm=0.553, train_wall=912, gb_free=7.9, wall=64999
2022-05-19 03:55:28 | INFO | train_inner | epoch 017:   1100 / 1550 loss=4.692, ppl=25.85, wps=12200.7, ups=0.19, wpb=65536, bsz=128, num_updates=25900, lr=0.000196494, gnorm=0.55, train_wall=393, gb_free=7.9, wall=65536
2022-05-19 04:01:43 | INFO | train_inner | epoch 017:   1200 / 1550 loss=4.699, ppl=25.98, wps=17450.9, ups=0.27, wpb=65536, bsz=128, num_updates=26000, lr=0.000196116, gnorm=0.553, train_wall=293, gb_free=7.9, wall=65912
2022-05-19 04:07:32 | INFO | train_inner | epoch 017:   1300 / 1550 loss=4.686, ppl=25.74, wps=18779.4, ups=0.29, wpb=65536, bsz=128, num_updates=26100, lr=0.00019574, gnorm=0.541, train_wall=277, gb_free=7.9, wall=66261
2022-05-19 04:13:14 | INFO | train_inner | epoch 017:   1400 / 1550 loss=4.696, ppl=25.92, wps=19146.9, ups=0.29, wpb=65536, bsz=128, num_updates=26200, lr=0.000195366, gnorm=0.553, train_wall=264, gb_free=7.9, wall=66603
2022-05-19 04:18:46 | INFO | train_inner | epoch 017:   1500 / 1550 loss=4.705, ppl=26.08, wps=19758.4, ups=0.3, wpb=65534.5, bsz=128, num_updates=26300, lr=0.000194994, gnorm=0.551, train_wall=263, gb_free=7.9, wall=66935
2022-05-19 04:21:05 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-05-19 04:22:34 | INFO | valid | epoch 017 | valid on 'valid' subset | loss 4.719 | ppl 26.34 | wps 63186 | wpb 2047.4 | bsz 4 | num_updates 26350 | best_loss 4.719
2022-05-19 04:22:35 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 17 @ 26350 updates
2022-05-19 04:22:35 | INFO | fairseq.trainer | Saving checkpoint to /cluster/work/cotterell/liam/master-thesis/checkpoints/language_model/bart/checkpoint_best_bart_32_5.0000E-04_full.pt
2022-05-19 04:22:36 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/work/cotterell/liam/master-thesis/checkpoints/language_model/bart/checkpoint_best_bart_32_5.0000E-04_full.pt
2022-05-19 04:22:36 | INFO | fairseq.checkpoint_utils | Saved checkpoint checkpoints/language_model/bart/checkpoint_best_bart_32_5.0000E-04_full.pt (epoch 17 @ 26350 updates, score 4.719) (writing took 1.8836441291496158 seconds)
2022-05-19 04:22:36 | INFO | fairseq_cli.train | end of epoch 17 (average epoch stats below)
2022-05-19 04:22:36 | INFO | train | epoch 017 | loss 4.679 | ppl 25.63 | wps 14556.3 | ups 0.22 | wpb 65522.7 | bsz 128 | num_updates 26350 | lr 0.000194809 | gnorm 0.547 | train_wall 5425 | gb_free 7.9 | wall 67165
2022-05-19 04:22:37 | INFO | fairseq.data.iterators | grouped total_num_itrs = 1550
2022-05-19 04:22:37 | INFO | fairseq.trainer | begin training epoch 18
2022-05-19 04:22:37 | INFO | fairseq_cli.train | Start iterating over samples
2022-05-19 04:25:30 | INFO | train_inner | epoch 018:     50 / 1550 loss=4.669, ppl=25.43, wps=16178.4, ups=0.25, wpb=65331.2, bsz=127.6, num_updates=26400, lr=0.000194625, gnorm=0.551, train_wall=248, gb_free=7.9, wall=67339
2022-05-19 04:31:58 | INFO | train_inner | epoch 018:    150 / 1550 loss=4.628, ppl=24.72, wps=16877.4, ups=0.26, wpb=65536, bsz=128, num_updates=26500, lr=0.000194257, gnorm=0.548, train_wall=286, gb_free=7.9, wall=67727
2022-05-19 04:45:49 | INFO | train_inner | epoch 018:    250 / 1550 loss=4.655, ppl=25.19, wps=7889, ups=0.12, wpb=65536, bsz=128, num_updates=26600, lr=0.000193892, gnorm=0.561, train_wall=687, gb_free=7.9, wall=68558
2022-05-19 04:55:50 | INFO | train_inner | epoch 018:    350 / 1550 loss=4.653, ppl=25.16, wps=10907.5, ups=0.17, wpb=65536, bsz=128, num_updates=26700, lr=0.000193528, gnorm=0.554, train_wall=467, gb_free=7.9, wall=69159
2022-05-19 05:02:05 | INFO | train_inner | epoch 018:    450 / 1550 loss=4.653, ppl=25.17, wps=17474.8, ups=0.27, wpb=65536, bsz=128, num_updates=26800, lr=0.000193167, gnorm=0.547, train_wall=287, gb_free=7.9, wall=69534
2022-05-19 05:08:15 | INFO | train_inner | epoch 018:    550 / 1550 loss=4.661, ppl=25.29, wps=17689.5, ups=0.27, wpb=65536, bsz=128, num_updates=26900, lr=0.000192807, gnorm=0.553, train_wall=273, gb_free=7.9, wall=69904
2022-05-19 05:14:08 | INFO | train_inner | epoch 018:    650 / 1550 loss=4.663, ppl=25.33, wps=18576.6, ups=0.28, wpb=65536, bsz=128, num_updates=27000, lr=0.00019245, gnorm=0.551, train_wall=269, gb_free=7.9, wall=70257
2022-05-19 05:19:47 | INFO | train_inner | epoch 018:    750 / 1550 loss=4.672, ppl=25.49, wps=19353.3, ups=0.3, wpb=65536, bsz=128, num_updates=27100, lr=0.000192095, gnorm=0.555, train_wall=263, gb_free=7.9, wall=70595
2022-05-19 05:25:31 | INFO | train_inner | epoch 018:    850 / 1550 loss=4.672, ppl=25.5, wps=19047.5, ups=0.29, wpb=65536, bsz=128, num_updates=27200, lr=0.000191741, gnorm=0.549, train_wall=272, gb_free=7.9, wall=70940
2022-05-19 05:30:50 | INFO | train_inner | epoch 018:    950 / 1550 loss=4.662, ppl=25.32, wps=20515.2, ups=0.31, wpb=65536, bsz=128, num_updates=27300, lr=0.00019139, gnorm=0.551, train_wall=254, gb_free=7.9, wall=71259
2022-05-19 05:36:08 | INFO | train_inner | epoch 018:   1050 / 1550 loss=4.68, ppl=25.64, wps=20611.1, ups=0.31, wpb=65536, bsz=128, num_updates=27400, lr=0.00019104, gnorm=0.548, train_wall=252, gb_free=7.9, wall=71577
2022-05-19 05:52:20 | INFO | train_inner | epoch 018:   1150 / 1550 loss=4.675, ppl=25.54, wps=6741.2, ups=0.1, wpb=65536, bsz=128, num_updates=27500, lr=0.000190693, gnorm=0.55, train_wall=749, gb_free=7.9, wall=72549
2022-05-19 06:01:00 | INFO | train_inner | epoch 018:   1250 / 1550 loss=4.686, ppl=25.74, wps=12614.4, ups=0.19, wpb=65534.5, bsz=128, num_updates=27600, lr=0.000190347, gnorm=0.549, train_wall=386, gb_free=7.9, wall=73069
2022-05-19 06:08:06 | INFO | train_inner | epoch 018:   1350 / 1550 loss=4.669, ppl=25.43, wps=15388.6, ups=0.23, wpb=65536, bsz=128, num_updates=27700, lr=0.000190003, gnorm=0.554, train_wall=327, gb_free=7.9, wall=73495
2022-05-19 06:14:35 | INFO | train_inner | epoch 018:   1450 / 1550 loss=4.676, ppl=25.57, wps=16839.5, ups=0.26, wpb=65536, bsz=128, num_updates=27800, lr=0.000189661, gnorm=0.558, train_wall=293, gb_free=7.9, wall=73884
2022-05-19 06:21:55 | INFO | train_inner | epoch 018:   1550 / 1550 loss=4.67, ppl=25.45, wps=14845.3, ups=0.23, wpb=65331.2, bsz=127.6, num_updates=27900, lr=0.000189321, gnorm=0.548, train_wall=322, gb_free=7.9, wall=74324
2022-05-19 06:21:55 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-05-19 06:23:19 | INFO | valid | epoch 018 | valid on 'valid' subset | loss 4.711 | ppl 26.19 | wps 67768.2 | wpb 2047.4 | bsz 4 | num_updates 27900 | best_loss 4.711
2022-05-19 06:23:19 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 18 @ 27900 updates
2022-05-19 06:23:19 | INFO | fairseq.trainer | Saving checkpoint to /cluster/work/cotterell/liam/master-thesis/checkpoints/language_model/bart/checkpoint_best_bart_32_5.0000E-04_full.pt
2022-05-19 06:23:20 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/work/cotterell/liam/master-thesis/checkpoints/language_model/bart/checkpoint_best_bart_32_5.0000E-04_full.pt
2022-05-19 06:23:20 | INFO | fairseq.checkpoint_utils | Saved checkpoint checkpoints/language_model/bart/checkpoint_best_bart_32_5.0000E-04_full.pt (epoch 18 @ 27900 updates, score 4.711) (writing took 1.615294437855482 seconds)
2022-05-19 06:23:20 | INFO | fairseq_cli.train | end of epoch 18 (average epoch stats below)
2022-05-19 06:23:20 | INFO | train | epoch 018 | loss 4.664 | ppl 25.36 | wps 14020 | ups 0.21 | wpb 65522.7 | bsz 128 | num_updates 27900 | lr 0.000189321 | gnorm 0.552 | train_wall 5519 | gb_free 7.9 | wall 74409
2022-05-19 06:23:21 | INFO | fairseq.data.iterators | grouped total_num_itrs = 1550
2022-05-19 06:23:21 | INFO | fairseq.trainer | begin training epoch 19
2022-05-19 06:23:21 | INFO | fairseq_cli.train | Start iterating over samples
2022-05-19 06:30:25 | INFO | train_inner | epoch 019:    100 / 1550 loss=4.615, ppl=24.5, wps=12843.1, ups=0.2, wpb=65536, bsz=128, num_updates=28000, lr=0.000188982, gnorm=0.552, train_wall=320, gb_free=7.9, wall=74834
2022-05-19 06:39:31 | INFO | train_inner | epoch 019:    200 / 1550 loss=4.634, ppl=24.83, wps=12001.5, ups=0.18, wpb=65536, bsz=128, num_updates=28100, lr=0.000188646, gnorm=0.553, train_wall=386, gb_free=7.9, wall=75380
2022-05-19 06:54:34 | INFO | train_inner | epoch 019:    300 / 1550 loss=4.633, ppl=24.82, wps=7256.9, ups=0.11, wpb=65536, bsz=128, num_updates=28200, lr=0.000188311, gnorm=0.545, train_wall=776, gb_free=7.9, wall=76283
2022-05-19 07:02:54 | INFO | train_inner | epoch 019:    400 / 1550 loss=4.644, ppl=25, wps=13108, ups=0.2, wpb=65536, bsz=128, num_updates=28300, lr=0.000187978, gnorm=0.563, train_wall=372, gb_free=7.9, wall=76783
2022-05-19 07:09:11 | INFO | train_inner | epoch 019:    500 / 1550 loss=4.637, ppl=24.88, wps=17417.7, ups=0.27, wpb=65536, bsz=128, num_updates=28400, lr=0.000187647, gnorm=0.549, train_wall=276, gb_free=7.9, wall=77159
2022-05-19 07:15:24 | INFO | train_inner | epoch 019:    600 / 1550 loss=4.647, ppl=25.05, wps=17550.7, ups=0.27, wpb=65536, bsz=128, num_updates=28500, lr=0.000187317, gnorm=0.549, train_wall=289, gb_free=7.9, wall=77533
2022-05-19 07:21:10 | INFO | train_inner | epoch 019:    700 / 1550 loss=4.644, ppl=25.01, wps=18958.3, ups=0.29, wpb=65536, bsz=128, num_updates=28600, lr=0.000186989, gnorm=0.551, train_wall=277, gb_free=7.9, wall=77879
2022-05-19 07:26:08 | INFO | train_inner | epoch 019:    800 / 1550 loss=4.653, ppl=25.16, wps=21982.2, ups=0.34, wpb=65536, bsz=128, num_updates=28700, lr=0.000186663, gnorm=0.554, train_wall=231, gb_free=7.9, wall=78177
2022-05-19 07:31:36 | INFO | train_inner | epoch 019:    900 / 1550 loss=4.662, ppl=25.32, wps=19968.3, ups=0.3, wpb=65536, bsz=128, num_updates=28800, lr=0.000186339, gnorm=0.556, train_wall=258, gb_free=7.9, wall=78505
2022-05-19 07:36:35 | INFO | train_inner | epoch 019:   1000 / 1550 loss=4.67, ppl=25.46, wps=21952.8, ups=0.33, wpb=65536, bsz=128, num_updates=28900, lr=0.000186016, gnorm=0.554, train_wall=240, gb_free=7.9, wall=78803
2022-05-19 07:41:30 | INFO | train_inner | epoch 019:   1100 / 1550 loss=4.65, ppl=25.1, wps=22172.9, ups=0.34, wpb=65536, bsz=128, num_updates=29000, lr=0.000185695, gnorm=0.559, train_wall=242, gb_free=7.9, wall=79099
2022-05-19 07:48:12 | INFO | train_inner | epoch 019:   1200 / 1550 loss=4.67, ppl=25.46, wps=16307.8, ups=0.25, wpb=65536, bsz=128, num_updates=29100, lr=0.000185376, gnorm=0.554, train_wall=327, gb_free=7.9, wall=79501
2022-05-19 08:01:28 | INFO | train_inner | epoch 019:   1300 / 1550 loss=4.656, ppl=25.22, wps=8231.9, ups=0.13, wpb=65534.5, bsz=128, num_updates=29200, lr=0.000185058, gnorm=0.551, train_wall=637, gb_free=7.9, wall=80297
2022-05-19 08:07:12 | INFO | train_inner | epoch 019:   1400 / 1550 loss=4.671, ppl=25.47, wps=19045.5, ups=0.29, wpb=65536, bsz=128, num_updates=29300, lr=0.000184742, gnorm=0.549, train_wall=260, gb_free=7.9, wall=80641
2022-05-19 08:13:11 | INFO | train_inner | epoch 019:   1500 / 1550 loss=4.67, ppl=25.46, wps=18289.9, ups=0.28, wpb=65536, bsz=128, num_updates=29400, lr=0.000184428, gnorm=0.555, train_wall=278, gb_free=7.9, wall=80999
2022-05-19 08:16:09 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-05-19 08:17:41 | INFO | valid | epoch 019 | valid on 'valid' subset | loss 4.704 | ppl 26.06 | wps 62591.5 | wpb 2047.4 | bsz 4 | num_updates 29450 | best_loss 4.704
2022-05-19 08:17:41 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 19 @ 29450 updates
2022-05-19 08:17:41 | INFO | fairseq.trainer | Saving checkpoint to /cluster/work/cotterell/liam/master-thesis/checkpoints/language_model/bart/checkpoint_best_bart_32_5.0000E-04_full.pt
2022-05-19 08:17:42 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/work/cotterell/liam/master-thesis/checkpoints/language_model/bart/checkpoint_best_bart_32_5.0000E-04_full.pt
2022-05-19 08:17:42 | INFO | fairseq.checkpoint_utils | Saved checkpoint checkpoints/language_model/bart/checkpoint_best_bart_32_5.0000E-04_full.pt (epoch 19 @ 29450 updates, score 4.704) (writing took 1.5276393345557153 seconds)
2022-05-19 08:17:42 | INFO | fairseq_cli.train | end of epoch 19 (average epoch stats below)
2022-05-19 08:17:42 | INFO | train | epoch 019 | loss 4.651 | ppl 25.12 | wps 14800.4 | ups 0.23 | wpb 65522.7 | bsz 128 | num_updates 29450 | lr 0.000184271 | gnorm 0.553 | train_wall 5307 | gb_free 7.9 | wall 81271
2022-05-19 08:17:43 | INFO | fairseq.data.iterators | grouped total_num_itrs = 1550
2022-05-19 08:17:43 | INFO | fairseq.trainer | begin training epoch 20
2022-05-19 08:17:43 | INFO | fairseq_cli.train | Start iterating over samples
2022-05-19 08:20:41 | INFO | train_inner | epoch 020:     50 / 1550 loss=4.643, ppl=24.98, wps=14507, ups=0.22, wpb=65331.2, bsz=127.6, num_updates=29500, lr=0.000184115, gnorm=0.555, train_wall=276, gb_free=7.9, wall=81450
2022-05-19 08:26:31 | INFO | train_inner | epoch 020:    150 / 1550 loss=4.619, ppl=24.58, wps=18727.7, ups=0.29, wpb=65536, bsz=128, num_updates=29600, lr=0.000183804, gnorm=0.552, train_wall=264, gb_free=7.9, wall=81800
2022-05-19 08:31:51 | INFO | train_inner | epoch 020:    250 / 1550 loss=4.62, ppl=24.59, wps=20475.9, ups=0.31, wpb=65536, bsz=128, num_updates=29700, lr=0.000183494, gnorm=0.559, train_wall=255, gb_free=7.9, wall=82120
2022-05-19 08:37:25 | INFO | train_inner | epoch 020:    350 / 1550 loss=4.625, ppl=24.67, wps=19611.7, ups=0.3, wpb=65536, bsz=128, num_updates=29800, lr=0.000183186, gnorm=0.555, train_wall=261, gb_free=7.9, wall=82454
2022-05-19 08:42:37 | INFO | train_inner | epoch 020:    450 / 1550 loss=4.63, ppl=24.76, wps=21032.4, ups=0.32, wpb=65536, bsz=128, num_updates=29900, lr=0.000182879, gnorm=0.55, train_wall=247, gb_free=7.9, wall=82765
2022-05-19 08:57:23 | INFO | train_inner | epoch 020:    550 / 1550 loss=4.636, ppl=24.87, wps=7392, ups=0.11, wpb=65534.5, bsz=128, num_updates=30000, lr=0.000182574, gnorm=0.555, train_wall=705, gb_free=7.9, wall=83652
2022-05-19 09:05:31 | INFO | train_inner | epoch 020:    650 / 1550 loss=4.63, ppl=24.77, wps=13432.8, ups=0.2, wpb=65536, bsz=128, num_updates=30100, lr=0.000182271, gnorm=0.556, train_wall=361, gb_free=7.9, wall=84140
2022-05-19 09:12:19 | INFO | train_inner | epoch 020:    750 / 1550 loss=4.632, ppl=24.8, wps=16079.7, ups=0.25, wpb=65536, bsz=128, num_updates=30200, lr=0.000181969, gnorm=0.554, train_wall=304, gb_free=7.9, wall=84547
2022-05-19 09:18:53 | INFO | train_inner | epoch 020:    850 / 1550 loss=4.635, ppl=24.84, wps=16620, ups=0.25, wpb=65536, bsz=128, num_updates=30300, lr=0.000181668, gnorm=0.558, train_wall=287, gb_free=7.9, wall=84942
2022-05-19 09:24:28 | INFO | train_inner | epoch 020:    950 / 1550 loss=4.646, ppl=25.04, wps=19560, ups=0.3, wpb=65536, bsz=128, num_updates=30400, lr=0.000181369, gnorm=0.556, train_wall=254, gb_free=7.9, wall=85277
2022-05-19 09:30:20 | INFO | train_inner | epoch 020:   1050 / 1550 loss=4.65, ppl=25.1, wps=18607.2, ups=0.28, wpb=65536, bsz=128, num_updates=30500, lr=0.000181071, gnorm=0.553, train_wall=264, gb_free=7.9, wall=85629
2022-05-19 09:36:12 | INFO | train_inner | epoch 020:   1150 / 1550 loss=4.656, ppl=25.21, wps=18655.6, ups=0.28, wpb=65536, bsz=128, num_updates=30600, lr=0.000180775, gnorm=0.551, train_wall=266, gb_free=7.9, wall=85980
2022-05-19 09:41:41 | INFO | train_inner | epoch 020:   1250 / 1550 loss=4.645, ppl=25.02, wps=19868.2, ups=0.3, wpb=65536, bsz=128, num_updates=30700, lr=0.000180481, gnorm=0.548, train_wall=247, gb_free=7.9, wall=86310
2022-05-19 09:47:28 | INFO | train_inner | epoch 020:   1350 / 1550 loss=4.649, ppl=25.08, wps=18917.6, ups=0.29, wpb=65536, bsz=128, num_updates=30800, lr=0.000180187, gnorm=0.557, train_wall=265, gb_free=7.9, wall=86657
2022-05-19 09:59:54 | INFO | train_inner | epoch 020:   1450 / 1550 loss=4.658, ppl=25.25, wps=8779.7, ups=0.13, wpb=65536, bsz=128, num_updates=30900, lr=0.000179896, gnorm=0.562, train_wall=565, gb_free=7.9, wall=87403
2022-05-19 10:09:32 | INFO | train_inner | epoch 020:   1550 / 1550 loss=4.649, ppl=25.1, wps=11314.5, ups=0.17, wpb=65331.2, bsz=127.6, num_updates=31000, lr=0.000179605, gnorm=0.546, train_wall=430, gb_free=7.9, wall=87981
2022-05-19 10:09:32 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-05-19 10:11:06 | INFO | valid | epoch 020 | valid on 'valid' subset | loss 4.697 | ppl 25.93 | wps 59872.6 | wpb 2047.4 | bsz 4 | num_updates 31000 | best_loss 4.697
2022-05-19 10:11:06 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 20 @ 31000 updates
2022-05-19 10:11:06 | INFO | fairseq.trainer | Saving checkpoint to /cluster/work/cotterell/liam/master-thesis/checkpoints/language_model/bart/checkpoint_best_bart_32_5.0000E-04_full.pt
2022-05-19 10:11:08 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/work/cotterell/liam/master-thesis/checkpoints/language_model/bart/checkpoint_best_bart_32_5.0000E-04_full.pt
2022-05-19 10:11:08 | INFO | fairseq.checkpoint_utils | Saved checkpoint checkpoints/language_model/bart/checkpoint_best_bart_32_5.0000E-04_full.pt (epoch 20 @ 31000 updates, score 4.697) (writing took 1.5501562068238854 seconds)
2022-05-19 10:11:08 | INFO | fairseq_cli.train | end of epoch 20 (average epoch stats below)
2022-05-19 10:11:08 | INFO | train | epoch 020 | loss 4.638 | ppl 24.9 | wps 14923.8 | ups 0.23 | wpb 65522.7 | bsz 128 | num_updates 31000 | lr 0.000179605 | gnorm 0.554 | train_wall 5110 | gb_free 7.9 | wall 88076
2022-05-19 10:11:08 | INFO | fairseq.data.iterators | grouped total_num_itrs = 1550
2022-05-19 10:11:08 | INFO | fairseq.trainer | begin training epoch 21
2022-05-19 10:11:08 | INFO | fairseq_cli.train | Start iterating over samples
2022-05-19 10:17:00 | INFO | train_inner | epoch 021:    100 / 1550 loss=4.597, ppl=24.21, wps=14613.5, ups=0.22, wpb=65536, bsz=128, num_updates=31100, lr=0.000179316, gnorm=0.556, train_wall=270, gb_free=7.9, wall=88429
2022-05-19 10:23:01 | INFO | train_inner | epoch 021:    200 / 1550 loss=4.611, ppl=24.44, wps=18170.3, ups=0.28, wpb=65536, bsz=128, num_updates=31200, lr=0.000179029, gnorm=0.558, train_wall=275, gb_free=7.9, wall=88790
2022-05-19 10:29:43 | INFO | train_inner | epoch 021:    300 / 1550 loss=4.615, ppl=24.51, wps=16308.9, ups=0.25, wpb=65536, bsz=128, num_updates=31300, lr=0.000178743, gnorm=0.56, train_wall=305, gb_free=7.9, wall=89192
2022-05-19 10:35:40 | INFO | train_inner | epoch 021:    400 / 1550 loss=4.613, ppl=24.47, wps=18333.8, ups=0.28, wpb=65536, bsz=128, num_updates=31400, lr=0.000178458, gnorm=0.56, train_wall=274, gb_free=7.9, wall=89549
2022-05-19 10:41:51 | INFO | train_inner | epoch 021:    500 / 1550 loss=4.624, ppl=24.66, wps=17669.2, ups=0.27, wpb=65536, bsz=128, num_updates=31500, lr=0.000178174, gnorm=0.553, train_wall=280, gb_free=7.9, wall=89920
2022-05-19 10:48:03 | INFO | train_inner | epoch 021:    600 / 1550 loss=4.628, ppl=24.73, wps=17605.9, ups=0.27, wpb=65536, bsz=128, num_updates=31600, lr=0.000177892, gnorm=0.56, train_wall=283, gb_free=7.9, wall=90292
2022-05-19 10:52:58 | INFO | train_inner | epoch 021:    700 / 1550 loss=4.607, ppl=24.37, wps=22266.6, ups=0.34, wpb=65536, bsz=128, num_updates=31700, lr=0.000177611, gnorm=0.554, train_wall=233, gb_free=7.9, wall=90586
2022-05-19 10:57:41 | INFO | train_inner | epoch 021:    800 / 1550 loss=4.633, ppl=24.81, wps=23151.4, ups=0.35, wpb=65536, bsz=128, num_updates=31800, lr=0.000177332, gnorm=0.562, train_wall=233, gb_free=7.9, wall=90870
2022-05-19 11:03:11 | INFO | train_inner | epoch 021:    900 / 1550 loss=4.633, ppl=24.82, wps=19831.6, ups=0.3, wpb=65536, bsz=128, num_updates=31900, lr=0.000177054, gnorm=0.555, train_wall=253, gb_free=7.9, wall=91200
2022-05-19 11:08:16 | INFO | train_inner | epoch 021:   1000 / 1550 loss=4.638, ppl=24.9, wps=21496.8, ups=0.33, wpb=65534.5, bsz=128, num_updates=32000, lr=0.000176777, gnorm=0.555, train_wall=240, gb_free=7.9, wall=91505
2022-05-19 11:13:35 | INFO | train_inner | epoch 021:   1100 / 1550 loss=4.632, ppl=24.8, wps=20525, ups=0.31, wpb=65536, bsz=128, num_updates=32100, lr=0.000176501, gnorm=0.56, train_wall=241, gb_free=7.9, wall=91824
2022-05-19 11:18:23 | INFO | train_inner | epoch 021:   1200 / 1550 loss=4.642, ppl=24.97, wps=22800.9, ups=0.35, wpb=65536, bsz=128, num_updates=32200, lr=0.000176227, gnorm=0.562, train_wall=232, gb_free=7.9, wall=92112
2022-05-19 11:25:42 | INFO | train_inner | epoch 021:   1300 / 1550 loss=4.633, ppl=24.81, wps=14934.7, ups=0.23, wpb=65536, bsz=128, num_updates=32300, lr=0.000175954, gnorm=0.557, train_wall=340, gb_free=7.9, wall=92550
2022-05-19 11:32:15 | INFO | train_inner | epoch 021:   1400 / 1550 loss=4.646, ppl=25.04, wps=16667.4, ups=0.25, wpb=65536, bsz=128, num_updates=32400, lr=0.000175682, gnorm=0.558, train_wall=295, gb_free=7.9, wall=92944
2022-05-19 11:38:35 | INFO | train_inner | epoch 021:   1500 / 1550 loss=4.638, ppl=24.9, wps=17239.2, ups=0.26, wpb=65536, bsz=128, num_updates=32500, lr=0.000175412, gnorm=0.554, train_wall=281, gb_free=7.9, wall=93324
2022-05-19 11:41:52 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-05-19 11:43:28 | INFO | valid | epoch 021 | valid on 'valid' subset | loss 4.687 | ppl 25.76 | wps 58759 | wpb 2047.4 | bsz 4 | num_updates 32550 | best_loss 4.687
2022-05-19 11:43:28 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 21 @ 32550 updates
2022-05-19 11:43:28 | INFO | fairseq.trainer | Saving checkpoint to /cluster/work/cotterell/liam/master-thesis/checkpoints/language_model/bart/checkpoint_best_bart_32_5.0000E-04_full.pt
2022-05-19 11:43:30 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/work/cotterell/liam/master-thesis/checkpoints/language_model/bart/checkpoint_best_bart_32_5.0000E-04_full.pt
2022-05-19 11:43:30 | INFO | fairseq.checkpoint_utils | Saved checkpoint checkpoints/language_model/bart/checkpoint_best_bart_32_5.0000E-04_full.pt (epoch 21 @ 32550 updates, score 4.687) (writing took 1.6694837962277234 seconds)
2022-05-19 11:43:30 | INFO | fairseq_cli.train | end of epoch 21 (average epoch stats below)
2022-05-19 11:43:30 | INFO | train | epoch 021 | loss 4.627 | ppl 24.7 | wps 18324.7 | ups 0.28 | wpb 65522.7 | bsz 128 | num_updates 32550 | lr 0.000175277 | gnorm 0.558 | train_wall 4179 | gb_free 7.9 | wall 93619
2022-05-19 11:43:30 | INFO | fairseq.data.iterators | grouped total_num_itrs = 1550
2022-05-19 11:43:30 | INFO | fairseq.trainer | begin training epoch 22
2022-05-19 11:43:30 | INFO | fairseq_cli.train | Start iterating over samples
2022-05-19 11:46:32 | INFO | train_inner | epoch 022:     50 / 1550 loss=4.61, ppl=24.42, wps=13690.2, ups=0.21, wpb=65331.2, bsz=127.6, num_updates=32600, lr=0.000175142, gnorm=0.564, train_wall=278, gb_free=7.9, wall=93801
2022-05-19 11:51:54 | INFO | train_inner | epoch 022:    150 / 1550 loss=4.596, ppl=24.19, wps=20377.9, ups=0.31, wpb=65536, bsz=128, num_updates=32700, lr=0.000174874, gnorm=0.558, train_wall=243, gb_free=7.9, wall=94123
2022-05-19 11:57:25 | INFO | train_inner | epoch 022:    250 / 1550 loss=4.607, ppl=24.38, wps=19781.3, ups=0.3, wpb=65536, bsz=128, num_updates=32800, lr=0.000174608, gnorm=0.559, train_wall=256, gb_free=7.9, wall=94454
2022-05-19 12:03:17 | INFO | train_inner | epoch 022:    350 / 1550 loss=4.598, ppl=24.22, wps=18630.8, ups=0.28, wpb=65536, bsz=128, num_updates=32900, lr=0.000174342, gnorm=0.565, train_wall=265, gb_free=7.9, wall=94806
2022-05-19 12:09:07 | INFO | train_inner | epoch 022:    450 / 1550 loss=4.613, ppl=24.46, wps=18715.6, ups=0.29, wpb=65536, bsz=128, num_updates=33000, lr=0.000174078, gnorm=0.554, train_wall=266, gb_free=7.9, wall=95156
2022-05-19 12:15:32 | INFO | train_inner | epoch 022:    550 / 1550 loss=4.622, ppl=24.63, wps=17016.9, ups=0.26, wpb=65536, bsz=128, num_updates=33100, lr=0.000173814, gnorm=0.556, train_wall=314, gb_free=7.9, wall=95541
2022-05-19 12:32:38 | INFO | train_inner | epoch 022:    650 / 1550 loss=4.601, ppl=24.27, wps=6388, ups=0.1, wpb=65536, bsz=128, num_updates=33200, lr=0.000173553, gnorm=0.557, train_wall=805, gb_free=7.9, wall=96567
2022-05-19 12:43:07 | INFO | train_inner | epoch 022:    750 / 1550 loss=4.608, ppl=24.39, wps=10414.8, ups=0.16, wpb=65536, bsz=128, num_updates=33300, lr=0.000173292, gnorm=0.56, train_wall=485, gb_free=7.9, wall=97196
2022-05-19 12:49:25 | INFO | train_inner | epoch 022:    850 / 1550 loss=4.623, ppl=24.64, wps=17334.6, ups=0.26, wpb=65536, bsz=128, num_updates=33400, lr=0.000173032, gnorm=0.559, train_wall=277, gb_free=7.9, wall=97574
2022-05-19 12:55:22 | INFO | train_inner | epoch 022:    950 / 1550 loss=4.624, ppl=24.66, wps=18384.3, ups=0.28, wpb=65536, bsz=128, num_updates=33500, lr=0.000172774, gnorm=0.561, train_wall=276, gb_free=7.9, wall=97931
2022-05-19 13:01:12 | INFO | train_inner | epoch 022:   1050 / 1550 loss=4.632, ppl=24.79, wps=18719, ups=0.29, wpb=65536, bsz=128, num_updates=33600, lr=0.000172516, gnorm=0.56, train_wall=272, gb_free=7.9, wall=98281
2022-05-19 13:06:35 | INFO | train_inner | epoch 022:   1150 / 1550 loss=4.625, ppl=24.68, wps=20306.5, ups=0.31, wpb=65536, bsz=128, num_updates=33700, lr=0.00017226, gnorm=0.562, train_wall=251, gb_free=7.9, wall=98603
2022-05-19 13:12:08 | INFO | train_inner | epoch 022:   1250 / 1550 loss=4.625, ppl=24.67, wps=19642.2, ups=0.3, wpb=65536, bsz=128, num_updates=33800, lr=0.000172005, gnorm=0.563, train_wall=254, gb_free=7.9, wall=98937
2022-05-19 13:18:04 | INFO | train_inner | epoch 022:   1350 / 1550 loss=4.619, ppl=24.58, wps=18410, ups=0.28, wpb=65534.5, bsz=128, num_updates=33900, lr=0.000171751, gnorm=0.566, train_wall=275, gb_free=7.9, wall=99293
2022-05-19 13:29:54 | INFO | train_inner | epoch 022:   1450 / 1550 loss=4.628, ppl=24.73, wps=9230.6, ups=0.14, wpb=65536, bsz=128, num_updates=34000, lr=0.000171499, gnorm=0.561, train_wall=515, gb_free=7.9, wall=100003
2022-05-19 13:36:50 | INFO | train_inner | epoch 022:   1550 / 1550 loss=4.627, ppl=24.7, wps=15708.9, ups=0.24, wpb=65331.2, bsz=127.6, num_updates=34100, lr=0.000171247, gnorm=0.553, train_wall=316, gb_free=7.9, wall=100419
2022-05-19 13:36:50 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-05-19 13:38:18 | INFO | valid | epoch 022 | valid on 'valid' subset | loss 4.681 | ppl 25.65 | wps 64192.5 | wpb 2047.4 | bsz 4 | num_updates 34100 | best_loss 4.681
2022-05-19 13:38:18 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 22 @ 34100 updates
2022-05-19 13:38:18 | INFO | fairseq.trainer | Saving checkpoint to /cluster/work/cotterell/liam/master-thesis/checkpoints/language_model/bart/checkpoint_best_bart_32_5.0000E-04_full.pt
2022-05-19 13:38:20 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/work/cotterell/liam/master-thesis/checkpoints/language_model/bart/checkpoint_best_bart_32_5.0000E-04_full.pt
2022-05-19 13:38:20 | INFO | fairseq.checkpoint_utils | Saved checkpoint checkpoints/language_model/bart/checkpoint_best_bart_32_5.0000E-04_full.pt (epoch 22 @ 34100 updates, score 4.681) (writing took 1.652498914860189 seconds)
2022-05-19 13:38:20 | INFO | fairseq_cli.train | end of epoch 22 (average epoch stats below)
2022-05-19 13:38:20 | INFO | train | epoch 022 | loss 4.615 | ppl 24.51 | wps 14740.4 | ups 0.22 | wpb 65522.7 | bsz 128 | num_updates 34100 | lr 0.000171247 | gnorm 0.56 | train_wall 5206 | gb_free 7.9 | wall 100509
2022-05-19 13:38:20 | INFO | fairseq.data.iterators | grouped total_num_itrs = 1550
2022-05-19 13:38:20 | INFO | fairseq.trainer | begin training epoch 23
2022-05-19 13:38:20 | INFO | fairseq_cli.train | Start iterating over samples
2022-05-19 13:43:53 | INFO | train_inner | epoch 023:    100 / 1550 loss=4.577, ppl=23.87, wps=15508.4, ups=0.24, wpb=65536, bsz=128, num_updates=34200, lr=0.000170996, gnorm=0.567, train_wall=257, gb_free=7.9, wall=100842
2022-05-19 13:48:50 | INFO | train_inner | epoch 023:    200 / 1550 loss=4.589, ppl=24.06, wps=22045.2, ups=0.34, wpb=65536, bsz=128, num_updates=34300, lr=0.000170747, gnorm=0.559, train_wall=239, gb_free=7.9, wall=101139
2022-05-19 13:54:12 | INFO | train_inner | epoch 023:    300 / 1550 loss=4.595, ppl=24.16, wps=20351.8, ups=0.31, wpb=65536, bsz=128, num_updates=34400, lr=0.000170499, gnorm=0.566, train_wall=248, gb_free=7.9, wall=101461
2022-05-19 13:59:13 | INFO | train_inner | epoch 023:    400 / 1550 loss=4.594, ppl=24.16, wps=21779.2, ups=0.33, wpb=65536, bsz=128, num_updates=34500, lr=0.000170251, gnorm=0.563, train_wall=238, gb_free=7.9, wall=101762
2022-05-19 14:04:20 | INFO | train_inner | epoch 023:    500 / 1550 loss=4.604, ppl=24.32, wps=21360.8, ups=0.33, wpb=65536, bsz=128, num_updates=34600, lr=0.000170005, gnorm=0.568, train_wall=242, gb_free=7.9, wall=102069
2022-05-19 14:09:24 | INFO | train_inner | epoch 023:    600 / 1550 loss=4.609, ppl=24.41, wps=21526.5, ups=0.33, wpb=65536, bsz=128, num_updates=34700, lr=0.00016976, gnorm=0.559, train_wall=239, gb_free=7.9, wall=102373
2022-05-19 14:14:22 | INFO | train_inner | epoch 023:    700 / 1550 loss=4.6, ppl=24.25, wps=22023.9, ups=0.34, wpb=65536, bsz=128, num_updates=34800, lr=0.000169516, gnorm=0.558, train_wall=236, gb_free=7.9, wall=102671
2022-05-19 14:19:20 | INFO | train_inner | epoch 023:    800 / 1550 loss=4.607, ppl=24.38, wps=21939.2, ups=0.33, wpb=65534.5, bsz=128, num_updates=34900, lr=0.000169273, gnorm=0.562, train_wall=237, gb_free=7.9, wall=102969
2022-05-19 14:25:29 | INFO | train_inner | epoch 023:    900 / 1550 loss=4.608, ppl=24.38, wps=17758.6, ups=0.27, wpb=65536, bsz=128, num_updates=35000, lr=0.000169031, gnorm=0.559, train_wall=297, gb_free=7.9, wall=103338
2022-05-19 14:35:38 | INFO | train_inner | epoch 023:   1000 / 1550 loss=4.605, ppl=24.33, wps=10763.6, ups=0.16, wpb=65536, bsz=128, num_updates=35100, lr=0.00016879, gnorm=0.555, train_wall=480, gb_free=7.9, wall=103947
2022-05-19 14:41:14 | INFO | train_inner | epoch 023:   1100 / 1550 loss=4.615, ppl=24.5, wps=19537.1, ups=0.3, wpb=65536, bsz=128, num_updates=35200, lr=0.00016855, gnorm=0.566, train_wall=253, gb_free=7.9, wall=104283
2022-05-19 14:46:47 | INFO | train_inner | epoch 023:   1200 / 1550 loss=4.621, ppl=24.62, wps=19652.4, ups=0.3, wpb=65536, bsz=128, num_updates=35300, lr=0.000168311, gnorm=0.563, train_wall=259, gb_free=7.9, wall=104616
2022-05-19 14:52:14 | INFO | train_inner | epoch 023:   1300 / 1550 loss=4.619, ppl=24.57, wps=20044.9, ups=0.31, wpb=65536, bsz=128, num_updates=35400, lr=0.000168073, gnorm=0.564, train_wall=253, gb_free=7.9, wall=104943
2022-05-19 14:57:47 | INFO | train_inner | epoch 023:   1400 / 1550 loss=4.613, ppl=24.46, wps=19664.6, ups=0.3, wpb=65536, bsz=128, num_updates=35500, lr=0.000167836, gnorm=0.561, train_wall=262, gb_free=7.9, wall=105276
2022-05-19 15:03:16 | INFO | train_inner | epoch 023:   1500 / 1550 loss=4.624, ppl=24.65, wps=19928.4, ups=0.3, wpb=65536, bsz=128, num_updates=35600, lr=0.0001676, gnorm=0.561, train_wall=256, gb_free=7.9, wall=105605
2022-05-19 15:05:39 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-05-19 15:06:54 | INFO | valid | epoch 023 | valid on 'valid' subset | loss 4.675 | ppl 25.54 | wps 75432.2 | wpb 2047.4 | bsz 4 | num_updates 35650 | best_loss 4.675
2022-05-19 15:06:54 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 23 @ 35650 updates
2022-05-19 15:06:54 | INFO | fairseq.trainer | Saving checkpoint to /cluster/work/cotterell/liam/master-thesis/checkpoints/language_model/bart/checkpoint_best_bart_32_5.0000E-04_full.pt
2022-05-19 15:06:57 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/work/cotterell/liam/master-thesis/checkpoints/language_model/bart/checkpoint_best_bart_32_5.0000E-04_full.pt
2022-05-19 15:06:57 | INFO | fairseq.checkpoint_utils | Saved checkpoint checkpoints/language_model/bart/checkpoint_best_bart_32_5.0000E-04_full.pt (epoch 23 @ 35650 updates, score 4.675) (writing took 2.963084240909666 seconds)
2022-05-19 15:06:57 | INFO | fairseq_cli.train | end of epoch 23 (average epoch stats below)
2022-05-19 15:06:57 | INFO | train | epoch 023 | loss 4.606 | ppl 24.35 | wps 19099.2 | ups 0.29 | wpb 65522.7 | bsz 128 | num_updates 35650 | lr 0.000167483 | gnorm 0.562 | train_wall 4110 | gb_free 7.9 | wall 105826
2022-05-19 15:06:57 | INFO | fairseq.data.iterators | grouped total_num_itrs = 1550
2022-05-19 15:06:57 | INFO | fairseq.trainer | begin training epoch 24
2022-05-19 15:06:57 | INFO | fairseq_cli.train | Start iterating over samples
2022-05-19 15:09:36 | INFO | train_inner | epoch 024:     50 / 1550 loss=4.593, ppl=24.14, wps=17207.9, ups=0.26, wpb=65331.2, bsz=127.6, num_updates=35700, lr=0.000167365, gnorm=0.565, train_wall=232, gb_free=7.9, wall=105985
2022-05-19 15:14:39 | INFO | train_inner | epoch 024:    150 / 1550 loss=4.569, ppl=23.74, wps=21647.8, ups=0.33, wpb=65536, bsz=128, num_updates=35800, lr=0.000167132, gnorm=0.565, train_wall=238, gb_free=7.9, wall=106288
2022-05-19 15:20:09 | INFO | train_inner | epoch 024:    250 / 1550 loss=4.567, ppl=23.7, wps=19840.5, ups=0.3, wpb=65536, bsz=128, num_updates=35900, lr=0.000166899, gnorm=0.567, train_wall=244, gb_free=7.9, wall=106618
2022-05-19 15:25:26 | INFO | train_inner | epoch 024:    350 / 1550 loss=4.593, ppl=24.13, wps=20686.8, ups=0.32, wpb=65536, bsz=128, num_updates=36000, lr=0.000166667, gnorm=0.563, train_wall=248, gb_free=7.9, wall=106935
2022-05-19 15:32:36 | INFO | train_inner | epoch 024:    450 / 1550 loss=4.59, ppl=24.08, wps=15237.4, ups=0.23, wpb=65536, bsz=128, num_updates=36100, lr=0.000166436, gnorm=0.568, train_wall=346, gb_free=7.9, wall=107365
2022-05-19 15:39:17 | INFO | train_inner | epoch 024:    550 / 1550 loss=4.589, ppl=24.07, wps=16327.3, ups=0.25, wpb=65536, bsz=128, num_updates=36200, lr=0.000166206, gnorm=0.572, train_wall=307, gb_free=7.9, wall=107766
2022-05-19 15:44:17 | INFO | train_inner | epoch 024:    650 / 1550 loss=4.607, ppl=24.36, wps=21899, ups=0.33, wpb=65536, bsz=128, num_updates=36300, lr=0.000165977, gnorm=0.57, train_wall=237, gb_free=7.9, wall=108065
2022-05-19 15:50:00 | INFO | train_inner | epoch 024:    750 / 1550 loss=4.601, ppl=24.27, wps=19082.6, ups=0.29, wpb=65536, bsz=128, num_updates=36400, lr=0.000165748, gnorm=0.564, train_wall=263, gb_free=7.9, wall=108409
2022-05-19 15:55:24 | INFO | train_inner | epoch 024:    850 / 1550 loss=4.592, ppl=24.12, wps=20239.5, ups=0.31, wpb=65536, bsz=128, num_updates=36500, lr=0.000165521, gnorm=0.565, train_wall=251, gb_free=7.9, wall=108733
2022-05-19 16:00:19 | INFO | train_inner | epoch 024:    950 / 1550 loss=4.599, ppl=24.23, wps=22209.2, ups=0.34, wpb=65534.5, bsz=128, num_updates=36600, lr=0.000165295, gnorm=0.567, train_wall=236, gb_free=7.9, wall=109028
2022-05-19 16:05:46 | INFO | train_inner | epoch 024:   1050 / 1550 loss=4.598, ppl=24.22, wps=20065.6, ups=0.31, wpb=65536, bsz=128, num_updates=36700, lr=0.00016507, gnorm=0.566, train_wall=257, gb_free=7.9, wall=109354
2022-05-19 16:10:59 | INFO | train_inner | epoch 024:   1150 / 1550 loss=4.6, ppl=24.26, wps=20910.8, ups=0.32, wpb=65536, bsz=128, num_updates=36800, lr=0.000164845, gnorm=0.573, train_wall=247, gb_free=7.9, wall=109668
2022-05-19 16:15:42 | INFO | train_inner | epoch 024:   1250 / 1550 loss=4.606, ppl=24.35, wps=23169.4, ups=0.35, wpb=65536, bsz=128, num_updates=36900, lr=0.000164622, gnorm=0.56, train_wall=234, gb_free=7.9, wall=109951
2022-05-19 16:20:32 | INFO | train_inner | epoch 024:   1350 / 1550 loss=4.625, ppl=24.67, wps=22544.5, ups=0.34, wpb=65536, bsz=128, num_updates=37000, lr=0.000164399, gnorm=0.564, train_wall=235, gb_free=7.9, wall=110241
2022-05-19 16:25:35 | INFO | train_inner | epoch 024:   1450 / 1550 loss=4.608, ppl=24.38, wps=21679.8, ups=0.33, wpb=65536, bsz=128, num_updates=37100, lr=0.000164177, gnorm=0.56, train_wall=232, gb_free=7.9, wall=110544
2022-05-19 16:30:28 | INFO | train_inner | epoch 024:   1550 / 1550 loss=4.613, ppl=24.48, wps=22250.6, ups=0.34, wpb=65331.2, bsz=127.6, num_updates=37200, lr=0.000163956, gnorm=0.57, train_wall=238, gb_free=7.9, wall=110837
2022-05-19 16:30:28 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-05-19 16:31:55 | INFO | valid | epoch 024 | valid on 'valid' subset | loss 4.668 | ppl 25.43 | wps 65488.1 | wpb 2047.4 | bsz 4 | num_updates 37200 | best_loss 4.668
2022-05-19 16:31:55 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 24 @ 37200 updates
2022-05-19 16:31:55 | INFO | fairseq.trainer | Saving checkpoint to /cluster/work/cotterell/liam/master-thesis/checkpoints/language_model/bart/checkpoint_best_bart_32_5.0000E-04_full.pt
2022-05-19 16:31:58 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/work/cotterell/liam/master-thesis/checkpoints/language_model/bart/checkpoint_best_bart_32_5.0000E-04_full.pt
2022-05-19 16:31:58 | INFO | fairseq.checkpoint_utils | Saved checkpoint checkpoints/language_model/bart/checkpoint_best_bart_32_5.0000E-04_full.pt (epoch 24 @ 37200 updates, score 4.668) (writing took 2.846191744785756 seconds)
2022-05-19 16:31:58 | INFO | fairseq_cli.train | end of epoch 24 (average epoch stats below)
2022-05-19 16:31:58 | INFO | train | epoch 024 | loss 4.596 | ppl 24.19 | wps 19912.6 | ups 0.3 | wpb 65522.7 | bsz 128 | num_updates 37200 | lr 0.000163956 | gnorm 0.566 | train_wall 3931 | gb_free 7.9 | wall 110926
2022-05-19 16:31:58 | INFO | fairseq.data.iterators | grouped total_num_itrs = 1550
2022-05-19 16:31:58 | INFO | fairseq.trainer | begin training epoch 25
2022-05-19 16:31:58 | INFO | fairseq_cli.train | Start iterating over samples
2022-05-19 16:40:47 | INFO | train_inner | epoch 025:    100 / 1550 loss=4.564, ppl=23.65, wps=10590.2, ups=0.16, wpb=65536, bsz=128, num_updates=37300, lr=0.000163737, gnorm=0.577, train_wall=405, gb_free=7.9, wall=111456
2022-05-19 16:46:53 | INFO | train_inner | epoch 025:    200 / 1550 loss=4.564, ppl=23.66, wps=17925.3, ups=0.27, wpb=65536, bsz=128, num_updates=37400, lr=0.000163517, gnorm=0.561, train_wall=279, gb_free=7.9, wall=111822
2022-05-19 16:52:25 | INFO | train_inner | epoch 025:    300 / 1550 loss=4.559, ppl=23.57, wps=19755, ups=0.3, wpb=65536, bsz=128, num_updates=37500, lr=0.000163299, gnorm=0.568, train_wall=254, gb_free=7.9, wall=112153
2022-05-19 16:57:44 | INFO | train_inner | epoch 025:    400 / 1550 loss=4.576, ppl=23.85, wps=20496.9, ups=0.31, wpb=65536, bsz=128, num_updates=37600, lr=0.000163082, gnorm=0.562, train_wall=252, gb_free=7.9, wall=112473
2022-05-19 17:03:04 | INFO | train_inner | epoch 025:    500 / 1550 loss=4.582, ppl=23.95, wps=20483.4, ups=0.31, wpb=65536, bsz=128, num_updates=37700, lr=0.000162866, gnorm=0.562, train_wall=246, gb_free=7.9, wall=112793
2022-05-19 17:08:06 | INFO | train_inner | epoch 025:    600 / 1550 loss=4.595, ppl=24.18, wps=21693.4, ups=0.33, wpb=65534.5, bsz=128, num_updates=37800, lr=0.00016265, gnorm=0.567, train_wall=232, gb_free=7.9, wall=113095
2022-05-19 17:13:09 | INFO | train_inner | epoch 025:    700 / 1550 loss=4.578, ppl=23.89, wps=21687, ups=0.33, wpb=65536, bsz=128, num_updates=37900, lr=0.000162435, gnorm=0.575, train_wall=243, gb_free=7.9, wall=113397
2022-05-19 17:18:11 | INFO | train_inner | epoch 025:    800 / 1550 loss=4.601, ppl=24.27, wps=21684.9, ups=0.33, wpb=65536, bsz=128, num_updates=38000, lr=0.000162221, gnorm=0.57, train_wall=242, gb_free=7.9, wall=113700
2022-05-19 17:23:33 | INFO | train_inner | epoch 025:    900 / 1550 loss=4.588, ppl=24.05, wps=20318.3, ups=0.31, wpb=65536, bsz=128, num_updates=38100, lr=0.000162008, gnorm=0.569, train_wall=256, gb_free=7.9, wall=114022
2022-05-19 17:28:57 | INFO | train_inner | epoch 025:   1000 / 1550 loss=4.591, ppl=24.11, wps=20249.2, ups=0.31, wpb=65536, bsz=128, num_updates=38200, lr=0.000161796, gnorm=0.569, train_wall=250, gb_free=7.9, wall=114346
2022-05-19 17:33:44 | INFO | train_inner | epoch 025:   1100 / 1550 loss=4.599, ppl=24.23, wps=22868, ups=0.35, wpb=65536, bsz=128, num_updates=38300, lr=0.000161585, gnorm=0.569, train_wall=227, gb_free=7.9, wall=114632
2022-05-19 17:47:51 | INFO | train_inner | epoch 025:   1200 / 1550 loss=4.616, ppl=24.52, wps=7732.3, ups=0.12, wpb=65536, bsz=128, num_updates=38400, lr=0.000161374, gnorm=0.57, train_wall=643, gb_free=7.9, wall=115480
2022-05-19 17:55:21 | INFO | train_inner | epoch 025:   1300 / 1550 loss=4.595, ppl=24.17, wps=14558.3, ups=0.22, wpb=65536, bsz=128, num_updates=38500, lr=0.000161165, gnorm=0.573, train_wall=328, gb_free=7.9, wall=115930
2022-05-19 18:02:25 | INFO | train_inner | epoch 025:   1400 / 1550 loss=4.593, ppl=24.14, wps=15459.8, ups=0.24, wpb=65536, bsz=128, num_updates=38600, lr=0.000160956, gnorm=0.568, train_wall=319, gb_free=7.9, wall=116354
2022-05-19 18:08:03 | INFO | train_inner | epoch 025:   1500 / 1550 loss=4.595, ppl=24.17, wps=19392.7, ups=0.3, wpb=65536, bsz=128, num_updates=38700, lr=0.000160748, gnorm=0.57, train_wall=262, gb_free=7.9, wall=116692
2022-05-19 18:11:05 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-05-19 18:12:30 | INFO | valid | epoch 025 | valid on 'valid' subset | loss 4.664 | ppl 25.35 | wps 66084.4 | wpb 2047.4 | bsz 4 | num_updates 38750 | best_loss 4.664
2022-05-19 18:12:30 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 25 @ 38750 updates
2022-05-19 18:12:30 | INFO | fairseq.trainer | Saving checkpoint to /cluster/work/cotterell/liam/master-thesis/checkpoints/language_model/bart/checkpoint_best_bart_32_5.0000E-04_full.pt
2022-05-19 18:12:33 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/work/cotterell/liam/master-thesis/checkpoints/language_model/bart/checkpoint_best_bart_32_5.0000E-04_full.pt
2022-05-19 18:12:33 | INFO | fairseq.checkpoint_utils | Saved checkpoint checkpoints/language_model/bart/checkpoint_best_bart_32_5.0000E-04_full.pt (epoch 25 @ 38750 updates, score 4.664) (writing took 2.754416288807988 seconds)
2022-05-19 18:12:33 | INFO | fairseq_cli.train | end of epoch 25 (average epoch stats below)
2022-05-19 18:12:33 | INFO | train | epoch 025 | loss 4.587 | ppl 24.04 | wps 16827.5 | ups 0.26 | wpb 65522.7 | bsz 128 | num_updates 38750 | lr 0.000160644 | gnorm 0.569 | train_wall 4579 | gb_free 7.9 | wall 116962
2022-05-19 18:12:33 | INFO | fairseq.data.iterators | grouped total_num_itrs = 1550
2022-05-19 18:12:33 | INFO | fairseq.trainer | begin training epoch 26
2022-05-19 18:12:33 | INFO | fairseq_cli.train | Start iterating over samples
2022-05-19 18:15:35 | INFO | train_inner | epoch 026:     50 / 1550 loss=4.589, ppl=24.07, wps=14451.5, ups=0.22, wpb=65331.2, bsz=127.6, num_updates=38800, lr=0.00016054, gnorm=0.576, train_wall=279, gb_free=7.9, wall=117144
2022-05-19 18:22:05 | INFO | train_inner | epoch 026:    150 / 1550 loss=4.558, ppl=23.56, wps=16831.9, ups=0.26, wpb=65536, bsz=128, num_updates=38900, lr=0.000160334, gnorm=0.57, train_wall=290, gb_free=7.9, wall=117533
2022-05-19 18:27:25 | INFO | train_inner | epoch 026:    250 / 1550 loss=4.55, ppl=23.42, wps=20437.2, ups=0.31, wpb=65536, bsz=128, num_updates=39000, lr=0.000160128, gnorm=0.572, train_wall=252, gb_free=7.9, wall=117854
2022-05-19 18:33:27 | INFO | train_inner | epoch 026:    350 / 1550 loss=4.561, ppl=23.6, wps=18112.5, ups=0.28, wpb=65536, bsz=128, num_updates=39100, lr=0.000159923, gnorm=0.567, train_wall=268, gb_free=7.9, wall=118216
2022-05-19 18:39:20 | INFO | train_inner | epoch 026:    450 / 1550 loss=4.578, ppl=23.88, wps=18578.3, ups=0.28, wpb=65536, bsz=128, num_updates=39200, lr=0.000159719, gnorm=0.577, train_wall=271, gb_free=7.9, wall=118569
2022-05-19 18:53:11 | INFO | train_inner | epoch 026:    550 / 1550 loss=4.57, ppl=23.75, wps=7886.7, ups=0.12, wpb=65536, bsz=128, num_updates=39300, lr=0.000159516, gnorm=0.569, train_wall=647, gb_free=7.9, wall=119400
2022-05-19 18:59:37 | INFO | train_inner | epoch 026:    650 / 1550 loss=4.584, ppl=23.98, wps=16958.9, ups=0.26, wpb=65536, bsz=128, num_updates=39400, lr=0.000159313, gnorm=0.573, train_wall=293, gb_free=7.9, wall=119786
2022-05-19 19:05:27 | INFO | train_inner | epoch 026:    750 / 1550 loss=4.577, ppl=23.87, wps=18711.6, ups=0.29, wpb=65536, bsz=128, num_updates=39500, lr=0.000159111, gnorm=0.57, train_wall=263, gb_free=7.9, wall=120136
2022-05-19 19:11:24 | INFO | train_inner | epoch 026:    850 / 1550 loss=4.579, ppl=23.89, wps=18404.3, ups=0.28, wpb=65536, bsz=128, num_updates=39600, lr=0.00015891, gnorm=0.575, train_wall=270, gb_free=7.9, wall=120492
2022-05-19 19:16:37 | INFO | train_inner | epoch 026:    950 / 1550 loss=4.582, ppl=23.96, wps=20926.7, ups=0.32, wpb=65536, bsz=128, num_updates=39700, lr=0.00015871, gnorm=0.571, train_wall=244, gb_free=7.9, wall=120806
2022-05-19 19:22:08 | INFO | train_inner | epoch 026:   1050 / 1550 loss=4.589, ppl=24.07, wps=19796, ups=0.3, wpb=65536, bsz=128, num_updates=39800, lr=0.000158511, gnorm=0.57, train_wall=261, gb_free=7.9, wall=121137
2022-05-19 19:27:13 | INFO | train_inner | epoch 026:   1150 / 1550 loss=4.582, ppl=23.96, wps=21472, ups=0.33, wpb=65536, bsz=128, num_updates=39900, lr=0.000158312, gnorm=0.572, train_wall=240, gb_free=7.9, wall=121442
2022-05-19 19:32:14 | INFO | train_inner | epoch 026:   1250 / 1550 loss=4.592, ppl=24.12, wps=21784.6, ups=0.33, wpb=65534.5, bsz=128, num_updates=40000, lr=0.000158114, gnorm=0.565, train_wall=241, gb_free=7.9, wall=121743
2022-05-19 19:37:27 | INFO | train_inner | epoch 026:   1350 / 1550 loss=4.585, ppl=24.01, wps=20932.9, ups=0.32, wpb=65536, bsz=128, num_updates=40100, lr=0.000157917, gnorm=0.578, train_wall=245, gb_free=7.9, wall=122056
2022-05-19 19:42:11 | INFO | train_inner | epoch 026:   1450 / 1550 loss=4.593, ppl=24.13, wps=23080.5, ups=0.35, wpb=65536, bsz=128, num_updates=40200, lr=0.00015772, gnorm=0.57, train_wall=232, gb_free=7.9, wall=122340
2022-05-19 19:48:00 | INFO | train_inner | epoch 026:   1550 / 1550 loss=4.614, ppl=24.49, wps=18690.6, ups=0.29, wpb=65331.2, bsz=127.6, num_updates=40300, lr=0.000157524, gnorm=0.568, train_wall=264, gb_free=7.9, wall=122689
2022-05-19 19:48:00 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-05-19 19:49:34 | INFO | valid | epoch 026 | valid on 'valid' subset | loss 4.658 | ppl 25.25 | wps 60812.9 | wpb 2047.4 | bsz 4 | num_updates 40300 | best_loss 4.658
2022-05-19 19:49:34 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 26 @ 40300 updates
2022-05-19 19:49:34 | INFO | fairseq.trainer | Saving checkpoint to /cluster/work/cotterell/liam/master-thesis/checkpoints/language_model/bart/checkpoint_best_bart_32_5.0000E-04_full.pt
2022-05-19 19:49:36 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/work/cotterell/liam/master-thesis/checkpoints/language_model/bart/checkpoint_best_bart_32_5.0000E-04_full.pt
2022-05-19 19:49:36 | INFO | fairseq.checkpoint_utils | Saved checkpoint checkpoints/language_model/bart/checkpoint_best_bart_32_5.0000E-04_full.pt (epoch 26 @ 40300 updates, score 4.658) (writing took 1.6797725027427077 seconds)
2022-05-19 19:49:36 | INFO | fairseq_cli.train | end of epoch 26 (average epoch stats below)
2022-05-19 19:49:36 | INFO | train | epoch 026 | loss 4.579 | ppl 23.9 | wps 17441.3 | ups 0.27 | wpb 65522.7 | bsz 128 | num_updates 40300 | lr 0.000157524 | gnorm 0.571 | train_wall 4417 | gb_free 7.9 | wall 122785
2022-05-19 19:49:36 | INFO | fairseq.data.iterators | grouped total_num_itrs = 1550
2022-05-19 19:49:36 | INFO | fairseq.trainer | begin training epoch 27
2022-05-19 19:49:36 | INFO | fairseq_cli.train | Start iterating over samples
2022-05-19 19:55:44 | INFO | train_inner | epoch 027:    100 / 1550 loss=4.551, ppl=23.44, wps=14144.9, ups=0.22, wpb=65536, bsz=128, num_updates=40400, lr=0.000157329, gnorm=0.579, train_wall=287, gb_free=7.9, wall=123153
2022-05-19 20:01:18 | INFO | train_inner | epoch 027:    200 / 1550 loss=4.556, ppl=23.52, wps=19601.2, ups=0.3, wpb=65536, bsz=128, num_updates=40500, lr=0.000157135, gnorm=0.576, train_wall=260, gb_free=7.9, wall=123487
2022-05-19 20:06:03 | INFO | train_inner | epoch 027:    300 / 1550 loss=4.55, ppl=23.43, wps=23010.7, ups=0.35, wpb=65536, bsz=128, num_updates=40600, lr=0.000156941, gnorm=0.57, train_wall=233, gb_free=7.9, wall=123772
2022-05-19 20:11:28 | INFO | train_inner | epoch 027:    400 / 1550 loss=4.578, ppl=23.88, wps=20138.1, ups=0.31, wpb=65536, bsz=128, num_updates=40700, lr=0.000156748, gnorm=0.574, train_wall=263, gb_free=7.9, wall=124097
2022-05-19 20:17:16 | INFO | train_inner | epoch 027:    500 / 1550 loss=4.552, ppl=23.46, wps=18873.5, ups=0.29, wpb=65536, bsz=128, num_updates=40800, lr=0.000156556, gnorm=0.57, train_wall=268, gb_free=7.9, wall=124444
2022-05-19 20:22:25 | INFO | train_inner | epoch 027:    600 / 1550 loss=4.568, ppl=23.72, wps=21184, ups=0.32, wpb=65536, bsz=128, num_updates=40900, lr=0.000156365, gnorm=0.573, train_wall=240, gb_free=7.9, wall=124754
2022-05-19 20:27:36 | INFO | train_inner | epoch 027:    700 / 1550 loss=4.562, ppl=23.63, wps=21075.3, ups=0.32, wpb=65536, bsz=128, num_updates=41000, lr=0.000156174, gnorm=0.569, train_wall=248, gb_free=7.9, wall=125065
2022-05-19 20:32:40 | INFO | train_inner | epoch 027:    800 / 1550 loss=4.579, ppl=23.89, wps=21531.5, ups=0.33, wpb=65536, bsz=128, num_updates=41100, lr=0.000155984, gnorm=0.572, train_wall=240, gb_free=7.9, wall=125369
2022-05-19 20:38:24 | INFO | train_inner | epoch 027:    900 / 1550 loss=4.569, ppl=23.73, wps=19070.7, ups=0.29, wpb=65536, bsz=128, num_updates=41200, lr=0.000155794, gnorm=0.565, train_wall=259, gb_free=7.9, wall=125713
2022-05-19 20:43:04 | INFO | train_inner | epoch 027:   1000 / 1550 loss=4.591, ppl=24.1, wps=23419.4, ups=0.36, wpb=65536, bsz=128, num_updates=41300, lr=0.000155606, gnorm=0.576, train_wall=226, gb_free=7.9, wall=125993
2022-05-19 20:48:39 | INFO | train_inner | epoch 027:   1100 / 1550 loss=4.579, ppl=23.91, wps=19530.1, ups=0.3, wpb=65536, bsz=128, num_updates=41400, lr=0.000155417, gnorm=0.572, train_wall=257, gb_free=7.9, wall=126328
2022-05-19 20:56:07 | INFO | train_inner | epoch 027:   1200 / 1550 loss=4.578, ppl=23.89, wps=14639, ups=0.22, wpb=65536, bsz=128, num_updates=41500, lr=0.00015523, gnorm=0.578, train_wall=342, gb_free=7.9, wall=126776
2022-05-19 21:02:07 | INFO | train_inner | epoch 027:   1300 / 1550 loss=4.575, ppl=23.83, wps=18217.5, ups=0.28, wpb=65534.5, bsz=128, num_updates=41600, lr=0.000155043, gnorm=0.569, train_wall=278, gb_free=7.9, wall=127136
2022-05-19 21:07:47 | INFO | train_inner | epoch 027:   1400 / 1550 loss=4.584, ppl=23.98, wps=19244.1, ups=0.29, wpb=65536, bsz=128, num_updates=41700, lr=0.000154857, gnorm=0.573, train_wall=267, gb_free=7.9, wall=127476
2022-05-19 21:13:25 | INFO | train_inner | epoch 027:   1500 / 1550 loss=4.594, ppl=24.15, wps=19382.2, ups=0.3, wpb=65536, bsz=128, num_updates=41800, lr=0.000154672, gnorm=0.575, train_wall=264, gb_free=7.9, wall=127814
2022-05-19 21:16:08 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-05-19 21:17:25 | INFO | valid | epoch 027 | valid on 'valid' subset | loss 4.656 | ppl 25.21 | wps 72805.1 | wpb 2047.4 | bsz 4 | num_updates 41850 | best_loss 4.656
2022-05-19 21:17:25 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 27 @ 41850 updates
2022-05-19 21:17:25 | INFO | fairseq.trainer | Saving checkpoint to /cluster/work/cotterell/liam/master-thesis/checkpoints/language_model/bart/checkpoint_best_bart_32_5.0000E-04_full.pt
2022-05-19 21:17:27 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/work/cotterell/liam/master-thesis/checkpoints/language_model/bart/checkpoint_best_bart_32_5.0000E-04_full.pt
2022-05-19 21:17:27 | INFO | fairseq.checkpoint_utils | Saved checkpoint checkpoints/language_model/bart/checkpoint_best_bart_32_5.0000E-04_full.pt (epoch 27 @ 41850 updates, score 4.656) (writing took 1.5462393229827285 seconds)
2022-05-19 21:17:27 | INFO | fairseq_cli.train | end of epoch 27 (average epoch stats below)
2022-05-19 21:17:27 | INFO | train | epoch 027 | loss 4.571 | ppl 23.78 | wps 19267.7 | ups 0.29 | wpb 65522.7 | bsz 128 | num_updates 41850 | lr 0.00015458 | gnorm 0.573 | train_wall 4062 | gb_free 7.9 | wall 128056
2022-05-19 21:17:27 | INFO | fairseq.data.iterators | grouped total_num_itrs = 1550
2022-05-19 21:17:27 | INFO | fairseq.trainer | begin training epoch 28
2022-05-19 21:17:27 | INFO | fairseq_cli.train | Start iterating over samples
2022-05-19 21:19:51 | INFO | train_inner | epoch 028:     50 / 1550 loss=4.557, ppl=23.54, wps=16938.6, ups=0.26, wpb=65331.2, bsz=127.6, num_updates=41900, lr=0.000154487, gnorm=0.58, train_wall=244, gb_free=7.9, wall=128200
2022-05-19 21:25:10 | INFO | train_inner | epoch 028:    150 / 1550 loss=4.52, ppl=22.94, wps=20577.9, ups=0.31, wpb=65536, bsz=128, num_updates=42000, lr=0.000154303, gnorm=0.572, train_wall=252, gb_free=7.9, wall=128518
2022-05-19 21:30:37 | INFO | train_inner | epoch 028:    250 / 1550 loss=4.545, ppl=23.35, wps=20014.7, ups=0.31, wpb=65536, bsz=128, num_updates=42100, lr=0.00015412, gnorm=0.573, train_wall=250, gb_free=7.9, wall=128846
2022-05-19 21:36:14 | INFO | train_inner | epoch 028:    350 / 1550 loss=4.562, ppl=23.62, wps=19423.6, ups=0.3, wpb=65536, bsz=128, num_updates=42200, lr=0.000153937, gnorm=0.581, train_wall=255, gb_free=7.9, wall=129183
2022-05-19 21:41:31 | INFO | train_inner | epoch 028:    450 / 1550 loss=4.551, ppl=23.44, wps=20712.1, ups=0.32, wpb=65536, bsz=128, num_updates=42300, lr=0.000153755, gnorm=0.572, train_wall=254, gb_free=7.9, wall=129500
2022-05-19 21:47:05 | INFO | train_inner | epoch 028:    550 / 1550 loss=4.554, ppl=23.49, wps=19626.7, ups=0.3, wpb=65536, bsz=128, num_updates=42400, lr=0.000153574, gnorm=0.574, train_wall=261, gb_free=7.9, wall=129834
2022-05-19 21:52:28 | INFO | train_inner | epoch 028:    650 / 1550 loss=4.565, ppl=23.68, wps=20275.3, ups=0.31, wpb=65536, bsz=128, num_updates=42500, lr=0.000153393, gnorm=0.577, train_wall=257, gb_free=7.9, wall=130157
2022-05-19 21:59:03 | INFO | train_inner | epoch 028:    750 / 1550 loss=4.565, ppl=23.66, wps=16582.1, ups=0.25, wpb=65534.5, bsz=128, num_updates=42600, lr=0.000153213, gnorm=0.572, train_wall=303, gb_free=7.9, wall=130552
2022-05-19 22:05:51 | INFO | train_inner | epoch 028:    850 / 1550 loss=4.559, ppl=23.57, wps=16070.3, ups=0.25, wpb=65536, bsz=128, num_updates=42700, lr=0.000153033, gnorm=0.57, train_wall=310, gb_free=7.9, wall=130960
2022-05-19 22:11:20 | INFO | train_inner | epoch 028:    950 / 1550 loss=4.584, ppl=23.98, wps=19925.4, ups=0.3, wpb=65536, bsz=128, num_updates=42800, lr=0.000152854, gnorm=0.568, train_wall=251, gb_free=7.9, wall=131289
2022-05-19 22:16:30 | INFO | train_inner | epoch 028:   1050 / 1550 loss=4.573, ppl=23.79, wps=21113.9, ups=0.32, wpb=65536, bsz=128, num_updates=42900, lr=0.000152676, gnorm=0.575, train_wall=239, gb_free=7.9, wall=131599
2022-05-19 22:21:44 | INFO | train_inner | epoch 028:   1150 / 1550 loss=4.575, ppl=23.83, wps=20880.1, ups=0.32, wpb=65536, bsz=128, num_updates=43000, lr=0.000152499, gnorm=0.565, train_wall=250, gb_free=7.9, wall=131913
2022-05-19 22:26:59 | INFO | train_inner | epoch 028:   1250 / 1550 loss=4.574, ppl=23.81, wps=20784.7, ups=0.32, wpb=65536, bsz=128, num_updates=43100, lr=0.000152322, gnorm=0.572, train_wall=248, gb_free=7.9, wall=132228
2022-05-19 22:32:34 | INFO | train_inner | epoch 028:   1350 / 1550 loss=4.578, ppl=23.88, wps=19572.3, ups=0.3, wpb=65536, bsz=128, num_updates=43200, lr=0.000152145, gnorm=0.575, train_wall=257, gb_free=7.9, wall=132563
2022-05-19 22:37:57 | INFO | train_inner | epoch 028:   1450 / 1550 loss=4.585, ppl=24, wps=20327.7, ups=0.31, wpb=65536, bsz=128, num_updates=43300, lr=0.000151969, gnorm=0.573, train_wall=254, gb_free=7.9, wall=132886
2022-05-19 22:43:15 | INFO | train_inner | epoch 028:   1550 / 1550 loss=4.585, ppl=24, wps=20558.6, ups=0.31, wpb=65331.2, bsz=127.6, num_updates=43400, lr=0.000151794, gnorm=0.57, train_wall=252, gb_free=7.9, wall=133203
2022-05-19 22:43:15 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-05-19 22:44:36 | INFO | valid | epoch 028 | valid on 'valid' subset | loss 4.651 | ppl 25.12 | wps 69667 | wpb 2047.4 | bsz 4 | num_updates 43400 | best_loss 4.651
2022-05-19 22:44:36 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 28 @ 43400 updates
2022-05-19 22:44:36 | INFO | fairseq.trainer | Saving checkpoint to /cluster/work/cotterell/liam/master-thesis/checkpoints/language_model/bart/checkpoint_best_bart_32_5.0000E-04_full.pt
2022-05-19 22:44:37 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/work/cotterell/liam/master-thesis/checkpoints/language_model/bart/checkpoint_best_bart_32_5.0000E-04_full.pt
2022-05-19 22:44:37 | INFO | fairseq.checkpoint_utils | Saved checkpoint checkpoints/language_model/bart/checkpoint_best_bart_32_5.0000E-04_full.pt (epoch 28 @ 43400 updates, score 4.651) (writing took 1.5637148120440543 seconds)
2022-05-19 22:44:37 | INFO | fairseq_cli.train | end of epoch 28 (average epoch stats below)
2022-05-19 22:44:37 | INFO | train | epoch 028 | loss 4.564 | ppl 23.65 | wps 19417.4 | ups 0.3 | wpb 65522.7 | bsz 128 | num_updates 43400 | lr 0.000151794 | gnorm 0.573 | train_wall 4005 | gb_free 7.9 | wall 133286
2022-05-19 22:44:37 | INFO | fairseq.data.iterators | grouped total_num_itrs = 1550
2022-05-19 22:44:37 | INFO | fairseq.trainer | begin training epoch 29
2022-05-19 22:44:37 | INFO | fairseq_cli.train | Start iterating over samples
2022-05-19 22:49:35 | INFO | train_inner | epoch 029:    100 / 1550 loss=4.533, ppl=23.15, wps=17213.9, ups=0.26, wpb=65536, bsz=128, num_updates=43500, lr=0.00015162, gnorm=0.582, train_wall=236, gb_free=7.9, wall=133584
2022-05-19 22:54:32 | INFO | train_inner | epoch 029:    200 / 1550 loss=4.537, ppl=23.21, wps=22051.8, ups=0.34, wpb=65536, bsz=128, num_updates=43600, lr=0.000151446, gnorm=0.575, train_wall=240, gb_free=7.9, wall=133881
2022-05-19 23:01:13 | INFO | train_inner | epoch 029:    300 / 1550 loss=4.534, ppl=23.16, wps=16355.2, ups=0.25, wpb=65536, bsz=128, num_updates=43700, lr=0.000151272, gnorm=0.58, train_wall=288, gb_free=7.9, wall=134282
2022-05-19 23:06:53 | INFO | train_inner | epoch 029:    400 / 1550 loss=4.54, ppl=23.26, wps=19254.8, ups=0.29, wpb=65536, bsz=128, num_updates=43800, lr=0.000151099, gnorm=0.569, train_wall=263, gb_free=7.9, wall=134622
2022-05-19 23:12:39 | INFO | train_inner | epoch 029:    500 / 1550 loss=4.554, ppl=23.49, wps=18953.1, ups=0.29, wpb=65536, bsz=128, num_updates=43900, lr=0.000150927, gnorm=0.57, train_wall=264, gb_free=7.9, wall=134968
2022-05-19 23:18:03 | INFO | train_inner | epoch 029:    600 / 1550 loss=4.544, ppl=23.32, wps=20262.8, ups=0.31, wpb=65536, bsz=128, num_updates=44000, lr=0.000150756, gnorm=0.572, train_wall=256, gb_free=7.9, wall=135292
2022-05-19 23:23:16 | INFO | train_inner | epoch 029:    700 / 1550 loss=4.561, ppl=23.61, wps=20885.8, ups=0.32, wpb=65536, bsz=128, num_updates=44100, lr=0.000150585, gnorm=0.576, train_wall=241, gb_free=7.9, wall=135605
2022-05-19 23:28:40 | INFO | train_inner | epoch 029:    800 / 1550 loss=4.548, ppl=23.39, wps=20280, ups=0.31, wpb=65536, bsz=128, num_updates=44200, lr=0.000150414, gnorm=0.576, train_wall=253, gb_free=7.9, wall=135928
2022-05-19 23:33:23 | INFO | train_inner | epoch 029:    900 / 1550 loss=4.563, ppl=23.63, wps=23096.4, ups=0.35, wpb=65536, bsz=128, num_updates=44300, lr=0.000150244, gnorm=0.577, train_wall=230, gb_free=7.9, wall=136212
2022-05-19 23:38:39 | INFO | train_inner | epoch 029:   1000 / 1550 loss=4.563, ppl=23.63, wps=20750.5, ups=0.32, wpb=65536, bsz=128, num_updates=44400, lr=0.000150075, gnorm=0.577, train_wall=253, gb_free=7.9, wall=136528
2022-05-19 23:43:28 | INFO | train_inner | epoch 029:   1100 / 1550 loss=4.565, ppl=23.67, wps=22663, ups=0.35, wpb=65534.5, bsz=128, num_updates=44500, lr=0.000149906, gnorm=0.572, train_wall=233, gb_free=7.9, wall=136817
2022-05-19 23:48:37 | INFO | train_inner | epoch 029:   1200 / 1550 loss=4.577, ppl=23.86, wps=21263.2, ups=0.32, wpb=65536, bsz=128, num_updates=44600, lr=0.000149738, gnorm=0.575, train_wall=235, gb_free=7.9, wall=137125
2022-05-19 23:54:02 | INFO | train_inner | epoch 029:   1300 / 1550 loss=4.577, ppl=23.87, wps=20146.6, ups=0.31, wpb=65536, bsz=128, num_updates=44700, lr=0.000149571, gnorm=0.579, train_wall=257, gb_free=7.9, wall=137451
2022-05-20 00:00:07 | INFO | train_inner | epoch 029:   1400 / 1550 loss=4.578, ppl=23.89, wps=17943.3, ups=0.27, wpb=65536, bsz=128, num_updates=44800, lr=0.000149404, gnorm=0.576, train_wall=282, gb_free=7.9, wall=137816
2022-05-20 00:07:40 | INFO | train_inner | epoch 029:   1500 / 1550 loss=4.571, ppl=23.76, wps=14456.3, ups=0.22, wpb=65536, bsz=128, num_updates=44900, lr=0.000149237, gnorm=0.574, train_wall=367, gb_free=7.9, wall=138269
2022-05-20 00:11:05 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-05-20 00:12:35 | INFO | valid | epoch 029 | valid on 'valid' subset | loss 4.647 | ppl 25.05 | wps 62701.7 | wpb 2047.4 | bsz 4 | num_updates 44950 | best_loss 4.647
2022-05-20 00:12:35 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 29 @ 44950 updates
2022-05-20 00:12:35 | INFO | fairseq.trainer | Saving checkpoint to /cluster/work/cotterell/liam/master-thesis/checkpoints/language_model/bart/checkpoint_best_bart_32_5.0000E-04_full.pt
2022-05-20 00:12:37 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/work/cotterell/liam/master-thesis/checkpoints/language_model/bart/checkpoint_best_bart_32_5.0000E-04_full.pt
2022-05-20 00:12:37 | INFO | fairseq.checkpoint_utils | Saved checkpoint checkpoints/language_model/bart/checkpoint_best_bart_32_5.0000E-04_full.pt (epoch 29 @ 44950 updates, score 4.647) (writing took 1.6889928258024156 seconds)
2022-05-20 00:12:37 | INFO | fairseq_cli.train | end of epoch 29 (average epoch stats below)
2022-05-20 00:12:37 | INFO | train | epoch 029 | loss 4.557 | ppl 23.54 | wps 19236.1 | ups 0.29 | wpb 65522.7 | bsz 128 | num_updates 44950 | lr 0.000149154 | gnorm 0.575 | train_wall 4059 | gb_free 7.9 | wall 138566
2022-05-20 00:12:37 | INFO | fairseq.data.iterators | grouped total_num_itrs = 1550
2022-05-20 00:12:37 | INFO | fairseq.trainer | begin training epoch 30
2022-05-20 00:12:37 | INFO | fairseq_cli.train | Start iterating over samples
2022-05-20 00:16:09 | INFO | train_inner | epoch 030:     50 / 1550 loss=4.552, ppl=23.46, wps=12848.6, ups=0.2, wpb=65331.2, bsz=127.6, num_updates=45000, lr=0.000149071, gnorm=0.572, train_wall=325, gb_free=7.9, wall=138778
2022-05-20 00:24:03 | INFO | train_inner | epoch 030:    150 / 1550 loss=4.515, ppl=22.86, wps=13830.1, ups=0.21, wpb=65536, bsz=128, num_updates=45100, lr=0.000148906, gnorm=0.573, train_wall=365, gb_free=7.9, wall=139252
2022-05-20 00:30:15 | INFO | train_inner | epoch 030:    250 / 1550 loss=4.536, ppl=23.2, wps=17594.8, ups=0.27, wpb=65534.5, bsz=128, num_updates=45200, lr=0.000148741, gnorm=0.573, train_wall=283, gb_free=7.9, wall=139624
2022-05-20 00:36:25 | INFO | train_inner | epoch 030:    350 / 1550 loss=4.534, ppl=23.17, wps=17715.7, ups=0.27, wpb=65536, bsz=128, num_updates=45300, lr=0.000148577, gnorm=0.581, train_wall=290, gb_free=7.9, wall=139994
2022-05-20 00:42:13 | INFO | train_inner | epoch 030:    450 / 1550 loss=4.547, ppl=23.38, wps=18866, ups=0.29, wpb=65536, bsz=128, num_updates=45400, lr=0.000148413, gnorm=0.58, train_wall=272, gb_free=7.9, wall=140341
2022-05-20 00:47:47 | INFO | train_inner | epoch 030:    550 / 1550 loss=4.552, ppl=23.46, wps=19597.4, ups=0.3, wpb=65536, bsz=128, num_updates=45500, lr=0.00014825, gnorm=0.576, train_wall=258, gb_free=7.9, wall=140676
2022-05-20 00:53:21 | INFO | train_inner | epoch 030:    650 / 1550 loss=4.536, ppl=23.2, wps=19608, ups=0.3, wpb=65536, bsz=128, num_updates=45600, lr=0.000148087, gnorm=0.585, train_wall=267, gb_free=7.9, wall=141010
2022-05-20 01:00:06 | INFO | train_inner | epoch 030:    750 / 1550 loss=4.548, ppl=23.4, wps=16193.8, ups=0.25, wpb=65536, bsz=128, num_updates=45700, lr=0.000147925, gnorm=0.583, train_wall=317, gb_free=7.9, wall=141415
2022-05-20 01:10:36 | INFO | train_inner | epoch 030:    850 / 1550 loss=4.566, ppl=23.69, wps=10394.1, ups=0.16, wpb=65536, bsz=128, num_updates=45800, lr=0.000147764, gnorm=0.576, train_wall=513, gb_free=7.9, wall=142045
2022-05-20 01:18:12 | INFO | train_inner | epoch 030:    950 / 1550 loss=4.563, ppl=23.64, wps=14389.4, ups=0.22, wpb=65536, bsz=128, num_updates=45900, lr=0.000147602, gnorm=0.577, train_wall=350, gb_free=7.9, wall=142501
2022-05-20 01:23:49 | INFO | train_inner | epoch 030:   1050 / 1550 loss=4.561, ppl=23.6, wps=19446.4, ups=0.3, wpb=65536, bsz=128, num_updates=46000, lr=0.000147442, gnorm=0.58, train_wall=251, gb_free=7.9, wall=142838
2022-05-20 01:29:21 | INFO | train_inner | epoch 030:   1150 / 1550 loss=4.574, ppl=23.82, wps=19710.6, ups=0.3, wpb=65536, bsz=128, num_updates=46100, lr=0.000147282, gnorm=0.578, train_wall=257, gb_free=7.9, wall=143170
2022-05-20 01:34:33 | INFO | train_inner | epoch 030:   1250 / 1550 loss=4.559, ppl=23.58, wps=21061.5, ups=0.32, wpb=65536, bsz=128, num_updates=46200, lr=0.000147122, gnorm=0.576, train_wall=246, gb_free=7.9, wall=143481
2022-05-20 01:39:31 | INFO | train_inner | epoch 030:   1350 / 1550 loss=4.546, ppl=23.36, wps=21994.8, ups=0.34, wpb=65536, bsz=128, num_updates=46300, lr=0.000146964, gnorm=0.573, train_wall=240, gb_free=7.9, wall=143779
2022-05-20 01:44:56 | INFO | train_inner | epoch 030:   1450 / 1550 loss=4.575, ppl=23.83, wps=20133.4, ups=0.31, wpb=65536, bsz=128, num_updates=46400, lr=0.000146805, gnorm=0.578, train_wall=252, gb_free=7.9, wall=144105
2022-05-20 01:50:41 | INFO | train_inner | epoch 030:   1550 / 1550 loss=4.554, ppl=23.5, wps=18948.4, ups=0.29, wpb=65331.2, bsz=127.6, num_updates=46500, lr=0.000146647, gnorm=0.581, train_wall=268, gb_free=7.9, wall=144450
2022-05-20 01:50:41 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-05-20 01:52:04 | INFO | valid | epoch 030 | valid on 'valid' subset | loss 4.642 | ppl 24.96 | wps 68221.4 | wpb 2047.4 | bsz 4 | num_updates 46500 | best_loss 4.642
2022-05-20 01:52:04 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 30 @ 46500 updates
2022-05-20 01:52:04 | INFO | fairseq.trainer | Saving checkpoint to /cluster/work/cotterell/liam/master-thesis/checkpoints/language_model/bart/checkpoint_best_bart_32_5.0000E-04_full.pt
2022-05-20 01:52:05 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/work/cotterell/liam/master-thesis/checkpoints/language_model/bart/checkpoint_best_bart_32_5.0000E-04_full.pt
2022-05-20 01:52:05 | INFO | fairseq.checkpoint_utils | Saved checkpoint checkpoints/language_model/bart/checkpoint_best_bart_32_5.0000E-04_full.pt (epoch 30 @ 46500 updates, score 4.642) (writing took 1.6170182968489826 seconds)
2022-05-20 01:52:05 | INFO | fairseq_cli.train | end of epoch 30 (average epoch stats below)
2022-05-20 01:52:05 | INFO | train | epoch 030 | loss 4.55 | ppl 23.43 | wps 17016.5 | ups 0.26 | wpb 65522.7 | bsz 128 | num_updates 46500 | lr 0.000146647 | gnorm 0.578 | train_wall 4589 | gb_free 7.9 | wall 144534
2022-05-20 01:52:06 | INFO | fairseq.data.iterators | grouped total_num_itrs = 1550
2022-05-20 01:52:06 | INFO | fairseq.trainer | begin training epoch 31
2022-05-20 01:52:06 | INFO | fairseq_cli.train | Start iterating over samples
2022-05-20 01:57:18 | INFO | train_inner | epoch 031:    100 / 1550 loss=4.519, ppl=22.92, wps=16512.9, ups=0.25, wpb=65536, bsz=128, num_updates=46600, lr=0.00014649, gnorm=0.576, train_wall=237, gb_free=7.9, wall=144847
2022-05-20 02:02:59 | INFO | train_inner | epoch 031:    200 / 1550 loss=4.521, ppl=22.95, wps=19188.9, ups=0.29, wpb=65536, bsz=128, num_updates=46700, lr=0.000146333, gnorm=0.579, train_wall=264, gb_free=7.9, wall=145188
2022-05-20 02:12:29 | INFO | train_inner | epoch 031:    300 / 1550 loss=4.528, ppl=23.07, wps=11494.4, ups=0.18, wpb=65536, bsz=128, num_updates=46800, lr=0.000146176, gnorm=0.576, train_wall=451, gb_free=7.9, wall=145758
2022-05-20 02:20:25 | INFO | train_inner | epoch 031:    400 / 1550 loss=4.54, ppl=23.26, wps=13774.5, ups=0.21, wpb=65536, bsz=128, num_updates=46900, lr=0.00014602, gnorm=0.579, train_wall=367, gb_free=7.9, wall=146234
2022-05-20 02:26:46 | INFO | train_inner | epoch 031:    500 / 1550 loss=4.536, ppl=23.19, wps=17190.3, ups=0.26, wpb=65536, bsz=128, num_updates=47000, lr=0.000145865, gnorm=0.579, train_wall=289, gb_free=7.9, wall=146615
2022-05-20 02:32:16 | INFO | train_inner | epoch 031:    600 / 1550 loss=4.545, ppl=23.35, wps=19890, ups=0.3, wpb=65536, bsz=128, num_updates=47100, lr=0.00014571, gnorm=0.584, train_wall=254, gb_free=7.9, wall=146945
2022-05-20 02:37:18 | INFO | train_inner | epoch 031:    700 / 1550 loss=4.538, ppl=23.23, wps=21700.6, ups=0.33, wpb=65536, bsz=128, num_updates=47200, lr=0.000145556, gnorm=0.575, train_wall=242, gb_free=7.9, wall=147247
2022-05-20 02:42:28 | INFO | train_inner | epoch 031:    800 / 1550 loss=4.549, ppl=23.4, wps=21131.5, ups=0.32, wpb=65536, bsz=128, num_updates=47300, lr=0.000145402, gnorm=0.576, train_wall=243, gb_free=7.9, wall=147557
2022-05-20 02:47:46 | INFO | train_inner | epoch 031:    900 / 1550 loss=4.555, ppl=23.51, wps=20601.2, ups=0.31, wpb=65536, bsz=128, num_updates=47400, lr=0.000145248, gnorm=0.572, train_wall=247, gb_free=7.9, wall=147875
2022-05-20 02:53:01 | INFO | train_inner | epoch 031:   1000 / 1550 loss=4.556, ppl=23.52, wps=20840.9, ups=0.32, wpb=65536, bsz=128, num_updates=47500, lr=0.000145095, gnorm=0.58, train_wall=248, gb_free=7.9, wall=148189
2022-05-20 02:57:57 | INFO | train_inner | epoch 031:   1100 / 1550 loss=4.548, ppl=23.4, wps=22144.9, ups=0.34, wpb=65536, bsz=128, num_updates=47600, lr=0.000144943, gnorm=0.578, train_wall=237, gb_free=7.9, wall=148485
2022-05-20 03:03:12 | INFO | train_inner | epoch 031:   1200 / 1550 loss=4.545, ppl=23.35, wps=20757.9, ups=0.32, wpb=65536, bsz=128, num_updates=47700, lr=0.000144791, gnorm=0.577, train_wall=239, gb_free=7.9, wall=148801
2022-05-20 03:08:52 | INFO | train_inner | epoch 031:   1300 / 1550 loss=4.555, ppl=23.51, wps=19315.3, ups=0.29, wpb=65536, bsz=128, num_updates=47800, lr=0.000144639, gnorm=0.583, train_wall=261, gb_free=7.9, wall=149140
2022-05-20 03:16:57 | INFO | train_inner | epoch 031:   1400 / 1550 loss=4.56, ppl=23.59, wps=13508.7, ups=0.21, wpb=65536, bsz=128, num_updates=47900, lr=0.000144488, gnorm=0.583, train_wall=365, gb_free=7.9, wall=149626
2022-05-20 03:22:18 | INFO | train_inner | epoch 031:   1500 / 1550 loss=4.56, ppl=23.58, wps=20393.5, ups=0.31, wpb=65536, bsz=128, num_updates=48000, lr=0.000144338, gnorm=0.574, train_wall=248, gb_free=7.9, wall=149947
2022-05-20 03:25:12 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-05-20 03:26:38 | INFO | valid | epoch 031 | valid on 'valid' subset | loss 4.638 | ppl 24.9 | wps 65677.8 | wpb 2047.4 | bsz 4 | num_updates 48050 | best_loss 4.638
2022-05-20 03:26:38 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 31 @ 48050 updates
2022-05-20 03:26:38 | INFO | fairseq.trainer | Saving checkpoint to /cluster/work/cotterell/liam/master-thesis/checkpoints/language_model/bart/checkpoint_best_bart_32_5.0000E-04_full.pt
2022-05-20 03:26:40 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/work/cotterell/liam/master-thesis/checkpoints/language_model/bart/checkpoint_best_bart_32_5.0000E-04_full.pt
2022-05-20 03:26:40 | INFO | fairseq.checkpoint_utils | Saved checkpoint checkpoints/language_model/bart/checkpoint_best_bart_32_5.0000E-04_full.pt (epoch 31 @ 48050 updates, score 4.638) (writing took 1.6015480388887227 seconds)
2022-05-20 03:26:40 | INFO | fairseq_cli.train | end of epoch 31 (average epoch stats below)
2022-05-20 03:26:40 | INFO | train | epoch 031 | loss 4.544 | ppl 23.33 | wps 17898.3 | ups 0.27 | wpb 65522.7 | bsz 128 | num_updates 48050 | lr 0.000144262 | gnorm 0.578 | train_wall 4328 | gb_free 7.9 | wall 150208
2022-05-20 03:26:40 | INFO | fairseq.data.iterators | grouped total_num_itrs = 1550
2022-05-20 03:26:40 | INFO | fairseq.trainer | begin training epoch 32
2022-05-20 03:26:40 | INFO | fairseq_cli.train | Start iterating over samples
2022-05-20 03:29:28 | INFO | train_inner | epoch 032:     50 / 1550 loss=4.539, ppl=23.25, wps=15188.2, ups=0.23, wpb=65329.7, bsz=127.6, num_updates=48100, lr=0.000144187, gnorm=0.58, train_wall=262, gb_free=7.9, wall=150377
2022-05-20 03:35:41 | INFO | train_inner | epoch 032:    150 / 1550 loss=4.512, ppl=22.82, wps=17580.6, ups=0.27, wpb=65536, bsz=128, num_updates=48200, lr=0.000144038, gnorm=0.579, train_wall=290, gb_free=7.9, wall=150750
2022-05-20 03:40:59 | INFO | train_inner | epoch 032:    250 / 1550 loss=4.53, ppl=23.11, wps=20585.7, ups=0.31, wpb=65536, bsz=128, num_updates=48300, lr=0.000143889, gnorm=0.579, train_wall=253, gb_free=7.9, wall=151068
2022-05-20 03:46:25 | INFO | train_inner | epoch 032:    350 / 1550 loss=4.531, ppl=23.13, wps=20118.9, ups=0.31, wpb=65536, bsz=128, num_updates=48400, lr=0.00014374, gnorm=0.578, train_wall=248, gb_free=7.9, wall=151394
2022-05-20 03:52:00 | INFO | train_inner | epoch 032:    450 / 1550 loss=4.528, ppl=23.07, wps=19568.7, ups=0.3, wpb=65536, bsz=128, num_updates=48500, lr=0.000143592, gnorm=0.587, train_wall=260, gb_free=7.9, wall=151729
2022-05-20 03:57:17 | INFO | train_inner | epoch 032:    550 / 1550 loss=4.55, ppl=23.43, wps=20668.4, ups=0.32, wpb=65536, bsz=128, num_updates=48600, lr=0.000143444, gnorm=0.578, train_wall=252, gb_free=7.9, wall=152046
2022-05-20 04:02:11 | INFO | train_inner | epoch 032:    650 / 1550 loss=4.529, ppl=23.08, wps=22272.3, ups=0.34, wpb=65536, bsz=128, num_updates=48700, lr=0.000143296, gnorm=0.583, train_wall=236, gb_free=7.9, wall=152340
2022-05-20 04:07:17 | INFO | train_inner | epoch 032:    750 / 1550 loss=4.539, ppl=23.24, wps=21471.8, ups=0.33, wpb=65536, bsz=128, num_updates=48800, lr=0.00014315, gnorm=0.577, train_wall=240, gb_free=7.9, wall=152645
2022-05-20 04:15:24 | INFO | train_inner | epoch 032:    850 / 1550 loss=4.529, ppl=23.09, wps=13434.8, ups=0.2, wpb=65536, bsz=128, num_updates=48900, lr=0.000143003, gnorm=0.583, train_wall=382, gb_free=7.9, wall=153133
2022-05-20 04:20:39 | INFO | train_inner | epoch 032:    950 / 1550 loss=4.537, ppl=23.22, wps=20819.7, ups=0.32, wpb=65536, bsz=128, num_updates=49000, lr=0.000142857, gnorm=0.576, train_wall=238, gb_free=7.9, wall=153448
2022-05-20 04:25:12 | INFO | train_inner | epoch 032:   1050 / 1550 loss=4.55, ppl=23.42, wps=23976.3, ups=0.37, wpb=65536, bsz=128, num_updates=49100, lr=0.000142712, gnorm=0.582, train_wall=226, gb_free=7.9, wall=153721
2022-05-20 04:29:36 | INFO | train_inner | epoch 032:   1150 / 1550 loss=4.546, ppl=23.36, wps=24901.8, ups=0.38, wpb=65536, bsz=128, num_updates=49200, lr=0.000142566, gnorm=0.579, train_wall=227, gb_free=7.9, wall=153984
2022-05-20 04:33:47 | INFO | train_inner | epoch 032:   1250 / 1550 loss=4.555, ppl=23.51, wps=26056.7, ups=0.4, wpb=65536, bsz=128, num_updates=49300, lr=0.000142422, gnorm=0.578, train_wall=225, gb_free=7.9, wall=154236
2022-05-20 04:37:56 | INFO | train_inner | epoch 032:   1350 / 1550 loss=4.553, ppl=23.48, wps=26300.9, ups=0.4, wpb=65536, bsz=128, num_updates=49400, lr=0.000142278, gnorm=0.581, train_wall=229, gb_free=7.9, wall=154485
2022-05-20 04:44:13 | INFO | train_inner | epoch 032:   1450 / 1550 loss=4.54, ppl=23.27, wps=17399.1, ups=0.27, wpb=65536, bsz=128, num_updates=49500, lr=0.000142134, gnorm=0.577, train_wall=297, gb_free=7.9, wall=154862
2022-05-20 04:48:38 | INFO | train_inner | epoch 032:   1550 / 1550 loss=4.553, ppl=23.48, wps=24610.7, ups=0.38, wpb=65329.7, bsz=127.6, num_updates=49600, lr=0.00014199, gnorm=0.589, train_wall=225, gb_free=7.9, wall=155127
2022-05-20 04:48:38 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-05-20 04:49:55 | INFO | valid | epoch 032 | valid on 'valid' subset | loss 4.637 | ppl 24.89 | wps 74047.4 | wpb 2047.4 | bsz 4 | num_updates 49600 | best_loss 4.637
2022-05-20 04:49:55 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 32 @ 49600 updates
2022-05-20 04:49:55 | INFO | fairseq.trainer | Saving checkpoint to /cluster/work/cotterell/liam/master-thesis/checkpoints/language_model/bart/checkpoint_best_bart_32_5.0000E-04_full.pt
2022-05-20 04:49:56 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/work/cotterell/liam/master-thesis/checkpoints/language_model/bart/checkpoint_best_bart_32_5.0000E-04_full.pt
2022-05-20 04:49:56 | INFO | fairseq.checkpoint_utils | Saved checkpoint checkpoints/language_model/bart/checkpoint_best_bart_32_5.0000E-04_full.pt (epoch 32 @ 49600 updates, score 4.637) (writing took 1.5220880350098014 seconds)
2022-05-20 04:49:56 | INFO | fairseq_cli.train | end of epoch 32 (average epoch stats below)
2022-05-20 04:49:56 | INFO | train | epoch 032 | loss 4.538 | ppl 23.24 | wps 20326.3 | ups 0.31 | wpb 65522.7 | bsz 128 | num_updates 49600 | lr 0.00014199 | gnorm 0.58 | train_wall 3954 | gb_free 7.9 | wall 155205
2022-05-20 04:49:56 | INFO | fairseq.data.iterators | grouped total_num_itrs = 1550
2022-05-20 04:49:56 | INFO | fairseq.trainer | begin training epoch 33
2022-05-20 04:49:56 | INFO | fairseq_cli.train | Start iterating over samples
2022-05-20 04:54:09 | INFO | train_inner | epoch 033:    100 / 1550 loss=4.499, ppl=22.6, wps=19814.5, ups=0.3, wpb=65536, bsz=128, num_updates=49700, lr=0.000141848, gnorm=0.579, train_wall=226, gb_free=7.9, wall=155458
2022-05-20 04:58:16 | INFO | train_inner | epoch 033:    200 / 1550 loss=4.525, ppl=23.02, wps=26598.8, ups=0.41, wpb=65536, bsz=128, num_updates=49800, lr=0.000141705, gnorm=0.583, train_wall=226, gb_free=7.9, wall=155704
2022-05-20 05:02:30 | INFO | train_inner | epoch 033:    300 / 1550 loss=4.51, ppl=22.79, wps=25793.4, ups=0.39, wpb=65536, bsz=128, num_updates=49900, lr=0.000141563, gnorm=0.587, train_wall=229, gb_free=7.9, wall=155959
2022-05-20 05:06:30 | INFO | train_inner | epoch 033:    400 / 1550 loss=4.527, ppl=23.06, wps=27254.7, ups=0.42, wpb=65536, bsz=128, num_updates=50000, lr=0.000141421, gnorm=0.59, train_wall=226, gb_free=7.9, wall=156199
2022-05-20 05:10:38 | INFO | train_inner | epoch 033:    500 / 1550 loss=4.528, ppl=23.07, wps=26411.5, ups=0.4, wpb=65536, bsz=128, num_updates=50100, lr=0.00014128, gnorm=0.583, train_wall=230, gb_free=7.9, wall=156447
2022-05-20 05:14:36 | INFO | train_inner | epoch 033:    600 / 1550 loss=4.532, ppl=23.14, wps=27521.6, ups=0.42, wpb=65536, bsz=128, num_updates=50200, lr=0.000141139, gnorm=0.584, train_wall=226, gb_free=7.9, wall=156685
2022-05-20 05:18:33 | INFO | train_inner | epoch 033:    700 / 1550 loss=4.533, ppl=23.15, wps=27747.7, ups=0.42, wpb=65536, bsz=128, num_updates=50300, lr=0.000140999, gnorm=0.587, train_wall=226, gb_free=7.9, wall=156921
2022-05-20 05:22:40 | INFO | train_inner | epoch 033:    800 / 1550 loss=4.533, ppl=23.15, wps=26486.9, ups=0.4, wpb=65536, bsz=128, num_updates=50400, lr=0.000140859, gnorm=0.585, train_wall=232, gb_free=7.9, wall=157169
2022-05-20 05:26:34 | INFO | train_inner | epoch 033:    900 / 1550 loss=4.535, ppl=23.19, wps=28004.5, ups=0.43, wpb=65536, bsz=128, num_updates=50500, lr=0.00014072, gnorm=0.586, train_wall=226, gb_free=7.9, wall=157403
2022-05-20 05:30:33 | INFO | train_inner | epoch 033:   1000 / 1550 loss=4.528, ppl=23.07, wps=27371.8, ups=0.42, wpb=65536, bsz=128, num_updates=50600, lr=0.00014058, gnorm=0.578, train_wall=229, gb_free=7.9, wall=157642
2022-05-20 05:34:35 | INFO | train_inner | epoch 033:   1100 / 1550 loss=4.543, ppl=23.3, wps=27142.3, ups=0.41, wpb=65534.5, bsz=128, num_updates=50700, lr=0.000140442, gnorm=0.578, train_wall=228, gb_free=7.9, wall=157884
2022-05-20 05:38:29 | INFO | train_inner | epoch 033:   1200 / 1550 loss=4.542, ppl=23.3, wps=28010.2, ups=0.43, wpb=65536, bsz=128, num_updates=50800, lr=0.000140303, gnorm=0.582, train_wall=226, gb_free=7.9, wall=158118
2022-05-20 05:42:22 | INFO | train_inner | epoch 033:   1300 / 1550 loss=4.547, ppl=23.38, wps=28067.9, ups=0.43, wpb=65536, bsz=128, num_updates=50900, lr=0.000140165, gnorm=0.579, train_wall=226, gb_free=7.9, wall=158351
2022-05-20 05:47:15 | INFO | train_inner | epoch 033:   1400 / 1550 loss=4.544, ppl=23.33, wps=22390.7, ups=0.34, wpb=65536, bsz=128, num_updates=51000, lr=0.000140028, gnorm=0.583, train_wall=230, gb_free=7.9, wall=158644
2022-05-20 05:51:43 | INFO | train_inner | epoch 033:   1500 / 1550 loss=4.557, ppl=23.54, wps=24433.7, ups=0.37, wpb=65536, bsz=128, num_updates=51100, lr=0.000139891, gnorm=0.588, train_wall=225, gb_free=7.9, wall=158912
2022-05-20 05:53:56 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-05-20 05:55:12 | INFO | valid | epoch 033 | valid on 'valid' subset | loss 4.632 | ppl 24.79 | wps 74065.8 | wpb 2047.4 | bsz 4 | num_updates 51150 | best_loss 4.632
2022-05-20 05:55:12 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 33 @ 51150 updates
2022-05-20 05:55:12 | INFO | fairseq.trainer | Saving checkpoint to /cluster/work/cotterell/liam/master-thesis/checkpoints/language_model/bart/checkpoint_best_bart_32_5.0000E-04_full.pt
2022-05-20 05:55:14 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/work/cotterell/liam/master-thesis/checkpoints/language_model/bart/checkpoint_best_bart_32_5.0000E-04_full.pt
2022-05-20 05:55:14 | INFO | fairseq.checkpoint_utils | Saved checkpoint checkpoints/language_model/bart/checkpoint_best_bart_32_5.0000E-04_full.pt (epoch 33 @ 51150 updates, score 4.632) (writing took 1.6038896939717233 seconds)
2022-05-20 05:55:14 | INFO | fairseq_cli.train | end of epoch 33 (average epoch stats below)
2022-05-20 05:55:14 | INFO | train | epoch 033 | loss 4.532 | ppl 23.14 | wps 25922.4 | ups 0.4 | wpb 65522.7 | bsz 128 | num_updates 51150 | lr 0.000139823 | gnorm 0.583 | train_wall 3525 | gb_free 7.9 | wall 159123
2022-05-20 05:55:14 | INFO | fairseq.data.iterators | grouped total_num_itrs = 1550
2022-05-20 05:55:14 | INFO | fairseq.trainer | begin training epoch 34
2022-05-20 05:55:14 | INFO | fairseq_cli.train | Start iterating over samples
2022-05-20 05:57:25 | INFO | train_inner | epoch 034:     50 / 1550 loss=4.513, ppl=22.82, wps=19116.3, ups=0.29, wpb=65331.2, bsz=127.6, num_updates=51200, lr=0.000139754, gnorm=0.575, train_wall=227, gb_free=7.9, wall=159254
2022-05-20 06:01:36 | INFO | train_inner | epoch 034:    150 / 1550 loss=4.493, ppl=22.52, wps=26146.3, ups=0.4, wpb=65536, bsz=128, num_updates=51300, lr=0.000139618, gnorm=0.583, train_wall=228, gb_free=7.9, wall=159505
2022-05-20 06:05:42 | INFO | train_inner | epoch 034:    250 / 1550 loss=4.503, ppl=22.68, wps=26657.2, ups=0.41, wpb=65536, bsz=128, num_updates=51400, lr=0.000139482, gnorm=0.584, train_wall=227, gb_free=7.9, wall=159750
2022-05-20 06:09:42 | INFO | train_inner | epoch 034:    350 / 1550 loss=4.509, ppl=22.78, wps=27279.2, ups=0.42, wpb=65536, bsz=128, num_updates=51500, lr=0.000139347, gnorm=0.583, train_wall=226, gb_free=7.9, wall=159991
2022-05-20 06:13:41 | INFO | train_inner | epoch 034:    450 / 1550 loss=4.522, ppl=22.97, wps=27443.9, ups=0.42, wpb=65536, bsz=128, num_updates=51600, lr=0.000139212, gnorm=0.589, train_wall=227, gb_free=7.9, wall=160229
2022-05-20 06:17:48 | INFO | train_inner | epoch 034:    550 / 1550 loss=4.532, ppl=23.14, wps=26535.1, ups=0.4, wpb=65536, bsz=128, num_updates=51700, lr=0.000139077, gnorm=0.586, train_wall=230, gb_free=7.9, wall=160476
2022-05-20 06:21:45 | INFO | train_inner | epoch 034:    650 / 1550 loss=4.526, ppl=23.05, wps=27609.8, ups=0.42, wpb=65536, bsz=128, num_updates=51800, lr=0.000138943, gnorm=0.585, train_wall=228, gb_free=7.9, wall=160714
2022-05-20 06:25:47 | INFO | train_inner | epoch 034:    750 / 1550 loss=4.533, ppl=23.15, wps=27055.3, ups=0.41, wpb=65536, bsz=128, num_updates=51900, lr=0.000138809, gnorm=0.587, train_wall=229, gb_free=7.9, wall=160956
2022-05-20 06:29:43 | INFO | train_inner | epoch 034:    850 / 1550 loss=4.534, ppl=23.16, wps=27817.8, ups=0.42, wpb=65534.5, bsz=128, num_updates=52000, lr=0.000138675, gnorm=0.582, train_wall=226, gb_free=7.9, wall=161192
2022-05-20 06:33:37 | INFO | train_inner | epoch 034:    950 / 1550 loss=4.531, ppl=23.12, wps=27956.9, ups=0.43, wpb=65536, bsz=128, num_updates=52100, lr=0.000138542, gnorm=0.589, train_wall=226, gb_free=7.9, wall=161426
2022-05-20 06:37:33 | INFO | train_inner | epoch 034:   1050 / 1550 loss=4.526, ppl=23.04, wps=27826.7, ups=0.42, wpb=65536, bsz=128, num_updates=52200, lr=0.000138409, gnorm=0.587, train_wall=227, gb_free=7.9, wall=161662
2022-05-20 06:41:34 | INFO | train_inner | epoch 034:   1150 / 1550 loss=4.543, ppl=23.32, wps=27158.8, ups=0.41, wpb=65536, bsz=128, num_updates=52300, lr=0.000138277, gnorm=0.59, train_wall=229, gb_free=7.9, wall=161903
2022-05-20 06:45:28 | INFO | train_inner | epoch 034:   1250 / 1550 loss=4.534, ppl=23.16, wps=28036.2, ups=0.43, wpb=65536, bsz=128, num_updates=52400, lr=0.000138145, gnorm=0.581, train_wall=226, gb_free=7.9, wall=162137
2022-05-20 06:49:27 | INFO | train_inner | epoch 034:   1350 / 1550 loss=4.548, ppl=23.4, wps=27396.4, ups=0.42, wpb=65536, bsz=128, num_updates=52500, lr=0.000138013, gnorm=0.586, train_wall=228, gb_free=7.9, wall=162376
2022-05-20 06:53:22 | INFO | train_inner | epoch 034:   1450 / 1550 loss=4.544, ppl=23.32, wps=27859.3, ups=0.43, wpb=65536, bsz=128, num_updates=52600, lr=0.000137882, gnorm=0.584, train_wall=228, gb_free=7.9, wall=162611
2022-05-20 06:57:16 | INFO | train_inner | epoch 034:   1550 / 1550 loss=4.542, ppl=23.29, wps=27897.4, ups=0.43, wpb=65331.2, bsz=127.6, num_updates=52700, lr=0.000137751, gnorm=0.588, train_wall=227, gb_free=7.9, wall=162845
2022-05-20 06:57:16 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-05-20 06:58:34 | INFO | valid | epoch 034 | valid on 'valid' subset | loss 4.629 | ppl 24.74 | wps 72496.7 | wpb 2047.4 | bsz 4 | num_updates 52700 | best_loss 4.629
2022-05-20 06:58:35 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 34 @ 52700 updates
2022-05-20 06:58:35 | INFO | fairseq.trainer | Saving checkpoint to /cluster/work/cotterell/liam/master-thesis/checkpoints/language_model/bart/checkpoint_best_bart_32_5.0000E-04_full.pt
2022-05-20 06:58:36 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/work/cotterell/liam/master-thesis/checkpoints/language_model/bart/checkpoint_best_bart_32_5.0000E-04_full.pt
2022-05-20 06:58:36 | INFO | fairseq.checkpoint_utils | Saved checkpoint checkpoints/language_model/bart/checkpoint_best_bart_32_5.0000E-04_full.pt (epoch 34 @ 52700 updates, score 4.629) (writing took 1.6003875071182847 seconds)
2022-05-20 06:58:36 | INFO | fairseq_cli.train | end of epoch 34 (average epoch stats below)
2022-05-20 06:58:36 | INFO | train | epoch 034 | loss 4.527 | ppl 23.05 | wps 26711.5 | ups 0.41 | wpb 65522.7 | bsz 128 | num_updates 52700 | lr 0.000137751 | gnorm 0.586 | train_wall 3525 | gb_free 7.9 | wall 162925
2022-05-20 06:58:36 | INFO | fairseq.data.iterators | grouped total_num_itrs = 1550
2022-05-20 06:58:36 | INFO | fairseq.trainer | begin training epoch 35
2022-05-20 06:58:36 | INFO | fairseq_cli.train | Start iterating over samples
2022-05-20 07:04:28 | INFO | train_inner | epoch 035:    100 / 1550 loss=4.495, ppl=22.56, wps=15180.2, ups=0.23, wpb=65536, bsz=128, num_updates=52800, lr=0.00013762, gnorm=0.586, train_wall=300, gb_free=7.9, wall=163277
2022-05-20 07:09:04 | INFO | train_inner | epoch 035:    200 / 1550 loss=4.502, ppl=22.66, wps=23760.7, ups=0.36, wpb=65536, bsz=128, num_updates=52900, lr=0.00013749, gnorm=0.586, train_wall=225, gb_free=7.9, wall=163553
2022-05-20 07:13:25 | INFO | train_inner | epoch 035:    300 / 1550 loss=4.511, ppl=22.79, wps=25067.9, ups=0.38, wpb=65534.5, bsz=128, num_updates=53000, lr=0.000137361, gnorm=0.584, train_wall=227, gb_free=7.9, wall=163814
2022-05-20 07:17:36 | INFO | train_inner | epoch 035:    400 / 1550 loss=4.517, ppl=22.9, wps=26140.7, ups=0.4, wpb=65536, bsz=128, num_updates=53100, lr=0.000137231, gnorm=0.586, train_wall=227, gb_free=7.9, wall=164065
2022-05-20 07:21:38 | INFO | train_inner | epoch 035:    500 / 1550 loss=4.514, ppl=22.84, wps=27087.4, ups=0.41, wpb=65536, bsz=128, num_updates=53200, lr=0.000137102, gnorm=0.58, train_wall=226, gb_free=7.9, wall=164307
2022-05-20 07:25:44 | INFO | train_inner | epoch 035:    600 / 1550 loss=4.51, ppl=22.79, wps=26635.7, ups=0.41, wpb=65536, bsz=128, num_updates=53300, lr=0.000136973, gnorm=0.584, train_wall=229, gb_free=7.9, wall=164553
2022-05-20 07:29:42 | INFO | train_inner | epoch 035:    700 / 1550 loss=4.518, ppl=22.92, wps=27516.5, ups=0.42, wpb=65536, bsz=128, num_updates=53400, lr=0.000136845, gnorm=0.588, train_wall=226, gb_free=7.9, wall=164791
2022-05-20 07:33:49 | INFO | train_inner | epoch 035:    800 / 1550 loss=4.537, ppl=23.21, wps=26600.4, ups=0.41, wpb=65536, bsz=128, num_updates=53500, lr=0.000136717, gnorm=0.589, train_wall=231, gb_free=7.9, wall=165037
2022-05-20 07:37:46 | INFO | train_inner | epoch 035:    900 / 1550 loss=4.533, ppl=23.15, wps=27571.8, ups=0.42, wpb=65536, bsz=128, num_updates=53600, lr=0.00013659, gnorm=0.587, train_wall=227, gb_free=7.9, wall=165275
2022-05-20 07:41:40 | INFO | train_inner | epoch 035:   1000 / 1550 loss=4.522, ppl=22.97, wps=27987.5, ups=0.43, wpb=65536, bsz=128, num_updates=53700, lr=0.000136462, gnorm=0.585, train_wall=226, gb_free=7.9, wall=165509
2022-05-20 07:45:40 | INFO | train_inner | epoch 035:   1100 / 1550 loss=4.52, ppl=22.94, wps=27368.4, ups=0.42, wpb=65536, bsz=128, num_updates=53800, lr=0.000136335, gnorm=0.581, train_wall=227, gb_free=7.9, wall=165749
2022-05-20 07:49:34 | INFO | train_inner | epoch 035:   1200 / 1550 loss=4.535, ppl=23.18, wps=27985.3, ups=0.43, wpb=65536, bsz=128, num_updates=53900, lr=0.000136209, gnorm=0.584, train_wall=226, gb_free=7.9, wall=165983
2022-05-20 07:53:33 | INFO | train_inner | epoch 035:   1300 / 1550 loss=4.529, ppl=23.09, wps=27386.3, ups=0.42, wpb=65536, bsz=128, num_updates=54000, lr=0.000136083, gnorm=0.586, train_wall=230, gb_free=7.9, wall=166222
2022-05-20 07:57:36 | INFO | train_inner | epoch 035:   1400 / 1550 loss=4.544, ppl=23.32, wps=27045.2, ups=0.41, wpb=65536, bsz=128, num_updates=54100, lr=0.000135957, gnorm=0.588, train_wall=226, gb_free=7.9, wall=166465
2022-05-20 08:01:31 | INFO | train_inner | epoch 035:   1500 / 1550 loss=4.534, ppl=23.17, wps=27820.1, ups=0.42, wpb=65536, bsz=128, num_updates=54200, lr=0.000135831, gnorm=0.592, train_wall=226, gb_free=7.9, wall=166700
2022-05-20 08:03:30 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-05-20 08:04:45 | INFO | valid | epoch 035 | valid on 'valid' subset | loss 4.625 | ppl 24.67 | wps 74986.3 | wpb 2047.4 | bsz 4 | num_updates 54250 | best_loss 4.625
2022-05-20 08:04:45 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 35 @ 54250 updates
2022-05-20 08:04:45 | INFO | fairseq.trainer | Saving checkpoint to /cluster/work/cotterell/liam/master-thesis/checkpoints/language_model/bart/checkpoint_best_bart_32_5.0000E-04_full.pt
2022-05-20 08:04:48 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/work/cotterell/liam/master-thesis/checkpoints/language_model/bart/checkpoint_best_bart_32_5.0000E-04_full.pt
2022-05-20 08:04:48 | INFO | fairseq.checkpoint_utils | Saved checkpoint checkpoints/language_model/bart/checkpoint_best_bart_32_5.0000E-04_full.pt (epoch 35 @ 54250 updates, score 4.625) (writing took 2.7828803309239447 seconds)
2022-05-20 08:04:48 | INFO | fairseq_cli.train | end of epoch 35 (average epoch stats below)
2022-05-20 08:04:48 | INFO | train | epoch 035 | loss 4.522 | ppl 22.97 | wps 25568.7 | ups 0.39 | wpb 65522.7 | bsz 128 | num_updates 54250 | lr 0.000135769 | gnorm 0.586 | train_wall 3592 | gb_free 7.9 | wall 166897
2022-05-20 08:04:48 | INFO | fairseq.data.iterators | grouped total_num_itrs = 1550
2022-05-20 08:04:48 | INFO | fairseq.trainer | begin training epoch 36
2022-05-20 08:04:48 | INFO | fairseq_cli.train | Start iterating over samples
2022-05-20 08:06:46 | INFO | train_inner | epoch 036:     50 / 1550 loss=4.502, ppl=22.65, wps=20787.5, ups=0.32, wpb=65331.2, bsz=127.6, num_updates=54300, lr=0.000135706, gnorm=0.588, train_wall=227, gb_free=7.9, wall=167014
2022-05-20 08:10:43 | INFO | train_inner | epoch 036:    150 / 1550 loss=4.504, ppl=22.68, wps=27590.7, ups=0.42, wpb=65536, bsz=128, num_updates=54400, lr=0.000135582, gnorm=0.59, train_wall=226, gb_free=7.9, wall=167252
2022-05-20 08:14:39 | INFO | train_inner | epoch 036:    250 / 1550 loss=4.496, ppl=22.57, wps=27803.3, ups=0.42, wpb=65536, bsz=128, num_updates=54500, lr=0.000135457, gnorm=0.586, train_wall=228, gb_free=7.9, wall=167488
2022-05-20 08:19:38 | INFO | train_inner | epoch 036:    350 / 1550 loss=4.504, ppl=22.7, wps=21920.5, ups=0.33, wpb=65534.5, bsz=128, num_updates=54600, lr=0.000135333, gnorm=0.589, train_wall=230, gb_free=7.9, wall=167787
2022-05-20 08:24:07 | INFO | train_inner | epoch 036:    450 / 1550 loss=4.498, ppl=22.6, wps=24328.4, ups=0.37, wpb=65536, bsz=128, num_updates=54700, lr=0.000135209, gnorm=0.584, train_wall=228, gb_free=7.9, wall=168056
2022-05-20 08:28:26 | INFO | train_inner | epoch 036:    550 / 1550 loss=4.516, ppl=22.88, wps=25280.7, ups=0.39, wpb=65536, bsz=128, num_updates=54800, lr=0.000135086, gnorm=0.589, train_wall=226, gb_free=7.9, wall=168315
2022-05-20 08:32:32 | INFO | train_inner | epoch 036:    650 / 1550 loss=4.512, ppl=22.81, wps=26719.2, ups=0.41, wpb=65536, bsz=128, num_updates=54900, lr=0.000134963, gnorm=0.594, train_wall=226, gb_free=7.9, wall=168561
2022-05-20 08:36:57 | INFO | train_inner | epoch 036:    750 / 1550 loss=4.52, ppl=22.95, wps=24735.9, ups=0.38, wpb=65536, bsz=128, num_updates=55000, lr=0.00013484, gnorm=0.587, train_wall=232, gb_free=7.9, wall=168825
2022-05-20 08:41:31 | INFO | train_inner | epoch 036:    850 / 1550 loss=4.52, ppl=22.95, wps=23873.4, ups=0.36, wpb=65536, bsz=128, num_updates=55100, lr=0.000134718, gnorm=0.588, train_wall=225, gb_free=7.9, wall=169100
2022-05-20 08:45:51 | INFO | train_inner | epoch 036:    950 / 1550 loss=4.519, ppl=22.92, wps=25224, ups=0.38, wpb=65536, bsz=128, num_updates=55200, lr=0.000134595, gnorm=0.593, train_wall=226, gb_free=7.9, wall=169360
2022-05-20 08:49:59 | INFO | train_inner | epoch 036:   1050 / 1550 loss=4.534, ppl=23.16, wps=26371.5, ups=0.4, wpb=65536, bsz=128, num_updates=55300, lr=0.000134474, gnorm=0.588, train_wall=227, gb_free=7.9, wall=169608
2022-05-20 08:54:04 | INFO | train_inner | epoch 036:   1150 / 1550 loss=4.529, ppl=23.09, wps=26763.2, ups=0.41, wpb=65536, bsz=128, num_updates=55400, lr=0.000134352, gnorm=0.59, train_wall=228, gb_free=7.9, wall=169853
2022-05-20 08:58:08 | INFO | train_inner | epoch 036:   1250 / 1550 loss=4.523, ppl=23, wps=26903.8, ups=0.41, wpb=65536, bsz=128, num_updates=55500, lr=0.000134231, gnorm=0.585, train_wall=229, gb_free=7.9, wall=170097
2022-05-20 09:02:04 | INFO | train_inner | epoch 036:   1350 / 1550 loss=4.53, ppl=23.11, wps=27811.4, ups=0.42, wpb=65536, bsz=128, num_updates=55600, lr=0.00013411, gnorm=0.585, train_wall=226, gb_free=7.9, wall=170332
2022-05-20 09:06:02 | INFO | train_inner | epoch 036:   1450 / 1550 loss=4.519, ppl=22.92, wps=27499.8, ups=0.42, wpb=65536, bsz=128, num_updates=55700, lr=0.00013399, gnorm=0.586, train_wall=226, gb_free=7.9, wall=170571
2022-05-20 09:09:59 | INFO | train_inner | epoch 036:   1550 / 1550 loss=4.546, ppl=23.37, wps=27494.2, ups=0.42, wpb=65331.2, bsz=127.6, num_updates=55800, lr=0.00013387, gnorm=0.588, train_wall=227, gb_free=7.9, wall=170808
2022-05-20 09:09:59 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-05-20 09:11:14 | INFO | valid | epoch 036 | valid on 'valid' subset | loss 4.624 | ppl 24.66 | wps 76393.6 | wpb 2047.4 | bsz 4 | num_updates 55800 | best_loss 4.624
2022-05-20 09:11:14 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 36 @ 55800 updates
2022-05-20 09:11:14 | INFO | fairseq.trainer | Saving checkpoint to /cluster/work/cotterell/liam/master-thesis/checkpoints/language_model/bart/checkpoint_best_bart_32_5.0000E-04_full.pt
2022-05-20 09:11:16 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/work/cotterell/liam/master-thesis/checkpoints/language_model/bart/checkpoint_best_bart_32_5.0000E-04_full.pt
2022-05-20 09:11:16 | INFO | fairseq.checkpoint_utils | Saved checkpoint checkpoints/language_model/bart/checkpoint_best_bart_32_5.0000E-04_full.pt (epoch 36 @ 55800 updates, score 4.624) (writing took 2.581158851739019 seconds)
2022-05-20 09:11:16 | INFO | fairseq_cli.train | end of epoch 36 (average epoch stats below)
2022-05-20 09:11:16 | INFO | train | epoch 036 | loss 4.517 | ppl 22.89 | wps 25466.4 | ups 0.39 | wpb 65522.7 | bsz 128 | num_updates 55800 | lr 0.00013387 | gnorm 0.588 | train_wall 3521 | gb_free 7.9 | wall 170885
2022-05-20 09:11:16 | INFO | fairseq.data.iterators | grouped total_num_itrs = 1550
2022-05-20 09:11:16 | INFO | fairseq.trainer | begin training epoch 37
2022-05-20 09:11:16 | INFO | fairseq_cli.train | Start iterating over samples
2022-05-20 09:15:12 | INFO | train_inner | epoch 037:    100 / 1550 loss=4.487, ppl=22.42, wps=20951.3, ups=0.32, wpb=65536, bsz=128, num_updates=55900, lr=0.00013375, gnorm=0.584, train_wall=228, gb_free=7.9, wall=171121
2022-05-20 09:19:05 | INFO | train_inner | epoch 037:    200 / 1550 loss=4.487, ppl=22.43, wps=28135.4, ups=0.43, wpb=65536, bsz=128, num_updates=56000, lr=0.000133631, gnorm=0.59, train_wall=225, gb_free=7.9, wall=171354
2022-05-20 09:23:10 | INFO | train_inner | epoch 037:    300 / 1550 loss=4.493, ppl=22.51, wps=26821.8, ups=0.41, wpb=65536, bsz=128, num_updates=56100, lr=0.000133511, gnorm=0.588, train_wall=229, gb_free=7.9, wall=171598
2022-05-20 09:27:06 | INFO | train_inner | epoch 037:    400 / 1550 loss=4.511, ppl=22.79, wps=27697.7, ups=0.42, wpb=65536, bsz=128, num_updates=56200, lr=0.000133393, gnorm=0.587, train_wall=227, gb_free=7.9, wall=171835
2022-05-20 09:31:01 | INFO | train_inner | epoch 037:    500 / 1550 loss=4.511, ppl=22.8, wps=27942.5, ups=0.43, wpb=65534.5, bsz=128, num_updates=56300, lr=0.000133274, gnorm=0.592, train_wall=226, gb_free=7.9, wall=172070
2022-05-20 09:34:57 | INFO | train_inner | epoch 037:    600 / 1550 loss=4.507, ppl=22.73, wps=27787.5, ups=0.42, wpb=65536, bsz=128, num_updates=56400, lr=0.000133156, gnorm=0.587, train_wall=227, gb_free=7.9, wall=172305
2022-05-20 09:38:57 | INFO | train_inner | epoch 037:    700 / 1550 loss=4.517, ppl=22.9, wps=27282.6, ups=0.42, wpb=65536, bsz=128, num_updates=56500, lr=0.000133038, gnorm=0.591, train_wall=228, gb_free=7.9, wall=172546
2022-05-20 09:42:52 | INFO | train_inner | epoch 037:    800 / 1550 loss=4.515, ppl=22.87, wps=27809, ups=0.42, wpb=65536, bsz=128, num_updates=56600, lr=0.00013292, gnorm=0.585, train_wall=226, gb_free=7.9, wall=172781
2022-05-20 09:46:48 | INFO | train_inner | epoch 037:    900 / 1550 loss=4.51, ppl=22.78, wps=27774.3, ups=0.42, wpb=65536, bsz=128, num_updates=56700, lr=0.000132803, gnorm=0.586, train_wall=228, gb_free=7.9, wall=173017
2022-05-20 09:50:50 | INFO | train_inner | epoch 037:   1000 / 1550 loss=4.516, ppl=22.88, wps=27075.4, ups=0.41, wpb=65536, bsz=128, num_updates=56800, lr=0.000132686, gnorm=0.586, train_wall=227, gb_free=7.9, wall=173259
2022-05-20 09:54:55 | INFO | train_inner | epoch 037:   1100 / 1550 loss=4.524, ppl=23.01, wps=26803, ups=0.41, wpb=65536, bsz=128, num_updates=56900, lr=0.00013257, gnorm=0.588, train_wall=226, gb_free=7.9, wall=173504
2022-05-20 09:58:56 | INFO | train_inner | epoch 037:   1200 / 1550 loss=4.525, ppl=23.02, wps=27231.2, ups=0.42, wpb=65536, bsz=128, num_updates=57000, lr=0.000132453, gnorm=0.587, train_wall=226, gb_free=7.9, wall=173744
2022-05-20 10:02:55 | INFO | train_inner | epoch 037:   1300 / 1550 loss=4.52, ppl=22.95, wps=27336, ups=0.42, wpb=65536, bsz=128, num_updates=57100, lr=0.000132337, gnorm=0.588, train_wall=227, gb_free=7.9, wall=173984
2022-05-20 10:06:54 | INFO | train_inner | epoch 037:   1400 / 1550 loss=4.519, ppl=22.93, wps=27434.6, ups=0.42, wpb=65536, bsz=128, num_updates=57200, lr=0.000132221, gnorm=0.594, train_wall=228, gb_free=7.9, wall=174223
2022-05-20 10:10:55 | INFO | train_inner | epoch 037:   1500 / 1550 loss=4.526, ppl=23.04, wps=27213.1, ups=0.42, wpb=65536, bsz=128, num_updates=57300, lr=0.000132106, gnorm=0.589, train_wall=227, gb_free=7.9, wall=174464
2022-05-20 10:12:53 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-05-20 10:14:09 | INFO | valid | epoch 037 | valid on 'valid' subset | loss 4.62 | ppl 24.59 | wps 73989.3 | wpb 2047.4 | bsz 4 | num_updates 57350 | best_loss 4.62
2022-05-20 10:14:09 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 37 @ 57350 updates
2022-05-20 10:14:09 | INFO | fairseq.trainer | Saving checkpoint to /cluster/work/cotterell/liam/master-thesis/checkpoints/language_model/bart/checkpoint_best_bart_32_5.0000E-04_full.pt
2022-05-20 10:14:12 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/work/cotterell/liam/master-thesis/checkpoints/language_model/bart/checkpoint_best_bart_32_5.0000E-04_full.pt
2022-05-20 10:14:12 | INFO | fairseq.checkpoint_utils | Saved checkpoint checkpoints/language_model/bart/checkpoint_best_bart_32_5.0000E-04_full.pt (epoch 37 @ 57350 updates, score 4.62) (writing took 2.575711702927947 seconds)
2022-05-20 10:14:12 | INFO | fairseq_cli.train | end of epoch 37 (average epoch stats below)
2022-05-20 10:14:12 | INFO | train | epoch 037 | loss 4.512 | ppl 22.81 | wps 26898.4 | ups 0.41 | wpb 65522.7 | bsz 128 | num_updates 57350 | lr 0.000132048 | gnorm 0.588 | train_wall 3517 | gb_free 7.9 | wall 174661
2022-05-20 10:14:12 | INFO | fairseq.data.iterators | grouped total_num_itrs = 1550
2022-05-20 10:14:12 | INFO | fairseq.trainer | begin training epoch 38
2022-05-20 10:14:12 | INFO | fairseq_cli.train | Start iterating over samples
2022-05-20 10:16:10 | INFO | train_inner | epoch 038:     50 / 1550 loss=4.508, ppl=22.75, wps=20734.9, ups=0.32, wpb=65329.7, bsz=127.6, num_updates=57400, lr=0.000131991, gnorm=0.594, train_wall=226, gb_free=7.9, wall=174779
2022-05-20 10:20:04 | INFO | train_inner | epoch 038:    150 / 1550 loss=4.488, ppl=22.43, wps=28035, ups=0.43, wpb=65536, bsz=128, num_updates=57500, lr=0.000131876, gnorm=0.59, train_wall=226, gb_free=7.9, wall=175013
2022-05-20 10:24:00 | INFO | train_inner | epoch 038:    250 / 1550 loss=4.492, ppl=22.5, wps=27715.7, ups=0.42, wpb=65536, bsz=128, num_updates=57600, lr=0.000131762, gnorm=0.587, train_wall=226, gb_free=7.9, wall=175249
2022-05-20 10:28:08 | INFO | train_inner | epoch 038:    350 / 1550 loss=4.492, ppl=22.5, wps=26481.5, ups=0.4, wpb=65536, bsz=128, num_updates=57700, lr=0.000131647, gnorm=0.59, train_wall=233, gb_free=7.9, wall=175497
2022-05-20 10:32:02 | INFO | train_inner | epoch 038:    450 / 1550 loss=4.501, ppl=22.64, wps=27998.5, ups=0.43, wpb=65536, bsz=128, num_updates=57800, lr=0.000131533, gnorm=0.596, train_wall=226, gb_free=7.9, wall=175731
2022-05-20 10:36:03 | INFO | train_inner | epoch 038:    550 / 1550 loss=4.516, ppl=22.87, wps=27199.6, ups=0.42, wpb=65536, bsz=128, num_updates=57900, lr=0.00013142, gnorm=0.598, train_wall=229, gb_free=7.9, wall=175972
2022-05-20 10:39:57 | INFO | train_inner | epoch 038:    650 / 1550 loss=4.494, ppl=22.53, wps=28040.4, ups=0.43, wpb=65536, bsz=128, num_updates=58000, lr=0.000131306, gnorm=0.589, train_wall=226, gb_free=7.9, wall=176205
2022-05-20 10:43:52 | INFO | train_inner | epoch 038:    750 / 1550 loss=4.505, ppl=22.71, wps=27841.7, ups=0.42, wpb=65536, bsz=128, num_updates=58100, lr=0.000131193, gnorm=0.591, train_wall=226, gb_free=7.9, wall=176441
2022-05-20 10:47:55 | INFO | train_inner | epoch 038:    850 / 1550 loss=4.508, ppl=22.75, wps=26999.4, ups=0.41, wpb=65536, bsz=128, num_updates=58200, lr=0.000131081, gnorm=0.589, train_wall=230, gb_free=7.9, wall=176684
2022-05-20 10:51:48 | INFO | train_inner | epoch 038:    950 / 1550 loss=4.508, ppl=22.75, wps=28072.6, ups=0.43, wpb=65536, bsz=128, num_updates=58300, lr=0.000130968, gnorm=0.592, train_wall=226, gb_free=7.9, wall=176917
2022-05-20 10:55:50 | INFO | train_inner | epoch 038:   1050 / 1550 loss=4.51, ppl=22.79, wps=27046.6, ups=0.41, wpb=65536, bsz=128, num_updates=58400, lr=0.000130856, gnorm=0.592, train_wall=228, gb_free=7.9, wall=177159
2022-05-20 10:59:48 | INFO | train_inner | epoch 038:   1150 / 1550 loss=4.521, ppl=22.97, wps=27636.5, ups=0.42, wpb=65536, bsz=128, num_updates=58500, lr=0.000130744, gnorm=0.592, train_wall=225, gb_free=7.9, wall=177396
2022-05-20 11:03:45 | INFO | train_inner | epoch 038:   1250 / 1550 loss=4.529, ppl=23.09, wps=27658.3, ups=0.42, wpb=65536, bsz=128, num_updates=58600, lr=0.000130632, gnorm=0.592, train_wall=226, gb_free=7.9, wall=177633
2022-05-20 11:07:58 | INFO | train_inner | epoch 038:   1350 / 1550 loss=4.528, ppl=23.07, wps=25890.6, ups=0.4, wpb=65536, bsz=128, num_updates=58700, lr=0.000130521, gnorm=0.593, train_wall=232, gb_free=7.9, wall=177887
2022-05-20 11:11:56 | INFO | train_inner | epoch 038:   1450 / 1550 loss=4.506, ppl=22.72, wps=27462.3, ups=0.42, wpb=65536, bsz=128, num_updates=58800, lr=0.00013041, gnorm=0.589, train_wall=226, gb_free=7.9, wall=178125
2022-05-20 11:16:00 | INFO | train_inner | epoch 038:   1550 / 1550 loss=4.523, ppl=23, wps=26805.2, ups=0.41, wpb=65331.2, bsz=127.6, num_updates=58900, lr=0.000130299, gnorm=0.586, train_wall=229, gb_free=7.9, wall=178369
2022-05-20 11:16:00 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-05-20 11:17:14 | INFO | valid | epoch 038 | valid on 'valid' subset | loss 4.618 | ppl 24.55 | wps 76316.8 | wpb 2047.4 | bsz 4 | num_updates 58900 | best_loss 4.618
2022-05-20 11:17:14 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 38 @ 58900 updates
2022-05-20 11:17:14 | INFO | fairseq.trainer | Saving checkpoint to /cluster/work/cotterell/liam/master-thesis/checkpoints/language_model/bart/checkpoint_best_bart_32_5.0000E-04_full.pt
2022-05-20 11:17:16 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/work/cotterell/liam/master-thesis/checkpoints/language_model/bart/checkpoint_best_bart_32_5.0000E-04_full.pt
2022-05-20 11:17:16 | INFO | fairseq.checkpoint_utils | Saved checkpoint checkpoints/language_model/bart/checkpoint_best_bart_32_5.0000E-04_full.pt (epoch 38 @ 58900 updates, score 4.618) (writing took 1.5059779812581837 seconds)
2022-05-20 11:17:16 | INFO | fairseq_cli.train | end of epoch 38 (average epoch stats below)
2022-05-20 11:17:16 | INFO | train | epoch 038 | loss 4.507 | ppl 22.74 | wps 26839.8 | ups 0.41 | wpb 65522.7 | bsz 128 | num_updates 58900 | lr 0.000130299 | gnorm 0.591 | train_wall 3526 | gb_free 7.9 | wall 178445
2022-05-20 11:17:16 | INFO | fairseq.data.iterators | grouped total_num_itrs = 1550
2022-05-20 11:17:16 | INFO | fairseq.trainer | begin training epoch 39
2022-05-20 11:17:16 | INFO | fairseq_cli.train | Start iterating over samples
2022-05-20 11:21:12 | INFO | train_inner | epoch 039:    100 / 1550 loss=4.473, ppl=22.21, wps=20986, ups=0.32, wpb=65536, bsz=128, num_updates=59000, lr=0.000130189, gnorm=0.595, train_wall=226, gb_free=7.9, wall=178681
2022-05-20 11:25:09 | INFO | train_inner | epoch 039:    200 / 1550 loss=4.488, ppl=22.43, wps=27738.7, ups=0.42, wpb=65536, bsz=128, num_updates=59100, lr=0.000130079, gnorm=0.595, train_wall=227, gb_free=7.9, wall=178917
2022-05-20 11:29:09 | INFO | train_inner | epoch 039:    300 / 1550 loss=4.482, ppl=22.35, wps=27315.5, ups=0.42, wpb=65536, bsz=128, num_updates=59200, lr=0.000129969, gnorm=0.592, train_wall=228, gb_free=7.9, wall=179157
2022-05-20 11:33:02 | INFO | train_inner | epoch 039:    400 / 1550 loss=4.48, ppl=22.31, wps=28095.1, ups=0.43, wpb=65536, bsz=128, num_updates=59300, lr=0.000129859, gnorm=0.595, train_wall=226, gb_free=7.9, wall=179391
2022-05-20 11:36:58 | INFO | train_inner | epoch 039:    500 / 1550 loss=4.495, ppl=22.55, wps=27688.8, ups=0.42, wpb=65536, bsz=128, num_updates=59400, lr=0.00012975, gnorm=0.589, train_wall=227, gb_free=7.9, wall=179627
2022-05-20 11:42:01 | INFO | train_inner | epoch 039:    600 / 1550 loss=4.496, ppl=22.57, wps=21649.5, ups=0.33, wpb=65536, bsz=128, num_updates=59500, lr=0.000129641, gnorm=0.591, train_wall=277, gb_free=7.9, wall=179930
2022-05-20 11:46:48 | INFO | train_inner | epoch 039:    700 / 1550 loss=4.502, ppl=22.66, wps=22857.4, ups=0.35, wpb=65536, bsz=128, num_updates=59600, lr=0.000129532, gnorm=0.593, train_wall=225, gb_free=7.9, wall=180217
2022-05-20 11:51:10 | INFO | train_inner | epoch 039:    800 / 1550 loss=4.513, ppl=22.82, wps=25000.6, ups=0.38, wpb=65536, bsz=128, num_updates=59700, lr=0.000129423, gnorm=0.593, train_wall=225, gb_free=7.9, wall=180479
2022-05-20 11:55:40 | INFO | train_inner | epoch 039:    900 / 1550 loss=4.511, ppl=22.8, wps=24241.2, ups=0.37, wpb=65536, bsz=128, num_updates=59800, lr=0.000129315, gnorm=0.596, train_wall=228, gb_free=7.9, wall=180749
2022-05-20 11:59:55 | INFO | train_inner | epoch 039:   1000 / 1550 loss=4.508, ppl=22.76, wps=25768.6, ups=0.39, wpb=65536, bsz=128, num_updates=59900, lr=0.000129207, gnorm=0.588, train_wall=225, gb_free=7.9, wall=181004
2022-05-20 12:04:11 | INFO | train_inner | epoch 039:   1100 / 1550 loss=4.507, ppl=22.75, wps=25538.6, ups=0.39, wpb=65536, bsz=128, num_updates=60000, lr=0.000129099, gnorm=0.593, train_wall=230, gb_free=7.9, wall=181260
2022-05-20 12:08:14 | INFO | train_inner | epoch 039:   1200 / 1550 loss=4.516, ppl=22.88, wps=26963.4, ups=0.41, wpb=65536, bsz=128, num_updates=60100, lr=0.000128992, gnorm=0.596, train_wall=226, gb_free=7.9, wall=181503
2022-05-20 12:12:16 | INFO | train_inner | epoch 039:   1300 / 1550 loss=4.519, ppl=22.92, wps=27127.1, ups=0.41, wpb=65534.5, bsz=128, num_updates=60200, lr=0.000128885, gnorm=0.59, train_wall=229, gb_free=7.9, wall=181745
2022-05-20 12:16:12 | INFO | train_inner | epoch 039:   1400 / 1550 loss=4.53, ppl=23.11, wps=27734.1, ups=0.42, wpb=65536, bsz=128, num_updates=60300, lr=0.000128778, gnorm=0.592, train_wall=225, gb_free=7.9, wall=181981
2022-05-20 12:20:08 | INFO | train_inner | epoch 039:   1500 / 1550 loss=4.512, ppl=22.82, wps=27774.2, ups=0.42, wpb=65536, bsz=128, num_updates=60400, lr=0.000128671, gnorm=0.597, train_wall=227, gb_free=7.9, wall=182217
2022-05-20 12:22:32 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-05-20 12:23:47 | INFO | valid | epoch 039 | valid on 'valid' subset | loss 4.617 | ppl 24.54 | wps 75814.6 | wpb 2047.4 | bsz 4 | num_updates 60450 | best_loss 4.617
2022-05-20 12:23:47 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 39 @ 60450 updates
2022-05-20 12:23:47 | INFO | fairseq.trainer | Saving checkpoint to /cluster/work/cotterell/liam/master-thesis/checkpoints/language_model/bart/checkpoint_best_bart_32_5.0000E-04_full.pt
2022-05-20 12:23:48 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/work/cotterell/liam/master-thesis/checkpoints/language_model/bart/checkpoint_best_bart_32_5.0000E-04_full.pt
2022-05-20 12:23:48 | INFO | fairseq.checkpoint_utils | Saved checkpoint checkpoints/language_model/bart/checkpoint_best_bart_32_5.0000E-04_full.pt (epoch 39 @ 60450 updates, score 4.617) (writing took 1.5422622081823647 seconds)
2022-05-20 12:23:48 | INFO | fairseq_cli.train | end of epoch 39 (average epoch stats below)
2022-05-20 12:23:48 | INFO | train | epoch 039 | loss 4.503 | ppl 22.67 | wps 25437.3 | ups 0.39 | wpb 65522.7 | bsz 128 | num_updates 60450 | lr 0.000128618 | gnorm 0.593 | train_wall 3566 | gb_free 7.9 | wall 182437
2022-05-20 12:23:49 | INFO | fairseq.data.iterators | grouped total_num_itrs = 1550
2022-05-20 12:23:49 | INFO | fairseq.trainer | begin training epoch 40
2022-05-20 12:23:49 | INFO | fairseq_cli.train | Start iterating over samples
2022-05-20 12:26:17 | INFO | train_inner | epoch 040:     50 / 1550 loss=4.496, ppl=22.57, wps=17736.2, ups=0.27, wpb=65331.2, bsz=127.6, num_updates=60500, lr=0.000128565, gnorm=0.591, train_wall=229, gb_free=7.9, wall=182585
2022-05-20 12:30:48 | INFO | train_inner | epoch 040:    150 / 1550 loss=4.479, ppl=22.31, wps=24175.4, ups=0.37, wpb=65536, bsz=128, num_updates=60600, lr=0.000128459, gnorm=0.596, train_wall=225, gb_free=7.9, wall=182857
2022-05-20 12:35:16 | INFO | train_inner | epoch 040:    250 / 1550 loss=4.478, ppl=22.28, wps=24394.4, ups=0.37, wpb=65536, bsz=128, num_updates=60700, lr=0.000128353, gnorm=0.593, train_wall=232, gb_free=7.9, wall=183125
2022-05-20 12:39:26 | INFO | train_inner | epoch 040:    350 / 1550 loss=4.487, ppl=22.42, wps=26234.7, ups=0.4, wpb=65536, bsz=128, num_updates=60800, lr=0.000128247, gnorm=0.598, train_wall=226, gb_free=7.9, wall=183375
2022-05-20 12:43:44 | INFO | train_inner | epoch 040:    450 / 1550 loss=4.492, ppl=22.5, wps=25428.2, ups=0.39, wpb=65536, bsz=128, num_updates=60900, lr=0.000128142, gnorm=0.591, train_wall=235, gb_free=7.9, wall=183633
2022-05-20 12:47:43 | INFO | train_inner | epoch 040:    550 / 1550 loss=4.495, ppl=22.55, wps=27354.7, ups=0.42, wpb=65536, bsz=128, num_updates=61000, lr=0.000128037, gnorm=0.6, train_wall=226, gb_free=7.9, wall=183872
2022-05-20 12:51:44 | INFO | train_inner | epoch 040:    650 / 1550 loss=4.499, ppl=22.61, wps=27265.7, ups=0.42, wpb=65536, bsz=128, num_updates=61100, lr=0.000127932, gnorm=0.591, train_wall=228, gb_free=7.9, wall=184113
2022-05-20 12:55:41 | INFO | train_inner | epoch 040:    750 / 1550 loss=4.5, ppl=22.62, wps=27591.9, ups=0.42, wpb=65536, bsz=128, num_updates=61200, lr=0.000127827, gnorm=0.594, train_wall=226, gb_free=7.9, wall=184350
2022-05-20 12:59:42 | INFO | train_inner | epoch 040:    850 / 1550 loss=4.504, ppl=22.69, wps=27285.6, ups=0.42, wpb=65536, bsz=128, num_updates=61300, lr=0.000127723, gnorm=0.595, train_wall=230, gb_free=7.9, wall=184590
2022-05-20 13:03:40 | INFO | train_inner | epoch 040:    950 / 1550 loss=4.515, ppl=22.87, wps=27454.3, ups=0.42, wpb=65534.5, bsz=128, num_updates=61400, lr=0.000127619, gnorm=0.596, train_wall=231, gb_free=7.9, wall=184829
2022-05-20 13:07:33 | INFO | train_inner | epoch 040:   1050 / 1550 loss=4.492, ppl=22.5, wps=28104.7, ups=0.43, wpb=65536, bsz=128, num_updates=61500, lr=0.000127515, gnorm=0.597, train_wall=226, gb_free=7.9, wall=185062
2022-05-20 13:11:28 | INFO | train_inner | epoch 040:   1150 / 1550 loss=4.511, ppl=22.8, wps=27902.7, ups=0.43, wpb=65536, bsz=128, num_updates=61600, lr=0.000127412, gnorm=0.588, train_wall=228, gb_free=7.9, wall=185297
2022-05-20 13:15:28 | INFO | train_inner | epoch 040:   1250 / 1550 loss=4.503, ppl=22.67, wps=27357.3, ups=0.42, wpb=65536, bsz=128, num_updates=61700, lr=0.000127309, gnorm=0.594, train_wall=228, gb_free=7.9, wall=185537
2022-05-20 13:19:23 | INFO | train_inner | epoch 040:   1350 / 1550 loss=4.514, ppl=22.85, wps=27919.8, ups=0.43, wpb=65536, bsz=128, num_updates=61800, lr=0.000127205, gnorm=0.591, train_wall=228, gb_free=7.9, wall=185771
2022-05-20 13:23:24 | INFO | train_inner | epoch 040:   1450 / 1550 loss=4.513, ppl=22.82, wps=27175.9, ups=0.41, wpb=65536, bsz=128, num_updates=61900, lr=0.000127103, gnorm=0.588, train_wall=228, gb_free=7.9, wall=186013
2022-05-20 13:27:32 | INFO | train_inner | epoch 040:   1550 / 1550 loss=4.511, ppl=22.79, wps=26367.1, ups=0.4, wpb=65331.2, bsz=127.6, num_updates=62000, lr=0.000127, gnorm=0.592, train_wall=227, gb_free=7.9, wall=186260
2022-05-20 13:27:32 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-05-20 13:28:47 | INFO | valid | epoch 040 | valid on 'valid' subset | loss 4.614 | ppl 24.49 | wps 74682 | wpb 2047.4 | bsz 4 | num_updates 62000 | best_loss 4.614
2022-05-20 13:28:47 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 40 @ 62000 updates
2022-05-20 13:28:47 | INFO | fairseq.trainer | Saving checkpoint to /cluster/work/cotterell/liam/master-thesis/checkpoints/language_model/bart/checkpoint_best_bart_32_5.0000E-04_full.pt
2022-05-20 13:28:49 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/work/cotterell/liam/master-thesis/checkpoints/language_model/bart/checkpoint_best_bart_32_5.0000E-04_full.pt
2022-05-20 13:28:49 | INFO | fairseq.checkpoint_utils | Saved checkpoint checkpoints/language_model/bart/checkpoint_best_bart_32_5.0000E-04_full.pt (epoch 40 @ 62000 updates, score 4.614) (writing took 1.6492540282197297 seconds)
2022-05-20 13:28:49 | INFO | fairseq_cli.train | end of epoch 40 (average epoch stats below)
2022-05-20 13:28:49 | INFO | train | epoch 040 | loss 4.499 | ppl 22.61 | wps 26036.4 | ups 0.4 | wpb 65522.7 | bsz 128 | num_updates 62000 | lr 0.000127 | gnorm 0.594 | train_wall 3538 | gb_free 7.9 | wall 186338
2022-05-20 13:28:49 | INFO | fairseq.data.iterators | grouped total_num_itrs = 1550
2022-05-20 13:28:49 | INFO | fairseq.trainer | begin training epoch 41
2022-05-20 13:28:49 | INFO | fairseq_cli.train | Start iterating over samples
2022-05-20 13:32:52 | INFO | train_inner | epoch 041:    100 / 1550 loss=4.469, ppl=22.15, wps=20441.2, ups=0.31, wpb=65536, bsz=128, num_updates=62100, lr=0.000126898, gnorm=0.591, train_wall=226, gb_free=7.9, wall=186581
2022-05-20 13:36:53 | INFO | train_inner | epoch 041:    200 / 1550 loss=4.471, ppl=22.17, wps=27219.1, ups=0.42, wpb=65536, bsz=128, num_updates=62200, lr=0.000126796, gnorm=0.593, train_wall=228, gb_free=7.9, wall=186822
2022-05-20 13:40:56 | INFO | train_inner | epoch 041:    300 / 1550 loss=4.491, ppl=22.49, wps=26978.2, ups=0.41, wpb=65536, bsz=128, num_updates=62300, lr=0.000126694, gnorm=0.592, train_wall=226, gb_free=7.9, wall=187065
2022-05-20 13:44:50 | INFO | train_inner | epoch 041:    400 / 1550 loss=4.47, ppl=22.16, wps=27954.4, ups=0.43, wpb=65536, bsz=128, num_updates=62400, lr=0.000126592, gnorm=0.596, train_wall=226, gb_free=7.9, wall=187299
2022-05-20 13:48:48 | INFO | train_inner | epoch 041:    500 / 1550 loss=4.496, ppl=22.56, wps=27609.7, ups=0.42, wpb=65534.5, bsz=128, num_updates=62500, lr=0.000126491, gnorm=0.597, train_wall=229, gb_free=7.9, wall=187536
2022-05-20 13:52:51 | INFO | train_inner | epoch 041:    600 / 1550 loss=4.488, ppl=22.43, wps=26887.8, ups=0.41, wpb=65536, bsz=128, num_updates=62600, lr=0.00012639, gnorm=0.597, train_wall=229, gb_free=7.9, wall=187780
2022-05-20 13:57:14 | INFO | train_inner | epoch 041:    700 / 1550 loss=4.501, ppl=22.64, wps=24978.8, ups=0.38, wpb=65536, bsz=128, num_updates=62700, lr=0.000126289, gnorm=0.594, train_wall=234, gb_free=7.9, wall=188043
2022-05-20 14:02:09 | INFO | train_inner | epoch 041:    800 / 1550 loss=4.504, ppl=22.69, wps=22213.7, ups=0.34, wpb=65536, bsz=128, num_updates=62800, lr=0.000126189, gnorm=0.591, train_wall=229, gb_free=7.9, wall=188338
2022-05-20 14:06:30 | INFO | train_inner | epoch 041:    900 / 1550 loss=4.493, ppl=22.52, wps=25085.6, ups=0.38, wpb=65536, bsz=128, num_updates=62900, lr=0.000126088, gnorm=0.594, train_wall=225, gb_free=7.9, wall=188599
2022-05-20 14:10:40 | INFO | train_inner | epoch 041:   1000 / 1550 loss=4.505, ppl=22.71, wps=26261.3, ups=0.4, wpb=65536, bsz=128, num_updates=63000, lr=0.000125988, gnorm=0.601, train_wall=226, gb_free=7.9, wall=188848
2022-05-20 14:14:55 | INFO | train_inner | epoch 041:   1100 / 1550 loss=4.507, ppl=22.74, wps=25646.3, ups=0.39, wpb=65536, bsz=128, num_updates=63100, lr=0.000125888, gnorm=0.588, train_wall=231, gb_free=7.9, wall=189104
2022-05-20 14:18:55 | INFO | train_inner | epoch 041:   1200 / 1550 loss=4.505, ppl=22.71, wps=27264.3, ups=0.42, wpb=65536, bsz=128, num_updates=63200, lr=0.000125789, gnorm=0.596, train_wall=226, gb_free=7.9, wall=189344
2022-05-20 14:23:07 | INFO | train_inner | epoch 041:   1300 / 1550 loss=4.497, ppl=22.58, wps=26038.5, ups=0.4, wpb=65536, bsz=128, num_updates=63300, lr=0.000125689, gnorm=0.593, train_wall=232, gb_free=7.9, wall=189596
2022-05-20 14:27:05 | INFO | train_inner | epoch 041:   1400 / 1550 loss=4.512, ppl=22.82, wps=27610.2, ups=0.42, wpb=65536, bsz=128, num_updates=63400, lr=0.00012559, gnorm=0.587, train_wall=226, gb_free=7.9, wall=189833
2022-05-20 14:31:00 | INFO | train_inner | epoch 041:   1500 / 1550 loss=4.509, ppl=22.77, wps=27857.4, ups=0.43, wpb=65536, bsz=128, num_updates=63500, lr=0.000125491, gnorm=0.594, train_wall=226, gb_free=7.9, wall=190069
2022-05-20 14:33:05 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-05-20 14:34:20 | INFO | valid | epoch 041 | valid on 'valid' subset | loss 4.611 | ppl 24.43 | wps 75732 | wpb 2047.4 | bsz 4 | num_updates 63550 | best_loss 4.611
2022-05-20 14:34:20 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 41 @ 63550 updates
2022-05-20 14:34:20 | INFO | fairseq.trainer | Saving checkpoint to /cluster/work/cotterell/liam/master-thesis/checkpoints/language_model/bart/checkpoint_best_bart_32_5.0000E-04_full.pt
2022-05-20 14:34:21 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/work/cotterell/liam/master-thesis/checkpoints/language_model/bart/checkpoint_best_bart_32_5.0000E-04_full.pt
2022-05-20 14:34:21 | INFO | fairseq.checkpoint_utils | Saved checkpoint checkpoints/language_model/bart/checkpoint_best_bart_32_5.0000E-04_full.pt (epoch 41 @ 63550 updates, score 4.611) (writing took 1.51193585107103 seconds)
2022-05-20 14:34:21 | INFO | fairseq_cli.train | end of epoch 41 (average epoch stats below)
2022-05-20 14:34:21 | INFO | train | epoch 041 | loss 4.495 | ppl 22.54 | wps 25826.4 | ups 0.39 | wpb 65522.7 | bsz 128 | num_updates 63550 | lr 0.000125442 | gnorm 0.594 | train_wall 3537 | gb_free 7.9 | wall 190270
2022-05-20 14:34:22 | INFO | fairseq.data.iterators | grouped total_num_itrs = 1550
2022-05-20 14:34:22 | INFO | fairseq.trainer | begin training epoch 42
2022-05-20 14:34:22 | INFO | fairseq_cli.train | Start iterating over samples
2022-05-20 14:36:20 | INFO | train_inner | epoch 042:     50 / 1550 loss=4.477, ppl=22.27, wps=20411.4, ups=0.31, wpb=65331.2, bsz=127.6, num_updates=63600, lr=0.000125392, gnorm=0.592, train_wall=230, gb_free=7.9, wall=190389
2022-05-20 14:40:14 | INFO | train_inner | epoch 042:    150 / 1550 loss=4.47, ppl=22.17, wps=28024.8, ups=0.43, wpb=65534.5, bsz=128, num_updates=63700, lr=0.000125294, gnorm=0.595, train_wall=226, gb_free=7.9, wall=190623
2022-05-20 14:44:10 | INFO | train_inner | epoch 042:    250 / 1550 loss=4.472, ppl=22.19, wps=27693.2, ups=0.42, wpb=65536, bsz=128, num_updates=63800, lr=0.000125196, gnorm=0.597, train_wall=229, gb_free=7.9, wall=190859
2022-05-20 14:48:06 | INFO | train_inner | epoch 042:    350 / 1550 loss=4.481, ppl=22.34, wps=27863.8, ups=0.43, wpb=65536, bsz=128, num_updates=63900, lr=0.000125098, gnorm=0.595, train_wall=227, gb_free=7.9, wall=191094
2022-05-20 14:52:05 | INFO | train_inner | epoch 042:    450 / 1550 loss=4.492, ppl=22.51, wps=27336.9, ups=0.42, wpb=65536, bsz=128, num_updates=64000, lr=0.000125, gnorm=0.604, train_wall=229, gb_free=7.9, wall=191334
2022-05-20 14:56:22 | INFO | train_inner | epoch 042:    550 / 1550 loss=4.493, ppl=22.51, wps=25511.4, ups=0.39, wpb=65536, bsz=128, num_updates=64100, lr=0.000124902, gnorm=0.597, train_wall=233, gb_free=7.9, wall=191591
2022-05-20 15:01:20 | INFO | train_inner | epoch 042:    650 / 1550 loss=4.485, ppl=22.4, wps=22020, ups=0.34, wpb=65536, bsz=128, num_updates=64200, lr=0.000124805, gnorm=0.595, train_wall=228, gb_free=7.9, wall=191889
2022-05-20 15:06:10 | INFO | train_inner | epoch 042:    750 / 1550 loss=4.49, ppl=22.47, wps=22561.7, ups=0.34, wpb=65536, bsz=128, num_updates=64300, lr=0.000124708, gnorm=0.598, train_wall=232, gb_free=7.9, wall=192179
2022-05-20 15:10:39 | INFO | train_inner | epoch 042:    850 / 1550 loss=4.483, ppl=22.37, wps=24378, ups=0.37, wpb=65536, bsz=128, num_updates=64400, lr=0.000124611, gnorm=0.598, train_wall=227, gb_free=7.9, wall=192448
2022-05-20 15:14:55 | INFO | train_inner | epoch 042:    950 / 1550 loss=4.499, ppl=22.61, wps=25605.8, ups=0.39, wpb=65536, bsz=128, num_updates=64500, lr=0.000124515, gnorm=0.593, train_wall=225, gb_free=7.9, wall=192704
2022-05-20 15:19:26 | INFO | train_inner | epoch 042:   1050 / 1550 loss=4.502, ppl=22.66, wps=24220.2, ups=0.37, wpb=65536, bsz=128, num_updates=64600, lr=0.000124418, gnorm=0.599, train_wall=231, gb_free=7.9, wall=192974
2022-05-20 15:24:09 | INFO | train_inner | epoch 042:   1150 / 1550 loss=4.495, ppl=22.56, wps=23143.7, ups=0.35, wpb=65536, bsz=128, num_updates=64700, lr=0.000124322, gnorm=0.599, train_wall=225, gb_free=7.9, wall=193258
2022-05-20 15:28:39 | INFO | train_inner | epoch 042:   1250 / 1550 loss=4.498, ppl=22.59, wps=24210.5, ups=0.37, wpb=65536, bsz=128, num_updates=64800, lr=0.000124226, gnorm=0.596, train_wall=228, gb_free=7.9, wall=193528
2022-05-20 15:32:53 | INFO | train_inner | epoch 042:   1350 / 1550 loss=4.508, ppl=22.75, wps=25839.5, ups=0.39, wpb=65536, bsz=128, num_updates=64900, lr=0.00012413, gnorm=0.595, train_wall=226, gb_free=7.9, wall=193782
2022-05-20 15:39:24 | INFO | train_inner | epoch 042:   1450 / 1550 loss=4.508, ppl=22.75, wps=16765.6, ups=0.26, wpb=65536, bsz=128, num_updates=65000, lr=0.000124035, gnorm=0.596, train_wall=248, gb_free=7.9, wall=194173
2022-05-20 15:44:53 | INFO | train_inner | epoch 042:   1550 / 1550 loss=4.498, ppl=22.6, wps=19882.1, ups=0.3, wpb=65331.2, bsz=127.6, num_updates=65100, lr=0.000123939, gnorm=0.598, train_wall=247, gb_free=7.9, wall=194501
2022-05-20 15:44:53 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-05-20 15:46:10 | INFO | valid | epoch 042 | valid on 'valid' subset | loss 4.608 | ppl 24.38 | wps 74461 | wpb 2047.4 | bsz 4 | num_updates 65100 | best_loss 4.608
2022-05-20 15:46:10 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 42 @ 65100 updates
2022-05-20 15:46:10 | INFO | fairseq.trainer | Saving checkpoint to /cluster/work/cotterell/liam/master-thesis/checkpoints/language_model/bart/checkpoint_best_bart_32_5.0000E-04_full.pt
2022-05-20 15:46:12 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/work/cotterell/liam/master-thesis/checkpoints/language_model/bart/checkpoint_best_bart_32_5.0000E-04_full.pt
2022-05-20 15:46:12 | INFO | fairseq.checkpoint_utils | Saved checkpoint checkpoints/language_model/bart/checkpoint_best_bart_32_5.0000E-04_full.pt (epoch 42 @ 65100 updates, score 4.608) (writing took 2.5917394128628075 seconds)
2022-05-20 15:46:12 | INFO | fairseq_cli.train | end of epoch 42 (average epoch stats below)
2022-05-20 15:46:13 | INFO | train | epoch 042 | loss 4.491 | ppl 22.48 | wps 23555.8 | ups 0.36 | wpb 65522.7 | bsz 128 | num_updates 65100 | lr 0.000123939 | gnorm 0.597 | train_wall 3575 | gb_free 7.9 | wall 194582
2022-05-20 15:46:14 | INFO | fairseq.data.iterators | grouped total_num_itrs = 1550
2022-05-20 15:46:14 | INFO | fairseq.trainer | begin training epoch 43
2022-05-20 15:46:14 | INFO | fairseq_cli.train | Start iterating over samples
2022-05-20 15:52:08 | INFO | train_inner | epoch 043:    100 / 1550 loss=4.463, ppl=22.06, wps=15051, ups=0.23, wpb=65534.5, bsz=128, num_updates=65200, lr=0.000123844, gnorm=0.594, train_wall=250, gb_free=7.9, wall=194937
2022-05-20 15:57:48 | INFO | train_inner | epoch 043:    200 / 1550 loss=4.466, ppl=22.11, wps=19298.6, ups=0.29, wpb=65536, bsz=128, num_updates=65300, lr=0.000123749, gnorm=0.595, train_wall=260, gb_free=7.9, wall=195276
2022-05-20 16:02:36 | INFO | train_inner | epoch 043:    300 / 1550 loss=4.465, ppl=22.08, wps=22758.3, ups=0.35, wpb=65536, bsz=128, num_updates=65400, lr=0.000123655, gnorm=0.603, train_wall=225, gb_free=7.9, wall=195564
2022-05-20 16:07:09 | INFO | train_inner | epoch 043:    400 / 1550 loss=4.472, ppl=22.19, wps=23990.1, ups=0.37, wpb=65536, bsz=128, num_updates=65500, lr=0.00012356, gnorm=0.6, train_wall=227, gb_free=7.9, wall=195838
2022-05-20 16:11:33 | INFO | train_inner | epoch 043:    500 / 1550 loss=4.481, ppl=22.34, wps=24787.2, ups=0.38, wpb=65536, bsz=128, num_updates=65600, lr=0.000123466, gnorm=0.595, train_wall=231, gb_free=7.9, wall=196102
2022-05-20 16:16:17 | INFO | train_inner | epoch 043:    600 / 1550 loss=4.482, ppl=22.35, wps=23099.8, ups=0.35, wpb=65536, bsz=128, num_updates=65700, lr=0.000123372, gnorm=0.6, train_wall=225, gb_free=7.9, wall=196386
2022-05-20 16:20:55 | INFO | train_inner | epoch 043:    700 / 1550 loss=4.495, ppl=22.55, wps=23582.9, ups=0.36, wpb=65536, bsz=128, num_updates=65800, lr=0.000123278, gnorm=0.595, train_wall=229, gb_free=7.9, wall=196664
2022-05-20 16:25:37 | INFO | train_inner | epoch 043:    800 / 1550 loss=4.487, ppl=22.42, wps=23217.5, ups=0.35, wpb=65536, bsz=128, num_updates=65900, lr=0.000123185, gnorm=0.592, train_wall=230, gb_free=7.9, wall=196946
2022-05-20 16:30:14 | INFO | train_inner | epoch 043:    900 / 1550 loss=4.486, ppl=22.4, wps=23657.4, ups=0.36, wpb=65536, bsz=128, num_updates=66000, lr=0.000123091, gnorm=0.596, train_wall=229, gb_free=7.9, wall=197223
2022-05-20 16:34:31 | INFO | train_inner | epoch 043:   1000 / 1550 loss=4.502, ppl=22.66, wps=25498.9, ups=0.39, wpb=65536, bsz=128, num_updates=66100, lr=0.000122998, gnorm=0.597, train_wall=225, gb_free=7.9, wall=197480
2022-05-20 16:38:46 | INFO | train_inner | epoch 043:   1100 / 1550 loss=4.492, ppl=22.5, wps=25730.8, ups=0.39, wpb=65536, bsz=128, num_updates=66200, lr=0.000122905, gnorm=0.595, train_wall=230, gb_free=7.9, wall=197735
2022-05-20 16:42:47 | INFO | train_inner | epoch 043:   1200 / 1550 loss=4.507, ppl=22.74, wps=27207.9, ups=0.42, wpb=65536, bsz=128, num_updates=66300, lr=0.000122813, gnorm=0.596, train_wall=226, gb_free=7.9, wall=197975
2022-05-20 16:47:38 | INFO | train_inner | epoch 043:   1300 / 1550 loss=4.499, ppl=22.6, wps=22471.3, ups=0.34, wpb=65536, bsz=128, num_updates=66400, lr=0.00012272, gnorm=0.601, train_wall=232, gb_free=7.9, wall=198267
2022-05-20 16:52:03 | INFO | train_inner | epoch 043:   1400 / 1550 loss=4.497, ppl=22.58, wps=24720.1, ups=0.38, wpb=65536, bsz=128, num_updates=66500, lr=0.000122628, gnorm=0.591, train_wall=226, gb_free=7.9, wall=198532
2022-05-20 16:56:19 | INFO | train_inner | epoch 043:   1500 / 1550 loss=4.503, ppl=22.67, wps=25628.4, ups=0.39, wpb=65536, bsz=128, num_updates=66600, lr=0.000122536, gnorm=0.595, train_wall=227, gb_free=7.9, wall=198788
2022-05-20 16:58:24 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-05-20 16:59:39 | INFO | valid | epoch 043 | valid on 'valid' subset | loss 4.606 | ppl 24.36 | wps 75861.1 | wpb 2047.4 | bsz 4 | num_updates 66650 | best_loss 4.606
2022-05-20 16:59:39 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 43 @ 66650 updates
2022-05-20 16:59:39 | INFO | fairseq.trainer | Saving checkpoint to /cluster/work/cotterell/liam/master-thesis/checkpoints/language_model/bart/checkpoint_best_bart_32_5.0000E-04_full.pt
2022-05-20 16:59:40 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/work/cotterell/liam/master-thesis/checkpoints/language_model/bart/checkpoint_best_bart_32_5.0000E-04_full.pt
2022-05-20 16:59:40 | INFO | fairseq.checkpoint_utils | Saved checkpoint checkpoints/language_model/bart/checkpoint_best_bart_32_5.0000E-04_full.pt (epoch 43 @ 66650 updates, score 4.606) (writing took 1.4600362852215767 seconds)
2022-05-20 16:59:40 | INFO | fairseq_cli.train | end of epoch 43 (average epoch stats below)
2022-05-20 16:59:40 | INFO | train | epoch 043 | loss 4.487 | ppl 22.42 | wps 23043.7 | ups 0.35 | wpb 65522.7 | bsz 128 | num_updates 66650 | lr 0.00012249 | gnorm 0.596 | train_wall 3583 | gb_free 7.9 | wall 198989
2022-05-20 16:59:40 | INFO | fairseq.data.iterators | grouped total_num_itrs = 1550
2022-05-20 16:59:40 | INFO | fairseq.trainer | begin training epoch 44
2022-05-20 16:59:40 | INFO | fairseq_cli.train | Start iterating over samples
2022-05-20 17:01:44 | INFO | train_inner | epoch 044:     50 / 1550 loss=4.477, ppl=22.28, wps=20077.7, ups=0.31, wpb=65331.2, bsz=127.6, num_updates=66700, lr=0.000122444, gnorm=0.597, train_wall=227, gb_free=7.9, wall=199113
2022-05-20 17:05:45 | INFO | train_inner | epoch 044:    150 / 1550 loss=4.468, ppl=22.13, wps=27302.2, ups=0.42, wpb=65536, bsz=128, num_updates=66800, lr=0.000122352, gnorm=0.59, train_wall=226, gb_free=7.9, wall=199353
2022-05-20 17:09:54 | INFO | train_inner | epoch 044:    250 / 1550 loss=4.473, ppl=22.2, wps=26311.3, ups=0.4, wpb=65536, bsz=128, num_updates=66900, lr=0.000122261, gnorm=0.603, train_wall=230, gb_free=7.9, wall=199602
2022-05-20 17:14:46 | INFO | train_inner | epoch 044:    350 / 1550 loss=4.457, ppl=21.96, wps=22420, ups=0.34, wpb=65534.5, bsz=128, num_updates=67000, lr=0.000122169, gnorm=0.594, train_wall=225, gb_free=7.9, wall=199895
2022-05-20 17:19:11 | INFO | train_inner | epoch 044:    450 / 1550 loss=4.488, ppl=22.44, wps=24683.6, ups=0.38, wpb=65536, bsz=128, num_updates=67100, lr=0.000122078, gnorm=0.599, train_wall=225, gb_free=7.9, wall=200160
2022-05-20 17:23:27 | INFO | train_inner | epoch 044:    550 / 1550 loss=4.476, ppl=22.25, wps=25679, ups=0.39, wpb=65536, bsz=128, num_updates=67200, lr=0.000121988, gnorm=0.597, train_wall=225, gb_free=7.9, wall=200415
2022-05-20 17:28:20 | INFO | train_inner | epoch 044:    650 / 1550 loss=4.467, ppl=22.11, wps=22340.8, ups=0.34, wpb=65536, bsz=128, num_updates=67300, lr=0.000121897, gnorm=0.594, train_wall=231, gb_free=7.9, wall=200709
2022-05-20 17:32:47 | INFO | train_inner | epoch 044:    750 / 1550 loss=4.484, ppl=22.37, wps=24519, ups=0.37, wpb=65536, bsz=128, num_updates=67400, lr=0.000121806, gnorm=0.599, train_wall=225, gb_free=7.9, wall=200976
2022-05-20 17:37:11 | INFO | train_inner | epoch 044:    850 / 1550 loss=4.481, ppl=22.33, wps=24895.5, ups=0.38, wpb=65536, bsz=128, num_updates=67500, lr=0.000121716, gnorm=0.601, train_wall=228, gb_free=7.9, wall=201239
2022-05-20 17:41:23 | INFO | train_inner | epoch 044:    950 / 1550 loss=4.502, ppl=22.66, wps=25911.6, ups=0.4, wpb=65536, bsz=128, num_updates=67600, lr=0.000121626, gnorm=0.598, train_wall=229, gb_free=7.9, wall=201492
2022-05-20 17:45:34 | INFO | train_inner | epoch 044:   1050 / 1550 loss=4.493, ppl=22.52, wps=26139.3, ups=0.4, wpb=65536, bsz=128, num_updates=67700, lr=0.000121536, gnorm=0.601, train_wall=230, gb_free=7.9, wall=201743
2022-05-20 17:49:35 | INFO | train_inner | epoch 044:   1150 / 1550 loss=4.495, ppl=22.54, wps=27219.7, ups=0.42, wpb=65536, bsz=128, num_updates=67800, lr=0.000121447, gnorm=0.597, train_wall=226, gb_free=7.9, wall=201984
2022-05-20 17:53:33 | INFO | train_inner | epoch 044:   1250 / 1550 loss=4.488, ppl=22.44, wps=27484.2, ups=0.42, wpb=65536, bsz=128, num_updates=67900, lr=0.000121357, gnorm=0.607, train_wall=226, gb_free=7.9, wall=202222
2022-05-20 17:58:22 | INFO | train_inner | epoch 044:   1350 / 1550 loss=4.486, ppl=22.41, wps=22705.3, ups=0.35, wpb=65536, bsz=128, num_updates=68000, lr=0.000121268, gnorm=0.597, train_wall=231, gb_free=7.9, wall=202511
2022-05-20 18:02:50 | INFO | train_inner | epoch 044:   1450 / 1550 loss=4.499, ppl=22.62, wps=24486.2, ups=0.37, wpb=65536, bsz=128, num_updates=68100, lr=0.000121179, gnorm=0.592, train_wall=225, gb_free=7.9, wall=202779
2022-05-20 18:07:35 | INFO | train_inner | epoch 044:   1550 / 1550 loss=4.502, ppl=22.65, wps=22893.4, ups=0.35, wpb=65331.2, bsz=127.6, num_updates=68200, lr=0.00012109, gnorm=0.594, train_wall=233, gb_free=7.9, wall=203064
2022-05-20 18:07:35 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-05-20 18:08:50 | INFO | valid | epoch 044 | valid on 'valid' subset | loss 4.605 | ppl 24.33 | wps 75672.8 | wpb 2047.4 | bsz 4 | num_updates 68200 | best_loss 4.605
2022-05-20 18:08:50 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 44 @ 68200 updates
2022-05-20 18:08:50 | INFO | fairseq.trainer | Saving checkpoint to /cluster/work/cotterell/liam/master-thesis/checkpoints/language_model/bart/checkpoint_best_bart_32_5.0000E-04_full.pt
2022-05-20 18:08:52 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/work/cotterell/liam/master-thesis/checkpoints/language_model/bart/checkpoint_best_bart_32_5.0000E-04_full.pt
2022-05-20 18:08:52 | INFO | fairseq.checkpoint_utils | Saved checkpoint checkpoints/language_model/bart/checkpoint_best_bart_32_5.0000E-04_full.pt (epoch 44 @ 68200 updates, score 4.605) (writing took 1.7528143860399723 seconds)
2022-05-20 18:08:52 | INFO | fairseq_cli.train | end of epoch 44 (average epoch stats below)
2022-05-20 18:08:52 | INFO | train | epoch 044 | loss 4.483 | ppl 22.36 | wps 24463 | ups 0.37 | wpb 65522.7 | bsz 128 | num_updates 68200 | lr 0.00012109 | gnorm 0.597 | train_wall 3529 | gb_free 7.9 | wall 203141
2022-05-20 18:08:52 | INFO | fairseq.data.iterators | grouped total_num_itrs = 1550
2022-05-20 18:08:52 | INFO | fairseq.trainer | begin training epoch 45
2022-05-20 18:08:52 | INFO | fairseq_cli.train | Start iterating over samples
2022-05-20 18:13:31 | INFO | train_inner | epoch 045:    100 / 1550 loss=4.462, ppl=22.03, wps=18393.4, ups=0.28, wpb=65536, bsz=128, num_updates=68300, lr=0.000121001, gnorm=0.599, train_wall=225, gb_free=7.9, wall=203420
2022-05-20 18:17:52 | INFO | train_inner | epoch 045:    200 / 1550 loss=4.463, ppl=22.06, wps=25149.9, ups=0.38, wpb=65536, bsz=128, num_updates=68400, lr=0.000120913, gnorm=0.607, train_wall=228, gb_free=7.9, wall=203681
2022-05-20 18:22:41 | INFO | train_inner | epoch 045:    300 / 1550 loss=4.476, ppl=22.26, wps=22676.9, ups=0.35, wpb=65536, bsz=128, num_updates=68500, lr=0.000120824, gnorm=0.602, train_wall=231, gb_free=7.9, wall=203970
2022-05-20 18:27:12 | INFO | train_inner | epoch 045:    400 / 1550 loss=4.466, ppl=22.11, wps=24172.9, ups=0.37, wpb=65534.5, bsz=128, num_updates=68600, lr=0.000120736, gnorm=0.602, train_wall=225, gb_free=7.9, wall=204241
2022-05-20 18:31:30 | INFO | train_inner | epoch 045:    500 / 1550 loss=4.482, ppl=22.35, wps=25357, ups=0.39, wpb=65536, bsz=128, num_updates=68700, lr=0.000120648, gnorm=0.603, train_wall=229, gb_free=7.9, wall=204499
2022-05-20 18:35:40 | INFO | train_inner | epoch 045:    600 / 1550 loss=4.474, ppl=22.22, wps=26288.3, ups=0.4, wpb=65536, bsz=128, num_updates=68800, lr=0.000120561, gnorm=0.596, train_wall=227, gb_free=7.9, wall=204749
2022-05-20 18:39:44 | INFO | train_inner | epoch 045:    700 / 1550 loss=4.484, ppl=22.37, wps=26889.6, ups=0.41, wpb=65536, bsz=128, num_updates=68900, lr=0.000120473, gnorm=0.601, train_wall=228, gb_free=7.9, wall=204992
2022-05-20 18:44:20 | INFO | train_inner | epoch 045:    800 / 1550 loss=4.468, ppl=22.13, wps=23660.4, ups=0.36, wpb=65536, bsz=128, num_updates=69000, lr=0.000120386, gnorm=0.602, train_wall=229, gb_free=7.9, wall=205269
2022-05-20 18:59:08 | INFO | train_inner | epoch 045:    900 / 1550 loss=4.486, ppl=22.41, wps=7385.9, ups=0.11, wpb=65536, bsz=128, num_updates=69100, lr=0.000120299, gnorm=0.599, train_wall=660, gb_free=7.9, wall=206157
2022-05-20 19:05:44 | INFO | train_inner | epoch 045:   1000 / 1550 loss=4.482, ppl=22.35, wps=16553.1, ups=0.25, wpb=65536, bsz=128, num_updates=69200, lr=0.000120212, gnorm=0.602, train_wall=298, gb_free=7.9, wall=206553
2022-05-20 19:11:09 | INFO | train_inner | epoch 045:   1100 / 1550 loss=4.489, ppl=22.46, wps=20125.7, ups=0.31, wpb=65536, bsz=128, num_updates=69300, lr=0.000120125, gnorm=0.604, train_wall=250, gb_free=7.9, wall=206878
2022-05-20 19:16:51 | INFO | train_inner | epoch 045:   1200 / 1550 loss=4.479, ppl=22.3, wps=19155.7, ups=0.29, wpb=65536, bsz=128, num_updates=69400, lr=0.000120038, gnorm=0.596, train_wall=263, gb_free=7.9, wall=207220
2022-05-20 19:21:48 | INFO | train_inner | epoch 045:   1300 / 1550 loss=4.49, ppl=22.47, wps=22114.1, ups=0.34, wpb=65536, bsz=128, num_updates=69500, lr=0.000119952, gnorm=0.598, train_wall=235, gb_free=7.9, wall=207517
2022-05-20 19:26:31 | INFO | train_inner | epoch 045:   1400 / 1550 loss=4.493, ppl=22.52, wps=23135.8, ups=0.35, wpb=65536, bsz=128, num_updates=69600, lr=0.000119866, gnorm=0.6, train_wall=232, gb_free=7.9, wall=207800
2022-05-20 19:31:05 | INFO | train_inner | epoch 045:   1500 / 1550 loss=4.494, ppl=22.54, wps=23970.9, ups=0.37, wpb=65536, bsz=128, num_updates=69700, lr=0.00011978, gnorm=0.598, train_wall=229, gb_free=7.9, wall=208073
2022-05-20 19:33:59 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-05-20 19:35:19 | INFO | valid | epoch 045 | valid on 'valid' subset | loss 4.604 | ppl 24.32 | wps 70223.4 | wpb 2047.4 | bsz 4 | num_updates 69750 | best_loss 4.604
2022-05-20 19:35:19 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 45 @ 69750 updates
2022-05-20 19:35:19 | INFO | fairseq.trainer | Saving checkpoint to /cluster/work/cotterell/liam/master-thesis/checkpoints/language_model/bart/checkpoint_best_bart_32_5.0000E-04_full.pt
2022-05-20 19:35:22 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/work/cotterell/liam/master-thesis/checkpoints/language_model/bart/checkpoint_best_bart_32_5.0000E-04_full.pt
2022-05-20 19:35:22 | INFO | fairseq.checkpoint_utils | Saved checkpoint checkpoints/language_model/bart/checkpoint_best_bart_32_5.0000E-04_full.pt (epoch 45 @ 69750 updates, score 4.604) (writing took 2.697316053789109 seconds)
2022-05-20 19:35:22 | INFO | fairseq_cli.train | end of epoch 45 (average epoch stats below)
2022-05-20 19:35:23 | INFO | train | epoch 045 | loss 4.479 | ppl 22.31 | wps 19563.4 | ups 0.3 | wpb 65522.7 | bsz 128 | num_updates 69750 | lr 0.000119737 | gnorm 0.6 | train_wall 4127 | gb_free 7.9 | wall 208332
2022-05-20 19:35:24 | INFO | fairseq.data.iterators | grouped total_num_itrs = 1550
2022-05-20 19:35:24 | INFO | fairseq.trainer | begin training epoch 46
2022-05-20 19:35:24 | INFO | fairseq_cli.train | Start iterating over samples
2022-05-20 19:38:22 | INFO | train_inner | epoch 046:     50 / 1550 loss=4.463, ppl=22.05, wps=14934.1, ups=0.23, wpb=65331.2, bsz=127.6, num_updates=69800, lr=0.000119694, gnorm=0.597, train_wall=267, gb_free=7.9, wall=208511
/cluster/shadow/.lsbatch/1652859603.218938764: line 8: 17498 Bus error               (core dumped) fairseq-train data/xsum-lang-bart-full --save-dir checkpoints/language_model/bart --update-freq 32 --lr 0.0005 --checkpoint-suffix _bart_32_5.0000E-04_full --restore-file checkpoints/language_model/bart/checkpoint_best.pt --task language_modeling --arch transformer_lm --share-decoder-input-output-embed --dropout 0.1 --optimizer adam --adam-betas "(0.9, 0.98)" --weight-decay 0.01 --clip-norm 0.0 --lr-scheduler inverse_sqrt --warmup-updates 4000 --warmup-init-lr 1e-07 --tokens-per-sample 512 --sample-break-mode none --max-tokens 2048 --no-epoch-checkpoints --no-last-checkpoints --patience 5
