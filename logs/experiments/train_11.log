Sender: LSF System <lsfadmin@eu-g3-003>
Subject: Job 210062966: <train_11> in cluster <euler> Exited

Job <train_11> was submitted from host <eu-login-18> by user <euler_username> in cluster <euler> at Sun Mar 20 17:11:05 2022
Job was executed on host(s) <4*eu-g3-003>, in queue <gpu.24h>, as user <euler_username> in cluster <euler> at Sun Mar 20 18:48:36 2022
</cluster/home/euler_username> was used as the home directory.
</cluster/work/cotterell/liam/master-thesis> was used as the working directory.
Started at Sun Mar 20 18:48:36 2022
Terminated at Mon Mar 21 18:49:06 2022
Results reported at Mon Mar 21 18:49:06 2022

Your job looked like:

------------------------------------------------------------
# LSBATCH: User input
fairseq-train data/xsum-summarizer --save-dir checkpoints/summarization_model/11 --criterion label_smoothed_cross_entropy --label-smoothing 0.1 --max-tokens 4096 --update-freq 64 --no-epoch-checkpoints --no-last-checkpoints --truncate-source --skip-invalid-size-inputs-valid-test --patience 5 --arch transformer_wmt_en_de --share-all-embeddings --optimizer adam --adam-betas "(0.9, 0.98)" --clip-norm 0.0 --lr-scheduler inverse_sqrt --warmup-init-lr 1e-07 --warmup-updates 4000 --lr 0.00007 --dropout 0.3000 --weight-decay 0.0 --restore-file checkpoints/summarization_model/10/checkpoint_best.pt
------------------------------------------------------------

TERM_RUNLIMIT: job killed after reaching LSF run time limit.
Exited with exit code 140.

Resource usage summary:

    CPU time :                                   69408.00 sec.
    Max Memory :                                 4598 MB
    Average Memory :                             2939.76 MB
    Total Requested Memory :                     8192.00 MB
    Delta Memory :                               3594.00 MB
    Max Swap :                                   -
    Max Processes :                              4
    Max Threads :                                14
    Run time :                                   86430 sec.
    Turnaround time :                            92281 sec.

The output (if any) follows:

2022-03-20 18:50:35 | INFO | fairseq.tasks.text_to_speech | Please install tensorboardX: pip install tensorboardX
2022-03-20 18:50:44 | INFO | fairseq_cli.train | {'_name': None, 'common': {'_name': None, 'no_progress_bar': False, 'log_interval': 100, 'log_format': None, 'log_file': None, 'tensorboard_logdir': None, 'wandb_project': None, 'azureml_logging': False, 'seed': 1, 'cpu': False, 'tpu': False, 'bf16': False, 'memory_efficient_bf16': False, 'fp16': False, 'memory_efficient_fp16': False, 'fp16_no_flatten_grads': False, 'fp16_init_scale': 128, 'fp16_scale_window': None, 'fp16_scale_tolerance': 0.0, 'on_cpu_convert_precision': False, 'min_loss_scale': 0.0001, 'threshold_loss_scale': None, 'amp': False, 'amp_batch_retries': 2, 'amp_init_scale': 128, 'amp_scale_window': None, 'user_dir': None, 'empty_cache_freq': 0, 'all_gather_list_size': 16384, 'model_parallel_size': 1, 'quantization_config_path': None, 'profile': False, 'reset_logging': False, 'suppress_crashes': False, 'use_plasma_view': False, 'plasma_path': '/tmp/plasma'}, 'common_eval': {'_name': None, 'path': None, 'post_process': None, 'quiet': False, 'model_overrides': '{}', 'results_path': None}, 'distributed_training': {'_name': None, 'distributed_world_size': 1, 'distributed_num_procs': 1, 'distributed_rank': 0, 'distributed_backend': 'nccl', 'distributed_init_method': None, 'distributed_port': -1, 'device_id': 0, 'distributed_no_spawn': False, 'ddp_backend': 'pytorch_ddp', 'ddp_comm_hook': 'none', 'bucket_cap_mb': 25, 'fix_batches_to_gpus': False, 'find_unused_parameters': False, 'gradient_as_bucket_view': False, 'fast_stat_sync': False, 'heartbeat_timeout': -1, 'broadcast_buffers': False, 'slowmo_momentum': None, 'slowmo_base_algorithm': 'localsgd', 'localsgd_frequency': 3, 'nprocs_per_node': 1, 'pipeline_model_parallel': False, 'pipeline_balance': None, 'pipeline_devices': None, 'pipeline_chunks': 0, 'pipeline_encoder_balance': None, 'pipeline_encoder_devices': None, 'pipeline_decoder_balance': None, 'pipeline_decoder_devices': None, 'pipeline_checkpoint': 'never', 'zero_sharding': 'none', 'fp16': False, 'memory_efficient_fp16': False, 'tpu': False, 'no_reshard_after_forward': False, 'fp32_reduce_scatter': False, 'cpu_offload': False, 'use_sharded_state': False, 'not_fsdp_flatten_parameters': False}, 'dataset': {'_name': None, 'num_workers': 1, 'skip_invalid_size_inputs_valid_test': True, 'max_tokens': 4096, 'batch_size': None, 'required_batch_size_multiple': 8, 'required_seq_len_multiple': 1, 'dataset_impl': None, 'data_buffer_size': 10, 'train_subset': 'train', 'valid_subset': 'valid', 'combine_valid_subsets': None, 'ignore_unused_valid_subsets': False, 'validate_interval': 1, 'validate_interval_updates': 0, 'validate_after_updates': 0, 'fixed_validation_seed': None, 'disable_validation': False, 'max_tokens_valid': 4096, 'batch_size_valid': None, 'max_valid_steps': None, 'curriculum': 0, 'gen_subset': 'test', 'num_shards': 1, 'shard_id': 0, 'grouped_shuffling': False, 'update_epoch_batch_itr': False, 'update_ordered_indices_seed': False}, 'optimization': {'_name': None, 'max_epoch': 0, 'max_update': 0, 'stop_time_hours': 0.0, 'clip_norm': 0.0, 'sentence_avg': False, 'update_freq': [64], 'lr': [7e-05], 'stop_min_lr': -1.0, 'use_bmuf': False, 'skip_remainder_batch': False}, 'checkpoint': {'_name': None, 'save_dir': 'checkpoints/summarization_model/11', 'restore_file': 'checkpoints/summarization_model/10/checkpoint_best.pt', 'continue_once': None, 'finetune_from_model': None, 'reset_dataloader': False, 'reset_lr_scheduler': False, 'reset_meters': False, 'reset_optimizer': False, 'optimizer_overrides': '{}', 'save_interval': 1, 'save_interval_updates': 0, 'keep_interval_updates': -1, 'keep_interval_updates_pattern': -1, 'keep_last_epochs': -1, 'keep_best_checkpoints': -1, 'no_save': False, 'no_epoch_checkpoints': True, 'no_last_checkpoints': True, 'no_save_optimizer_state': False, 'best_checkpoint_metric': 'loss', 'maximize_best_checkpoint_metric': False, 'patience': 5, 'checkpoint_suffix': '', 'checkpoint_shard_count': 1, 'load_checkpoint_on_all_dp_ranks': False, 'write_checkpoints_asynchronously': False, 'model_parallel_size': 1}, 'bmuf': {'_name': None, 'block_lr': 1.0, 'block_momentum': 0.875, 'global_sync_iter': 50, 'warmup_iterations': 500, 'use_nbm': False, 'average_sync': False, 'distributed_world_size': 1}, 'generation': {'_name': None, 'beam': 5, 'nbest': 1, 'max_len_a': 0.0, 'max_len_b': 200, 'min_len': 1, 'match_source_len': False, 'unnormalized': False, 'no_early_stop': False, 'no_beamable_mm': False, 'lenpen': 1.0, 'unkpen': 0.0, 'replace_unk': None, 'sacrebleu': False, 'score_reference': False, 'prefix_size': 0, 'no_repeat_ngram_size': 0, 'sampling': False, 'sampling_topk': -1, 'sampling_topp': -1.0, 'constraints': None, 'temperature': 1.0, 'diverse_beam_groups': -1, 'diverse_beam_strength': 0.5, 'diversity_rate': -1.0, 'print_alignment': None, 'print_step': False, 'lm_path': None, 'lm_weight': 0.0, 'iter_decode_eos_penalty': 0.0, 'iter_decode_max_iter': 10, 'iter_decode_force_max_iter': False, 'iter_decode_with_beam': 1, 'iter_decode_with_external_reranker': False, 'retain_iter_history': False, 'retain_dropout': False, 'retain_dropout_modules': None, 'decoding_format': None, 'no_seed_provided': False}, 'eval_lm': {'_name': None, 'output_word_probs': False, 'output_word_stats': False, 'context_window': 0, 'softmax_batch': 9223372036854775807}, 'interactive': {'_name': None, 'buffer_size': 0, 'input': '-'}, 'model': Namespace(_name='transformer_wmt_en_de', activation_dropout=0.0, activation_fn='relu', adam_betas='(0.9, 0.98)', adam_eps=1e-08, adaptive_input=False, adaptive_softmax_cutoff=None, adaptive_softmax_dropout=0, all_gather_list_size=16384, amp=False, amp_batch_retries=2, amp_init_scale=128, amp_scale_window=None, arch='transformer_wmt_en_de', attention_dropout=0.0, azureml_logging=False, batch_size=None, batch_size_valid=None, best_checkpoint_metric='loss', bf16=False, bpe=None, broadcast_buffers=False, bucket_cap_mb=25, checkpoint_activations=False, checkpoint_shard_count=1, checkpoint_suffix='', clip_norm=0.0, combine_valid_subsets=None, continue_once=None, cpu=False, cpu_offload=False, criterion='label_smoothed_cross_entropy', cross_self_attention=False, curriculum=0, data='data/xsum-summarizer', data_buffer_size=10, dataset_impl=None, ddp_backend='pytorch_ddp', ddp_comm_hook='none', decoder_attention_heads=8, decoder_embed_dim=512, decoder_embed_path=None, decoder_ffn_embed_dim=2048, decoder_input_dim=512, decoder_layerdrop=0, decoder_layers=6, decoder_layers_to_keep=None, decoder_learned_pos=False, decoder_normalize_before=False, decoder_output_dim=512, device_id=0, disable_validation=False, distributed_backend='nccl', distributed_init_method=None, distributed_no_spawn=False, distributed_num_procs=1, distributed_port=-1, distributed_rank=0, distributed_world_size=1, dropout=0.3, ema_decay=0.9999, ema_fp32=False, ema_seed_model=None, ema_start_update=0, ema_update_freq=1, empty_cache_freq=0, encoder_attention_heads=8, encoder_embed_dim=512, encoder_embed_path=None, encoder_ffn_embed_dim=2048, encoder_layerdrop=0, encoder_layers=6, encoder_layers_to_keep=None, encoder_learned_pos=False, encoder_normalize_before=False, eos=2, eval_bleu=False, eval_bleu_args='{}', eval_bleu_detok='space', eval_bleu_detok_args='{}', eval_bleu_print_samples=False, eval_bleu_remove_bpe=None, eval_tokenized_bleu=False, fast_stat_sync=False, find_unused_parameters=False, finetune_from_model=None, fix_batches_to_gpus=False, fixed_validation_seed=None, fp16=False, fp16_adam_stats=False, fp16_init_scale=128, fp16_no_flatten_grads=False, fp16_scale_tolerance=0.0, fp16_scale_window=None, fp32_reduce_scatter=False, gen_subset='test', gradient_as_bucket_view=False, grouped_shuffling=False, heartbeat_timeout=-1, ignore_prefix_size=0, ignore_unused_valid_subsets=False, keep_best_checkpoints=-1, keep_interval_updates=-1, keep_interval_updates_pattern=-1, keep_last_epochs=-1, label_smoothing=0.1, layernorm_embedding=False, left_pad_source=True, left_pad_target=False, load_alignments=False, load_checkpoint_on_all_dp_ranks=False, localsgd_frequency=3, log_file=None, log_format=None, log_interval=100, lr=[7e-05], lr_scheduler='inverse_sqrt', max_epoch=0, max_tokens=4096, max_tokens_valid=4096, max_update=0, max_valid_steps=None, maximize_best_checkpoint_metric=False, memory_efficient_bf16=False, memory_efficient_fp16=False, min_loss_scale=0.0001, model_parallel_size=1, no_cross_attention=False, no_epoch_checkpoints=True, no_last_checkpoints=True, no_progress_bar=False, no_reshard_after_forward=False, no_save=False, no_save_optimizer_state=False, no_scale_embedding=False, no_seed_provided=False, no_token_positional_embeddings=False, not_fsdp_flatten_parameters=False, nprocs_per_node=1, num_batch_buckets=0, num_shards=1, num_workers=1, offload_activations=False, on_cpu_convert_precision=False, optimizer='adam', optimizer_overrides='{}', pad=1, patience=5, pipeline_balance=None, pipeline_checkpoint='never', pipeline_chunks=0, pipeline_decoder_balance=None, pipeline_decoder_devices=None, pipeline_devices=None, pipeline_encoder_balance=None, pipeline_encoder_devices=None, pipeline_model_parallel=False, plasma_path='/tmp/plasma', profile=False, quant_noise_pq=0, quant_noise_pq_block_size=8, quant_noise_scalar=0, quantization_config_path=None, report_accuracy=False, required_batch_size_multiple=8, required_seq_len_multiple=1, reset_dataloader=False, reset_logging=False, reset_lr_scheduler=False, reset_meters=False, reset_optimizer=False, restore_file='checkpoints/summarization_model/10/checkpoint_best.pt', save_dir='checkpoints/summarization_model/11', save_interval=1, save_interval_updates=0, scoring='bleu', seed=1, sentence_avg=False, shard_id=0, share_all_embeddings=True, share_decoder_input_output_embed=False, simul_type=None, skip_invalid_size_inputs_valid_test=True, skip_remainder_batch=False, slowmo_base_algorithm='localsgd', slowmo_momentum=None, source_lang=None, stop_min_lr=-1.0, stop_time_hours=0, store_ema=False, suppress_crashes=False, target_lang=None, task='translation', tensorboard_logdir=None, threshold_loss_scale=None, tie_adaptive_weights=False, tokenizer=None, tpu=False, train_subset='train', truncate_source=True, unk=3, update_epoch_batch_itr=False, update_freq=[64], update_ordered_indices_seed=False, upsample_primary=-1, use_bmuf=False, use_old_adam=False, use_plasma_view=False, use_sharded_state=False, user_dir=None, valid_subset='valid', validate_after_updates=0, validate_interval=1, validate_interval_updates=0, wandb_project=None, warmup_init_lr=1e-07, warmup_updates=4000, weight_decay=0.0, write_checkpoints_asynchronously=False, zero_sharding='none'), 'task': {'_name': 'translation', 'data': 'data/xsum-summarizer', 'source_lang': None, 'target_lang': None, 'load_alignments': False, 'left_pad_source': True, 'left_pad_target': False, 'max_source_positions': 1024, 'max_target_positions': 1024, 'upsample_primary': -1, 'truncate_source': True, 'num_batch_buckets': 0, 'train_subset': 'train', 'dataset_impl': None, 'required_seq_len_multiple': 1, 'eval_bleu': False, 'eval_bleu_args': '{}', 'eval_bleu_detok': 'space', 'eval_bleu_detok_args': '{}', 'eval_tokenized_bleu': False, 'eval_bleu_remove_bpe': None, 'eval_bleu_print_samples': False}, 'criterion': {'_name': 'label_smoothed_cross_entropy', 'label_smoothing': 0.1, 'report_accuracy': False, 'ignore_prefix_size': 0, 'sentence_avg': False}, 'optimizer': {'_name': 'adam', 'adam_betas': '(0.9, 0.98)', 'adam_eps': 1e-08, 'weight_decay': 0.0, 'use_old_adam': False, 'fp16_adam_stats': False, 'tpu': False, 'lr': [7e-05]}, 'lr_scheduler': {'_name': 'inverse_sqrt', 'warmup_updates': 4000, 'warmup_init_lr': 1e-07, 'lr': [7e-05]}, 'scoring': {'_name': 'bleu', 'pad': 1, 'eos': 2, 'unk': 3}, 'bpe': None, 'tokenizer': None, 'ema': {'_name': None, 'store_ema': False, 'ema_decay': 0.9999, 'ema_start_update': 0, 'ema_seed_model': None, 'ema_update_freq': 1, 'ema_fp32': False}, 'simul_type': None}
2022-03-20 18:50:44 | INFO | fairseq.tasks.translation | [source] dictionary: 49992 types
2022-03-20 18:50:44 | INFO | fairseq.tasks.translation | [target] dictionary: 49992 types
2022-03-20 18:50:48 | INFO | fairseq_cli.train | TransformerModel(
  (encoder): TransformerEncoderBase(
    (dropout_module): FairseqDropout()
    (embed_tokens): Embedding(49992, 512, padding_idx=1)
    (embed_positions): SinusoidalPositionalEmbedding()
    (layers): ModuleList(
      (0): TransformerEncoderLayerBase(
        (self_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=512, out_features=512, bias=True)
          (v_proj): Linear(in_features=512, out_features=512, bias=True)
          (q_proj): Linear(in_features=512, out_features=512, bias=True)
          (out_proj): Linear(in_features=512, out_features=512, bias=True)
        )
        (self_attn_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
        (dropout_module): FairseqDropout()
        (activation_dropout_module): FairseqDropout()
        (fc1): Linear(in_features=512, out_features=2048, bias=True)
        (fc2): Linear(in_features=2048, out_features=512, bias=True)
        (final_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
      )
      (1): TransformerEncoderLayerBase(
        (self_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=512, out_features=512, bias=True)
          (v_proj): Linear(in_features=512, out_features=512, bias=True)
          (q_proj): Linear(in_features=512, out_features=512, bias=True)
          (out_proj): Linear(in_features=512, out_features=512, bias=True)
        )
        (self_attn_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
        (dropout_module): FairseqDropout()
        (activation_dropout_module): FairseqDropout()
        (fc1): Linear(in_features=512, out_features=2048, bias=True)
        (fc2): Linear(in_features=2048, out_features=512, bias=True)
        (final_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
      )
      (2): TransformerEncoderLayerBase(
        (self_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=512, out_features=512, bias=True)
          (v_proj): Linear(in_features=512, out_features=512, bias=True)
          (q_proj): Linear(in_features=512, out_features=512, bias=True)
          (out_proj): Linear(in_features=512, out_features=512, bias=True)
        )
        (self_attn_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
        (dropout_module): FairseqDropout()
        (activation_dropout_module): FairseqDropout()
        (fc1): Linear(in_features=512, out_features=2048, bias=True)
        (fc2): Linear(in_features=2048, out_features=512, bias=True)
        (final_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
      )
      (3): TransformerEncoderLayerBase(
        (self_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=512, out_features=512, bias=True)
          (v_proj): Linear(in_features=512, out_features=512, bias=True)
          (q_proj): Linear(in_features=512, out_features=512, bias=True)
          (out_proj): Linear(in_features=512, out_features=512, bias=True)
        )
        (self_attn_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
        (dropout_module): FairseqDropout()
        (activation_dropout_module): FairseqDropout()
        (fc1): Linear(in_features=512, out_features=2048, bias=True)
        (fc2): Linear(in_features=2048, out_features=512, bias=True)
        (final_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
      )
      (4): TransformerEncoderLayerBase(
        (self_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=512, out_features=512, bias=True)
          (v_proj): Linear(in_features=512, out_features=512, bias=True)
          (q_proj): Linear(in_features=512, out_features=512, bias=True)
          (out_proj): Linear(in_features=512, out_features=512, bias=True)
        )
        (self_attn_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
        (dropout_module): FairseqDropout()
        (activation_dropout_module): FairseqDropout()
        (fc1): Linear(in_features=512, out_features=2048, bias=True)
        (fc2): Linear(in_features=2048, out_features=512, bias=True)
        (final_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
      )
      (5): TransformerEncoderLayerBase(
        (self_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=512, out_features=512, bias=True)
          (v_proj): Linear(in_features=512, out_features=512, bias=True)
          (q_proj): Linear(in_features=512, out_features=512, bias=True)
          (out_proj): Linear(in_features=512, out_features=512, bias=True)
        )
        (self_attn_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
        (dropout_module): FairseqDropout()
        (activation_dropout_module): FairseqDropout()
        (fc1): Linear(in_features=512, out_features=2048, bias=True)
        (fc2): Linear(in_features=2048, out_features=512, bias=True)
        (final_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
      )
    )
  )
  (decoder): TransformerDecoderBase(
    (dropout_module): FairseqDropout()
    (embed_tokens): Embedding(49992, 512, padding_idx=1)
    (embed_positions): SinusoidalPositionalEmbedding()
    (layers): ModuleList(
      (0): TransformerDecoderLayerBase(
        (dropout_module): FairseqDropout()
        (self_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=512, out_features=512, bias=True)
          (v_proj): Linear(in_features=512, out_features=512, bias=True)
          (q_proj): Linear(in_features=512, out_features=512, bias=True)
          (out_proj): Linear(in_features=512, out_features=512, bias=True)
        )
        (activation_dropout_module): FairseqDropout()
        (self_attn_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
        (encoder_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=512, out_features=512, bias=True)
          (v_proj): Linear(in_features=512, out_features=512, bias=True)
          (q_proj): Linear(in_features=512, out_features=512, bias=True)
          (out_proj): Linear(in_features=512, out_features=512, bias=True)
        )
        (encoder_attn_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
        (fc1): Linear(in_features=512, out_features=2048, bias=True)
        (fc2): Linear(in_features=2048, out_features=512, bias=True)
        (final_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
      )
      (1): TransformerDecoderLayerBase(
        (dropout_module): FairseqDropout()
        (self_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=512, out_features=512, bias=True)
          (v_proj): Linear(in_features=512, out_features=512, bias=True)
          (q_proj): Linear(in_features=512, out_features=512, bias=True)
          (out_proj): Linear(in_features=512, out_features=512, bias=True)
        )
        (activation_dropout_module): FairseqDropout()
        (self_attn_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
        (encoder_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=512, out_features=512, bias=True)
          (v_proj): Linear(in_features=512, out_features=512, bias=True)
          (q_proj): Linear(in_features=512, out_features=512, bias=True)
          (out_proj): Linear(in_features=512, out_features=512, bias=True)
        )
        (encoder_attn_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
        (fc1): Linear(in_features=512, out_features=2048, bias=True)
        (fc2): Linear(in_features=2048, out_features=512, bias=True)
        (final_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
      )
      (2): TransformerDecoderLayerBase(
        (dropout_module): FairseqDropout()
        (self_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=512, out_features=512, bias=True)
          (v_proj): Linear(in_features=512, out_features=512, bias=True)
          (q_proj): Linear(in_features=512, out_features=512, bias=True)
          (out_proj): Linear(in_features=512, out_features=512, bias=True)
        )
        (activation_dropout_module): FairseqDropout()
        (self_attn_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
        (encoder_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=512, out_features=512, bias=True)
          (v_proj): Linear(in_features=512, out_features=512, bias=True)
          (q_proj): Linear(in_features=512, out_features=512, bias=True)
          (out_proj): Linear(in_features=512, out_features=512, bias=True)
        )
        (encoder_attn_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
        (fc1): Linear(in_features=512, out_features=2048, bias=True)
        (fc2): Linear(in_features=2048, out_features=512, bias=True)
        (final_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
      )
      (3): TransformerDecoderLayerBase(
        (dropout_module): FairseqDropout()
        (self_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=512, out_features=512, bias=True)
          (v_proj): Linear(in_features=512, out_features=512, bias=True)
          (q_proj): Linear(in_features=512, out_features=512, bias=True)
          (out_proj): Linear(in_features=512, out_features=512, bias=True)
        )
        (activation_dropout_module): FairseqDropout()
        (self_attn_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
        (encoder_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=512, out_features=512, bias=True)
          (v_proj): Linear(in_features=512, out_features=512, bias=True)
          (q_proj): Linear(in_features=512, out_features=512, bias=True)
          (out_proj): Linear(in_features=512, out_features=512, bias=True)
        )
        (encoder_attn_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
        (fc1): Linear(in_features=512, out_features=2048, bias=True)
        (fc2): Linear(in_features=2048, out_features=512, bias=True)
        (final_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
      )
      (4): TransformerDecoderLayerBase(
        (dropout_module): FairseqDropout()
        (self_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=512, out_features=512, bias=True)
          (v_proj): Linear(in_features=512, out_features=512, bias=True)
          (q_proj): Linear(in_features=512, out_features=512, bias=True)
          (out_proj): Linear(in_features=512, out_features=512, bias=True)
        )
        (activation_dropout_module): FairseqDropout()
        (self_attn_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
        (encoder_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=512, out_features=512, bias=True)
          (v_proj): Linear(in_features=512, out_features=512, bias=True)
          (q_proj): Linear(in_features=512, out_features=512, bias=True)
          (out_proj): Linear(in_features=512, out_features=512, bias=True)
        )
        (encoder_attn_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
        (fc1): Linear(in_features=512, out_features=2048, bias=True)
        (fc2): Linear(in_features=2048, out_features=512, bias=True)
        (final_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
      )
      (5): TransformerDecoderLayerBase(
        (dropout_module): FairseqDropout()
        (self_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=512, out_features=512, bias=True)
          (v_proj): Linear(in_features=512, out_features=512, bias=True)
          (q_proj): Linear(in_features=512, out_features=512, bias=True)
          (out_proj): Linear(in_features=512, out_features=512, bias=True)
        )
        (activation_dropout_module): FairseqDropout()
        (self_attn_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
        (encoder_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=512, out_features=512, bias=True)
          (v_proj): Linear(in_features=512, out_features=512, bias=True)
          (q_proj): Linear(in_features=512, out_features=512, bias=True)
          (out_proj): Linear(in_features=512, out_features=512, bias=True)
        )
        (encoder_attn_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
        (fc1): Linear(in_features=512, out_features=2048, bias=True)
        (fc2): Linear(in_features=2048, out_features=512, bias=True)
        (final_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
      )
    )
    (output_projection): Linear(in_features=512, out_features=49992, bias=False)
  )
)
2022-03-20 18:50:48 | INFO | fairseq_cli.train | task: TranslationTask
2022-03-20 18:50:48 | INFO | fairseq_cli.train | model: TransformerModel
2022-03-20 18:50:48 | INFO | fairseq_cli.train | criterion: LabelSmoothedCrossEntropyCriterion
2022-03-20 18:50:48 | INFO | fairseq_cli.train | num. shared model params: 69,734,400 (num. trained: 69,734,400)
2022-03-20 18:50:48 | INFO | fairseq_cli.train | num. expert model params: 0 (num. trained: 0)
2022-03-20 18:50:48 | INFO | fairseq.data.data_utils | loaded 11,332 examples from: data/xsum-summarizer/valid.source-target.source
2022-03-20 18:50:48 | INFO | fairseq.data.data_utils | loaded 11,332 examples from: data/xsum-summarizer/valid.source-target.target
2022-03-20 18:50:48 | INFO | fairseq.tasks.translation | data/xsum-summarizer valid source-target 11332 examples
2022-03-20 18:51:25 | INFO | fairseq.trainer | detected shared parameter: encoder.embed_tokens.weight <- decoder.embed_tokens.weight
2022-03-20 18:51:25 | INFO | fairseq.trainer | detected shared parameter: encoder.embed_tokens.weight <- decoder.output_projection.weight
2022-03-20 18:51:25 | INFO | fairseq.utils | ***********************CUDA enviroments for all 1 workers***********************
2022-03-20 18:51:25 | INFO | fairseq.utils | rank   0: capabilities =  7.5  ; total memory = 10.761 GB ; name = NVIDIA GeForce RTX 2080 Ti              
2022-03-20 18:51:25 | INFO | fairseq.utils | ***********************CUDA enviroments for all 1 workers***********************
2022-03-20 18:51:25 | INFO | fairseq_cli.train | training on 1 devices (GPUs/TPUs)
2022-03-20 18:51:25 | INFO | fairseq_cli.train | max tokens per device = 4096 and max sentences per device = None
2022-03-20 18:51:25 | INFO | fairseq.trainer | Preparing to load checkpoint checkpoints/summarization_model/10/checkpoint_best.pt
2022-03-20 18:51:25 | INFO | fairseq.trainer | NOTE: your device may support faster training with --fp16 or --amp
2022-03-20 18:51:26 | INFO | fairseq.trainer | Loaded checkpoint checkpoints/summarization_model/10/checkpoint_best.pt (epoch 24 @ 9062 updates)
2022-03-20 18:51:26 | INFO | fairseq.trainer | loading train data for epoch 24
2022-03-20 18:51:26 | INFO | fairseq.data.data_utils | loaded 204,045 examples from: data/xsum-summarizer/train.source-target.source
2022-03-20 18:51:26 | INFO | fairseq.data.data_utils | loaded 204,045 examples from: data/xsum-summarizer/train.source-target.target
2022-03-20 18:51:26 | INFO | fairseq.tasks.translation | data/xsum-summarizer train source-target 204045 examples
2022-03-20 18:51:27 | INFO | fairseq.data.iterators | grouped total_num_itrs = 394
2022-03-20 18:51:27 | INFO | fairseq.trainer | begin training epoch 24
2022-03-20 18:51:27 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-20 18:55:20 | INFO | train_inner | epoch 024:     38 / 394 loss=6.618, nll_loss=5.213, ppl=37.08, wps=2190.1, ups=0.17, wpb=13164.3, bsz=524.7, num_updates=9100, lr=4.64095e-05, gnorm=0.926, train_wall=224, gb_free=7.2, wall=235
2022-03-20 19:12:11 | INFO | train_inner | epoch 024:    138 / 394 loss=6.613, nll_loss=5.207, ppl=36.94, wps=1299.8, ups=0.1, wpb=13142.3, bsz=524.9, num_updates=9200, lr=4.61566e-05, gnorm=0.909, train_wall=594, gb_free=7.2, wall=1246
2022-03-20 19:24:42 | INFO | train_inner | epoch 024:    238 / 394 loss=6.61, nll_loss=5.203, ppl=36.84, wps=1714.5, ups=0.13, wpb=12873.2, bsz=513.9, num_updates=9300, lr=4.59078e-05, gnorm=0.923, train_wall=601, gb_free=7.2, wall=1997
2022-03-20 19:35:52 | INFO | train_inner | epoch 024:    338 / 394 loss=6.598, nll_loss=5.189, ppl=36.48, wps=1917.4, ups=0.15, wpb=12853.3, bsz=515.3, num_updates=9400, lr=4.5663e-05, gnorm=0.923, train_wall=593, gb_free=7.2, wall=2667
2022-03-20 19:41:55 | INFO | fairseq_cli.train | begin validation on "valid" subset
/cluster/work/cotterell/liam/fairseq/fairseq/utils.py:374: UserWarning: amp_C fused kernels unavailable, disabling multi_tensor_l2norm; you may get better performance by installing NVIDIA's apex library
  warnings.warn(
2022-03-20 19:42:51 | INFO | valid | epoch 024 | valid on 'valid' subset | loss 6.656 | nll_loss 5.198 | ppl 36.72 | wps 5313.5 | wpb 203.8 | bsz 8.1 | num_updates 9456 | best_loss 6.656
2022-03-20 19:42:51 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 24 @ 9456 updates
2022-03-20 19:42:51 | INFO | fairseq.trainer | Saving checkpoint to /cluster/work/cotterell/liam/master-thesis/checkpoints/summarization_model/11/checkpoint_best.pt
2022-03-20 19:42:54 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/work/cotterell/liam/master-thesis/checkpoints/summarization_model/11/checkpoint_best.pt
2022-03-20 19:42:54 | INFO | fairseq.checkpoint_utils | Saved checkpoint checkpoints/summarization_model/11/checkpoint_best.pt (epoch 24 @ 9456 updates, score 6.656) (writing took 2.7245526295155287 seconds)
2022-03-20 19:42:54 | INFO | fairseq_cli.train | end of epoch 24 (average epoch stats below)
2022-03-20 19:42:54 | INFO | train | epoch 024 | loss 6.61 | nll_loss 5.202 | ppl 36.82 | wps 1655.4 | ups 0.13 | wpb 12960 | bsz 517.9 | num_updates 9456 | lr 4.55276e-05 | gnorm 0.918 | train_wall 2342 | gb_free 7.2 | wall 3089
2022-03-20 19:42:54 | INFO | fairseq.data.iterators | grouped total_num_itrs = 394
2022-03-20 19:42:54 | INFO | fairseq.trainer | begin training epoch 25
2022-03-20 19:42:54 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-20 19:47:42 | INFO | train_inner | epoch 025:     44 / 394 loss=6.595, nll_loss=5.185, ppl=36.38, wps=1817, ups=0.14, wpb=12894, bsz=515.3, num_updates=9500, lr=4.5422e-05, gnorm=0.9, train_wall=591, gb_free=7.2, wall=3377
2022-03-20 20:03:38 | INFO | train_inner | epoch 025:    144 / 394 loss=6.555, nll_loss=5.14, ppl=35.25, wps=1360.5, ups=0.1, wpb=13009.1, bsz=520.3, num_updates=9600, lr=4.51848e-05, gnorm=0.903, train_wall=602, gb_free=7.2, wall=4333
2022-03-20 20:16:56 | INFO | train_inner | epoch 025:    244 / 394 loss=6.568, nll_loss=5.154, ppl=35.6, wps=1633.9, ups=0.13, wpb=13039.5, bsz=520.8, num_updates=9700, lr=4.49513e-05, gnorm=0.908, train_wall=595, gb_free=7.2, wall=5131
2022-03-20 20:28:02 | INFO | train_inner | epoch 025:    344 / 394 loss=6.549, nll_loss=5.131, ppl=35.05, wps=1939.7, ups=0.15, wpb=12924.3, bsz=518, num_updates=9800, lr=4.47214e-05, gnorm=0.926, train_wall=594, gb_free=7.2, wall=5798
2022-03-20 20:33:28 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-20 20:34:24 | INFO | valid | epoch 025 | valid on 'valid' subset | loss 6.618 | nll_loss 5.145 | ppl 35.38 | wps 5297.8 | wpb 203.8 | bsz 8.1 | num_updates 9850 | best_loss 6.618
2022-03-20 20:34:24 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 25 @ 9850 updates
2022-03-20 20:34:24 | INFO | fairseq.trainer | Saving checkpoint to /cluster/work/cotterell/liam/master-thesis/checkpoints/summarization_model/11/checkpoint_best.pt
2022-03-20 20:34:26 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/work/cotterell/liam/master-thesis/checkpoints/summarization_model/11/checkpoint_best.pt
2022-03-20 20:34:26 | INFO | fairseq.checkpoint_utils | Saved checkpoint checkpoints/summarization_model/11/checkpoint_best.pt (epoch 25 @ 9850 updates, score 6.618) (writing took 2.748964412137866 seconds)
2022-03-20 20:34:26 | INFO | fairseq_cli.train | end of epoch 25 (average epoch stats below)
2022-03-20 20:34:26 | INFO | train | epoch 025 | loss 6.56 | nll_loss 5.145 | ppl 35.37 | wps 1651.2 | ups 0.13 | wpb 12960 | bsz 517.9 | num_updates 9850 | lr 4.46077e-05 | gnorm 0.916 | train_wall 2341 | gb_free 7.2 | wall 6182
2022-03-20 20:34:28 | INFO | fairseq.data.iterators | grouped total_num_itrs = 394
2022-03-20 20:34:28 | INFO | fairseq.trainer | begin training epoch 26
2022-03-20 20:34:28 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-20 20:40:01 | INFO | train_inner | epoch 026:     50 / 394 loss=6.543, nll_loss=5.124, ppl=34.88, wps=1796.9, ups=0.14, wpb=12909.5, bsz=512.8, num_updates=9900, lr=4.44949e-05, gnorm=0.947, train_wall=585, gb_free=6.5, wall=6516
2022-03-20 20:51:02 | INFO | train_inner | epoch 026:    150 / 394 loss=6.51, nll_loss=5.086, ppl=33.97, wps=1961, ups=0.15, wpb=12963.2, bsz=520.5, num_updates=10000, lr=4.42719e-05, gnorm=0.911, train_wall=603, gb_free=7.2, wall=7177
2022-03-20 21:01:43 | INFO | train_inner | epoch 026:    250 / 394 loss=6.507, nll_loss=5.083, ppl=33.89, wps=2023.9, ups=0.16, wpb=12990.1, bsz=519.4, num_updates=10100, lr=4.40522e-05, gnorm=0.914, train_wall=597, gb_free=7.2, wall=7819
2022-03-20 21:12:27 | INFO | train_inner | epoch 026:    350 / 394 loss=6.518, nll_loss=5.095, ppl=34.17, wps=2016.3, ups=0.16, wpb=12970.5, bsz=517, num_updates=10200, lr=4.38357e-05, gnorm=0.906, train_wall=587, gb_free=7.2, wall=8462
2022-03-20 21:17:06 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-20 21:18:01 | INFO | valid | epoch 026 | valid on 'valid' subset | loss 6.584 | nll_loss 5.108 | ppl 34.48 | wps 5373.5 | wpb 203.8 | bsz 8.1 | num_updates 10244 | best_loss 6.584
2022-03-20 21:18:01 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 26 @ 10244 updates
2022-03-20 21:18:01 | INFO | fairseq.trainer | Saving checkpoint to /cluster/work/cotterell/liam/master-thesis/checkpoints/summarization_model/11/checkpoint_best.pt
2022-03-20 21:18:04 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/work/cotterell/liam/master-thesis/checkpoints/summarization_model/11/checkpoint_best.pt
2022-03-20 21:18:04 | INFO | fairseq.checkpoint_utils | Saved checkpoint checkpoints/summarization_model/11/checkpoint_best.pt (epoch 26 @ 10244 updates, score 6.584) (writing took 3.5680834371596575 seconds)
2022-03-20 21:18:04 | INFO | fairseq_cli.train | end of epoch 26 (average epoch stats below)
2022-03-20 21:18:04 | INFO | train | epoch 026 | loss 6.513 | nll_loss 5.089 | ppl 34.04 | wps 1950.4 | ups 0.15 | wpb 12960 | bsz 517.9 | num_updates 10244 | lr 4.37415e-05 | gnorm 0.919 | train_wall 2347 | gb_free 7.2 | wall 8800
2022-03-20 21:18:05 | INFO | fairseq.data.iterators | grouped total_num_itrs = 394
2022-03-20 21:18:05 | INFO | fairseq.trainer | begin training epoch 27
2022-03-20 21:18:05 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-20 21:26:23 | INFO | train_inner | epoch 027:     56 / 394 loss=6.492, nll_loss=5.065, ppl=33.47, wps=1542.4, ups=0.12, wpb=12900.6, bsz=514.6, num_updates=10300, lr=4.36224e-05, gnorm=0.942, train_wall=600, gb_free=7.2, wall=9299
2022-03-20 21:42:35 | INFO | train_inner | epoch 027:    156 / 394 loss=6.463, nll_loss=5.032, ppl=32.72, wps=1350.3, ups=0.1, wpb=13118.6, bsz=525.8, num_updates=10400, lr=4.34122e-05, gnorm=0.921, train_wall=600, gb_free=7.2, wall=10270
2022-03-20 21:54:32 | INFO | train_inner | epoch 027:    256 / 394 loss=6.476, nll_loss=5.046, ppl=33.04, wps=1789.7, ups=0.14, wpb=12834.7, bsz=512.9, num_updates=10500, lr=4.32049e-05, gnorm=0.925, train_wall=596, gb_free=7.2, wall=10987
2022-03-20 22:05:41 | INFO | train_inner | epoch 027:    356 / 394 loss=6.459, nll_loss=5.026, ppl=32.58, wps=1948.5, ups=0.15, wpb=13029.8, bsz=519.1, num_updates=10600, lr=4.30007e-05, gnorm=0.919, train_wall=598, gb_free=7.2, wall=11656
2022-03-20 22:09:34 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-20 22:10:30 | INFO | valid | epoch 027 | valid on 'valid' subset | loss 6.546 | nll_loss 5.067 | ppl 33.52 | wps 5384.3 | wpb 203.8 | bsz 8.1 | num_updates 10638 | best_loss 6.546
2022-03-20 22:10:30 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 27 @ 10638 updates
2022-03-20 22:10:30 | INFO | fairseq.trainer | Saving checkpoint to /cluster/work/cotterell/liam/master-thesis/checkpoints/summarization_model/11/checkpoint_best.pt
2022-03-20 22:10:34 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/work/cotterell/liam/master-thesis/checkpoints/summarization_model/11/checkpoint_best.pt
2022-03-20 22:10:34 | INFO | fairseq.checkpoint_utils | Saved checkpoint checkpoints/summarization_model/11/checkpoint_best.pt (epoch 27 @ 10638 updates, score 6.546) (writing took 3.8862219341099262 seconds)
2022-03-20 22:10:34 | INFO | fairseq_cli.train | end of epoch 27 (average epoch stats below)
2022-03-20 22:10:34 | INFO | train | epoch 027 | loss 6.468 | nll_loss 5.038 | ppl 32.84 | wps 1621.4 | ups 0.13 | wpb 12960 | bsz 517.9 | num_updates 10638 | lr 4.29238e-05 | gnorm 0.925 | train_wall 2349 | gb_free 7.2 | wall 11949
2022-03-20 22:10:35 | INFO | fairseq.data.iterators | grouped total_num_itrs = 394
2022-03-20 22:10:35 | INFO | fairseq.trainer | begin training epoch 28
2022-03-20 22:10:35 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-20 22:20:15 | INFO | train_inner | epoch 028:     62 / 394 loss=6.441, nll_loss=5.006, ppl=32.14, wps=1488.2, ups=0.11, wpb=13008.9, bsz=521.5, num_updates=10700, lr=4.27992e-05, gnorm=0.926, train_wall=592, gb_free=7.2, wall=12530
2022-03-20 22:36:34 | INFO | train_inner | epoch 028:    162 / 394 loss=6.429, nll_loss=4.992, ppl=31.83, wps=1319.4, ups=0.1, wpb=12919.5, bsz=515.3, num_updates=10800, lr=4.26006e-05, gnorm=0.913, train_wall=604, gb_free=7.2, wall=13509
2022-03-20 22:48:41 | INFO | train_inner | epoch 028:    262 / 394 loss=6.432, nll_loss=4.994, ppl=31.87, wps=1768.1, ups=0.14, wpb=12860.6, bsz=513.8, num_updates=10900, lr=4.24048e-05, gnorm=0.92, train_wall=603, gb_free=6.8, wall=14237
2022-03-20 23:00:02 | INFO | train_inner | epoch 028:    362 / 394 loss=6.42, nll_loss=4.981, ppl=31.58, wps=1920.2, ups=0.15, wpb=13064.1, bsz=521.2, num_updates=11000, lr=4.22116e-05, gnorm=0.921, train_wall=605, gb_free=7.2, wall=14917
2022-03-20 23:03:21 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-20 23:04:15 | INFO | valid | epoch 028 | valid on 'valid' subset | loss 6.539 | nll_loss 5.055 | ppl 33.25 | wps 5407 | wpb 203.8 | bsz 8.1 | num_updates 11032 | best_loss 6.539
2022-03-20 23:04:15 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 28 @ 11032 updates
2022-03-20 23:04:15 | INFO | fairseq.trainer | Saving checkpoint to /cluster/work/cotterell/liam/master-thesis/checkpoints/summarization_model/11/checkpoint_best.pt
2022-03-20 23:04:18 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/work/cotterell/liam/master-thesis/checkpoints/summarization_model/11/checkpoint_best.pt
2022-03-20 23:04:18 | INFO | fairseq.checkpoint_utils | Saved checkpoint checkpoints/summarization_model/11/checkpoint_best.pt (epoch 28 @ 11032 updates, score 6.539) (writing took 2.5247078761458397 seconds)
2022-03-20 23:04:18 | INFO | fairseq_cli.train | end of epoch 28 (average epoch stats below)
2022-03-20 23:04:18 | INFO | train | epoch 028 | loss 6.425 | nll_loss 4.987 | ppl 31.72 | wps 1583.7 | ups 0.12 | wpb 12960 | bsz 517.9 | num_updates 11032 | lr 4.21503e-05 | gnorm 0.92 | train_wall 2370 | gb_free 7.2 | wall 15173
2022-03-20 23:04:18 | INFO | fairseq.data.iterators | grouped total_num_itrs = 394
2022-03-20 23:04:18 | INFO | fairseq.trainer | begin training epoch 29
2022-03-20 23:04:18 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-20 23:11:32 | INFO | train_inner | epoch 029:     68 / 394 loss=6.398, nll_loss=4.955, ppl=31.02, wps=1856.2, ups=0.14, wpb=12821.5, bsz=513.1, num_updates=11100, lr=4.2021e-05, gnorm=0.927, train_wall=593, gb_free=7.2, wall=15608
2022-03-20 23:22:25 | INFO | train_inner | epoch 029:    168 / 394 loss=6.396, nll_loss=4.953, ppl=30.97, wps=1993.2, ups=0.15, wpb=13007.8, bsz=518.2, num_updates=11200, lr=4.1833e-05, gnorm=0.927, train_wall=601, gb_free=5.5, wall=16260
2022-03-20 23:33:08 | INFO | train_inner | epoch 029:    268 / 394 loss=6.393, nll_loss=4.95, ppl=30.9, wps=2034.2, ups=0.16, wpb=13083.2, bsz=521, num_updates=11300, lr=4.16475e-05, gnorm=0.921, train_wall=600, gb_free=7.2, wall=16904
2022-03-20 23:43:52 | INFO | train_inner | epoch 029:    368 / 394 loss=6.378, nll_loss=4.932, ppl=30.52, wps=2001.5, ups=0.16, wpb=12882.9, bsz=517.1, num_updates=11400, lr=4.14644e-05, gnorm=0.916, train_wall=589, gb_free=7.2, wall=17547
2022-03-20 23:46:38 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-20 23:47:33 | INFO | valid | epoch 029 | valid on 'valid' subset | loss 6.507 | nll_loss 5.015 | ppl 32.35 | wps 5385 | wpb 203.8 | bsz 8.1 | num_updates 11426 | best_loss 6.507
2022-03-20 23:47:33 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 29 @ 11426 updates
2022-03-20 23:47:33 | INFO | fairseq.trainer | Saving checkpoint to /cluster/work/cotterell/liam/master-thesis/checkpoints/summarization_model/11/checkpoint_best.pt
2022-03-20 23:47:36 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/work/cotterell/liam/master-thesis/checkpoints/summarization_model/11/checkpoint_best.pt
2022-03-20 23:47:36 | INFO | fairseq.checkpoint_utils | Saved checkpoint checkpoints/summarization_model/11/checkpoint_best.pt (epoch 29 @ 11426 updates, score 6.507) (writing took 2.8840468022972345 seconds)
2022-03-20 23:47:36 | INFO | fairseq_cli.train | end of epoch 29 (average epoch stats below)
2022-03-20 23:47:36 | INFO | train | epoch 029 | loss 6.387 | nll_loss 4.942 | ppl 30.75 | wps 1965.6 | ups 0.15 | wpb 12960 | bsz 517.9 | num_updates 11426 | lr 4.14172e-05 | gnorm 0.922 | train_wall 2344 | gb_free 7.2 | wall 17771
2022-03-20 23:47:37 | INFO | fairseq.data.iterators | grouped total_num_itrs = 394
2022-03-20 23:47:37 | INFO | fairseq.trainer | begin training epoch 30
2022-03-20 23:47:37 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-21 00:01:04 | INFO | train_inner | epoch 030:     74 / 394 loss=6.362, nll_loss=4.913, ppl=30.14, wps=1229.3, ups=0.1, wpb=12692.1, bsz=506.1, num_updates=11500, lr=4.12837e-05, gnorm=0.928, train_wall=601, gb_free=7.2, wall=18580
2022-03-21 00:14:08 | INFO | train_inner | epoch 030:    174 / 394 loss=6.344, nll_loss=4.893, ppl=29.7, wps=1645.7, ups=0.13, wpb=12898.4, bsz=517.3, num_updates=11600, lr=4.11054e-05, gnorm=0.922, train_wall=599, gb_free=7.2, wall=19364
2022-03-21 00:25:20 | INFO | train_inner | epoch 030:    274 / 394 loss=6.351, nll_loss=4.901, ppl=29.87, wps=1950.5, ups=0.15, wpb=13112, bsz=522.2, num_updates=11700, lr=4.09294e-05, gnorm=0.909, train_wall=589, gb_free=7.2, wall=20036
2022-03-21 00:36:24 | INFO | train_inner | epoch 030:    374 / 394 loss=6.343, nll_loss=4.891, ppl=29.66, wps=1974.1, ups=0.15, wpb=13104.7, bsz=523.9, num_updates=11800, lr=4.07556e-05, gnorm=0.917, train_wall=601, gb_free=7.2, wall=20700
2022-03-21 00:38:23 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-21 00:39:18 | INFO | valid | epoch 030 | valid on 'valid' subset | loss 6.482 | nll_loss 4.988 | ppl 31.72 | wps 5384.1 | wpb 203.8 | bsz 8.1 | num_updates 11820 | best_loss 6.482
2022-03-21 00:39:18 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 30 @ 11820 updates
2022-03-21 00:39:18 | INFO | fairseq.trainer | Saving checkpoint to /cluster/work/cotterell/liam/master-thesis/checkpoints/summarization_model/11/checkpoint_best.pt
2022-03-21 00:39:21 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/work/cotterell/liam/master-thesis/checkpoints/summarization_model/11/checkpoint_best.pt
2022-03-21 00:39:21 | INFO | fairseq.checkpoint_utils | Saved checkpoint checkpoints/summarization_model/11/checkpoint_best.pt (epoch 30 @ 11820 updates, score 6.482) (writing took 2.8776398804038763 seconds)
2022-03-21 00:39:21 | INFO | fairseq_cli.train | end of epoch 30 (average epoch stats below)
2022-03-21 00:39:21 | INFO | train | epoch 030 | loss 6.347 | nll_loss 4.896 | ppl 29.78 | wps 1644.4 | ups 0.13 | wpb 12960 | bsz 517.9 | num_updates 11820 | lr 4.07211e-05 | gnorm 0.918 | train_wall 2351 | gb_free 7.2 | wall 20876
2022-03-21 00:39:22 | INFO | fairseq.data.iterators | grouped total_num_itrs = 394
2022-03-21 00:39:22 | INFO | fairseq.trainer | begin training epoch 31
2022-03-21 00:39:22 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-21 00:52:20 | INFO | train_inner | epoch 031:     80 / 394 loss=6.301, nll_loss=4.842, ppl=28.69, wps=1346.5, ups=0.1, wpb=12866.7, bsz=517.1, num_updates=11900, lr=4.0584e-05, gnorm=0.922, train_wall=594, gb_free=7.2, wall=21655
2022-03-21 01:07:27 | INFO | train_inner | epoch 031:    180 / 394 loss=6.317, nll_loss=4.861, ppl=29.06, wps=1410.8, ups=0.11, wpb=12799, bsz=510.5, num_updates=12000, lr=4.04145e-05, gnorm=0.926, train_wall=603, gb_free=7.2, wall=22562
2022-03-21 01:19:17 | INFO | train_inner | epoch 031:    280 / 394 loss=6.326, nll_loss=4.87, ppl=29.25, wps=1838.3, ups=0.14, wpb=13055.3, bsz=519, num_updates=12100, lr=4.02472e-05, gnorm=0.927, train_wall=601, gb_free=7.2, wall=23273
2022-03-21 01:30:13 | INFO | train_inner | epoch 031:    380 / 394 loss=6.301, nll_loss=4.842, ppl=28.67, wps=1995.2, ups=0.15, wpb=13092.2, bsz=525.2, num_updates=12200, lr=4.00819e-05, gnorm=0.912, train_wall=590, gb_free=7.2, wall=23929
2022-03-21 01:31:34 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-21 01:32:29 | INFO | valid | epoch 031 | valid on 'valid' subset | loss 6.45 | nll_loss 4.95 | ppl 30.91 | wps 5353.5 | wpb 203.8 | bsz 8.1 | num_updates 12214 | best_loss 6.45
2022-03-21 01:32:29 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 31 @ 12214 updates
2022-03-21 01:32:29 | INFO | fairseq.trainer | Saving checkpoint to /cluster/work/cotterell/liam/master-thesis/checkpoints/summarization_model/11/checkpoint_best.pt
2022-03-21 01:32:32 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/work/cotterell/liam/master-thesis/checkpoints/summarization_model/11/checkpoint_best.pt
2022-03-21 01:32:32 | INFO | fairseq.checkpoint_utils | Saved checkpoint checkpoints/summarization_model/11/checkpoint_best.pt (epoch 31 @ 12214 updates, score 6.45) (writing took 2.7918847762048244 seconds)
2022-03-21 01:32:32 | INFO | fairseq_cli.train | end of epoch 31 (average epoch stats below)
2022-03-21 01:32:32 | INFO | train | epoch 031 | loss 6.311 | nll_loss 4.854 | ppl 28.91 | wps 1600.1 | ups 0.12 | wpb 12960 | bsz 517.9 | num_updates 12214 | lr 4.00589e-05 | gnorm 0.924 | train_wall 2354 | gb_free 7.2 | wall 24068
2022-03-21 01:32:33 | INFO | fairseq.data.iterators | grouped total_num_itrs = 394
2022-03-21 01:32:33 | INFO | fairseq.trainer | begin training epoch 32
2022-03-21 01:32:33 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-21 01:46:15 | INFO | train_inner | epoch 032:     86 / 394 loss=6.285, nll_loss=4.823, ppl=28.31, wps=1341.8, ups=0.1, wpb=12904.2, bsz=514.5, num_updates=12300, lr=3.99186e-05, gnorm=0.934, train_wall=597, gb_free=7.2, wall=24890
2022-03-21 02:01:10 | INFO | train_inner | epoch 032:    186 / 394 loss=6.272, nll_loss=4.808, ppl=28.02, wps=1442.4, ups=0.11, wpb=12914.6, bsz=518.6, num_updates=12400, lr=3.97573e-05, gnorm=0.936, train_wall=611, gb_free=7.2, wall=25786
2022-03-21 02:12:49 | INFO | train_inner | epoch 032:    286 / 394 loss=6.269, nll_loss=4.804, ppl=27.94, wps=1869.8, ups=0.14, wpb=13073.1, bsz=524.4, num_updates=12500, lr=3.9598e-05, gnorm=0.915, train_wall=590, gb_free=6.6, wall=26485
2022-03-21 02:23:53 | INFO | train_inner | epoch 032:    386 / 394 loss=6.289, nll_loss=4.826, ppl=28.37, wps=1965.5, ups=0.15, wpb=13036.5, bsz=517, num_updates=12600, lr=3.94405e-05, gnorm=0.929, train_wall=597, gb_free=7.2, wall=27148
2022-03-21 02:24:44 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-21 02:25:40 | INFO | valid | epoch 032 | valid on 'valid' subset | loss 6.438 | nll_loss 4.934 | ppl 30.56 | wps 5370.1 | wpb 203.8 | bsz 8.1 | num_updates 12608 | best_loss 6.438
2022-03-21 02:25:40 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 32 @ 12608 updates
2022-03-21 02:25:40 | INFO | fairseq.trainer | Saving checkpoint to /cluster/work/cotterell/liam/master-thesis/checkpoints/summarization_model/11/checkpoint_best.pt
2022-03-21 02:25:43 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/work/cotterell/liam/master-thesis/checkpoints/summarization_model/11/checkpoint_best.pt
2022-03-21 02:25:43 | INFO | fairseq.checkpoint_utils | Saved checkpoint checkpoints/summarization_model/11/checkpoint_best.pt (epoch 32 @ 12608 updates, score 6.438) (writing took 2.8093514516949654 seconds)
2022-03-21 02:25:43 | INFO | fairseq_cli.train | end of epoch 32 (average epoch stats below)
2022-03-21 02:25:43 | INFO | train | epoch 032 | loss 6.277 | nll_loss 4.814 | ppl 28.13 | wps 1600.4 | ups 0.12 | wpb 12960 | bsz 517.9 | num_updates 12608 | lr 3.9428e-05 | gnorm 0.928 | train_wall 2368 | gb_free 7.2 | wall 27258
2022-03-21 02:25:44 | INFO | fairseq.data.iterators | grouped total_num_itrs = 394
2022-03-21 02:25:44 | INFO | fairseq.trainer | begin training epoch 33
2022-03-21 02:25:44 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-21 02:39:09 | INFO | train_inner | epoch 033:     92 / 394 loss=6.242, nll_loss=4.773, ppl=27.34, wps=1405.4, ups=0.11, wpb=12870.9, bsz=514.2, num_updates=12700, lr=3.92849e-05, gnorm=0.935, train_wall=593, gb_free=7.2, wall=28064
2022-03-21 02:54:11 | INFO | train_inner | epoch 033:    192 / 394 loss=6.242, nll_loss=4.773, ppl=27.33, wps=1448.3, ups=0.11, wpb=13071, bsz=523.5, num_updates=12800, lr=3.91312e-05, gnorm=0.941, train_wall=602, gb_free=7.2, wall=28967
2022-03-21 03:05:56 | INFO | train_inner | epoch 033:    292 / 394 loss=6.246, nll_loss=4.778, ppl=27.43, wps=1830.4, ups=0.14, wpb=12897.2, bsz=515.9, num_updates=12900, lr=3.89792e-05, gnorm=0.937, train_wall=601, gb_free=7.2, wall=29671
2022-03-21 03:16:58 | INFO | train_inner | epoch 033:    392 / 394 loss=6.249, nll_loss=4.78, ppl=27.47, wps=1958.7, ups=0.15, wpb=12970.6, bsz=517.4, num_updates=13000, lr=3.8829e-05, gnorm=0.932, train_wall=599, gb_free=7.2, wall=30333
2022-03-21 03:17:05 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-21 03:18:00 | INFO | valid | epoch 033 | valid on 'valid' subset | loss 6.406 | nll_loss 4.894 | ppl 29.72 | wps 5377.3 | wpb 203.8 | bsz 8.1 | num_updates 13002 | best_loss 6.406
2022-03-21 03:18:00 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 33 @ 13002 updates
2022-03-21 03:18:00 | INFO | fairseq.trainer | Saving checkpoint to /cluster/work/cotterell/liam/master-thesis/checkpoints/summarization_model/11/checkpoint_best.pt
2022-03-21 03:18:03 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/work/cotterell/liam/master-thesis/checkpoints/summarization_model/11/checkpoint_best.pt
2022-03-21 03:18:03 | INFO | fairseq.checkpoint_utils | Saved checkpoint checkpoints/summarization_model/11/checkpoint_best.pt (epoch 33 @ 13002 updates, score 6.406) (writing took 2.757522689178586 seconds)
2022-03-21 03:18:03 | INFO | fairseq_cli.train | end of epoch 33 (average epoch stats below)
2022-03-21 03:18:03 | INFO | train | epoch 033 | loss 6.244 | nll_loss 4.775 | ppl 27.39 | wps 1626.3 | ups 0.13 | wpb 12960 | bsz 517.9 | num_updates 13002 | lr 3.8826e-05 | gnorm 0.936 | train_wall 2352 | gb_free 7.2 | wall 30398
2022-03-21 03:18:04 | INFO | fairseq.data.iterators | grouped total_num_itrs = 394
2022-03-21 03:18:04 | INFO | fairseq.trainer | begin training epoch 34
2022-03-21 03:18:04 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-21 03:28:45 | INFO | train_inner | epoch 034:     98 / 394 loss=6.216, nll_loss=4.743, ppl=26.77, wps=1812.9, ups=0.14, wpb=12824.8, bsz=512.9, num_updates=13100, lr=3.86805e-05, gnorm=0.941, train_wall=594, gb_free=7.2, wall=31041
2022-03-21 03:39:23 | INFO | train_inner | epoch 034:    198 / 394 loss=6.208, nll_loss=4.733, ppl=26.6, wps=1989.3, ups=0.16, wpb=12693, bsz=507.8, num_updates=13200, lr=3.85337e-05, gnorm=0.94, train_wall=589, gb_free=7.2, wall=31679
2022-03-21 03:50:15 | INFO | train_inner | epoch 034:    298 / 394 loss=6.222, nll_loss=4.749, ppl=26.88, wps=2009.2, ups=0.15, wpb=13093.4, bsz=521.8, num_updates=13300, lr=3.83886e-05, gnorm=0.932, train_wall=595, gb_free=7.2, wall=32331
2022-03-21 04:00:39 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-21 04:01:34 | INFO | valid | epoch 034 | valid on 'valid' subset | loss 6.393 | nll_loss 4.881 | ppl 29.47 | wps 5381.2 | wpb 203.8 | bsz 8.1 | num_updates 13396 | best_loss 6.393
2022-03-21 04:01:34 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 34 @ 13396 updates
2022-03-21 04:01:34 | INFO | fairseq.trainer | Saving checkpoint to /cluster/work/cotterell/liam/master-thesis/checkpoints/summarization_model/11/checkpoint_best.pt
2022-03-21 04:01:41 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/work/cotterell/liam/master-thesis/checkpoints/summarization_model/11/checkpoint_best.pt
2022-03-21 04:01:41 | INFO | fairseq.checkpoint_utils | Saved checkpoint checkpoints/summarization_model/11/checkpoint_best.pt (epoch 34 @ 13396 updates, score 6.393) (writing took 6.521714191883802 seconds)
2022-03-21 04:01:41 | INFO | fairseq_cli.train | end of epoch 34 (average epoch stats below)
2022-03-21 04:01:41 | INFO | train | epoch 034 | loss 6.213 | nll_loss 4.738 | ppl 26.69 | wps 1950.2 | ups 0.15 | wpb 12960 | bsz 517.9 | num_updates 13396 | lr 3.82508e-05 | gnorm 0.933 | train_wall 2344 | gb_free 7.2 | wall 33016
2022-03-21 04:01:41 | INFO | fairseq.data.iterators | grouped total_num_itrs = 394
2022-03-21 04:01:41 | INFO | fairseq.trainer | begin training epoch 35
2022-03-21 04:01:41 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-21 04:02:06 | INFO | train_inner | epoch 035:      4 / 394 loss=6.204, nll_loss=4.728, ppl=26.49, wps=1849.5, ups=0.14, wpb=13144, bsz=525.4, num_updates=13400, lr=3.82451e-05, gnorm=0.932, train_wall=596, gb_free=7.2, wall=33041
2022-03-21 04:19:23 | INFO | train_inner | epoch 035:    104 / 394 loss=6.185, nll_loss=4.707, ppl=26.12, wps=1250.6, ups=0.1, wpb=12970.4, bsz=517.9, num_updates=13500, lr=3.81032e-05, gnorm=0.936, train_wall=607, gb_free=7.2, wall=34078
2022-03-21 04:34:26 | INFO | train_inner | epoch 035:    204 / 394 loss=6.168, nll_loss=4.686, ppl=25.75, wps=1440, ups=0.11, wpb=13007.8, bsz=522, num_updates=13600, lr=3.79628e-05, gnorm=0.945, train_wall=602, gb_free=7.2, wall=34982
2022-03-21 04:46:21 | INFO | train_inner | epoch 035:    304 / 394 loss=6.2, nll_loss=4.723, ppl=26.41, wps=1802.1, ups=0.14, wpb=12882.5, bsz=513.4, num_updates=13700, lr=3.7824e-05, gnorm=0.946, train_wall=596, gb_free=6, wall=35697
2022-03-21 04:56:05 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-21 04:57:01 | INFO | valid | epoch 035 | valid on 'valid' subset | loss 6.379 | nll_loss 4.858 | ppl 29.01 | wps 5297.5 | wpb 203.8 | bsz 8.1 | num_updates 13790 | best_loss 6.379
2022-03-21 04:57:01 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 35 @ 13790 updates
2022-03-21 04:57:01 | INFO | fairseq.trainer | Saving checkpoint to /cluster/work/cotterell/liam/master-thesis/checkpoints/summarization_model/11/checkpoint_best.pt
2022-03-21 04:57:04 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/work/cotterell/liam/master-thesis/checkpoints/summarization_model/11/checkpoint_best.pt
2022-03-21 04:57:05 | INFO | fairseq.checkpoint_utils | Saved checkpoint checkpoints/summarization_model/11/checkpoint_best.pt (epoch 35 @ 13790 updates, score 6.379) (writing took 3.1865752823650837 seconds)
2022-03-21 04:57:05 | INFO | fairseq_cli.train | end of epoch 35 (average epoch stats below)
2022-03-21 04:57:05 | INFO | train | epoch 035 | loss 6.183 | nll_loss 4.704 | ppl 26.06 | wps 1536.3 | ups 0.12 | wpb 12960 | bsz 517.9 | num_updates 13790 | lr 3.77004e-05 | gnorm 0.941 | train_wall 2365 | gb_free 7.2 | wall 36340
2022-03-21 04:57:05 | INFO | fairseq.data.iterators | grouped total_num_itrs = 394
2022-03-21 04:57:05 | INFO | fairseq.trainer | begin training epoch 36
2022-03-21 04:57:05 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-21 04:58:15 | INFO | train_inner | epoch 036:     10 / 394 loss=6.173, nll_loss=4.692, ppl=25.84, wps=1819.7, ups=0.14, wpb=13000.3, bsz=519.4, num_updates=13800, lr=3.76867e-05, gnorm=0.936, train_wall=596, gb_free=7.2, wall=36411
2022-03-21 05:09:08 | INFO | train_inner | epoch 036:    110 / 394 loss=6.157, nll_loss=4.674, ppl=25.53, wps=1977.4, ups=0.15, wpb=12904.1, bsz=514.3, num_updates=13900, lr=3.75509e-05, gnorm=0.931, train_wall=601, gb_free=7.2, wall=37064
2022-03-21 05:20:01 | INFO | train_inner | epoch 036:    210 / 394 loss=6.15, nll_loss=4.665, ppl=25.37, wps=2022.6, ups=0.15, wpb=13212.5, bsz=528.1, num_updates=14000, lr=3.74166e-05, gnorm=0.931, train_wall=600, gb_free=7.2, wall=37717
2022-03-21 05:30:57 | INFO | train_inner | epoch 036:    310 / 394 loss=6.159, nll_loss=4.675, ppl=25.55, wps=1973.6, ups=0.15, wpb=12938.8, bsz=517.3, num_updates=14100, lr=3.72837e-05, gnorm=0.943, train_wall=595, gb_free=7.2, wall=38372
2022-03-21 05:39:59 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-21 05:40:54 | INFO | valid | epoch 036 | valid on 'valid' subset | loss 6.368 | nll_loss 4.846 | ppl 28.75 | wps 5363.4 | wpb 203.8 | bsz 8.1 | num_updates 14184 | best_loss 6.368
2022-03-21 05:40:54 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 36 @ 14184 updates
2022-03-21 05:40:54 | INFO | fairseq.trainer | Saving checkpoint to /cluster/work/cotterell/liam/master-thesis/checkpoints/summarization_model/11/checkpoint_best.pt
2022-03-21 05:40:56 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/work/cotterell/liam/master-thesis/checkpoints/summarization_model/11/checkpoint_best.pt
2022-03-21 05:40:56 | INFO | fairseq.checkpoint_utils | Saved checkpoint checkpoints/summarization_model/11/checkpoint_best.pt (epoch 36 @ 14184 updates, score 6.368) (writing took 2.657820425927639 seconds)
2022-03-21 05:40:56 | INFO | fairseq_cli.train | end of epoch 36 (average epoch stats below)
2022-03-21 05:40:56 | INFO | train | epoch 036 | loss 6.153 | nll_loss 4.669 | ppl 25.44 | wps 1940.2 | ups 0.15 | wpb 12960 | bsz 517.9 | num_updates 14184 | lr 3.71731e-05 | gnorm 0.94 | train_wall 2355 | gb_free 7.2 | wall 38972
2022-03-21 05:40:57 | INFO | fairseq.data.iterators | grouped total_num_itrs = 394
2022-03-21 05:40:57 | INFO | fairseq.trainer | begin training epoch 37
2022-03-21 05:40:57 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-21 05:42:46 | INFO | train_inner | epoch 037:     16 / 394 loss=6.143, nll_loss=4.657, ppl=25.23, wps=1799.3, ups=0.14, wpb=12754, bsz=510.1, num_updates=14200, lr=3.71521e-05, gnorm=0.955, train_wall=593, gb_free=7.2, wall=39081
2022-03-21 05:59:42 | INFO | train_inner | epoch 037:    116 / 394 loss=6.128, nll_loss=4.64, ppl=24.94, wps=1280.5, ups=0.1, wpb=13015.5, bsz=520.8, num_updates=14300, lr=3.7022e-05, gnorm=0.941, train_wall=608, gb_free=7.2, wall=40098
2022-03-21 06:12:27 | INFO | train_inner | epoch 037:    216 / 394 loss=6.108, nll_loss=4.616, ppl=24.52, wps=1686.7, ups=0.13, wpb=12898.1, bsz=516.7, num_updates=14400, lr=3.68932e-05, gnorm=0.941, train_wall=605, gb_free=6.6, wall=40862
2022-03-21 06:23:37 | INFO | train_inner | epoch 037:    316 / 394 loss=6.141, nll_loss=4.654, ppl=25.17, wps=1936.2, ups=0.15, wpb=12971.5, bsz=516.7, num_updates=14500, lr=3.67658e-05, gnorm=0.933, train_wall=595, gb_free=5.5, wall=41532
2022-03-21 06:31:44 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-21 06:32:40 | INFO | valid | epoch 037 | valid on 'valid' subset | loss 6.351 | nll_loss 4.823 | ppl 28.3 | wps 5373.2 | wpb 203.8 | bsz 8.1 | num_updates 14578 | best_loss 6.351
2022-03-21 06:32:40 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 37 @ 14578 updates
2022-03-21 06:32:40 | INFO | fairseq.trainer | Saving checkpoint to /cluster/work/cotterell/liam/master-thesis/checkpoints/summarization_model/11/checkpoint_best.pt
2022-03-21 06:32:43 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/work/cotterell/liam/master-thesis/checkpoints/summarization_model/11/checkpoint_best.pt
2022-03-21 06:32:43 | INFO | fairseq.checkpoint_utils | Saved checkpoint checkpoints/summarization_model/11/checkpoint_best.pt (epoch 37 @ 14578 updates, score 6.351) (writing took 3.1004050374031067 seconds)
2022-03-21 06:32:43 | INFO | fairseq_cli.train | end of epoch 37 (average epoch stats below)
2022-03-21 06:32:43 | INFO | train | epoch 037 | loss 6.125 | nll_loss 4.636 | ppl 24.86 | wps 1643.8 | ups 0.13 | wpb 12960 | bsz 517.9 | num_updates 14578 | lr 3.66673e-05 | gnorm 0.943 | train_wall 2353 | gb_free 7.2 | wall 42078
2022-03-21 06:32:43 | INFO | fairseq.data.iterators | grouped total_num_itrs = 394
2022-03-21 06:32:43 | INFO | fairseq.trainer | begin training epoch 38
2022-03-21 06:32:43 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-21 06:35:06 | INFO | train_inner | epoch 038:     22 / 394 loss=6.119, nll_loss=4.628, ppl=24.74, wps=1874.4, ups=0.15, wpb=12924.9, bsz=516.1, num_updates=14600, lr=3.66397e-05, gnorm=0.957, train_wall=586, gb_free=7.2, wall=42222
2022-03-21 06:52:08 | INFO | train_inner | epoch 038:    122 / 394 loss=6.082, nll_loss=4.586, ppl=24.02, wps=1251.8, ups=0.1, wpb=12786.3, bsz=513.7, num_updates=14700, lr=3.65148e-05, gnorm=0.937, train_wall=599, gb_free=7.2, wall=43243
2022-03-21 07:05:01 | INFO | train_inner | epoch 038:    222 / 394 loss=6.112, nll_loss=4.62, ppl=24.6, wps=1699, ups=0.13, wpb=13143.3, bsz=522.2, num_updates=14800, lr=3.63913e-05, gnorm=0.931, train_wall=599, gb_free=7.2, wall=44017
2022-03-21 07:16:07 | INFO | train_inner | epoch 038:    322 / 394 loss=6.089, nll_loss=4.594, ppl=24.15, wps=1952.3, ups=0.15, wpb=12988.7, bsz=521.1, num_updates=14900, lr=3.62689e-05, gnorm=0.952, train_wall=600, gb_free=7.2, wall=44682
2022-03-21 07:23:48 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-21 07:24:45 | INFO | valid | epoch 038 | valid on 'valid' subset | loss 6.335 | nll_loss 4.811 | ppl 28.07 | wps 5254.1 | wpb 203.8 | bsz 8.1 | num_updates 14972 | best_loss 6.335
2022-03-21 07:24:45 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 38 @ 14972 updates
2022-03-21 07:24:45 | INFO | fairseq.trainer | Saving checkpoint to /cluster/work/cotterell/liam/master-thesis/checkpoints/summarization_model/11/checkpoint_best.pt
2022-03-21 07:24:48 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/work/cotterell/liam/master-thesis/checkpoints/summarization_model/11/checkpoint_best.pt
2022-03-21 07:24:48 | INFO | fairseq.checkpoint_utils | Saved checkpoint checkpoints/summarization_model/11/checkpoint_best.pt (epoch 38 @ 14972 updates, score 6.335) (writing took 2.888604123145342 seconds)
2022-03-21 07:24:48 | INFO | fairseq_cli.train | end of epoch 38 (average epoch stats below)
2022-03-21 07:24:48 | INFO | train | epoch 038 | loss 6.098 | nll_loss 4.604 | ppl 24.33 | wps 1634 | ups 0.13 | wpb 12960 | bsz 517.9 | num_updates 14972 | lr 3.61816e-05 | gnorm 0.944 | train_wall 2352 | gb_free 7.2 | wall 45203
2022-03-21 07:24:48 | INFO | fairseq.data.iterators | grouped total_num_itrs = 394
2022-03-21 07:24:48 | INFO | fairseq.trainer | begin training epoch 39
2022-03-21 07:24:48 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-21 07:27:41 | INFO | train_inner | epoch 039:     28 / 394 loss=6.085, nll_loss=4.589, ppl=24.07, wps=1878.5, ups=0.14, wpb=13037.9, bsz=521.5, num_updates=15000, lr=3.61478e-05, gnorm=0.951, train_wall=584, gb_free=7.2, wall=45376
2022-03-21 07:38:45 | INFO | train_inner | epoch 039:    128 / 394 loss=6.074, nll_loss=4.576, ppl=23.86, wps=1935.8, ups=0.15, wpb=12857.7, bsz=513.8, num_updates=15100, lr=3.6028e-05, gnorm=0.956, train_wall=592, gb_free=7.2, wall=46040
2022-03-21 07:50:03 | INFO | train_inner | epoch 039:    228 / 394 loss=6.071, nll_loss=4.573, ppl=23.8, wps=1888.4, ups=0.15, wpb=12808.5, bsz=512.8, num_updates=15200, lr=3.59092e-05, gnorm=0.951, train_wall=597, gb_free=7.2, wall=46719
2022-03-21 08:00:49 | INFO | train_inner | epoch 039:    328 / 394 loss=6.084, nll_loss=4.587, ppl=24.04, wps=2040.2, ups=0.15, wpb=13170.1, bsz=524.2, num_updates=15300, lr=3.57917e-05, gnorm=0.944, train_wall=594, gb_free=7.2, wall=47364
2022-03-21 08:07:49 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-21 08:08:44 | INFO | valid | epoch 039 | valid on 'valid' subset | loss 6.323 | nll_loss 4.794 | ppl 27.74 | wps 5375.1 | wpb 203.8 | bsz 8.1 | num_updates 15366 | best_loss 6.323
2022-03-21 08:08:44 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 39 @ 15366 updates
2022-03-21 08:08:44 | INFO | fairseq.trainer | Saving checkpoint to /cluster/work/cotterell/liam/master-thesis/checkpoints/summarization_model/11/checkpoint_best.pt
2022-03-21 08:08:47 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/work/cotterell/liam/master-thesis/checkpoints/summarization_model/11/checkpoint_best.pt
2022-03-21 08:08:47 | INFO | fairseq.checkpoint_utils | Saved checkpoint checkpoints/summarization_model/11/checkpoint_best.pt (epoch 39 @ 15366 updates, score 6.323) (writing took 2.5411403458565474 seconds)
2022-03-21 08:08:47 | INFO | fairseq_cli.train | end of epoch 39 (average epoch stats below)
2022-03-21 08:08:47 | INFO | train | epoch 039 | loss 6.072 | nll_loss 4.573 | ppl 23.81 | wps 1935 | ups 0.15 | wpb 12960 | bsz 517.9 | num_updates 15366 | lr 3.57148e-05 | gnorm 0.953 | train_wall 2332 | gb_free 7.2 | wall 47842
2022-03-21 08:08:47 | INFO | fairseq.data.iterators | grouped total_num_itrs = 394
2022-03-21 08:08:47 | INFO | fairseq.trainer | begin training epoch 40
2022-03-21 08:08:47 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-21 08:12:26 | INFO | train_inner | epoch 040:     34 / 394 loss=6.063, nll_loss=4.563, ppl=23.64, wps=1842.5, ups=0.14, wpb=12839.8, bsz=512, num_updates=15400, lr=3.56753e-05, gnorm=0.962, train_wall=585, gb_free=7.2, wall=48061
2022-03-21 08:23:07 | INFO | train_inner | epoch 040:    134 / 394 loss=6.03, nll_loss=4.525, ppl=23.03, wps=2043.5, ups=0.16, wpb=13104.9, bsz=526, num_updates=15500, lr=3.556e-05, gnorm=0.942, train_wall=593, gb_free=7.2, wall=48702
2022-03-21 08:40:37 | INFO | train_inner | epoch 040:    234 / 394 loss=6.038, nll_loss=4.534, ppl=23.17, wps=1245.5, ups=0.1, wpb=13077, bsz=521.5, num_updates=15600, lr=3.54459e-05, gnorm=0.932, train_wall=601, gb_free=7.2, wall=49752
2022-03-21 08:54:01 | INFO | train_inner | epoch 040:    334 / 394 loss=6.058, nll_loss=4.557, ppl=23.53, wps=1606.6, ups=0.12, wpb=12917, bsz=516.3, num_updates=15700, lr=3.53328e-05, gnorm=0.943, train_wall=618, gb_free=7.2, wall=50556
2022-03-21 09:00:55 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-21 09:01:54 | INFO | valid | epoch 040 | valid on 'valid' subset | loss 6.308 | nll_loss 4.773 | ppl 27.34 | wps 5516.8 | wpb 203.8 | bsz 8.1 | num_updates 15760 | best_loss 6.308
2022-03-21 09:01:54 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 40 @ 15760 updates
2022-03-21 09:01:54 | INFO | fairseq.trainer | Saving checkpoint to /cluster/work/cotterell/liam/master-thesis/checkpoints/summarization_model/11/checkpoint_best.pt
2022-03-21 09:01:57 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/work/cotterell/liam/master-thesis/checkpoints/summarization_model/11/checkpoint_best.pt
2022-03-21 09:01:57 | INFO | fairseq.checkpoint_utils | Saved checkpoint checkpoints/summarization_model/11/checkpoint_best.pt (epoch 40 @ 15760 updates, score 6.308) (writing took 3.34610727801919 seconds)
2022-03-21 09:01:57 | INFO | fairseq_cli.train | end of epoch 40 (average epoch stats below)
2022-03-21 09:01:57 | INFO | train | epoch 040 | loss 6.047 | nll_loss 4.544 | ppl 23.33 | wps 1600.5 | ups 0.12 | wpb 12960 | bsz 517.9 | num_updates 15760 | lr 3.52655e-05 | gnorm 0.946 | train_wall 2372 | gb_free 7.2 | wall 51032
2022-03-21 09:02:00 | INFO | fairseq.data.iterators | grouped total_num_itrs = 394
2022-03-21 09:02:00 | INFO | fairseq.trainer | begin training epoch 41
2022-03-21 09:02:00 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-21 09:07:07 | INFO | train_inner | epoch 041:     40 / 394 loss=6.055, nll_loss=4.553, ppl=23.47, wps=1646.3, ups=0.13, wpb=12942.9, bsz=513.3, num_updates=15800, lr=3.52208e-05, gnorm=0.965, train_wall=600, gb_free=7.2, wall=51343
2022-03-21 09:19:18 | INFO | train_inner | epoch 041:    140 / 394 loss=6.026, nll_loss=4.52, ppl=22.94, wps=1749.4, ups=0.14, wpb=12791.6, bsz=512.1, num_updates=15900, lr=3.51099e-05, gnorm=0.964, train_wall=628, gb_free=7.2, wall=52074
2022-03-21 09:31:41 | INFO | train_inner | epoch 041:    240 / 394 loss=6.017, nll_loss=4.51, ppl=22.78, wps=1772.2, ups=0.13, wpb=13158.2, bsz=527.6, num_updates=16000, lr=3.5e-05, gnorm=0.936, train_wall=616, gb_free=7.2, wall=52816
2022-03-21 09:43:13 | INFO | train_inner | epoch 041:    340 / 394 loss=6.015, nll_loss=4.507, ppl=22.74, wps=1870.2, ups=0.14, wpb=12947.6, bsz=518.6, num_updates=16100, lr=3.48911e-05, gnorm=0.95, train_wall=598, gb_free=7.2, wall=53509
2022-03-21 09:49:55 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-21 09:50:54 | INFO | valid | epoch 041 | valid on 'valid' subset | loss 6.303 | nll_loss 4.765 | ppl 27.2 | wps 5114 | wpb 203.8 | bsz 8.1 | num_updates 16154 | best_loss 6.303
2022-03-21 09:50:54 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 41 @ 16154 updates
2022-03-21 09:50:54 | INFO | fairseq.trainer | Saving checkpoint to /cluster/work/cotterell/liam/master-thesis/checkpoints/summarization_model/11/checkpoint_best.pt
2022-03-21 09:50:57 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/work/cotterell/liam/master-thesis/checkpoints/summarization_model/11/checkpoint_best.pt
2022-03-21 09:50:57 | INFO | fairseq.checkpoint_utils | Saved checkpoint checkpoints/summarization_model/11/checkpoint_best.pt (epoch 41 @ 16154 updates, score 6.303) (writing took 2.8802353013306856 seconds)
2022-03-21 09:50:57 | INFO | fairseq_cli.train | end of epoch 41 (average epoch stats below)
2022-03-21 09:50:57 | INFO | train | epoch 041 | loss 6.022 | nll_loss 4.516 | ppl 22.87 | wps 1736.9 | ups 0.13 | wpb 12960 | bsz 517.9 | num_updates 16154 | lr 3.48328e-05 | gnorm 0.953 | train_wall 2406 | gb_free 7.2 | wall 53972
2022-03-21 09:50:57 | INFO | fairseq.data.iterators | grouped total_num_itrs = 394
2022-03-21 09:50:57 | INFO | fairseq.trainer | begin training epoch 42
2022-03-21 09:50:57 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-21 09:56:44 | INFO | train_inner | epoch 042:     46 / 394 loss=6.018, nll_loss=4.51, ppl=22.78, wps=1595, ups=0.12, wpb=12932.2, bsz=514.8, num_updates=16200, lr=3.47833e-05, gnorm=0.96, train_wall=609, gb_free=6.5, wall=54319
2022-03-21 10:08:16 | INFO | train_inner | epoch 042:    146 / 394 loss=6, nll_loss=4.49, ppl=22.47, wps=1858, ups=0.14, wpb=12853.9, bsz=513, num_updates=16300, lr=3.46764e-05, gnorm=0.954, train_wall=595, gb_free=7.2, wall=55011
2022-03-21 10:19:45 | INFO | train_inner | epoch 042:    246 / 394 loss=6.004, nll_loss=4.494, ppl=22.53, wps=1893.8, ups=0.15, wpb=13050.2, bsz=521.3, num_updates=16400, lr=3.45705e-05, gnorm=0.956, train_wall=599, gb_free=7.2, wall=55700
2022-03-21 10:31:11 | INFO | train_inner | epoch 042:    346 / 394 loss=5.991, nll_loss=4.479, ppl=22.3, wps=1880.9, ups=0.15, wpb=12907.3, bsz=518.5, num_updates=16500, lr=3.44656e-05, gnorm=0.95, train_wall=605, gb_free=7.2, wall=56386
2022-03-21 10:40:18 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-21 10:41:17 | INFO | valid | epoch 042 | valid on 'valid' subset | loss 6.286 | nll_loss 4.744 | ppl 26.79 | wps 5210.6 | wpb 203.8 | bsz 8.1 | num_updates 16548 | best_loss 6.286
2022-03-21 10:41:17 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 42 @ 16548 updates
2022-03-21 10:41:17 | INFO | fairseq.trainer | Saving checkpoint to /cluster/work/cotterell/liam/master-thesis/checkpoints/summarization_model/11/checkpoint_best.pt
2022-03-21 10:41:20 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/work/cotterell/liam/master-thesis/checkpoints/summarization_model/11/checkpoint_best.pt
2022-03-21 10:41:20 | INFO | fairseq.checkpoint_utils | Saved checkpoint checkpoints/summarization_model/11/checkpoint_best.pt (epoch 42 @ 16548 updates, score 6.286) (writing took 3.3684059232473373 seconds)
2022-03-21 10:41:20 | INFO | fairseq_cli.train | end of epoch 42 (average epoch stats below)
2022-03-21 10:41:20 | INFO | train | epoch 042 | loss 5.998 | nll_loss 4.487 | ppl 22.43 | wps 1689 | ups 0.13 | wpb 12960 | bsz 517.9 | num_updates 16548 | lr 3.44156e-05 | gnorm 0.955 | train_wall 2394 | gb_free 7.2 | wall 56996
2022-03-21 10:41:23 | INFO | fairseq.data.iterators | grouped total_num_itrs = 394
2022-03-21 10:41:23 | INFO | fairseq.trainer | begin training epoch 43
2022-03-21 10:41:23 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-21 10:55:01 | INFO | train_inner | epoch 043:     52 / 394 loss=5.993, nll_loss=4.481, ppl=22.34, wps=900.1, ups=0.07, wpb=12866.5, bsz=509.9, num_updates=16600, lr=3.43616e-05, gnorm=0.963, train_wall=641, gb_free=5.3, wall=57816
2022-03-21 11:12:25 | INFO | train_inner | epoch 043:    152 / 394 loss=5.975, nll_loss=4.46, ppl=22.01, wps=1246, ups=0.1, wpb=13013.2, bsz=518.5, num_updates=16700, lr=3.42586e-05, gnorm=0.959, train_wall=625, gb_free=7.2, wall=58860
2022-03-21 11:24:04 | INFO | train_inner | epoch 043:    252 / 394 loss=5.977, nll_loss=4.462, ppl=22.05, wps=1866.9, ups=0.14, wpb=13053.3, bsz=523.5, num_updates=16800, lr=3.41565e-05, gnorm=0.945, train_wall=605, gb_free=7.2, wall=59560
2022-03-21 11:35:41 | INFO | train_inner | epoch 043:    352 / 394 loss=5.97, nll_loss=4.454, ppl=21.91, wps=1854.7, ups=0.14, wpb=12925.1, bsz=517.5, num_updates=16900, lr=3.40553e-05, gnorm=0.96, train_wall=612, gb_free=7.2, wall=60256
2022-03-21 11:41:53 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-21 11:42:51 | INFO | valid | epoch 043 | valid on 'valid' subset | loss 6.277 | nll_loss 4.73 | ppl 26.54 | wps 5245.1 | wpb 203.8 | bsz 8.1 | num_updates 16942 | best_loss 6.277
2022-03-21 11:42:51 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 43 @ 16942 updates
2022-03-21 11:42:51 | INFO | fairseq.trainer | Saving checkpoint to /cluster/work/cotterell/liam/master-thesis/checkpoints/summarization_model/11/checkpoint_best.pt
2022-03-21 11:42:54 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/work/cotterell/liam/master-thesis/checkpoints/summarization_model/11/checkpoint_best.pt
2022-03-21 11:42:54 | INFO | fairseq.checkpoint_utils | Saved checkpoint checkpoints/summarization_model/11/checkpoint_best.pt (epoch 43 @ 16942 updates, score 6.277) (writing took 3.1563082188367844 seconds)
2022-03-21 11:42:54 | INFO | fairseq_cli.train | end of epoch 43 (average epoch stats below)
2022-03-21 11:42:54 | INFO | train | epoch 043 | loss 5.976 | nll_loss 4.461 | ppl 22.03 | wps 1382.2 | ups 0.11 | wpb 12960 | bsz 517.9 | num_updates 16942 | lr 3.40131e-05 | gnorm 0.957 | train_wall 2425 | gb_free 7.6 | wall 60690
2022-03-21 11:42:55 | INFO | fairseq.data.iterators | grouped total_num_itrs = 394
2022-03-21 11:42:55 | INFO | fairseq.trainer | begin training epoch 44
2022-03-21 11:42:55 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-21 11:55:12 | INFO | train_inner | epoch 044:     58 / 394 loss=5.958, nll_loss=4.44, ppl=21.71, wps=1110.4, ups=0.09, wpb=13002.9, bsz=522.7, num_updates=17000, lr=3.3955e-05, gnorm=0.973, train_wall=621, gb_free=7.2, wall=61427
2022-03-21 12:14:14 | INFO | train_inner | epoch 044:    158 / 394 loss=5.955, nll_loss=4.436, ppl=21.65, wps=1122.9, ups=0.09, wpb=12820, bsz=510.9, num_updates=17100, lr=3.38556e-05, gnorm=0.957, train_wall=617, gb_free=7.2, wall=62569
2022-03-21 12:28:19 | INFO | train_inner | epoch 044:    258 / 394 loss=5.949, nll_loss=4.43, ppl=21.56, wps=1528.4, ups=0.12, wpb=12920, bsz=516.1, num_updates=17200, lr=3.3757e-05, gnorm=0.972, train_wall=618, gb_free=7.2, wall=63414
2022-03-21 12:39:42 | INFO | train_inner | epoch 044:    358 / 394 loss=5.957, nll_loss=4.438, ppl=21.67, wps=1908.4, ups=0.15, wpb=13035.7, bsz=522.3, num_updates=17300, lr=3.36593e-05, gnorm=0.961, train_wall=604, gb_free=7.2, wall=64098
2022-03-21 12:43:30 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-21 12:44:28 | INFO | valid | epoch 044 | valid on 'valid' subset | loss 6.269 | nll_loss 4.719 | ppl 26.34 | wps 5347.6 | wpb 203.8 | bsz 8.1 | num_updates 17336 | best_loss 6.269
2022-03-21 12:44:28 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 44 @ 17336 updates
2022-03-21 12:44:28 | INFO | fairseq.trainer | Saving checkpoint to /cluster/work/cotterell/liam/master-thesis/checkpoints/summarization_model/11/checkpoint_best.pt
2022-03-21 12:44:30 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/work/cotterell/liam/master-thesis/checkpoints/summarization_model/11/checkpoint_best.pt
2022-03-21 12:44:30 | INFO | fairseq.checkpoint_utils | Saved checkpoint checkpoints/summarization_model/11/checkpoint_best.pt (epoch 44 @ 17336 updates, score 6.269) (writing took 2.648710884153843 seconds)
2022-03-21 12:44:30 | INFO | fairseq_cli.train | end of epoch 44 (average epoch stats below)
2022-03-21 12:44:30 | INFO | train | epoch 044 | loss 5.954 | nll_loss 4.435 | ppl 21.63 | wps 1381.5 | ups 0.11 | wpb 12960 | bsz 517.9 | num_updates 17336 | lr 3.36243e-05 | gnorm 0.964 | train_wall 2419 | gb_free 7.2 | wall 64386
2022-03-21 12:44:31 | INFO | fairseq.data.iterators | grouped total_num_itrs = 394
2022-03-21 12:44:31 | INFO | fairseq.trainer | begin training epoch 45
2022-03-21 12:44:31 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-21 12:52:08 | INFO | train_inner | epoch 045:     64 / 394 loss=5.938, nll_loss=4.417, ppl=21.37, wps=1731.3, ups=0.13, wpb=12913.5, bsz=514.5, num_updates=17400, lr=3.35624e-05, gnorm=0.968, train_wall=600, gb_free=7.1, wall=64843
2022-03-21 13:03:01 | INFO | train_inner | epoch 045:    164 / 394 loss=5.923, nll_loss=4.4, ppl=21.11, wps=2010.1, ups=0.15, wpb=13130.8, bsz=524.5, num_updates=17500, lr=3.34664e-05, gnorm=0.954, train_wall=602, gb_free=7.2, wall=65497
2022-03-21 13:14:18 | INFO | train_inner | epoch 045:    264 / 394 loss=5.926, nll_loss=4.403, ppl=21.16, wps=1947, ups=0.15, wpb=13178.2, bsz=527.9, num_updates=17600, lr=3.33712e-05, gnorm=0.953, train_wall=613, gb_free=7, wall=66173
2022-03-21 13:25:00 | INFO | train_inner | epoch 045:    364 / 394 loss=5.946, nll_loss=4.425, ppl=21.48, wps=1978.6, ups=0.16, wpb=12704.6, bsz=507.2, num_updates=17700, lr=3.32768e-05, gnorm=0.965, train_wall=592, gb_free=7.2, wall=66816
2022-03-21 13:29:53 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-21 13:30:50 | INFO | valid | epoch 045 | valid on 'valid' subset | loss 6.266 | nll_loss 4.715 | ppl 26.27 | wps 5248.4 | wpb 203.8 | bsz 8.1 | num_updates 17730 | best_loss 6.266
2022-03-21 13:30:50 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 45 @ 17730 updates
2022-03-21 13:30:50 | INFO | fairseq.trainer | Saving checkpoint to /cluster/work/cotterell/liam/master-thesis/checkpoints/summarization_model/11/checkpoint_best.pt
2022-03-21 13:30:53 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/work/cotterell/liam/master-thesis/checkpoints/summarization_model/11/checkpoint_best.pt
2022-03-21 13:30:53 | INFO | fairseq.checkpoint_utils | Saved checkpoint checkpoints/summarization_model/11/checkpoint_best.pt (epoch 45 @ 17730 updates, score 6.266) (writing took 3.142580198124051 seconds)
2022-03-21 13:30:53 | INFO | fairseq_cli.train | end of epoch 45 (average epoch stats below)
2022-03-21 13:30:53 | INFO | train | epoch 045 | loss 5.932 | nll_loss 4.409 | ppl 21.25 | wps 1835.1 | ups 0.14 | wpb 12960 | bsz 517.9 | num_updates 17730 | lr 3.32486e-05 | gnorm 0.962 | train_wall 2374 | gb_free 7.2 | wall 67169
2022-03-21 13:30:55 | INFO | fairseq.data.iterators | grouped total_num_itrs = 394
2022-03-21 13:30:55 | INFO | fairseq.trainer | begin training epoch 46
2022-03-21 13:30:55 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-21 13:49:09 | INFO | train_inner | epoch 046:     70 / 394 loss=5.922, nll_loss=4.398, ppl=21.08, wps=882.1, ups=0.07, wpb=12776.6, bsz=509, num_updates=17800, lr=3.31832e-05, gnorm=0.982, train_wall=619, gb_free=7.2, wall=68264
2022-03-21 14:05:14 | INFO | train_inner | epoch 046:    170 / 394 loss=5.91, nll_loss=4.384, ppl=20.88, wps=1367.6, ups=0.1, wpb=13206.1, bsz=527.4, num_updates=17900, lr=3.30904e-05, gnorm=0.973, train_wall=607, gb_free=7.2, wall=69230
2022-03-21 14:17:37 | INFO | train_inner | epoch 046:    270 / 394 loss=5.895, nll_loss=4.367, ppl=20.63, wps=1747.8, ups=0.13, wpb=12978.8, bsz=520.3, num_updates=18000, lr=3.29983e-05, gnorm=0.97, train_wall=595, gb_free=7.2, wall=69972
2022-03-21 14:28:59 | INFO | train_inner | epoch 046:    370 / 394 loss=5.931, nll_loss=4.407, ppl=21.21, wps=1891.7, ups=0.15, wpb=12900.5, bsz=514.8, num_updates=18100, lr=3.2907e-05, gnorm=0.962, train_wall=595, gb_free=7.2, wall=70654
2022-03-21 14:31:33 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-21 14:32:32 | INFO | valid | epoch 046 | valid on 'valid' subset | loss 6.249 | nll_loss 4.702 | ppl 26.02 | wps 5159.6 | wpb 203.8 | bsz 8.1 | num_updates 18124 | best_loss 6.249
2022-03-21 14:32:32 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 46 @ 18124 updates
2022-03-21 14:32:32 | INFO | fairseq.trainer | Saving checkpoint to /cluster/work/cotterell/liam/master-thesis/checkpoints/summarization_model/11/checkpoint_best.pt
2022-03-21 14:32:35 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/work/cotterell/liam/master-thesis/checkpoints/summarization_model/11/checkpoint_best.pt
2022-03-21 14:32:35 | INFO | fairseq.checkpoint_utils | Saved checkpoint checkpoints/summarization_model/11/checkpoint_best.pt (epoch 46 @ 18124 updates, score 6.249) (writing took 3.0420342460274696 seconds)
2022-03-21 14:32:35 | INFO | fairseq_cli.train | end of epoch 46 (average epoch stats below)
2022-03-21 14:32:35 | INFO | train | epoch 046 | loss 5.911 | nll_loss 4.385 | ppl 20.9 | wps 1379.5 | ups 0.11 | wpb 12960 | bsz 517.9 | num_updates 18124 | lr 3.28852e-05 | gnorm 0.971 | train_wall 2380 | gb_free 7.2 | wall 70870
2022-03-21 14:32:36 | INFO | fairseq.data.iterators | grouped total_num_itrs = 394
2022-03-21 14:32:36 | INFO | fairseq.trainer | begin training epoch 47
2022-03-21 14:32:36 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-21 14:46:50 | INFO | train_inner | epoch 047:     76 / 394 loss=5.895, nll_loss=4.367, ppl=20.64, wps=1195.2, ups=0.09, wpb=12802.3, bsz=512.8, num_updates=18200, lr=3.28165e-05, gnorm=0.972, train_wall=604, gb_free=7.2, wall=71725
2022-03-21 15:05:13 | INFO | train_inner | epoch 047:    176 / 394 loss=5.879, nll_loss=4.347, ppl=20.36, wps=1184, ups=0.09, wpb=13064.8, bsz=522.3, num_updates=18300, lr=3.27267e-05, gnorm=0.963, train_wall=643, gb_free=7.2, wall=72829
2022-03-21 15:18:11 | INFO | train_inner | epoch 047:    276 / 394 loss=5.895, nll_loss=4.366, ppl=20.62, wps=1670.6, ups=0.13, wpb=13000.1, bsz=518.4, num_updates=18400, lr=3.26377e-05, gnorm=0.975, train_wall=599, gb_free=7.2, wall=73607
2022-03-21 15:30:02 | INFO | train_inner | epoch 047:    376 / 394 loss=5.898, nll_loss=4.369, ppl=20.66, wps=1832.9, ups=0.14, wpb=13018, bsz=519.5, num_updates=18500, lr=3.25493e-05, gnorm=0.97, train_wall=607, gb_free=7.2, wall=74317
2022-03-21 15:31:52 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-21 15:32:51 | INFO | valid | epoch 047 | valid on 'valid' subset | loss 6.242 | nll_loss 4.689 | ppl 25.79 | wps 5074.8 | wpb 203.8 | bsz 8.1 | num_updates 18518 | best_loss 6.242
2022-03-21 15:32:51 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 47 @ 18518 updates
2022-03-21 15:32:51 | INFO | fairseq.trainer | Saving checkpoint to /cluster/work/cotterell/liam/master-thesis/checkpoints/summarization_model/11/checkpoint_best.pt
2022-03-21 15:32:55 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/work/cotterell/liam/master-thesis/checkpoints/summarization_model/11/checkpoint_best.pt
2022-03-21 15:32:55 | INFO | fairseq.checkpoint_utils | Saved checkpoint checkpoints/summarization_model/11/checkpoint_best.pt (epoch 47 @ 18518 updates, score 6.242) (writing took 3.466747220605612 seconds)
2022-03-21 15:32:55 | INFO | fairseq_cli.train | end of epoch 47 (average epoch stats below)
2022-03-21 15:32:55 | INFO | train | epoch 047 | loss 5.891 | nll_loss 4.362 | ppl 20.56 | wps 1410.5 | ups 0.11 | wpb 12960 | bsz 517.9 | num_updates 18518 | lr 3.25335e-05 | gnorm 0.971 | train_wall 2410 | gb_free 7.2 | wall 74490
2022-03-21 15:32:56 | INFO | fairseq.data.iterators | grouped total_num_itrs = 394
2022-03-21 15:32:56 | INFO | fairseq.trainer | begin training epoch 48
2022-03-21 15:32:56 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-21 15:42:29 | INFO | train_inner | epoch 048:     82 / 394 loss=5.877, nll_loss=4.345, ppl=20.33, wps=1689.9, ups=0.13, wpb=12632.5, bsz=505.1, num_updates=18600, lr=3.24617e-05, gnorm=0.992, train_wall=604, gb_free=7.2, wall=75065
2022-03-21 15:53:29 | INFO | train_inner | epoch 048:    182 / 394 loss=5.866, nll_loss=4.333, ppl=20.15, wps=2001.1, ups=0.15, wpb=13209, bsz=527.9, num_updates=18700, lr=3.23748e-05, gnorm=0.974, train_wall=605, gb_free=7.2, wall=75725
2022-03-21 16:14:08 | INFO | train_inner | epoch 048:    282 / 394 loss=5.873, nll_loss=4.34, ppl=20.25, wps=1056.1, ups=0.08, wpb=13086.2, bsz=523.6, num_updates=18800, lr=3.22886e-05, gnorm=0.965, train_wall=615, gb_free=7.2, wall=76964
2022-03-21 16:29:20 | INFO | train_inner | epoch 048:    382 / 394 loss=5.872, nll_loss=4.339, ppl=20.23, wps=1414, ups=0.11, wpb=12885.2, bsz=514.5, num_updates=18900, lr=3.22031e-05, gnorm=0.984, train_wall=616, gb_free=7.2, wall=77875
2022-03-21 16:30:46 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-21 16:31:45 | INFO | valid | epoch 048 | valid on 'valid' subset | loss 6.24 | nll_loss 4.684 | ppl 25.71 | wps 5090.5 | wpb 203.8 | bsz 8.1 | num_updates 18912 | best_loss 6.24
2022-03-21 16:31:45 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 48 @ 18912 updates
2022-03-21 16:31:45 | INFO | fairseq.trainer | Saving checkpoint to /cluster/work/cotterell/liam/master-thesis/checkpoints/summarization_model/11/checkpoint_best.pt
2022-03-21 16:31:48 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/work/cotterell/liam/master-thesis/checkpoints/summarization_model/11/checkpoint_best.pt
2022-03-21 16:31:48 | INFO | fairseq.checkpoint_utils | Saved checkpoint checkpoints/summarization_model/11/checkpoint_best.pt (epoch 48 @ 18912 updates, score 6.24) (writing took 2.7918057702481747 seconds)
2022-03-21 16:31:48 | INFO | fairseq_cli.train | end of epoch 48 (average epoch stats below)
2022-03-21 16:31:48 | INFO | train | epoch 048 | loss 5.871 | nll_loss 4.338 | ppl 20.23 | wps 1445.2 | ups 0.11 | wpb 12960 | bsz 517.9 | num_updates 18912 | lr 3.21928e-05 | gnorm 0.978 | train_wall 2413 | gb_free 7.2 | wall 78024
2022-03-21 16:31:49 | INFO | fairseq.data.iterators | grouped total_num_itrs = 394
2022-03-21 16:31:49 | INFO | fairseq.trainer | begin training epoch 49
2022-03-21 16:31:49 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-21 16:42:01 | INFO | train_inner | epoch 049:     88 / 394 loss=5.854, nll_loss=4.319, ppl=19.96, wps=1692.5, ups=0.13, wpb=12889.8, bsz=514.1, num_updates=19000, lr=3.21182e-05, gnorm=0.97, train_wall=594, gb_free=7.2, wall=78637
2022-03-21 16:54:55 | INFO | train_inner | epoch 049:    188 / 394 loss=5.85, nll_loss=4.314, ppl=19.89, wps=1643.9, ups=0.13, wpb=12713.7, bsz=509.8, num_updates=19100, lr=3.2034e-05, gnorm=0.985, train_wall=617, gb_free=7.2, wall=79410
2022-03-21 17:06:35 | INFO | train_inner | epoch 049:    288 / 394 loss=5.852, nll_loss=4.316, ppl=19.91, wps=1852, ups=0.14, wpb=12970.1, bsz=517.6, num_updates=19200, lr=3.19505e-05, gnorm=0.966, train_wall=602, gb_free=7.2, wall=80110
2022-03-21 17:17:36 | INFO | train_inner | epoch 049:    388 / 394 loss=5.85, nll_loss=4.313, ppl=19.88, wps=2006.8, ups=0.15, wpb=13262, bsz=529.9, num_updates=19300, lr=3.18676e-05, gnorm=0.958, train_wall=603, gb_free=6.5, wall=80771
2022-03-21 17:18:08 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-21 17:19:05 | INFO | valid | epoch 049 | valid on 'valid' subset | loss 6.225 | nll_loss 4.667 | ppl 25.41 | wps 5249.7 | wpb 203.8 | bsz 8.1 | num_updates 19306 | best_loss 6.225
2022-03-21 17:19:05 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 49 @ 19306 updates
2022-03-21 17:19:05 | INFO | fairseq.trainer | Saving checkpoint to /cluster/work/cotterell/liam/master-thesis/checkpoints/summarization_model/11/checkpoint_best.pt
2022-03-21 17:19:08 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/work/cotterell/liam/master-thesis/checkpoints/summarization_model/11/checkpoint_best.pt
2022-03-21 17:19:08 | INFO | fairseq.checkpoint_utils | Saved checkpoint checkpoints/summarization_model/11/checkpoint_best.pt (epoch 49 @ 19306 updates, score 6.225) (writing took 2.7066494189202785 seconds)
2022-03-21 17:19:08 | INFO | fairseq_cli.train | end of epoch 49 (average epoch stats below)
2022-03-21 17:19:08 | INFO | train | epoch 049 | loss 5.851 | nll_loss 4.314 | ppl 19.9 | wps 1798.1 | ups 0.14 | wpb 12960 | bsz 517.9 | num_updates 19306 | lr 3.18626e-05 | gnorm 0.971 | train_wall 2375 | gb_free 7.2 | wall 80863
2022-03-21 17:19:09 | INFO | fairseq.data.iterators | grouped total_num_itrs = 394
2022-03-21 17:19:09 | INFO | fairseq.trainer | begin training epoch 50
2022-03-21 17:19:09 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-21 17:39:02 | INFO | train_inner | epoch 050:     94 / 394 loss=5.827, nll_loss=4.287, ppl=19.52, wps=1003.8, ups=0.08, wpb=12909.2, bsz=517.7, num_updates=19400, lr=3.17854e-05, gnorm=0.987, train_wall=605, gb_free=7.2, wall=82057
2022-03-21 17:54:14 | INFO | train_inner | epoch 050:    194 / 394 loss=5.839, nll_loss=4.3, ppl=19.7, wps=1402.3, ups=0.11, wpb=12786.1, bsz=509.9, num_updates=19500, lr=3.17038e-05, gnorm=0.984, train_wall=617, gb_free=7.1, wall=82969
2022-03-21 18:06:10 | INFO | train_inner | epoch 050:    294 / 394 loss=5.833, nll_loss=4.293, ppl=19.61, wps=1800.1, ups=0.14, wpb=12900.2, bsz=513.7, num_updates=19600, lr=3.16228e-05, gnorm=0.974, train_wall=601, gb_free=7.2, wall=83686
2022-03-21 18:17:10 | INFO | train_inner | epoch 050:    394 / 394 loss=5.833, nll_loss=4.293, ppl=19.6, wps=1989, ups=0.15, wpb=13119.6, bsz=525.4, num_updates=19700, lr=3.15424e-05, gnorm=0.994, train_wall=585, gb_free=7.2, wall=84345
2022-03-21 18:17:10 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-21 18:18:07 | INFO | valid | epoch 050 | valid on 'valid' subset | loss 6.225 | nll_loss 4.67 | ppl 25.45 | wps 5248 | wpb 203.8 | bsz 8.1 | num_updates 19700 | best_loss 6.225
2022-03-21 18:18:07 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 50 @ 19700 updates
2022-03-21 18:18:07 | INFO | fairseq.trainer | Saving checkpoint to /cluster/work/cotterell/liam/master-thesis/checkpoints/summarization_model/11/checkpoint_best.pt
2022-03-21 18:18:10 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/work/cotterell/liam/master-thesis/checkpoints/summarization_model/11/checkpoint_best.pt
2022-03-21 18:18:11 | INFO | fairseq.checkpoint_utils | Saved checkpoint checkpoints/summarization_model/11/checkpoint_best.pt (epoch 50 @ 19700 updates, score 6.225) (writing took 3.758741866797209 seconds)
2022-03-21 18:18:11 | INFO | fairseq_cli.train | end of epoch 50 (average epoch stats below)
2022-03-21 18:18:11 | INFO | train | epoch 050 | loss 5.832 | nll_loss 4.293 | ppl 19.6 | wps 1441.3 | ups 0.11 | wpb 12960 | bsz 517.9 | num_updates 19700 | lr 3.15424e-05 | gnorm 0.981 | train_wall 2376 | gb_free 7.2 | wall 84406
2022-03-21 18:18:12 | INFO | fairseq.data.iterators | grouped total_num_itrs = 394
2022-03-21 18:18:12 | INFO | fairseq.trainer | begin training epoch 51
2022-03-21 18:18:12 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-21 18:40:07 | INFO | train_inner | epoch 051:    100 / 394 loss=5.8, nll_loss=4.256, ppl=19.11, wps=953.8, ups=0.07, wpb=13136.3, bsz=526.3, num_updates=19800, lr=3.14627e-05, gnorm=0.976, train_wall=623, gb_free=7.2, wall=85723
User defined signal 2
